<!DOCTYPE html> <html lang="en"> <head> <title>Debezium Blog</title> <meta charset="utf-8"> <meta content="width=device-width, initial-scale=1.0" name="viewport"> <meta content="" name="description"> <meta content="" name="author"> <link href="https://static.jboss.org/theme/css/bootstrap-community/3.2.0.2/bootstrap-community.min.css" media="screen" rel="stylesheet"> <!--[if lt IE 9]><script src="https://static.jboss.org/theme/js/libs/html5/pre3.6/html5.min.js"></script><![endif]--> <link href="https://static.jboss.org/example/images/favicon.ico" rel="shortcut icon"> <link href="https://static.jboss.org/example/images/apple-touch-icon-144x144-precomposed.png" rel="apple-touch-icon-precomposed" sizes="144x144"> <link href="https://static.jboss.org/example/images/apple-touch-icon-114x114-precomposed.png" rel="apple-touch-icon-precomposed" sizes="114x114"> <link href="https://static.jboss.org/example/images/apple-touch-icon-72x72-precomposed.png" rel="apple-touch-icon-precomposed" sizes="72x72"> <link href="https://static.jboss.org/example/images/apple-touch-icon-precomposed.png" rel="apple-touch-icon-precomposed"> <link href="/stylesheets/debezium.css" rel="stylesheet" type="text/css"> <style>
      @media (min-width: 980px) {
        .banner { background-image: url(https://static.jboss.org/example/images/debezium-banner-1180px.png); height: 110px;  }
      }
      @media (max-width: 979px) {
        .banner { background-image: url(https://static.jboss.org/example/images/debezium-logo.png); background-repeat:no-repeat; background-position: center bottom; height: 60px; }
      }
      @media (max-width: 650px) {
        .banner { width: 100%; margin: 0px auto; }
      }
      @media (max-width: 450px) {
        .banner { height: 90px; }
      }
    </style> <link href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/styles/default.min.css" rel="stylesheet"> <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/highlight.min.js"></script> <script>
      hljs.initHighlightingOnLoad();
    </script> <script src="https://static.jboss.org/theme/js/libs/jquery/jquery-1.9.1.min.js"></script> <style>
      /* adjusting the vertical spacing for when a stickynav is engaged */
      .breadcrumb-fixed > .active {
        color: #8c8f91;
      }
      .breadcrumb-fixed {
        margin: 70px 0 10px;
        padding: 8px 15px;
        margin-bottom: 20px;
        list-style: none;
        background-color: #f5f5f5;
        border-radius: 4px;
      }
      
      .breadcrumb-fixed > li {
        display: inline-block;
      }
    </style> </head> <body> <div id="rhbar"> <a class="jbdevlogo" href="https://www.jboss.org/projects/about"></a> <a class="rhlogo" href="https://www.redhat.com/"></a> </div> <div id=""> <ul class="visuallyhidden" id="top"> <li> <a accesskey="n" href="#nav" title="Skip to navigation">Skip to navigation</a> </li> <li> <a accesskey="c" href="#page" title="Skip to content">Skip to content</a> </li> </ul> <div class="container" id="content"> <div class="navbar navbar-inverse navbar-fix"> <div class="container-fluid"> <div class="navbar-header"> <button class="navbar-toggle collapsed" data-target="#navbar-1" data-toggle="collapse"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button> <a class="navbar-brand" href="/"> Debezium </a> </div> <div class="collapse navbar-collapse" id="navbar-1"> <ul class="nav navbar-nav pull-right"> <li class=""><a href="/docs/faq">FAQ</a></li> <li class=""><a href="/docs">DOCS</a></li> <li class=""><a href="/community">COMMUNITY</a></li> <li class="active"><a href="/blog">BLOG</a></li> </ul> </div> </div> </div> <div id="equalHeightsLayout"> <div class="row post-text-padding row-no-expand"> <div class="hidden-xs col-sm-3 no-right-padding" id="leftdocnav"> <div class="panel-docnav"> <div class="panel-heading"> <h3 class="panel-title"> Latest posts </h3> </div> <div class="panel-body"> <ul class="list-group"> <li class="list-group-item"> <a href="/blog/2018/10/04/debezium-0-9-0-alpha2-released/" rel="tooltip" title="Click to go to post">Debezium 0.9.0.Alpha2 Released</a> </li> <li class="list-group-item"> <a href="/blog/2018/09/20/materializing-aggregate-views-with-hibernate-and-debezium/" rel="tooltip" title="Click to go to post">Materializing Aggregate Views With Hibernate and Debezium</a> </li> <li class="list-group-item"> <a href="/blog/2018/09/19/debezium-0-8-3-final-released/" rel="tooltip" title="Click to go to post">Debezium 0.8.3.Final Released</a> </li> <li class="list-group-item"> <a href="/blog/2018/08/30/streaming-mysql-data-changes-into-kinesis/" rel="tooltip" title="Click to go to post">Streaming MySQL Data Changes to Amazon Kinesis</a> </li> <li class="list-group-item"> <a href="/blog/2018/08/30/debezium-0-8-2-released/" rel="tooltip" title="Click to go to post">Debezium 0.8.2 Released</a> </li> <li class="list-group-item"> <a href="/blog/2018/07/26/debezium-0-9-0-alpha1-released/" rel="tooltip" title="Click to go to post">Debezium 0.9 Alpha1 and 0.8.1 Released</a> </li> <li class="list-group-item"> <a href="/blog/2018/07/19/advantages-of-log-based-change-data-capture/" rel="tooltip" title="Click to go to post">Five Advantages of Log-Based Change Data Capture</a> </li> <li class="list-group-item"> <a href="/blog/2018/07/12/debezium-0-8-0-final-released/" rel="tooltip" title="Click to go to post">Debezium 0.8 Final Is Released</a> </li> </ul> </div> </div> </div> <div class="col-xs-12 col-sm-9" id="maincol"> <div class="text-right"> <h3> Subscribe <a class="rss" href="/blog.atom"> <i class="icon-rss"></i> </a> </h3> </div> <hr> <div class="post"> <h1 class="title"> <a href="/blog/2018/10/04/debezium-0-9-0-alpha2-released/">Debezium 0.9.0.Alpha2 Released</a> </h1> <div class="byline"> <p> <em> October 04, 2018 by Gunnar Morling </em> <em> under&nbsp; </em> <a class="label label-info" href="/blog/tags/releases/">releases</a> <a class="label label-info" href="/blog/tags/mysql/">mysql</a> <a class="label label-info" href="/blog/tags/mongodb/">mongodb</a> <a class="label label-info" href="/blog/tags/postgres/">postgres</a> <a class="label label-info" href="/blog/tags/sqlserver/">sqlserver</a> <a class="label label-info" href="/blog/tags/oracle/">oracle</a> <a class="label label-info" href="/blog/tags/docker/">docker</a> </p> </div> <div id="preamble"> <div class="sectionbody"> <div class="paragraph"> <p>It&#8217;s my pleasure to announce the release of Debezium <strong>0.9.0.Alpha2</strong>!</p> </div> <div class="paragraph"> <p>While the work on the connectors for SQL Server and Oracle continues, we decided to do another Alpha release, as lots of fixes and new features - many of them contributed by community members - have piled up, which we wanted to get into your hands as quickly as possible.</p> </div> <div class="paragraph"> <p>This release supports Apache Kafka 2.0, comes with support for Postgres' HSTORE column type, allows to rename and filter fields from change data messages for MongoDB and contains multiple bug fixes and performance improvements. Overall, this release contains <a href="/docs/releases/#release-0-9-0-alpha2">55 fixes</a> (note that a few of these have been merged back to 0.8.x and are contained in earlier 0.8 releases, too).</p> </div> <div class="paragraph"> <p>A big "Thank You" is in order to community members <a href="https://github.com/jchipmunk">Andrey Pustovetov</a>, <a href="https://github.com/artiship">Artiship Artiship</a>, <a href="https://github.com/CliffWheadon">Cliff Wheadon</a>, <a href="https://github.com/deepakbarr">Deepak Barr</a>, <a href="https://github.com/ian-axelrod">Ian Axelrod</a>, <a href="https://github.com/ooooorz">Liu Hanlin</a>, <a href="https://github.com/maver1ck">Maciej Bry≈Ñski</a>, <a href="https://github.com/oripwk">Ori Popowski</a>, <a href="https://github.com/PengLyu">Peng Lyu</a>, <a href="https://github.com/PSanetra">Philip Sanetra</a>, <a href="https://github.com/sagarrao">Sagar Rao</a> and <a href="https://github.com/SyedMuhammadSufyian">Syed Muhammad Sufyian</a> for their contributions to this release. We salute you!</p> </div> </div> </div> <div class="sect1"> <h2 id="kafka_upgrade"><a class="anchor" href="#kafka_upgrade"></a>Kafka Upgrade</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium runs with and has been tested on top of the recently released Apache Kafka 2.0 (<a href="https://issues.jboss.org/browse/DBZ-858">DBZ-858</a>). The widely used version Kafka 1.x continues to be supported as well.</p> </div> <div class="paragraph"> <p>Note that 0.10.x is not supported due to Debezium&#8217;s usage of the admin client API which is only available in later versions. It shouldn&#8217;t be too hard to work around this, so if someone is interested in helping out with this, this would be a great contribution (see <a href="https://issues.jboss.org/browse/DBZ-883">DBZ-883</a>).</p> </div> </div> </div> <div class="sect1"> <h2 id="support_for_hstore_columns_in_postgres"><a class="anchor" href="#support_for_hstore_columns_in_postgres"></a>Support for HSTORE columns in Postgres</h2> <div class="sectionbody"> <div class="paragraph"> <p>Postgres is an amazingly powerful and flexible RDBMS, not the least due to its wide range of column types which go far beyond what&#8217;s defined by the SQL standard. One of these types being <a href="https://www.postgresql.org/docs/current/static/hstore.html">HSTORE</a>, which is a string-to-string map essentially.</p> </div> <div class="paragraph"> <p>Debezium can capture changes to columns of this type now (<a href="https://issues.jboss.org/browse/DBZ-898">DBZ-898</a>). By default, the field values will be represented using Kafka Connect&#8217;s map data type. As this may not be supported by all sink connectors, you might alternatively represent them as a string-ified JSON by setting the new <code>hstore.handling.mode</code> connector option to <code>json</code>. In this case, you&#8217;d see HSTORE columns represented as values in change messages like so: <code>{ "key1" : "val1", "key2" : "val2" }</code>.</p> </div> </div> </div> <div class="sect1"> <h2 id="field_filtering_and_renaming_for_mongodb"><a class="anchor" href="#field_filtering_and_renaming_for_mongodb"></a>Field filtering and renaming for MongoDB</h2> <div class="sectionbody"> <div class="paragraph"> <p>Unlike the connectors for MySQL and Postgres, the Debezium MongoDB connector so far didn&#8217;t allow to exclude single fields of captured collections from CDC messages. Also renaming them wasn&#8217;t supported e.g. by means of Kafka&#8217;s <code>ReplaceField</code> SMT. The reason being that MongoDB doesn&#8217;t mandate a fixed schema for the documents of a given collection, and documents therefore are represented in change messages using a single string-ified JSON field.</p> </div> <div class="paragraph"> <p>Thanks to the fantastic work of community member Andrey Pustovetov, this finally has changed, i.e. you can remove given fields (<a href="https://issues.jboss.org/browse/DBZ-633">DBZ-633</a>) now from the CDC messages of given collections or have them renamed (<a href="https://issues.jboss.org/browse/DBZ-881">DBZ-881</a>). Please refer to the description of the new connector options <code>field.blacklist</code> and <code>field.renames</code> in the <a href="/docs/connectors/mongodb/">MongoDB connector documentation</a> to learn more.</p> </div> </div> </div> <div class="sect1"> <h2 id="extended_source_info"><a class="anchor" href="#extended_source_info"></a>Extended source info</h2> <div class="sectionbody"> <div class="paragraph"> <p>Another contribution by Andrey is the new optional <code>connector</code> field within the source info block of CDC messages (<a href="https://issues.jboss.org/browse/DBZ-918">DBZ-918</a>). This tells the type of source connector that produced the messages ("mysql", "postgres" etc.), which can come in handy in cases where specific semantics need to be applied on the consumer side depending on the type of source database.</p> </div> </div> </div> <div class="sect1"> <h2 id="bug_fixes_and_version_upgrades"><a class="anchor" href="#bug_fixes_and_version_upgrades"></a>Bug fixes and version upgrades</h2> <div class="sectionbody"> <div class="paragraph"> <p>The new release contains a good number of bug fixes and other smaller improvements. Amongst them are</p> </div> <div class="ulist"> <ul> <li> <p>correct handling of invalid temporal default values with MySQL (<a href="https://issues.jboss.org/browse/DBZ-927">DBZ-927</a>),</p> </li> <li> <p>support for table/collection names with special characters for MySQL (<a href="https://issues.jboss.org/browse/DBZ-878">DBZ-878</a>) and MongoDB (<a href="https://issues.jboss.org/browse/DBZ-865">DBZ-865</a>) and</p> </li> <li> <p>fixed handling of blacklisted tables with the new Antlr-based DDL parser (<a href="https://issues.jboss.org/browse/DBZ-872">DBZ-872</a>).</p> </li> </ul> </div> <div class="paragraph"> <p>Community member Ian Axelrod provided a fix for a potential performance issue, where changes to tables with TOAST columns in Postgres would cause repeated updates to the connector&#8217;s internal schema metadata, which can be a costly operation (<a href="https://issues.jboss.org/browse/DBZ-911">DBZ-911</a>). Please refer to the <a href="/docs/connectors/postgres/">Postgres connector documentation</a> for details on the new <code>schema.refresh.mode</code> option, which deals with this issue.</p> </div> <div class="paragraph"> <p>In terms of version upgrades we migrated to the latest releases of the MySQL (<a href="https://issues.jboss.org/browse/DBZ-763">DBZ-763</a>, <a href="https://issues.jboss.org/browse/DBZ-764">DBZ-764</a>) and Postgres drivers (<a href="https://issues.jboss.org/browse/DBZ-912">DBZ-912</a>). The former is part of a longer stream of work leading towards support of MySQL 8 which should be finished in one of the next Debezium releases. For Postgres we provide a Docker image with Debezium&#8217;s supported logical decoding plug-ins based on Alpine now, which might be interesting to those concerned about container size (<a href="https://issues.jboss.org/browse/DBZ-705">DBZ-705</a>).</p> </div> <div class="paragraph"> <p>Please see the change log for the complete list of fixed issues.</p> </div> </div> </div> <div class="sect1"> <h2 id="what_s_next"><a class="anchor" href="#what_s_next"></a>What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>The work towards Debezium 0.9 continues, and we&#8217;ll focus mostly on improvements to the SQL Server and Oracle connectors. Other potential topics include support for MySQL 8 and native logical decoding as introduced with Postgres 10, which should greatly help with using the Debezium Postgres connectors in cloud environments such as Amazon RDS.</p> </div> <div class="paragraph"> <p>We&#8217;ll also be talking about Debezium at the following conferences:</p> </div> <div class="ulist"> <ul> <li> <p><a href="https://kafka-summit.org/sessions/change-data-streaming-patterns-microservices-debezium/">Kafka Summit</a>; San Francisco, Cal.; Oct. 17</p> </li> <li> <p><a href="https://vxdms2018.confinabox.com/talk/INI-9172/Data_Streaming_for_Microservices_using_Debezium">VoxxedDays Microservices</a>; Paris, France; Oct. 29 - 31</p> </li> <li> <p><a href="https://cfp.devoxx.ma/2018/talk/AEY-4477/Change_Data_Streaming_Patterns_for_Microservices_With_Debezium">Devoxx Morocco</a>; Marrakesh, Morocco; Nov. 27 - 29</p> </li> </ul> </div> <div class="paragraph"> <p>Already last week I had the opportunity to present Debezium at <a href="https://jug-saxony-day.org/programm/#!/P31">JUG Saxony Day</a>. If you are interested, you can find the (German) <a href="https://speakerdeck.com/gunnarmorling/streaming-von-datenbankanderungen-mit-debezium-jug-saxony-day">slideset of that talk</a> on Speaker Deck.</p> </div> </div> </div> <div class="sect1"> <h2 id="about_debezium"><a class="anchor" href="#about_debezium"></a>About Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of <a href="http://kafka.apache.org/">Kafka</a> and provides <a href="http://kafka.apache.org/documentation.html#connect">Kafka Connect</a> compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is <a href="/license">open source</a> under the <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, Version 2.0</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="get_involved"><a class="anchor" href="#get_involved"></a>Get involved</h2> <div class="sectionbody"> <div class="paragraph"> <p>We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter <a href="https://twitter.com/debezium">@debezium</a>, <a href="https://gitter.im/debezium/user">chat with us on Gitter</a>, or join our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> to talk with the community. All of the code is open source <a href="https://github.com/debezium/">on GitHub</a>, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or <a href="https://issues.jboss.org/projects/DBZ/issues/">log an issue</a>.</p> </div> </div> </div> </div> <hr> <div class="post"> <h1 class="title"> <a href="/blog/2018/09/20/materializing-aggregate-views-with-hibernate-and-debezium/">Materializing Aggregate Views With Hibernate and Debezium</a> </h1> <div class="byline"> <p> <em> September 20, 2018 by Gunnar Morling </em> <em> under&nbsp; </em> <a class="label label-info" href="/blog/tags/discussion/">discussion</a> <a class="label label-info" href="/blog/tags/examples/">examples</a> </p> </div> <div id="preamble"> <div class="sectionbody"> <div class="paragraph"> <p>Updating external full text search indexes (e.g. <a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a>) after data changes is a very popular use case for change data capture (CDC).</p> </div> <div class="paragraph"> <p>As we&#8217;ve discussed in a <a href="/blog/2018/01/17/streaming-to-elasticsearch/">blog post</a> a while ago, the combination of Debezium&#8217;s CDC source connectors and Confluent&#8217;s <a href="https://docs.confluent.io/current/connect/connect-elasticsearch/docs/index.html">sink connector for Elasticsearch</a> makes it straight forward to capture data changes in MySQL, Postgres etc. and push them towards Elastisearch in near real-time. This results in a 1:1 relationship between tables in the source database and a corresponding search index in Elasticsearch, which is perfectly fine for many use cases.</p> </div> <div class="paragraph"> <p>It gets more challenging though if you&#8217;d like to put entire aggregates into a single index. An example could be a customer and all their addresses; those would typically be stored in two separate tables in an RDBMS, linked by a foreign key, whereas you&#8217;d like to have just one index in Elasticsearch, containing documents of customers with their addresses embedded, allowing you to efficiently search for customers based on their address.</p> </div> <div class="paragraph"> <p>Following up to the <a href="/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/">KStreams-based solution</a> to this we described recently, we&#8217;d like to present in this post an alternative for materializing such aggregate views driven by the application layer.</p> </div> </div> </div> <div class="sect1"> <h2 id="overview"><a class="anchor" href="#overview"></a>Overview</h2> <div class="sectionbody"> <div class="paragraph"> <p>The idea is to materialize views in a separate table in the source database, right in the moment the original data is altered.</p> </div> <div class="paragraph"> <p>Aggregates are serialized as JSON structures (which naturally can represent any nested object structure) and stored in a specific table. This is done within the actual transaction altering the data, which means the aggregate view is always consistent with the primary data. In particular this approach isn&#8217;t prone to exposing intermediary aggregations as the KStreams-based solution discussed in the post linked above.</p> </div> <div class="paragraph"> <p>The following picture shows the overall architecture:</p> </div> <img src="/images/jpa_aggregations.png" style="max-width:100%; margin-bottom:10px;" class="responsive-image" alt="Streaming Materialized Aggregate Views to Elastisearch"> <div class="paragraph"> <p>Here the aggregate views are materialized by means of a small extension to <a href="http://hibernate.org/orm/">Hibernate ORM</a>, which stores the JSON aggregates within the source database (note "aggregate views" can be considered conceptually the same as "materialized views" as known from different RDBMS, as in that they materialize the result of a "join" operation, but technically we&#8217;re not using the latter to store aggregate views, but a regular table). Changes to that aggregate table are then captured by Debezium and streamed to one topic per aggregate type. The Elasticsearch sink connector can subscribe to these topics and update corresponding full-text indexes.</p> </div> <div class="paragraph"> <p>You can find a proof-of-concept implementation (said Hibernate extension and related code) of this idea in our <a href="https://github.com/debezium/debezium-examples/tree/master/jpa-aggregations">examples repository</a>. Of course the general idea isn&#8217;t limited to Hibernate ORM or JPA, you could implement something similar with any other API you&#8217;re using to access your data.</p> </div> </div> </div> <div class="sect1"> <h2 id="creating_aggregate_views_via_hibernate_orm"><a class="anchor" href="#creating_aggregate_views_via_hibernate_orm"></a>Creating Aggregate Views via Hibernate ORM</h2> <div class="sectionbody"> <div class="paragraph"> <p>For the following let&#8217;s assume we&#8217;re persisting a simple domain model (comprising a <code>Customer</code> entity and a few related ones like <code>Address</code>, (customer) <code>Category</code> etc.) in a database. Using Hibernate for that allows us to make the creation of aggregates fully transparent to the actual application code using a <a href="http://docs.jboss.org/hibernate/orm/current/userguide/html_single/Hibernate_User_Guide.html#events-events">Hibernate event listener</a>. Thanks to its extensible architecture, we can plug such listener into Hibernate just by adding it to the classpath, from where it will be picked up automatically when bootstrapping the entity manager / session factory.</p> </div> <div class="paragraph"> <p>Our example listener reacts to an annotation, <code>@MaterializeAggregate</code>, which marks those entity types that should be the roots of materialized aggregates.</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@Entity&#x000A;@MaterializeAggregate(aggregateName="customers-complete")&#x000A;public class Customer {&#x000A;&#x000A;    @Id&#x000A;    private long id;&#x000A;&#x000A;    private String firstName;&#x000A;&#x000A;    @OneToMany(mappedBy = "customer", fetch = FetchType.EAGER, cascade = CascadeType.ALL)&#x000A;    private Set&lt;Address&gt; addresses;&#x000A;&#x000A;    @ManyToOne&#x000A;    private Category category;&#x000A;&#x000A;    ...&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>Now if any entity annotated with <code>@MaterializeAggregate</code> is inserted, updated or deleted via Hibernate, the listener will kick in and materialize a JSON view of the aggregate root (customer) and its associated entities (addresses, category).</p> </div> <div class="paragraph"> <p>Under the hood the <a href="https://github.com/FasterXML/jackson">Jackson API</a> is used for serializing the model into JSON. This means you can use any of its annotations to customize the JSON output, e.g. <code>@JsonIgnore</code> to exclude the inverse relationship from <code>Address</code> to <code>Customer</code>:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@Entity&#x000A;public class Address {&#x000A;&#x000A;    @Id&#x000A;    private long id;&#x000A;&#x000A;    @ManyToOne&#x000A;    @JoinColumn(name = "customer_id")&#x000A;    @JsonIgnore&#x000A;    private Customer customer;&#x000A;&#x000A;    private String street;&#x000A;&#x000A;    private String city;&#x000A;&#x000A;    ...&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>Note that <code>Address</code> itself isn&#8217;t marked with <code>@MaterializeAggregate</code>, i.e. it won&#8217;t be materialized into an aggregate view by itself.</p> </div> <div class="paragraph"> <p>After using JPA&#8217;s <code>EntityManager</code> to insert or update a few customers, let&#8217;s take a look at the <code>aggregates</code> table which has been populated by the listener (value schema omitted for the sake of brevity):</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-sql" data-lang="sql">&gt; select * from aggregates;&#x000A;&#x000A;| rootType | keySchema | rootId | materialization | valueSchema |&#x000A;&#x000A;| customers-complete&#x000A;&#x000A;| {&#x000A;  "schema" : {&#x000A;    "type" : "struct",&#x000A;    "fields" : [ {&#x000A;      "type" : "int64",&#x000A;      "optional" : false,&#x000A;      "field" : "id"&#x000A;    } ],&#x000A;    "optional" : false,&#x000A;    "name" : "customers-complete.Key"&#x000A;  }&#x000A;}&#x000A;&#x000A;| { "id" : 1004 }&#x000A;&#x000A;| { "schema" : { ... } }&#x000A;&#x000A;| {&#x000A;  "id" : 1004,&#x000A;  "firstName" : "Anne",&#x000A;  "lastName" : "Kretchmar",&#x000A;  "email" : "annek@noanswer.org",&#x000A;  "tags" : [ "long-term", "vip" ],&#x000A;  "birthday" : 5098,&#x000A;  "category" : {&#x000A;    "id" : 100001,&#x000A;    "name" : "Retail"&#x000A;  },&#x000A;  "addresses" : [ {&#x000A;    "id" : 16,&#x000A;    "street" : "1289 University Hill Road",&#x000A;    "city" : "Canehill",&#x000A;    "state" : "Arkansas",&#x000A;    "zip" : "72717",&#x000A;    "type" : "SHIPPING"&#x000A;  } ]&#x000A;} |</code></pre> </div> </div> <div class="paragraph"> <p>The table contains these columns:</p> </div> <div class="ulist"> <ul> <li> <p><code>rootType</code>: The name of the aggregate as given in the <code>@MaterializeAggregate</code> annotation</p> </li> <li> <p><code>rootId</code>: The aggregate&#8217;s id as serialized JSON</p> </li> <li> <p><code>materialization</code>: The aggregate itself as serialized JSON; in this case a customer and their addresses, category etc.</p> </li> <li> <p><code>keySchema</code>: The Kafka Connect schema of the row&#8217;s key</p> </li> <li> <p><code>valueSchema</code>: The Kafka Connect schema of the materialization</p> </li> </ul> </div> <div class="paragraph"> <p>Let&#8217;s talk about the two schema columns for a bit. JSON itself is quite limited as far as its supported data types are concerned. So for instance we&#8217;d loose information about a numeric field&#8217;s value range (int vs. long etc.) without any additional information. Therefore the listener derives the corresponding schema information for key and aggregate view from the entity model and stores it within the aggregate records.</p> </div> <div class="paragraph"> <p>Now Jackson itself only supports JSON Schema, which would be a bit too limited for our purposes. Hence the example implementation provides custom serializers for Jackson&#8217;s schema system, which allow us to emit Kafka Connect&#8217;s schema representation (with more precise type information) instead of plain JSON Schema. This will come in handy in the following when we&#8217;d like to expand the string-based JSON representations of key and value into properly typed Kafka Connect records.</p> </div> </div> </div> <div class="sect1"> <h2 id="capturing_changes_to_the_aggregate_table"><a class="anchor" href="#capturing_changes_to_the_aggregate_table"></a>Capturing Changes to the Aggregate Table</h2> <div class="sectionbody"> <div class="paragraph"> <p>We now have a mechanism in place which transparently persists aggregates into a separate table within the source database, whenever the application data is changed through Hibernate. Note that this happens within the boundaries of the source transaction, so if the same would be rolled back for some reason, also the aggregate view would not be updated.</p> </div> <div class="paragraph"> <p>The Hibernate listener uses insert-or-update semantics when writing an aggregate view, i.e. for a given aggregate root there&#8217;ll always be exactly one corresponding entry in the aggregate table which reflects its current state. If an aggregate root entity is deleted, the listener will also drop the entry from the aggregate table.</p> </div> <div class="paragraph"> <p>So let&#8217;s set up Debezium now to capture any changes to the <code>aggregates</code> table:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">curl -i -X POST \&#x000A;  -H "Accept:application/json" \&#x000A;  -H "Content-Type:application/json" \&#x000A;  http://localhost:8083/connectors/ -d @- &lt;&lt;-EOF&#x000A;  {&#x000A;      "name": "inventory-connector",&#x000A;      "config": {&#x000A;          "connector.class": "io.debezium.connector.mysql.MySqlConnector",&#x000A;          "tasks.max": "1",&#x000A;          "database.hostname": "mysql",&#x000A;          "database.port": "3306",&#x000A;          "database.user": "debezium",&#x000A;          "database.password": "dbz",&#x000A;          "database.server.id": "184054",&#x000A;          "database.server.name": "dbserver1",&#x000A;          "database.whitelist": "inventory",&#x000A;          "table.whitelist": ".*aggregates",&#x000A;          "database.history.kafka.bootstrap.servers": "kafka:9092",&#x000A;          "database.history.kafka.topic": "schema-changes.inventory"&#x000A;      }&#x000A;  }&#x000A;EOF</code></pre> </div> </div> <div class="paragraph"> <p>This registers the MySQL connector with the "inventory" database (we&#8217;re using an expanded version of the schema from the <a href="/docs/tutorial">Debezium tutorial</a>), capturing any changes to the "aggregates" table.</p> </div> </div> </div> <div class="sect1"> <h2 id="expanding_json"><a class="anchor" href="#expanding_json"></a>Expanding JSON</h2> <div class="sectionbody"> <div class="paragraph"> <p>If we now were to browse the corresponding Kafka topic, we&#8217;d see data change events in the known Debezium format for all the changes to the <code>aggregates</code> table.</p> </div> <div class="paragraph"> <p>The "materialization" field with the records' "after" state still is a single field containing a JSON string, though. What we&#8217;d rather like to have is a strongly typed Kafka Connect record, whose schema exactly describes the aggregate structure and the types of its fields. For that purpose the example project provides an SMT (single message transform) which takes the JSON materialization and the corresponding <code>valueSchema</code> and converts this into a full-blown Kafka Connect record. The same is done for keys. DELETE events are rewritten into tombstone events. Finally, the SMT re-routes every record to a topic named after the aggregate root, allowing consumers to subscribe just to changes to specific aggregate types.</p> </div> <div class="paragraph"> <p>So let&#8217;s add that SMT when registering the Debezium CDC connector:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">...&#x000A;"transforms":"expandjson",&#x000A;"transforms.expandjson.type":"io.debezium.aggregation.smt.ExpandJsonSmt",&#x000A;...</code></pre> </div> </div> <div class="paragraph"> <p>When now browsing the "customers-complete" topic, we&#8217;ll see the strongly typed Kafka Connect records we&#8217;d expect:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">{&#x000A;    "schema": {&#x000A;        "type": "struct",&#x000A;        "fields": [&#x000A;            {&#x000A;                "type": "int64",&#x000A;                "optional": false,&#x000A;                "field": "id"&#x000A;            }&#x000A;        ],&#x000A;        "optional": false,&#x000A;        "name": "customers-complete.Key"&#x000A;    },&#x000A;    "payload": {&#x000A;        "id": 1004&#x000A;    }&#x000A;}&#x000A;{&#x000A;    "schema": {&#x000A;        "type": "struct",&#x000A;        "fields": [ ... ],&#x000A;        "optional": true,&#x000A;        "name": "urn:jsonschema:com:example:domain:Customer"&#x000A;    },&#x000A;    "payload": {&#x000A;        "id": 1004,&#x000A;        "firstName": "Anne",&#x000A;        "lastName": "Kretchmar",&#x000A;        "email": "annek@noanswer.org",&#x000A;        "active": true,&#x000A;        "tags" : [ "long-term", "vip" ],&#x000A;        "birthday" : 5098,&#x000A;        "category": {&#x000A;            "id": 100001,&#x000A;            "name": "Retail"&#x000A;        },&#x000A;        "addresses": [&#x000A;            {&#x000A;                "id": 16,&#x000A;                "street": "1289 University Hill Road",&#x000A;                "city": "Canehill",&#x000A;                "state": "Arkansas",&#x000A;                "zip": "72717",&#x000A;                "type": "LIVING"&#x000A;            }&#x000A;        ]&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>To confirm that these are actual typed Kafka Connect records and not just a single JSON string field, you could for instance use the <a href="/docs/configuration/avro/">Avro message converter</a> and examine the message schemas in the schema registry.</p> </div> </div> </div> <div class="sect1"> <h2 id="sinking_aggregate_messages_into_elasticsearch"><a class="anchor" href="#sinking_aggregate_messages_into_elasticsearch"></a>Sinking Aggregate Messages Into Elasticsearch</h2> <div class="sectionbody"> <div class="paragraph"> <p>The last missing step is to register the Confluent Elasticsearch sink connector, hooking it up with the "customers-complete" topic and letting it push any changes to the corresponding index:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">curl -i -X POST \&#x000A;  -H "Accept:application/json" \&#x000A;  -H "Content-Type:application/json" \&#x000A;  http://localhost:8083/connectors/ -d @- &lt;&lt;-EOF&#x000A;  {&#x000A;      "name": "es-customers",&#x000A;      "config": {&#x000A;          "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",&#x000A;          "tasks.max": "1",&#x000A;          "topics": "customers-complete",&#x000A;          "connection.url": "http://elastic:9200",&#x000A;          "key.ignore": "false",&#x000A;          "schema.ignore" : "false",&#x000A;          "behavior.on.null.values" : "delete",&#x000A;          "type.name": "customer-with-addresses",&#x000A;          "transforms" : "key",&#x000A;          "transforms.key.type": "org.apache.kafka.connect.transforms.ExtractField$Key",&#x000A;          "transforms.key.field": "id"&#x000A;      }&#x000A;  }&#x000A;EOF</code></pre> </div> </div> <div class="paragraph"> <p>This uses Connect&#8217;s <code>ExtractField</code> transformation to obtain just the actual id value from the key struct and use it as key for the corresponding Elasticsearch documents. Specifying the "behavior.on.null.values" option will let the connector delete the corresponding document from the index when encountering a tombstone message (i.e. a message with a key but without value).</p> </div> <div class="paragraph"> <p>Finally, we can use the Elasticsearch REST API to browse the index and of course use its powerful full-text query language to find customers by the address or any other property embedded into the aggregate structure:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">&gt; curl -X GET -H "Accept:application/json" \&#x000A;  http://localhost:9200/customers-complete/_search?pretty&#x000A;&#x000A;  {&#x000A;      "_shards": {&#x000A;          "failed": 0,&#x000A;          "successful": 5,&#x000A;          "total": 5&#x000A;      },&#x000A;      "hits": {&#x000A;          "hits": [&#x000A;              {&#x000A;                  "_id": "1004",&#x000A;                  "_index": "customers-complete",&#x000A;                  "_score": 1.0,&#x000A;                  "_source": {&#x000A;                      "active": true,&#x000A;                      "addresses": [&#x000A;                          {&#x000A;                              "city": "Canehill",&#x000A;                              "id": 16,&#x000A;                              "state": "Arkansas",&#x000A;                              "street": "1289 University Hill Road",&#x000A;                              "type": "LIVING",&#x000A;                              "zip": "72717"&#x000A;                          }&#x000A;                      ],&#x000A;                      "tags" : [ "long-term", "vip" ],&#x000A;                      "birthday" : 5098,&#x000A;                      "category": {&#x000A;                          "id": 100001,&#x000A;                          "name": "Retail"&#x000A;                      },&#x000A;                      "email": "annek@noanswer.org",&#x000A;                      "firstName": "Anne",&#x000A;                      "id": 1004,&#x000A;                      "lastName": "Kretchmar",&#x000A;                      "scores": [],&#x000A;                      "someBlob": null,&#x000A;                      "tags": []&#x000A;                  },&#x000A;                  "_type": "customer-with-addresses"&#x000A;              }&#x000A;          ],&#x000A;          "max_score": 1.0,&#x000A;          "total": 1&#x000A;      },&#x000A;      "timed_out": false,&#x000A;      "took": 11&#x000A;  }</code></pre> </div> </div> <div class="paragraph"> <p>And there you have it: a customer&#8217;s complete data, including their addresses, categories, tags etc., materialized into a single document within Elasticsearch. If you&#8217;re using JPA to update the customer, you&#8217;ll see the data in the index being updated accordingly in near-realtime.</p> </div> </div> </div> <div class="sect1"> <h2 id="pros_and_cons"><a class="anchor" href="#pros_and_cons"></a>Pros and Cons</h2> <div class="sectionbody"> <div class="paragraph"> <p>So what are the advantages and disadvantages of this approach for materializing aggregates from multiple source tables compared to the <a href="/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/">KStreams-based approach</a>?</p> </div> <div class="paragraph"> <p>The big advantage is consistency and awareness of transactional boundaries, whereas the KStreams-based solution in its suggested form was prone to exposing intermediary aggregates. For instance, if you&#8217;re storing a customer and three addresses, it might happen that the streaming query first creates an aggregation of the customer and the two addresses inserted first, and shortly thereafter the complete aggregate with all three addresses. This not the case for the approach discussed here, as you&#8217;ll only ever stream complete aggregates to Kafka. Also this approach feels a bit more "light-weight", i.e. a simple marker annotation (together with some Jackson annotations for fine-tuning the emitted JSON structures) is enough in order to materialize aggregates from your domain model, whereas some more effort was needed to set up the required streams, temporary tables etc. with the KStreams solution.</p> </div> <div class="paragraph"> <p>The downside of driving aggregations through the application layer is that it&#8217;s not fully agnostic to the way you access the primary data. If you bypass the application, e.g. by patching data directly in the database, naturally these updates would be missed, requiring a refresh of affected aggregates. Although this again could be done through change data capture and Debezium: change events to source tables could be captured and consumed by the application itself, allowing it to re-materialize aggregates after external data changes. You also might argue that running JSON serializations within source transactions and storing aggregates within the source database represents some overhead. This often may be acceptable, though.</p> </div> <div class="paragraph"> <p>Another question to ask is what&#8217;s the advantage of using change data capture on an intermediary aggregate table over simply posting REST requests to Elasticsearch. The answer is the highly increased robustness and fault tolerance. If the Elasticsearch cluster can&#8217;t be accessed for some reason, the machinery of Kafka and Kafka Connect will ensure that any change events will be propagated eventually, once the sink is up again. Also other consumers than Elasticsearch can subscribe to the aggregate topic, the log can be replayed from the beginning etc.</p> </div> <div class="paragraph"> <p>Note that while we&#8217;ve been talking primarily about using Elasticsearch as a data sink, there are also other datastores and connectors that support complexly structured records. One example would be MongoDB and the <a href="https://github.com/hpgrahsl/kafka-connect-mongodb">sink connector</a> maintained by Hans-Peter Grahsl, which one could use to sink customer aggregates into MongoDB, for instance enabling efficient retrieval of a customer and all their associated data with a single primary key look-up.</p> </div> </div> </div> <div class="sect1"> <h2 id="outlook"><a class="anchor" href="#outlook"></a>Outlook</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Hibernate ORM extension as well as the SMT discussed in this post can be found in our <a href="https://github.com/debezium/debezium-examples/tree/master/jpa-aggregations">examples repository</a>. They should be considered to be at "proof-of-concept" level currently.</p> </div> <div class="paragraph"> <p>That being said, we&#8217;re considering to make this a Debezium component proper, allowing you to employ this aggregation approach within your Hibernate-based applications just by pulling in this new component. For that we&#8217;d have to improve a few things first, though. Most importantly, an API is needed which will let you (re-)create aggregates on demand, e.g. for existing data or for data updated by bulk updates via the Criteria API / JPQL (which will be missed by listeners). Also aggregates should be re-created automatically, if any of the referenced entities change (with the current PoC, only a change to the customer instance itself will trigger its aggregate view to be rebuilt, but not a change to one of its addresses).</p> </div> <div class="paragraph"> <p>If you like this idea, then let us know about it, so we can gauge the general interest in this. Also, this would be a great item to work on, if you&#8217;re interested in contributing to the Debezium project. Looking forward to hearing from you, e.g. in the comment section below or on our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a>.</p> </div> <div class="paragraph"> <p>Thanks a lot to Hans-Peter Grahsl for his feedback on an earlier version of this post!</p> </div> </div> </div> <div class="sect1"> <h2 id="about_debezium"><a class="anchor" href="#about_debezium"></a>About Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of <a href="http://kafka.apache.org/">Kafka</a> and provides <a href="http://kafka.apache.org/documentation.html#connect">Kafka Connect</a> compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is <a href="/license">open source</a> under the <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, Version 2.0</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="get_involved"><a class="anchor" href="#get_involved"></a>Get involved</h2> <div class="sectionbody"> <div class="paragraph"> <p>We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter <a href="https://twitter.com/debezium">@debezium</a>, <a href="https://gitter.im/debezium/user">chat with us on Gitter</a>, or join our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> to talk with the community. All of the code is open source <a href="https://github.com/debezium/">on GitHub</a>, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or <a href="https://issues.jboss.org/projects/DBZ/issues/">log an issue</a>.</p> </div> </div> </div> </div> <hr> <div class="post"> <h1 class="title"> <a href="/blog/2018/09/19/debezium-0-8-3-final-released/">Debezium 0.8.3.Final Released</a> </h1> <div class="byline"> <p> <em> September 19, 2018 by Gunnar Morling </em> <em> under&nbsp; </em> <a class="label label-info" href="/blog/tags/releases/">releases</a> <a class="label label-info" href="/blog/tags/mysql/">mysql</a> <a class="label label-info" href="/blog/tags/mongodb/">mongodb</a> <a class="label label-info" href="/blog/tags/postgres/">postgres</a> <a class="label label-info" href="/blog/tags/docker/">docker</a> </p> </div> <div id="preamble"> <div class="sectionbody"> <div class="paragraph"> <p>As temperatures are cooling off, the Debezium team is getting into full swing again and we&#8217;re happy to announce the release of Debezium <strong>0.8.3.Final</strong>!</p> </div> <div class="paragraph"> <p>This is a bugfix release to the current stable release line of Debezium, 0.8.x, while the work on Debezium 0.9 goes on in parallel. There are <a href="/docs/releases/#release-0-8-3-final">14 fixes</a> in this release. As in earlier 0.8.x releases, we&#8217;ve further improved the new Antlr-based DDL parser used by the <a href="/docs/connectors/mysql/">MySQL connector</a> (see <a href="https://issues.jboss.org/browse/DBZ-901">DBZ-901</a>, <a href="https://issues.jboss.org/browse/DBZ-903">DBZ-903</a> and <a href="https://issues.jboss.org/browse/DBZ-910">DBZ-910</a>).</p> </div> <div class="paragraph"> <p>The <a href="/docs/connectors/postgres/">Postgres connector</a> saw a huge improvement to its start-up time for databases with lots of custom types (<a href="https://issues.jboss.org/browse/DBZ-899">DBZ-899</a>). The user reporting this issue had nearly 200K entries in pg_catalog.pg_type, and due to an N + 1 SELECT issue within the Postgres driver itself, this caused the connector to take 24 minutes to start. By using a custom query for obtaining the type metadata, we were able to cut down this time to 5 seconds! Right now we&#8217;re working with the maintainers of the Postgres driver to get this issue fixed upstream, too.</p> </div> </div> </div> <div class="sect1"> <h2 id="more_flexible_propagation_of_deletes"><a class="anchor" href="#more_flexible_propagation_of_deletes"></a>More Flexible Propagation of DELETEs</h2> <div class="sectionbody"> <div class="paragraph"> <p>Besides those bug fixes we decided to also merge one new feature from the 0.9.x branch into the 0.8.3.Final release, which those of you may find useful who are using the <a href="/docs/configuration/event-flattening/">SMT for extracting the "after" state</a> from change events (<a href="https://issues.jboss.org/browse/DBZ-857">DBZ-857</a>).</p> </div> <div class="paragraph"> <p>This SMT can be employed to stream changes to sink connectors which expect just a "flat" row representation of data instead of Debezium&#8217;s complex event structure. Not all sink connectors support the handling of deletions, though. E.g. some connectors will fail when encountering tombstone events. Therefore the SMT can now optionally rewrite delete events into updates of a special "deleted" marker field.</p> </div> <div class="paragraph"> <p>For that, set the <code>delete.handling.mode</code> option of the SMT to "rewrite":</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">...&#x000A;"transforms" : "unwrap",&#x000A;"transforms.unwrap.type": "io.debezium.transforms.UnwrapFromEnvelope",&#x000A;"transforms.unwrap.delete.handling.mode" : "rewrite",&#x000A;...</code></pre> </div> </div> <div class="paragraph"> <p>When a DELETE event is propagated, the "__deleted" field of outgoing records will be set to true. So when for instance consuming the events with the JDBC sink connector, you&#8217;d see this being reflected in a corresponding column in the sink tables:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code>__deleted | last_name |  id  | first_name |         email&#x000A;-----------+-----------+------+------------+-----------------------&#x000A;false     | Thomas    | 1001 | Sally      | sally.thomas@acme.com&#x000A;false     | Bailey    | 1002 | George     | gbailey@foobar.com&#x000A;false     | Kretchmar | 1004 | Anne       | annek@noanswer.org&#x000A;true      | Walker    | 1003 | Edward     | ed@walker.com</code></pre> </div> </div> <div class="paragraph"> <p>You then for instance can use a batch job running on your sink to remove all records flagged as deleted.</p> </div> </div> </div> <div class="sect1"> <h2 id="what_s_next"><a class="anchor" href="#what_s_next"></a>What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>We&#8217;re continuing the work on Debezium 0.9, which will mostly be about improvements to the SQL Server and Oracle connectors. The current plan is to do the next 0.9 release (either Alpha2 or Beta1) in two weeks from now.</p> </div> <div class="paragraph"> <p>Also it&#8217;s the beginning of the conference season, so we&#8217;ll spend some time with preparing demos and presenting Debezium at multiple locations. There will be sessions on change data capture with Debezium a these conferences:</p> </div> <div class="ulist"> <ul> <li> <p><a href="https://jug-saxony-day.org/programm/#!/P31">JUG Saxony Day</a>; Dresden, Germany; Sept. 28</p> </li> <li> <p><a href="https://kafka-summit.org/sessions/change-data-streaming-patterns-microservices-debezium/">Kafka Summit</a>; San Francisco, Cal.; Oct. 17</p> </li> <li> <p><a href="https://vxdms2018.confinabox.com/talk/INI-9172/Data_Streaming_for_Microservices_using_Debezium">VoxxedDays Microservices</a>; Paris, France; Oct. 29 - 31</p> </li> <li> <p><a href="https://cfp.devoxx.ma/2018/talk/AEY-4477/Change_Data_Streaming_Patterns_for_Microservices_With_Debezium">Devoxx Morocco</a>; Marrakesh, Morocco; Nov. 27 - 29</p> </li> </ul> </div> <div class="paragraph"> <p>If you are at any of these conferences, come and say Hi; we&#8217;d love to exchange with you about your use cases, feature requests, feedback on our <a href="/docs/roadmap/">roadmap</a> and any other ideas around Debezium.</p> </div> <div class="paragraph"> <p>Finally, a big "Thank You" goes to our fantastic community members <a href="https://github.com/jchipmunk">Andrey Pustovetov</a>, <a href="https://github.com/maver1ck">Maciej Bry≈Ñski</a> and <a href="https://github.com/PengLyu">Peng Lyu</a> for their contributions to this release!</p> </div> </div> </div> <div class="sect1"> <h2 id="about_debezium"><a class="anchor" href="#about_debezium"></a>About Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of <a href="http://kafka.apache.org/">Kafka</a> and provides <a href="http://kafka.apache.org/documentation.html#connect">Kafka Connect</a> compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is <a href="/license">open source</a> under the <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, Version 2.0</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="get_involved"><a class="anchor" href="#get_involved"></a>Get involved</h2> <div class="sectionbody"> <div class="paragraph"> <p>We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter <a href="https://twitter.com/debezium">@debezium</a>, <a href="https://gitter.im/debezium/user">chat with us on Gitter</a>, or join our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> to talk with the community. All of the code is open source <a href="https://github.com/debezium/">on GitHub</a>, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or <a href="https://issues.jboss.org/projects/DBZ/issues/">log an issue</a>.</p> </div> </div> </div> </div> <hr> <div class="post"> <h1 class="title"> <a href="/blog/2018/08/30/streaming-mysql-data-changes-into-kinesis/">Streaming MySQL Data Changes to Amazon Kinesis</a> </h1> <div class="byline"> <p> <em> August 30, 2018 by Gunnar Morling </em> <em> under&nbsp; </em> <a class="label label-info" href="/blog/tags/discussion/">discussion</a> <a class="label label-info" href="/blog/tags/examples/">examples</a> </p> </div> <div id="preamble"> <div class="sectionbody"> <div class="paragraph"> <p>Most of the times Debezium is used to stream data changes into <a href="http://kafka.apache.org/">Apache Kafka</a>. What though if you&#8217;re using another streaming platform such as <a href="https://pulsar.incubator.apache.org/">Apache Pulsar</a> or a cloud-based solution such as <a href="https://aws.amazon.com/kinesis/">Amazon Kinesis</a>, <a href="https://azure.microsoft.com/services/event-hubs/">Azure Event Hubs</a> and the like? Can you still benefit from Debezium&#8217;s powerful change data capture (CDC) capabilities and ingest changes from databases such as MySQL, Postgres, SQL Server etc.?</p> </div> <div class="paragraph"> <p>Turns out, with just a bit of glue code, you can! In the following we&#8217;ll discuss how to use Debezium to capture changes in a MySQL database and stream the change events into Kinesis, a fully-managed data streaming service available on the Amazon cloud.</p> </div> </div> </div> <div class="sect1"> <h2 id="introducing_the_debezium_embedded_engine"><a class="anchor" href="#introducing_the_debezium_embedded_engine"></a>Introducing the Debezium Embedded Engine</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is implemented as a set of connectors for Kafka and thus usually is run via <a href="https://kafka.apache.org/documentation/#connect">Kafka Connect</a>. But there&#8217;s one little gem in Debezium which isn&#8217;t as widely known yet, which is the <a href="/docs/embedded/">embedded engine</a>.</p> </div> <div class="paragraph"> <p>When using this engine, the Debezium connectors are not executed within Kafka Connect, but as a library embedded into your own Java application. For this purpose, the <em>debezium-embedded</em> module provides a small runtime environment which performs the tasks that&#8217;d otherwise be handled by the Kafka Connect framework: requesting change records from the connector, committing offsets etc. Each change record produced by the connector is passed to a configured event handler method, which in our case will convert the record into its JSON representation and submit it to a Kinesis stream, using the Kinesis Java API.</p> </div> <div class="paragraph"> <p>The overall architecture looks like so:</p> </div> <img src="/images/debezium-embedded.png" style="max-width:100%; margin-bottom:10px;" class="responsive-image" alt="Debezium Embedded Engine Streaming to Amazon Kinesis"> <div class="paragraph"> <p>Now let&#8217;s walk through the relevant parts of the code required for that. A complete executable example can be found in the <a href="https://github.com/debezium/debezium-examples/tree/master/kinesis">debezium-examples</a> repo on GitHub.</p> </div> </div> </div> <div class="sect1"> <h2 id="set_up"><a class="anchor" href="#set_up"></a>Set-Up</h2> <div class="sectionbody"> <div class="paragraph"> <p>In order to use Debezium&#8217;s embedded engine, add the <em>debezium-embedded</em> dependency as well as the Debezium connector of your choice to your project&#8217;s <em>pom.xml</em>. In the following we&#8217;re going to use the connector for MySQL. We also need to add a dependency to the <a href="https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/kinesis/package-summary.html">Kinesis Client API</a>, so these are the dependencies needed:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-xml" data-lang="xml">...&#x000A;&lt;dependency&gt;&#x000A;    &lt;groupId&gt;io.debezium&lt;/groupId&gt;&#x000A;    &lt;artifactId&gt;debezium-embedded&lt;/artifactId&gt;&#x000A;    &lt;version&gt;0.8.3.Final&lt;/version&gt;&#x000A;&lt;/dependency&gt;&#x000A;&lt;dependency&gt;&#x000A;    &lt;groupId&gt;io.debezium&lt;/groupId&gt;&#x000A;    &lt;artifactId&gt;debezium-connector-mysql&lt;/artifactId&gt;&#x000A;    &lt;version&gt;0.8.3.Final&lt;/version&gt;&#x000A;&lt;/dependency&gt;&#x000A;&lt;dependency&gt;&#x000A;    &lt;groupId&gt;com.amazonaws&lt;/groupId&gt;&#x000A;    &lt;artifactId&gt;amazon-kinesis-client&lt;/artifactId&gt;&#x000A;    &lt;version&gt;1.9.0&lt;/version&gt;&#x000A;&lt;/dependency&gt;&#x000A;...</code></pre> </div> </div> </div> </div> <div class="sect1"> <h2 id="configuring_the_embedded_engine"><a class="anchor" href="#configuring_the_embedded_engine"></a>Configuring the Embedded Engine</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Debezium embedded engine is configured through an instance of <code>io.debezium.config.Configuration</code>. This class can obtain values from system properties or from a given config file, but for the sake of the example we&#8217;ll simply pass all required values via its fluent builder API:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">Configuration config = Configuration.create()&#x000A;    .with(EmbeddedEngine.CONNECTOR_CLASS, "io.debezium.connector.mysql.MySqlConnector")&#x000A;    .with(EmbeddedEngine.ENGINE_NAME, "kinesis")&#x000A;    .with(MySqlConnectorConfig.SERVER_NAME, "kinesis")&#x000A;    .with(MySqlConnectorConfig.SERVER_ID, 8192)&#x000A;    .with(MySqlConnectorConfig.HOSTNAME, "localhost")&#x000A;    .with(MySqlConnectorConfig.PORT, 3306)&#x000A;    .with(MySqlConnectorConfig.USER, "debezium")&#x000A;    .with(MySqlConnectorConfig.PASSWORD, "dbz")&#x000A;    .with(MySqlConnectorConfig.DATABASE_WHITELIST, "inventory")&#x000A;    .with(MySqlConnectorConfig.TABLE_WHITELIST, "inventory.customers")&#x000A;    .with(EmbeddedEngine.OFFSET_STORAGE,&#x000A;        "org.apache.kafka.connect.storage.MemoryOffsetBackingStore")&#x000A;    .with(MySqlConnectorConfig.DATABASE_HISTORY,&#x000A;        MemoryDatabaseHistory.class.getName())&#x000A;    .with("schemas.enable", false)&#x000A;    .build();</code></pre> </div> </div> <div class="paragraph"> <p>If you&#8217;ve ever set up the Debezium MySQL connector in Kafka Connect, most of the properties will look familiar to you.</p> </div> <div class="paragraph"> <p>But let&#8217;s talk about the <code>OFFSET_STORAGE</code> and <code>DATABASE_HISTORY</code> options in a bit more detail. They deal with how connector offsets and the database history should be persisted. When running the connector via Kafka Connect, both would typically be stored in specific Kafka topics. But that&#8217;s not an option here, so an alternative is needed. For this example we&#8217;re simply going to keep the offsets and database history in memory. I.e. if the engine is restarted, this information will be lost and the connector will start from scratch, e.g. with a new initial snapshot.</p> </div> <div class="paragraph"> <p>While out of scope for this blog post, it wouldn&#8217;t be too difficult to create alternative implementations of the <code>OffsetBackingStore</code> and <code>DatabaseHistory</code> contracts, respectively. For instance if you&#8217;re fully committed into the AWS cloud services, you could think of storing offsets and database history in the DynamoDB NoSQL store. Note that, different from Kafka, a Kinesis stream wouldn&#8217;t be suitable for storing the database history. The reason being, that the maximum retention period for Kinesis data streams is seven days, whereas the database history must be kept for the entire lifetime of the connector. Another alternative could be to use the existing filesystem based implementations <code>FileOffsetBackingStore</code> and <code>FileDatabaseHistory</code>, respectively.</p> </div> <div class="paragraph"> <p>The next step is to build an <code>EmbeddedEngine</code> instance from the configuration. Again this is done using a fluent API:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">EmbeddedEngine engine = EmbeddedEngine.create()&#x000A;    .using(config)&#x000A;    .using(this.getClass().getClassLoader())&#x000A;    .using(Clock.SYSTEM)&#x000A;    .notifying(this::sendRecord)&#x000A;    .build();</code></pre> </div> </div> <div class="paragraph"> <p>The most interesting part here is the <code>notifying</code> call. The method passed here is the one which will be invoked by the engine for each emitted data change record. So let&#8217;s take a look at the implementation of this method.</p> </div> </div> </div> <div class="sect1"> <h2 id="sending_change_records_to_kinesis"><a class="anchor" href="#sending_change_records_to_kinesis"></a>Sending Change Records to Kinesis</h2> <div class="sectionbody"> <div class="paragraph"> <p>The <code>sendRecord()</code> method is where the magic happens. We&#8217;ll convert the incoming <code>SourceRecord</code> into an equivalent JSON representation and propagate it to a Kinesis stream.</p> </div> <div class="paragraph"> <p>For that, it&#8217;s important to understand some conceptual differences between Apache Kafka and Kinesis. Specifically, messages in Kafka have a <em>key</em> and a <em>value</em> (which both are arbitrary byte arrays). In case of Debezium, the key of data change events represents the primary key of the affected record and the value is a structure comprising of old and new row state as well as some additional metadata.</p> </div> <div class="paragraph"> <p>In Kinesis on the other hand a message contains a <em>data blob</em> (again an arbitrary byte sequence) and a <em>partition key</em>. Kinesis streams can be split up into multiple shards and the partition key is used to determine into which shard a given message should go.</p> </div> <div class="paragraph"> <p>Now one could think of mapping the key from Debezium&#8217;s change data events to the Kinesis partition key, but partition keys are limited to a length of 256 bytes. Depending on the length of primary key column(s) in the captured tables, this might not be enough. So a safer option is to create a hash value from the change message key and use that as the partition key. This in turn means that the change message key structure should be added next to the actual value to the Kinesis message&#8217;s data blob. While the key column values themselves are part of the value structure, too, a consumer otherwise wouldn&#8217;t know which column(s) make up the primary key.</p> </div> <div class="paragraph"> <p>With that in mind, let&#8217;s take a look at the <code>sendRecord()</code> implementation:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">private void sendRecord(SourceRecord record) {&#x000A;    // We are interested only in data events not schema change events&#x000A;    if (record.topic().equals("kinesis")) {&#x000A;        return;&#x000A;    }&#x000A;&#x000A;    // create schema for container with key *and* value&#x000A;    Schema schema = SchemaBuilder.struct()&#x000A;        .field("key", record.keySchema())&#x000A;        .field("value", record.valueSchema())&#x000A;        .build();&#x000A;&#x000A;    Struct message = new Struct(schema);&#x000A;    message.put("key", record.key());&#x000A;    message.put("value", record.value());&#x000A;&#x000A;    // create partition key by hashing the record's key&#x000A;    String partitionKey = String.valueOf(&#x000A;        record.key() != null ? record.key().hashCode() : -1);&#x000A;&#x000A;    // create data blob representing the container by using Kafka Connect's&#x000A;    // JSON converter&#x000A;    final byte[] payload = valueConverter.fromConnectData(&#x000A;        "dummy", schema, message);&#x000A;&#x000A;    // Assemble the put-record request ...&#x000A;    PutRecordRequest putRecord = new PutRecordRequest();&#x000A;&#x000A;    putRecord.setStreamName(record.topic());&#x000A;    putRecord.setPartitionKey(partitionKey);&#x000A;    putRecord.setData(ByteBuffer.wrap(payload));&#x000A;&#x000A;    // ... and execute it&#x000A;    kinesisClient.putRecord(putRecord);&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>The code is quite straight-forward; as discussed above it&#8217;s first creating a container structure containing key <em>and</em> value of the incoming source record. This structure then is converted into a binary representation using the JSON converter provided by Kafka Connect (an instance of <code>JsonConverter</code>). Then a <code>PutRecordRequest</code> is assembled from that blob, the partition key and the change record&#8217;s topic name, which finally is sent to Kinesis.</p> </div> <div class="paragraph"> <p>The Kinesis client object can be re-used and is set up once like so:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">// Uses the credentials from the local "default" AWS profile&#x000A;AWSCredentialsProvider credentialsProvider =&#x000A;    new ProfileCredentialsProvider("default");&#x000A;&#x000A;this.kinesisClient = AmazonKinesisClientBuilder.standard()&#x000A;    .withCredentials(credentialsProvider)&#x000A;    .withRegion("eu-central-1") // use your AWS region here&#x000A;    .build();</code></pre> </div> </div> <div class="paragraph"> <p>With that, we&#8217;ve set up an instance of Debezium&#8217;s <code>EmbeddedEngine</code> which runs the configured MySQL connector and passes each emitted change event to Amazon Kinesis. The last missing step is to actually run the engine. This is done on a separate thread using an <code>Executor</code>, e.g. like so:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">ExecutorService executor = Executors.newSingleThreadExecutor();&#x000A;executor.execute(engine);</code></pre> </div> </div> <div class="paragraph"> <p>Note you also should make sure to properly shut down the engine eventually. How that can be done <a href="https://github.com/debezium/debezium-examples/blob/master/kinesis/src/main/java/io/debezium/examples/kinesis/ChangeDataSender.java#L83-L88">is shown</a> in the accompanying example in the <em>debezium-examples</em> repo.</p> </div> </div> </div> <div class="sect1"> <h2 id="running_the_example"><a class="anchor" href="#running_the_example"></a>Running the Example</h2> <div class="sectionbody"> <div class="paragraph"> <p>Finally let&#8217;s take a look at running the complete example and consuming the Debezium CDC events from the Kinesis stream. Start by cloning the examples repository and go to the <em>kinesis</em> directory:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">git clone https://github.com/debezium/debezium-examples.git&#x000A;cd debezium-examples/kinesis</code></pre> </div> </div> <div class="paragraph"> <p>Make sure you&#8217;ve met the <a href="https://github.com/debezium/debezium-examples/tree/master/kinesis#prerequisites">prerequisites</a> described in the example&#8217;s <em>README.md</em>; most notably you should have a local Docker installation and you&#8217;ll need to have set up an AWS account as well as have the AWS client tools installed. Note that Kinesis isn&#8217;t part of the free tier when registering with AWS, i.e. you&#8217;ll pay a (small) amount of money when executing the example. Don&#8217;t forget to delete the streams you&#8217;ve set up once done, we won&#8217;t pay your AWS bills :)</p> </div> <div class="paragraph"> <p>Now run Debezium&#8217;s MySQL example database to have some data to play with:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">docker run -it --rm --name mysql -p 3306:3306 \&#x000A;  -e MYSQL_ROOT_PASSWORD=debezium \&#x000A;  -e MYSQL_USER=mysqluser \&#x000A;  -e MYSQL_PASSWORD=mysqlpw \&#x000A;  debezium/example-mysql:0.8</code></pre> </div> </div> <div class="paragraph"> <p>Create a Kinesis stream for change events from the <code>customers</code> table:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">aws kinesis create-stream --stream-name kinesis.inventory.customers \&#x000A;  --shard-count 1</code></pre> </div> </div> <div class="paragraph"> <p>Execute the Java application that runs the Debezium embedded engine (if needed, adjust the value of the <code>kinesis.region</code> property in <em>pom.xml</em> to your own region first):</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">mvn exec:java</code></pre> </div> </div> <div class="paragraph"> <p>This will start up the engine and the MySQL connector, which takes an initial snapshot of the captured database.</p> </div> <div class="paragraph"> <p>In order to take a look at the CDC events in the Kinesis stream, the AWS CLI can be used (usually, you&#8217;d implement a Kinesis Streams application for consuming the events). To do so, set up a <a href="https://docs.aws.amazon.com/streams/latest/dev/developing-consumers-with-sdk.html#kinesis-using-sdk-java-get-data-shard-iterators">shard iterator</a> first:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">ITERATOR=$(aws kinesis get-shard-iterator --stream-name kinesis.inventory.customers --shard-id 0 --shard-iterator-type TRIM_HORIZON | jq '.ShardIterator')</code></pre> </div> </div> <div class="paragraph"> <p>Note how the <a href="https://stedolan.github.io/jq/">jq</a> utility is used to obtain the generated id of the iterator from the JSON structure returned by the Kinesis API. Next that iterator can be used to examine the stream:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">aws kinesis get-records --shard-iterator $ITERATOR</code></pre> </div> </div> <div class="paragraph"> <p>You should receive an array of records like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">{&#x000A;    "Records": [&#x000A;        {&#x000A;            "SequenceNumber":&#x000A;                "49587760482547027816046765529422807492446419903410339842",&#x000A;            "ApproximateArrivalTimestamp": 1535551896.475,&#x000A;            "Data": "eyJiZWZvcm...4OTI3MzN9",&#x000A;            "PartitionKey": "eyJpZCI6MTAwMX0="&#x000A;        },&#x000A;        ...&#x000A;    ]&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>The <code>Data</code> element is a Base64-encoded representation of the message&#8217;s data blob. Again <em>jq</em> comes in handy: we can use it to just extract the <code>Data</code> part of each record and decode the Base64 representation (make sure to use jq 1.6 or newer):</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">aws kinesis get-records --shard-iterator $ITERATOR | \&#x000A;  jq -r '.Records[].Data | @base64d' | jq .</code></pre> </div> </div> <div class="paragraph"> <p>Now you should see the change events as JSON, each one with key and value:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">{&#x000A;  "key": {&#x000A;    "id": 1001&#x000A;  },&#x000A;  "value": {&#x000A;    "before": null,&#x000A;    "after": {&#x000A;      "id": 1001,&#x000A;      "first_name": "Sally",&#x000A;      "last_name": "Thomas",&#x000A;      "email": "sally.thomas@acme.com"&#x000A;    },&#x000A;    "source": {&#x000A;      "version": "0.8.1.Final",&#x000A;      "name": "kinesis",&#x000A;      "server_id": 0,&#x000A;      "ts_sec": 0,&#x000A;      "gtid": null,&#x000A;      "file": "mysql-bin.000003",&#x000A;      "pos": 154,&#x000A;      "row": 0,&#x000A;      "snapshot": true,&#x000A;      "thread": null,&#x000A;      "db": "inventory",&#x000A;      "table": "customers",&#x000A;      "query": null&#x000A;    },&#x000A;    "op": "c",&#x000A;    "ts_ms": 1535555325628&#x000A;  }&#x000A;}&#x000A;...</code></pre> </div> </div> <div class="paragraph"> <p>Next let&#8217;s try and update a record in MySQL:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell"># Start MySQL CLI client&#x000A;docker run -it --rm --name mysqlterm --link mysql --rm mysql:5.7 \&#x000A;  sh -c 'exec mysql -h"$MYSQL_PORT_3306_TCP_ADDR" \&#x000A;  -P"$MYSQL_PORT_3306_TCP_PORT" -uroot -p"$MYSQL_ENV_MYSQL_ROOT_PASSWORD"'&#x000A;&#x000A;# In the MySQL client&#x000A;use inventory;&#x000A;update customers set first_name = 'Trudy' where id = 1001;</code></pre> </div> </div> <div class="paragraph"> <p>If you now fetch the iterator again, you should see one more data change event representing that update:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">...&#x000A;&#x000A;{&#x000A;  "key": {&#x000A;    "id": 1001&#x000A;  },&#x000A;  "value": {&#x000A;    "before": {&#x000A;      "id": 1001,&#x000A;      "first_name": "Sally",&#x000A;      "last_name": "Thomas",&#x000A;      "email": "sally.thomas@acme.com"&#x000A;    },&#x000A;    "after": {&#x000A;      "id": 1001,&#x000A;      "first_name": "Trudy",&#x000A;      "last_name": "Thomas",&#x000A;      "email": "sally.thomas@acme.com"&#x000A;    },&#x000A;    "source": {&#x000A;      "version": "0.8.1.Final",&#x000A;      "name": "kinesis",&#x000A;      "server_id": 223344,&#x000A;      "ts_sec": 1535627629,&#x000A;      "gtid": null,&#x000A;      "file": "mysql-bin.000003",&#x000A;      "pos": 364,&#x000A;      "row": 0,&#x000A;      "snapshot": false,&#x000A;      "thread": 10,&#x000A;      "db": "inventory",&#x000A;      "table": "customers",&#x000A;      "query": null&#x000A;    },&#x000A;    "op": "u",&#x000A;    "ts_ms": 1535627622546&#x000A;  }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>Once you&#8217;re done, stop the embedded engine application by hitting Ctrl + C, stop the MySQL server by running <code>docker stop mysql</code> and delete the <em>kinesis.inventory.customers</em> stream in Kinesis.</p> </div> </div> </div> <div class="sect1"> <h2 id="summary_and_outlook"><a class="anchor" href="#summary_and_outlook"></a>Summary and Outlook</h2> <div class="sectionbody"> <div class="paragraph"> <p>In this blog post we&#8217;ve demonstrated that Debezium cannot only be used to stream data changes into Apache Kafka, but also into other streaming platforms such as Amazon Kinesis. Leveraging its embedded engine and by implementing a bit of glue code, you can benefit from <a href="/docs/connectors/">all the CDC connectors</a> provided by Debezium and their capabilities and connect them to the streaming solution of your choice.</p> </div> <div class="paragraph"> <p>And we&#8217;re thinking about even further simplifying this usage of Debezium. Instead of requiring you to implement your own application that invokes the embedded engine API, we&#8217;re considering to provide a small self-contained Debezium runtime which you can simply execute. It&#8217;d be configured with the source connector to run and make use of an outbound plug-in SPI with ready-to-use implementations for Kinesis, Apache Pulsar and others. Of course such runtime would also provide suitable implementations for safely persisting offsets and database history, and it&#8217;d offer means of monitoring, health checks etc. Meaning you could connect the Debezium source connectors with your preferred streaming platform in a robust and reliable way, without any manual coding required!</p> </div> <div class="paragraph"> <p>If you like this idea, then please check out JIRA issue <a href="https://issues.jboss.org/browse/DBZ-651">DBZ-651</a> and let us know about your thoughts, e.g. by leaving a comment on the issue, in the comment section below or on our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="about_debezium"><a class="anchor" href="#about_debezium"></a>About Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of <a href="http://kafka.apache.org/">Kafka</a> and provides <a href="http://kafka.apache.org/documentation.html#connect">Kafka Connect</a> compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is <a href="/license">open source</a> under the <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, Version 2.0</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="get_involved"><a class="anchor" href="#get_involved"></a>Get involved</h2> <div class="sectionbody"> <div class="paragraph"> <p>We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter <a href="https://twitter.com/debezium">@debezium</a>, <a href="https://gitter.im/debezium/user">chat with us on Gitter</a>, or join our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> to talk with the community. All of the code is open source <a href="https://github.com/debezium/">on GitHub</a>, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or <a href="https://issues.jboss.org/projects/DBZ/issues/">log an issue</a>.</p> </div> </div> </div> </div> <hr> <div class="post"> <h1 class="title"> <a href="/blog/2018/08/30/debezium-0-8-2-released/">Debezium 0.8.2 Released</a> </h1> <div class="byline"> <p> <em> August 30, 2018 by Gunnar Morling </em> <em> under&nbsp; </em> <a class="label label-info" href="/blog/tags/releases/">releases</a> <a class="label label-info" href="/blog/tags/mysql/">mysql</a> <a class="label label-info" href="/blog/tags/mongodb/">mongodb</a> <a class="label label-info" href="/blog/tags/postgres/">postgres</a> <a class="label label-info" href="/blog/tags/docker/">docker</a> </p> </div> <div id="preamble"> <div class="sectionbody"> <div class="paragraph"> <p>The Debezium team is back from summer holidays and we&#8217;re happy to announce the release of Debezium <strong>0.8.2</strong>!</p> </div> <div class="paragraph"> <p>This is a bugfix release to the current stable release line of Debezium, 0.8.x, while the work on Debezium 0.9 is continuing.</p> </div> <div class="paragraph"> <p><em>Note:</em> By accident the version of the release artifacts is <em>0.8.2</em> instead of <em>0.8.2.Final</em>. This is not in line with our recently established convention of always letting release versions end with qualifiers such as <em>Alpha1</em>, <em>Beta1</em>, <em>CR1</em> or <em>Final</em>. The next version in the 0.8 line will be <em>0.8.3.Final</em> and we&#8217;ll improve our release pipeline to make sure that this situation doesn&#8217;t occur again.</p> </div> <div class="paragraph"> <p>The 0.8.2 release contains <a href="/docs/releases/#release-0-8-2">10 fixes</a> overall, most of them dealing with issues related to DDL parsing as done by the Debezium <a href="/docs/connectors/mysql/">MySQL connector</a>. For instance, implicit non-nullable primary key columns will be handled correctly now using the new Antlr-based DDL parser (<a href="https://issues.jboss.org/browse/DBZ-860">DBZ-860</a>). Also the <a href="/docs/connectors/mongodb/">MongoDB connector</a> saw a bug fix (<a href="https://issues.jboss.org/browse/DBZ-838">DBZ-838</a>): initial snapshots will be interrupted now if the connector is requested to stop (e.g. when shutting down Kafka Connect). More a useful improvement rather than a bug fix is the <a href="/docs/connectors/postgres/">Postgres connector&#8217;s</a> capability to add the table, schema and database names to the <code>source</code> block of emitted CDC events (<a href="https://issues.jboss.org/browse/DBZ-866">DBZ-866</a>).</p> </div> <div class="paragraph"> <p>Thanks a lot to community members <a href="https://github.com/jchipmunk">Andrey Pustovetov</a>, <a href="https://github.com/CliffWheadon">Cliff Wheadon</a> and <a href="https://github.com/oripwk">Ori Popowski</a> for their contributions to this release!</p> </div> </div> </div> <div class="sect1"> <h2 id="what_s_next"><a class="anchor" href="#what_s_next"></a>What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>We&#8217;re continuing the work on Debezium 0.9, which will mostly be about improvements to the SQL Server and Oracle connectors. Both will get support for handling structural changes to captured tables while the connectors are running. Also the exploration of alternatives to using the XStream API for the Oracle connector continues.</p> </div> <div class="paragraph"> <p>Finally, a recurring theme of our work is to further consolidate the code bases of the different connectors, which will allow us to roll out new and improved features more quickly across all the Debezium connectors. The recently added Oracle and SQL Server connectors already share a lot of code, and in the next step we&#8217;ve planned to move the existing Postgres connector to the new basis established for these two connectors.</p> </div> <div class="paragraph"> <p>If you&#8217;d like to learn more about some middle and long term ideas, please check out our <a href="/docs/roadmap/">roadmap</a>. Also please get in touch with us if you got any ideas or suggestions for future development.</p> </div> </div> </div> <div class="sect1"> <h2 id="about_debezium"><a class="anchor" href="#about_debezium"></a>About Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of <a href="http://kafka.apache.org/">Kafka</a> and provides <a href="http://kafka.apache.org/documentation.html#connect">Kafka Connect</a> compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is <a href="/license">open source</a> under the <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, Version 2.0</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="get_involved"><a class="anchor" href="#get_involved"></a>Get involved</h2> <div class="sectionbody"> <div class="paragraph"> <p>We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter <a href="https://twitter.com/debezium">@debezium</a>, <a href="https://gitter.im/debezium/user">chat with us on Gitter</a>, or join our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> to talk with the community. All of the code is open source <a href="https://github.com/debezium/">on GitHub</a>, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or <a href="https://issues.jboss.org/projects/DBZ/issues/">log an issue</a>.</p> </div> </div> </div> </div> <hr> <ul class="pager"> <li class="previous"> <a href="/blog/page/2/">&laquo; Older</a> </li> <li class="pages">Page 1 of 11</li> <li class="disabled next"> <a href="#">Newer &raquo;</a> </li> </ul> </div> </div> </div> </div> <footer class="container"> <div class="row"> <div class="col-md-5 col-md-offset-1"> <h4>Debezium</h4> <p> &#169; 2018 Debezium Community <br> <br> <i class="icon-fire"></i> Mixed with <a href="http://twitter.github.com/bootstrap">Bootstrap</a>, baked by <a href="http://awestruct.org">Awestruct</a>. <br> <i class="icon-flag"></i> Website and docs licensed under <a href="http://creativecommons.org/licenses/by/3.0/">CC BY 3.0</a>. <br> <i class="icon-flag-alt"></i> Code released under <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, v2.0</a>. <br> <i class="icon-file-alt"></i> <a href="https://www.redhat.com/legal/legal_statement.html" title="Terms">Terms</a> | <a href="https://www.redhat.com/legal/privacy_statement.html" title="Privacy Policy">Privacy</a> </p> </div> <div class="col-md-3"> <h4>Documentation</h4> <ul class="list-unstyled"> <li> <a href="/docs/features" title="Features">Features</a> </li> <li> <a href="/docs/install" title="Install">Install</a> </li> <li> <a href="/docs/manage" title="Manage">Manage</a> </li> <li> <a href="/docs/architecture" title="Architecture">Architecture</a> </li> <li> <a href="/docs/faq" title="FAQ">FAQ</a> </li> <li> <a href="/docs/contribute" title="Contribute">Contribute</a> </li> </ul> </div> <div class="col-md-3"> <h4>Connect</h4> <ul class="list-unstyled"> <li> <a href="/blog" title="Blog">Blog</a> </li> <li> <a href="http://twitter.com/debezium" title="Twitter">Twitter</a> </li> <li> <a href="http://github.com/debezium" title="GitHub">GitHub</a> </li> <li> <a href="https://gitter.im/debezium/dev" title="Chat">Chat</a> </li> <li> <a href="https://groups.google.com/forum/#!forum/debezium" title="Google Groups">Google Groups</a> </li> <li> <a href="http://stackoverflow.com/questions/tagged/debezium" title="StackOverflow">StackOverflow</a> </li> </ul> </div> </div> </footer> <div class="container" id="companyfooter"> <div class="redhatlogo"> <div id="logospacer"></div> <a href="https://www.redhat.com/"><img src="https://static.jboss.org/theme/images/common/redhat_logo.png"></a> </div> </div> <span class="backToTop"> <a href="#top">back to top</a> </span> <script src="https://static.jboss.org/theme/js/libs/bootstrap-community/3.2.0.2/bootstrap-community.min.js"></script> <script type='text/javascript' language='JavaScript' src='https://www.redhat.com/j/elqNow/elqCfg.js'></script> <script type='text/javascript' language='JavaScript' src='https://www.redhat.com/j/elqNow/elqImg.js'></script> <div id="oTags"> <script type="text/javascript" src="//www.redhat.com/j/s_code.js"></script> <script type="text/javascript"><!--
        var coreUrl = encodeURI(document.URL.split("?")[0]).replace(/-/g," ");
        var urlSplit = coreUrl.toLowerCase().split(/\//);
        var urlLast = urlSplit[urlSplit.length-1];
        var pageNameString = "";
        var siteName = "";
        var minorSectionIndex = 3
        if (urlLast == "") {
            urlSplit.splice(-1,1);
        }
        if (urlLast.search(/\./) >= 0) {
            if (urlLast == "index.html") {
                urlSplit.splice(-1,1);
            }
            else {
                urlSplit[urlSplit.length-1] = urlLast.split(".").splice(0,1);
            }
        }
        siteName = urlSplit[2].split(".")[1];
        s.prop14 = s.eVar27 = siteName || "";
        s.prop15 = s.eVar28 = urlSplit[minorSectionIndex] || "";
        s.prop16 = s.eVar29 = urlSplit[minorSectionIndex+1] || "";
        pageNameString = urlSplit.splice(3).join(" | ");
        s.pageName = "jboss | community | " + siteName + " | " + pageNameString;
        s.server = "jboss";
        s.channel = "jboss | community";
        s.prop4 = s.eVar23 = encodeURI(document.URL);
        s.prop21 = s.eVar18 = coreUrl;
        s.prop2 = s.eVar22 = "en";
        s.prop3 = s.eVar19 = "us";
        //--></script> <script type="text/javascript" src="//www.redhat.com/j/rh_omni_footer.js"></script> <script language="JavaScript" type="text/javascript"><!--
        if(navigator.appVersion.indexOf('MSIE')>=0)document.write(unescape('%3C')+'\!-'+'-')
        //--></script> <noscript><a href="http://www.omniture.com" title="Web Analytics"><img src="https://smtrcs.redhat.com/b/ss/redhatcom,redhatglobal/1/H.25.4--NS/0?[AQB]&cdp=3&[AQE]" height="1" width="1" border="0" alt=""/></a></noscript> </div> <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
      document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
      </script> <script type="text/javascript">
      try {
      var pageTracker = _gat._getTracker("UA-10656779-1");
      pageTracker._trackPageview();
      } catch(err) {}</script> <script>
       (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
       
      ga('create', 'UA-76464546-1', 'auto');
      ga('send', 'pageview');
      ga('set', 'anonymizeIp', true);
      ga('require', 'linkid', 'linkid.js');
      
      </script> </div> </body> </html>