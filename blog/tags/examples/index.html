<!DOCTYPE html> <html lang="en"> <head> <title>Debezium Blog</title> <meta charset="utf-8"> <meta content="width=device-width, initial-scale=1.0" name="viewport"> <meta content="" name="description"> <meta content="" name="author"> <link href="https://static.jboss.org/theme/css/bootstrap-community/3.2.0.2/bootstrap-community.min.css" media="screen" rel="stylesheet"> <!--[if lt IE 9]><script src="https://static.jboss.org/theme/js/libs/html5/pre3.6/html5.min.js"></script><![endif]--> <link href="https://static.jboss.org/example/images/favicon.ico" rel="shortcut icon"> <link href="https://static.jboss.org/example/images/apple-touch-icon-144x144-precomposed.png" rel="apple-touch-icon-precomposed" sizes="144x144"> <link href="https://static.jboss.org/example/images/apple-touch-icon-114x114-precomposed.png" rel="apple-touch-icon-precomposed" sizes="114x114"> <link href="https://static.jboss.org/example/images/apple-touch-icon-72x72-precomposed.png" rel="apple-touch-icon-precomposed" sizes="72x72"> <link href="https://static.jboss.org/example/images/apple-touch-icon-precomposed.png" rel="apple-touch-icon-precomposed"> <link href="/stylesheets/debezium.css" rel="stylesheet" type="text/css"> <style>
      @media (min-width: 980px) {
        .banner { background-image: url(https://static.jboss.org/example/images/debezium-banner-1180px.png); height: 110px;  }
      }
      @media (max-width: 979px) {
        .banner { background-image: url(https://static.jboss.org/example/images/debezium-logo.png); background-repeat:no-repeat; background-position: center bottom; height: 60px; }
      }
      @media (max-width: 650px) {
        .banner { width: 100%; margin: 0px auto; }
      }
      @media (max-width: 450px) {
        .banner { height: 90px; }
      }
    </style> <link href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/styles/default.min.css" rel="stylesheet"> <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/highlight.min.js"></script> <script>
      hljs.initHighlightingOnLoad();
    </script> <script src="https://static.jboss.org/theme/js/libs/jquery/jquery-1.9.1.min.js"></script> <style>
      /* adjusting the vertical spacing for when a stickynav is engaged */
      .breadcrumb-fixed > .active {
        color: #8c8f91;
      }
      .breadcrumb-fixed {
        margin: 70px 0 10px;
        padding: 8px 15px;
        margin-bottom: 20px;
        list-style: none;
        background-color: #f5f5f5;
        border-radius: 4px;
      }
      
      .breadcrumb-fixed > li {
        display: inline-block;
      }
    </style> </head> <body> <div id="rhbar"> <a class="jbdevlogo" href="https://www.jboss.org/projects/about"></a> <a class="rhlogo" href="https://www.redhat.com/"></a> </div> <div id=""> <ul class="visuallyhidden" id="top"> <li> <a accesskey="n" href="#nav" title="Skip to navigation">Skip to navigation</a> </li> <li> <a accesskey="c" href="#page" title="Skip to content">Skip to content</a> </li> </ul> <div class="container" id="content"> <div class="navbar navbar-inverse navbar-fix"> <div class="container-fluid"> <div class="navbar-header"> <button class="navbar-toggle collapsed" data-target="#navbar-1" data-toggle="collapse"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button> <a class="navbar-brand" href="/"> Debezium </a> </div> <div class="collapse navbar-collapse" id="navbar-1"> <ul class="nav navbar-nav pull-right"> <li class=""><a href="/docs/faq/">FAQ</a></li> <li class=""><a href="/docs/">DOCS</a></li> <li class=""><a href="/community/">COMMUNITY</a></li> <li class="active"><a href="/blog/">BLOG</a></li> </ul> </div> </div> </div> <div id="equalHeightsLayout"> <div class="row post-text-padding row-no-expand"> <div class="hidden-xs col-sm-3 no-right-padding" id="leftdocnav"> <div class="panel-docnav"> <div class="panel-heading"> <h3 class="panel-title"> Latest posts </h3> </div> <div class="panel-body"> <ul class="list-group"> <li class="list-group-item"> <a href="/blog/2019/05/29/debezium-0-10-0-alpha1-released/" rel="tooltip" title="Click to go to post">Debezium 0.10.0.Alpha1 "Spring Clean-Up" Edition Released</a> </li> <li class="list-group-item"> <a href="/blog/2019/05/23/tutorial-using-debezium-connectors-with-apache-pulsar/" rel="tooltip" title="Click to go to post">Tutorial for Using Debezium Connectors With Apache Pulsar</a> </li> <li class="list-group-item"> <a href="/blog/2019/05/06/debezium-0-9-5-final-released/" rel="tooltip" title="Click to go to post">Debezium 0.9.5.Final Released</a> </li> <li class="list-group-item"> <a href="/blog/2019/04/18/hello-debezium/" rel="tooltip" title="Click to go to post">Debezium&#8217;s Team Grows</a> </li> <li class="list-group-item"> <a href="/blog/2019/04/11/debezium-0-9-4-final-released/" rel="tooltip" title="Click to go to post">Debezium 0.9.4.Final Released</a> </li> <li class="list-group-item"> <a href="/blog/2019/03/26/debezium-0-9-3-final-released/" rel="tooltip" title="Click to go to post">Debezium 0.9.3.Final Released</a> </li> <li class="list-group-item"> <a href="/blog/2019/03/14/debezium-meets-quarkus/" rel="tooltip" title="Click to go to post">Debezium meets Quarkus</a> </li> <li class="list-group-item"> <a href="/blog/2019/02/25/debezium-0-9-2-final-released/" rel="tooltip" title="Click to go to post">Debezium 0.9.2.Final Released</a> </li> </ul> </div> </div> </div> <div class="col-xs-12 col-sm-9" id="maincol"> <div class="text-right"> <h3> Subscribe <a class="rss" href="/blog.atom"> <i class="icon-rss"></i> </a> </h3> </div> <hr> <div class="post"> <h1 class="title"> <a href="/blog/2019/05/23/tutorial-using-debezium-connectors-with-apache-pulsar/">Tutorial for Using Debezium Connectors With Apache Pulsar</a> </h1> <div class="byline"> <p> <em> May 23, 2019 by Jia Zhai, StreamNative </em> <em> under&nbsp; </em> <a class="label label-info" href="/blog/tags/discussion/">discussion</a> <a class="label label-info" href="/blog/tags/examples/">examples</a> </p> </div> <div id="preamble"> <div class="sectionbody"> <div class="paragraph"> <p><strong><em>This is a guest post by Apache Pulsar PMC Member and Committer Jia Zhai.</em></strong></p> </div> <div class="paragraph"> <p><a href="https://debezium.io">Debezium</a> is an open source project for change data capture (CDC). It is built on <a href="https://kafka.apache.org/documentation/#connectapi">Apache Kafka Connect</a> and supports multiple databases, such as MySQL, MongoDB, PostgreSQL, Oracle, and SQL Server. <a href="http://pulsar.apache.org">Apache Pulsar</a> includes a set of <a href="https://pulsar.apache.org/docs/en/io-connectors">built-in connectors</a> based on Pulsar IO framework, which is counter part to Apache Kafka Connect.</p> </div> <div class="paragraph"> <p>As of version 2.3.0, Pulsar IO comes with support for the <a href="http://pulsar.apache.org/docs/en/2.3.0/io-cdc-debezium">Debezium source connectors</a> out of the box, so you can leverage Debezium to stream changes from your databases into Apache Pulsar. This tutorial walks you through setting up the Debezium connector for MySQL with Pulsar IO.</p> </div> </div> </div> <div class="sect1"> <h2 id="tutorial_steps"><a class="anchor" href="#tutorial_steps"></a>Tutorial Steps</h2> <div class="sectionbody"> <div class="paragraph"> <p>This tutorial is similar to the <a href="https://debezium.io/docs/tutorial">Debezium tutorial</a>, except that storage of event streams is changed from Kafka to Pulsar. It mainly includes six steps:</p> </div> <div class="olist arabic"> <ol class="arabic"> <li> <p>Start a MySQL server;</p> </li> <li> <p>Start standalone Pulsar service;</p> </li> <li> <p>Start the Debezium connector in Pulsar IO. Pulsar IO reads database changes existing in MySQL server;</p> </li> <li> <p>Subscribe Pulsar topics to monitor MySQL changes;</p> </li> <li> <p>Make changes in MySQL server, and verify that changes are recorded in Pulsar topics immediately;</p> </li> <li> <p>Clean up.</p> </li> </ol> </div> <div class="sect2"> <h3 id="step_1_start_a_mysql_server"><a class="anchor" href="#step_1_start_a_mysql_server"></a>Step 1: Start a MySQL server</h3> <div class="paragraph"> <p>Start a MySQL server that contains a database example, from which Debezium captures changes. Open a new terminal to start a new container that runs a MySQL database server pre-configured with a database named inventory:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">docker run --rm --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=debezium -e MYSQL_USER=mysqluser -e MYSQL_PASSWORD=mysqlpw debezium/example-mysql:0.9</code></pre> </div> </div> <div class="paragraph"> <p>The following information is displayed:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">2019-03-25T14:12:41.178325Z 0 [Note] Event Scheduler: Loaded 0 events&#x000A;2019-03-25T14:12:41.178670Z 0 [Note] mysqld: ready for connections.&#x000A;Version: '5.7.25-log'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server (GPL)</code></pre> </div> </div> </div> <div class="sect2"> <h3 id="step_2_start_standalone_pulsar_service"><a class="anchor" href="#step_2_start_standalone_pulsar_service"></a>Step 2: Start standalone Pulsar service</h3> <div class="paragraph"> <p>Start Pulsar service locally in standalone mode. Support for running Debezium connectors in Pulsar IO is introduced in Pulsar 2.3.0. Download <a href="https://archive.apache.org/dist/pulsar/pulsar-2.3.0/apache-pulsar-2.3.0-bin.tar.gz">Pulsar binary of 2.3.0 release</a> and <a href="https://archive.apache.org/dist/pulsar/pulsar-2.3.0/connectors/pulsar-io-kafka-connect-adaptor-2.3.0.nar">pulsar-io-kafka-connect-adaptor-2.3.0.nar of 2.3.0 release</a>. In Pulsar, all Pulsar IO connectors are packaged as separate <a href="https://medium.com/hashmapinc/nifi-nar-files-explained-14113f7796fd">NAR</a> files.</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">$ wget https://archive.apache.org/dist/pulsar/pulsar-2.3.0/apache-pulsar-2.3.0-bin.tar.gz&#x000A;$ wget https://archive.apache.org/dist/pulsar/pulsar-2.3.0/connectors/pulsar-io-kafka-connect-adaptor-2.3.0.nar&#x000A;$ tar zxf apache-pulsar-2.3.0-bin.tar.gz&#x000A;$ cd apache-pulsar-2.3.0&#x000A;$ mkdir connectors&#x000A;$ cp ../pulsar-io-kafka-connect-adaptor-2.3.0.nar connectors&#x000A;$ bin/pulsar standalone</code></pre> </div> </div> <div class="imageblock centered-image"> <img src="/images/pulsar_tutorial/pulsar-mysql-1.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="start pulsar standalone]"> </div> </div> <div class="sect2"> <h3 id="step_3_start_the_debezium_mysql_connector_in_pulsar_io"><a class="anchor" href="#step_3_start_the_debezium_mysql_connector_in_pulsar_io"></a>Step 3: Start the Debezium MySQL connector in Pulsar IO</h3> <div class="paragraph"> <p>Start the Debezium MySQL connector in Pulsar IO, with local run mode, in another terminal tab. The “debezium-mysql-source-config.yaml” file contains all the configuration, and main parameters are listed under the “configs” node. The .yaml file contains the "task.class" parameter. The configuration file also includes MySQL related parameters (like server, port, user, password) and two names of Pulsar topics for "history" and "offset" storage.</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">$ bin/pulsar-admin source localrun  --sourceConfigFile debezium-mysql-source-config.yaml</code></pre> </div> </div> <div class="paragraph"> <p>The content in the “debezium-mysql-source-config.yaml” file is as follows.</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">tenant: "test"&#x000A;namespace: "test-namespace"&#x000A;name: "debezium-kafka-source"&#x000A;topicName: "kafka-connect-topic"&#x000A;archive: "connectors/pulsar-io-kafka-connect-adaptor-2.3.0.nar"&#x000A;&#x000A;parallelism: 1&#x000A;&#x000A;configs:&#x000A;  ## sourceTask&#x000A;  task.class: "io.debezium.connector.mysql.MySqlConnectorTask"&#x000A;&#x000A;  ## config for mysql, docker image: debezium/example-mysql:0.8&#x000A;  database.hostname: "localhost"&#x000A;  database.port: "3306"&#x000A;  database.user: "debezium"&#x000A;  database.password: "dbz"&#x000A;  database.server.id: "184054"&#x000A;  database.server.name: "dbserver1"&#x000A;  database.whitelist: "inventory"&#x000A;&#x000A;  database.history: "org.apache.pulsar.io.debezium.PulsarDatabaseHistory"&#x000A;  database.history.pulsar.topic: "history-topic"&#x000A;  database.history.pulsar.service.url: "pulsar://127.0.0.1:6650"&#x000A;  ## KEY_CONVERTER_CLASS_CONFIG, VALUE_CONVERTER_CLASS_CONFIG&#x000A;  key.converter: "org.apache.kafka.connect.json.JsonConverter"&#x000A;  value.converter: "org.apache.kafka.connect.json.JsonConverter"&#x000A;  ## PULSAR_SERVICE_URL_CONFIG&#x000A;  pulsar.service.url: "pulsar://127.0.0.1:6650"&#x000A;  ## OFFSET_STORAGE_TOPIC_CONFIG&#x000A;  offset.storage.topic: "offset-topic"</code></pre> </div> </div> <div class="paragraph"> <p>Tables are created automatically in the aforementioned MySQL server. So Debezium connector reads history records from MySQL binlog file from the beginning. In the output you will find the connector has already been triggered and processed in 47 records.</p> </div> <div class="imageblock centered-image"> <img src="/images/pulsar_tutorial/pulsar-mysql-2.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="connector start process records"> </div> <div class="paragraph"> <p>For more information on how to manage connectors, see the <a href="http://pulsar.apache.org/docs/en/io-managing/">Pulsar IO documentation</a>.</p> </div> <div class="paragraph"> <p>Records that have been captured and read by Debezium are automatically published to Pulsar topics. When you start a new terminal, you will find the current topics in Pulsar with the following command:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">$ bin/pulsar-admin topics list public/default</code></pre> </div> </div> <div class="imageblock centered-image"> <img src="/images/pulsar_tutorial/pulsar-mysql-3.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="list Pulsar topics"> </div> <div class="paragraph"> <p>For each table, which has been changed, the change data is stored in a separate Pulsar topic. Except database table related topics, another two topics named “history-topic” and “offset-topic” are used to store history and offset related data.</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">persistent://public/default/history-topic&#x000A;persistent://public/default/offset-topic</code></pre> </div> </div> </div> <div class="sect2"> <h3 id="step_4_subscribe_pulsar_topics_to_monitor_mysql_changes"><a class="anchor" href="#step_4_subscribe_pulsar_topics_to_monitor_mysql_changes"></a>Step 4: Subscribe Pulsar topics to monitor MySQL changes</h3> <div class="paragraph"> <p>Take the <code>persistent://public/default/dbserver1.inventory.products</code> topic as an example. Use the CLI command to consume this topic and monitor changes while the “products” table changes.</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash"> $ bin/pulsar-client consume -s "sub-products" public/default/dbserver1.inventory.products -n 0</code></pre> </div> </div> <div class="paragraph"> <p>The output is as follows:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">…&#x000A;22:17:41.201 [pulsar-client-io-1-1] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [public/default/dbserver1.inventory.products][sub-products] Subscribing to topic on cnx [id: 0xfe0b4feb, L:/127.0.0.1:55585 - R:localhost/127.0.0.1:6650]&#x000A;22:17:41.223 [pulsar-client-io-1-1] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [public/default/dbserver1.inventory.products][sub-products] Subscribed to topic on localhost/127.0.0.1:6650 -- consumer: 0</code></pre> </div> </div> <div class="paragraph"> <p>You can also consume the offset topic to monitor the offset changes while the table changes are stored in the <code>persistent://public/default/dbserver1.inventory.products</code> Pulsar topic.</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">$ bin/pulsar-client consume -s "sub-offset" offset-topic -n 0</code></pre> </div> </div> </div> <div class="sect2"> <h3 id="step_5_make_changes_in_mysql_server_and_verify_that_changes_are_recorded_in_pulsar_topics_immediately"><a class="anchor" href="#step_5_make_changes_in_mysql_server_and_verify_that_changes_are_recorded_in_pulsar_topics_immediately"></a>Step 5: Make changes in MySQL server, and verify that changes are recorded in Pulsar topics immediately</h3> <div class="paragraph"> <p>Start a MySQL CLI docker connector, and you can make changes to the “products” table in MySQL server.</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">$docker run -it --rm --name mysqlterm --link mysql --rm mysql:5.7 sh -c 'exec mysql -h"$MYSQL_PORT_3306_TCP_ADDR" -P"$MYSQL_PORT_3306_TCP_PORT" -uroot -p"$MYSQL_ENV_MYSQL_ROOT_PASSWORD"'</code></pre> </div> </div> <div class="paragraph"> <p>After running the command, MySQL CLI is displayed, and you can change the names of the two items in the “products” table.</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">mysql&gt; use inventory;&#x000A;mysql&gt; show tables;&#x000A;mysql&gt; SELECT * FROM  products ;&#x000A;mysql&gt; UPDATE products SET name='1111111111' WHERE id=101;&#x000A;mysql&gt; UPDATE products SET name='1111111111' WHERE id=107;</code></pre> </div> </div> <div class="imageblock centered-image"> <img src="/images/pulsar_tutorial/pulsar-mysql-4.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="mysql updates"> </div> <div class="paragraph"> <p>In the terminal where you consume products topic, you find that two changes have been added.</p> </div> <div class="imageblock centered-image"> <img src="/images/pulsar_tutorial/pulsar-mysql-5.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="table topic stores mysql updates"> </div> <div class="paragraph"> <p>In the terminal where you consume the offset topic, you find that two offsets have been added.</p> </div> <div class="imageblock centered-image"> <img src="/images/pulsar_tutorial/pulsar-mysql-6.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="offset topic get updated"> </div> <div class="paragraph"> <p>In the terminal where you local-run the connector, you find two more records have been processed.</p> </div> <div class="imageblock centered-image"> <img src="/images/pulsar_tutorial/pulsar-mysql-7.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="table topic get more records"> </div> </div> <div class="sect2"> <h3 id="step_6_clean_up"><a class="anchor" href="#step_6_clean_up"></a>Step 6: Clean up.</h3> <div class="paragraph"> <p>Use “Ctrl + C” to close terminals. Use “docker ps” and “docker kill” to stop MySQL related containers.</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">mysql&gt; quit&#x000A;&#x000A;$ docker ps&#x000A;CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                               NAMES&#x000A;84d66c2f591d        debezium/example-mysql:0.8   "docker-entrypoint.s…"   About an hour ago   Up About an hour    0.0.0.0:3306-&gt;3306/tcp, 33060/tcp   mysql&#x000A;&#x000A;$ docker kill 84d66c2f591d</code></pre> </div> </div> <div class="paragraph"> <p>To delete Pulsar data, delete data directory in the Pulsar binary directory.</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">$ pwd&#x000A;/Users/jia/ws/releases/apache-pulsar-2.3.0&#x000A;&#x000A;$ rm -rf data</code></pre> </div> </div> </div> </div> </div> <div class="sect1"> <h2 id="conclusion"><a class="anchor" href="#conclusion"></a>Conclusion</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Pulsar IO framework allows to run the Debezium connectors for change data capture, streaming data changes from different databases into Apache Pulsar. In this tutorial you&#8217;ve learned how to capture data changes in a MySQL database and propagate them to Pulsar. We are improving support for running the Debezium connectors with Apache Pulsar continuously, it will be much easier to use after Pulsar 2.4.0 release.</p> </div> </div> </div> </div> <hr> <div class="post"> <h1 class="title"> <a href="/blog/2019/03/14/debezium-meets-quarkus/">Debezium meets Quarkus</a> </h1> <div class="byline"> <p> <em> March 14, 2019 by Jiri Pechanec </em> <em> under&nbsp; </em> <a class="label label-info" href="/blog/tags/quarkus/">quarkus</a> <a class="label label-info" href="/blog/tags/examples/">examples</a> <a class="label label-info" href="/blog/tags/microservices/">microservices</a> <a class="label label-info" href="/blog/tags/apache-kafka/">apache-kafka</a> </p> </div> <div id="preamble"> <div class="sectionbody"> <div class="openblock teaser"> <div class="content"> <div class="paragraph"> <p>Last week&#8217;s announcement of <a href="https://quarkus.io/">Quarkus</a> sparked a great amount of interest in the Java community: crafted from the best of breed Java libraries and standards, it allows to build Kubernetes-native applications based on GraalVM &amp; OpenJDK HotSpot. In this blog post we are going to demonstrate how a Quarkus-based microservice can consume Debezium&#8217;s data change events via Apache Kafka. For that purpose, we&#8217;ll see what it takes to convert the shipment microservice from our recent post about the <a href="2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern">outbox pattern</a> into Quarkus-based service.</p> </div> </div> </div> <div class="paragraph"> <p>Quarkus is a Java stack designed for the development of cloud-native applications based on the Java platform. It combines and tightly integrates mature libraries such Hibernate ORM, Vert.x, Netty, RESTEasy and Apache Camel as well as the APIs from the <a href="https://microprofile.io/">Eclipse MicroProfile</a> initiative, such as <a href="https://github.com/eclipse/microprofile-config">Config</a> or <a href="https://github.com/eclipse/microprofile-reactive-messaging">Reactive Messaging</a>. Using Quarkus, you can develop applications using both imperative and reactive styles, also combining both approaches as needed.</p> </div> <div class="paragraph"> <p>It is designed for significantly reduced memory consumption and improved startup time. Last but not least, Quarkus supports both OpenJDK HotSpot and GraalVM virtual machines. With GraalVM it is possible to compile the application into a native binary and thus reduce the resource consumption and startup time even more.</p> </div> <div class="paragraph"> <p>To learn more about Quarkus itself, we recommend to take a look at its excellent <a href="https://quarkus.io/get-started/">Getting Started</a> guide.</p> </div> </div> </div> <div class="sect1"> <h2 id="consuming_kafka_messages_with_quarkus"><a class="anchor" href="#consuming_kafka_messages_with_quarkus"></a>Consuming Kafka Messages with Quarkus</h2> <div class="sectionbody"> <div class="paragraph"> <p>In the original <a href="https://github.com/debezium/debezium-examples/tree/master/outbox">example application</a> demonstrating the outbox pattern, there was a microservice ("shipment") based on Thorntail that consumed the events produced by the Debezium connector. We&#8217;ve extended the example with a new service named "shipment-service-quarkus". It provides the same functionality as the "shipment-service" but is implemented as a microservice based on Quarkus instead of Thorntail.</p> </div> <div class="paragraph"> <p>This makes the overall architecture look like so:</p> </div> <div class="imageblock centered-image"> <img src="/images/outbox_pattern_quarkus.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Outbox Pattern Overview"> </div> <div class="paragraph"> <p>To retrofit the original service into a Quarkus-based application, only a few changes were needed:</p> </div> <div class="ulist"> <ul> <li> <p>Quarkus right now supports only MariaDB but not MySQL; hence we have included an instance of MariaDB to which the service is writing</p> </li> <li> <p>The <a href="https://javaee.github.io/jsonp/">JSON-P API</a> used do deserialize incoming JSON messages can currently not be used without RESTEasy (see <a href="https://github.com/quarkusio/quarkus/issues/1480">issue #1480</a>, which should be fixed soon); so the code has been modified to use the Jackson API instead</p> </li> <li> <p>Instead of the Kafka consumer API, the <a href="https://github.com/eclipse/microprofile-reactive-messaging">Reactive Messaging API</a> defined by MicroProfile is used to receive messages from Apache Kafka; as an implementation of that API, the one provided by the <a href="https://github.com/smallrye/smallrye-reactive-messaging">SmallRye project</a> is used, which is bundled as a Quarkus extension</p> </li> </ul> </div> <div class="paragraph"> <p>While the first two steps are mere technicalities, the Reactive Messaging API is a nice simplification over the polling loop in the original consumer. All that&#8217;s needed to consume messages from a Kafka topic is to annotate a method with <code>@Incoming</code>, and it will automatically be invoked when a new message arrives:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@ApplicationScoped&#x000A;public class KafkaEventConsumer {&#x000A;&#x000A;    @Incoming("orders")&#x000A;    public CompletionStage&lt;Void&gt; onMessage(KafkaMessage&lt;String, String&gt; message)&#x000A;            throws IOException {&#x000A;        // handle message...&#x000A;&#x000A;        return message.ack();&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>The "orders" message source is configured via the MicroProfile Config API, which resolves it to the "OrderEvents" topic already known from the original outbox example.</p> </div> </div> </div> <div class="sect1"> <h2 id="build_process"><a class="anchor" href="#build_process"></a>Build Process</h2> <div class="sectionbody"> <div class="paragraph"> <p>The build process is mostly the same as it was before. Instead of using the Thorntail Maven plug-in, the Quarkus Maven plug-in is used now.</p> </div> <div class="paragraph"> <p>The following Quarkus extensions are used:</p> </div> <div class="ulist"> <ul> <li> <p><em>io.quarkus:quarkus-hibernate-orm</em>: support for Hibernate ORM and JPA</p> </li> <li> <p><em>io.quarkus:quarkus-jdbc-mariadb</em>: support for accessing MariaDB through JDBC</p> </li> <li> <p><em>io.quarkus:quarkus-smallrye-reactive-messaging-kafka</em>: support for accessing Kafka through the MicroProfile Reactive Messaging API</p> </li> </ul> </div> <div class="paragraph"> <p>They pull in some other extensions too, e.g. <em>quarkus-arc</em> (the Quarkus CDI runtime) and <em>quarkus-vertx</em> (used by the reactive messaging support).</p> </div> <div class="paragraph"> <p>In addition, two more changes were needed:</p> </div> <div class="ulist"> <ul> <li> <p>A new build profile named <code>native</code> has been added; this is used to compile the service into a native binary image using the Quarkus Maven plug-in</p> </li> <li> <p>the <code>native-image.docker-build</code> system property is enabled when running the build; this means that the native image build is done inside of a Docker container, so that GraalVM doesn&#8217;t have to be installed on the developer&#8217;s machine</p> </li> </ul> </div> <div class="paragraph"> <p>All the heavy-lifting is done by the Quarkus Maven plug-in which is configured in <em>pom.xml</em> like so:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-xml" data-lang="xml">  &lt;build&gt;&#x000A;    &lt;finalName&gt;shipment&lt;/finalName&gt;&#x000A;    &lt;plugins&gt;&#x000A;      ...&#x000A;      &lt;plugin&gt;&#x000A;        &lt;groupId&gt;io.quarkus&lt;/groupId&gt;&#x000A;        &lt;artifactId&gt;quarkus-maven-plugin&lt;/artifactId&gt;&#x000A;        &lt;version&gt;${version.quarkus}&lt;/version&gt;&#x000A;        &lt;executions&gt;&#x000A;          &lt;execution&gt;&#x000A;            &lt;goals&gt;&#x000A;              &lt;goal&gt;build&lt;/goal&gt;&#x000A;            &lt;/goals&gt;&#x000A;          &lt;/execution&gt;&#x000A;        &lt;/executions&gt;&#x000A;      &lt;/plugin&gt;&#x000A;    &lt;/plugins&gt;&#x000A;  &lt;/build&gt;&#x000A;  ...&#x000A;    &lt;profile&gt;&#x000A;      &lt;id&gt;native&lt;/id&gt;&#x000A;      &lt;build&gt;&#x000A;        &lt;plugins&gt;&#x000A;          &lt;plugin&gt;&#x000A;            &lt;groupId&gt;io.quarkus&lt;/groupId&gt;&#x000A;            &lt;artifactId&gt;quarkus-maven-plugin&lt;/artifactId&gt;&#x000A;            &lt;version&gt;${version.quarkus}&lt;/version&gt;&#x000A;            &lt;executions&gt;&#x000A;              &lt;execution&gt;&#x000A;                &lt;goals&gt;&#x000A;                  &lt;goal&gt;native-image&lt;/goal&gt;&#x000A;                &lt;/goals&gt;&#x000A;                &lt;configuration&gt;&#x000A;                  &lt;enableHttpUrlHandler&gt;true&lt;/enableHttpUrlHandler&gt;&#x000A;                  &lt;autoServiceLoaderRegistration&gt;false&lt;/autoServiceLoaderRegistration&gt;&#x000A;                &lt;/configuration&gt;&#x000A;              &lt;/execution&gt;&#x000A;            &lt;/executions&gt;&#x000A;          &lt;/plugin&gt;&#x000A;        &lt;/plugins&gt;&#x000A;      &lt;/build&gt;&#x000A;    &lt;/profile&gt;</code></pre> </div> </div> </div> </div> <div class="sect1"> <h2 id="configuration"><a class="anchor" href="#configuration"></a>Configuration</h2> <div class="sectionbody"> <div class="paragraph"> <p>As any Quarkus application, the shipment service is configured via the <em>application.properties</em> file:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-xml" data-lang="xml">quarkus.datasource.url: jdbc:mariadb://shipment-db-quarkus:3306/shipmentdb&#x000A;quarkus.datasource.driver: org.mariadb.jdbc.Driver&#x000A;quarkus.datasource.username: mariadbuser&#x000A;quarkus.datasource.password: mariadbpw&#x000A;quarkus.hibernate-orm.database.generation=drop-and-create&#x000A;quarkus.hibernate-orm.log.sql=true&#x000A;&#x000A;smallrye.messaging.source.orders.type=io.smallrye.reactive.messaging.kafka.Kafka&#x000A;smallrye.messaging.source.orders.topic=OrderEvents&#x000A;smallrye.messaging.source.orders.bootstrap.servers=kafka:9092&#x000A;smallrye.messaging.source.orders.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer&#x000A;smallrye.messaging.source.orders.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer&#x000A;smallrye.messaging.source.orders.group.id=shipment-service-quarkus</code></pre> </div> </div> <div class="paragraph"> <p>In our case it contains</p> </div> <div class="ulist"> <ul> <li> <p>the definition of a datasource (based on MariaDB) to which the shipment service writes its data,</p> </li> <li> <p>the definition of a messaging source, which is backed by the "OrderEvents" Kafka topic, using the given bootstrap server, deserializers and Kafka consumer group id.</p> </li> </ul> </div> </div> </div> <div class="sect1"> <h2 id="execution"><a class="anchor" href="#execution"></a>Execution</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Docker Compose config file has been enriched with two services, MariaDB and the new Quarkus-based shipment service. So when <code>docker-compose up</code> is executed, two shipment services are started side-by-side: the original Thorntail-based one and the new one using Quarkus. When the order services receives a new purchase order and exports a corresponding event to Apache Kafka via the outbox table, that message is processed by both shipment services, as they are using distinct consumer group ids.</p> </div> </div> </div> <div class="sect1"> <h2 id="performance_numbers"><a class="anchor" href="#performance_numbers"></a>Performance Numbers</h2> <div class="sectionbody"> <div class="paragraph"> <p>The numbers are definitely not scientific, but provide a good indication of the order-of-magnitude difference between the native Quarkus-based application and the Thorntail service running on the JVM:</p> </div> <table class="tableblock frame-all grid-all spread table table-bordered table-striped"> <colgroup> <col style="width: 30%;"> <col style="width: 35%;"> <col style="width: 35%;"> </colgroup> <thead> <tr> <th class="tableblock halign-left valign-top"></th> <th class="tableblock halign-left valign-top">Quarkus service</th> <th class="tableblock halign-left valign-top">Thorntail service</th> </tr> </thead> <tfoot> <tr> <td class="tableblock halign-left valign-top"><div><div class="paragraph"> <p>application package size [MB]</p> </div></div></td> <td class="tableblock halign-left valign-top"><div><div class="paragraph"> <p>54</p> </div></div></td> <td class="tableblock halign-left valign-top"><div><div class="paragraph"> <p>131</p> </div></div></td> </tr> </tfoot> <tbody> <tr> <td class="tableblock halign-left valign-top"><div><div class="paragraph"> <p>memory [MB]</p> </div></div></td> <td class="tableblock halign-left valign-top"><div><div class="paragraph"> <p>33.8</p> </div></div></td> <td class="tableblock halign-left valign-top"><div><div class="paragraph"> <p>1257</p> </div></div></td> </tr> <tr> <td class="tableblock halign-left valign-top"><div><div class="paragraph"> <p>start time [ms]</p> </div></div></td> <td class="tableblock halign-left valign-top"><div><div class="paragraph"> <p>260</p> </div></div></td> <td class="tableblock halign-left valign-top"><div><div class="paragraph"> <p>5746</p> </div></div></td> </tr> </tbody> </table> <div class="paragraph"> <p>The memory data were obtained via <code>htop</code> utility. The startup time was measured till the message about application readiness was printed. As with all performance measurements, you should run your own comparisons based on your set-up and workload to gain insight into the actual differences for your specific use cases.</p> </div> </div> </div> <div class="sect1"> <h2 id="summary"><a class="anchor" href="#summary"></a>Summary</h2> <div class="sectionbody"> <div class="paragraph"> <p>In this post we have successfully demonstrated that it is possible to consume Debezium-generated events in a Java application written with the Quarkus Java stack. We have also shown that it is possible to provide such application as a binary image and provided back-of-the-envelope performance numbers demonstrating significant savings in resources.</p> </div> <div class="paragraph"> <p>If you&#8217;d like to see the awesomeness of deploying Java microservices as native images by yourself, you can find the complete <a href="https://github.com/debezium/debezium-examples/tree/master/outbox/shipment-service-quarkus">source code</a> of the implementation in the Debezium examples repo. If you got any questions or feedback, please let us know in the comments below; looking forward to hearing from you!</p> </div> <div class="paragraph"> <p><em>Many thanks to Guillaume Smet for reviewing an earlier version of this post!</em></p> </div> </div> </div> <div class="sect1"> <h2 id="about_debezium"><a class="anchor" href="#about_debezium"></a>About Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of <a href="http://kafka.apache.org/">Kafka</a> and provides <a href="http://kafka.apache.org/documentation.html#connect">Kafka Connect</a> compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is <a href="/license/">open source</a> under the <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, Version 2.0</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="get_involved"><a class="anchor" href="#get_involved"></a>Get involved</h2> <div class="sectionbody"> <div class="paragraph"> <p>We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter <a href="https://twitter.com/debezium">@debezium</a>, <a href="https://gitter.im/debezium/user">chat with us on Gitter</a>, or join our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> to talk with the community. All of the code is open source <a href="https://github.com/debezium/">on GitHub</a>, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or <a href="https://issues.jboss.org/projects/DBZ/issues/">log an issue</a>.</p> </div> </div> </div> </div> <hr> <div class="post"> <h1 class="title"> <a href="/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/">Reliable Microservices Data Exchange With the Outbox Pattern</a> </h1> <div class="byline"> <p> <em> February 19, 2019 by Gunnar Morling </em> <em> under&nbsp; </em> <a class="label label-info" href="/blog/tags/discussion/">discussion</a> <a class="label label-info" href="/blog/tags/examples/">examples</a> <a class="label label-info" href="/blog/tags/microservices/">microservices</a> <a class="label label-info" href="/blog/tags/apache-kafka/">apache-kafka</a> </p> </div> <div id="preamble"> <div class="sectionbody"> <div class="openblock teaser"> <div class="content"> <div class="paragraph"> <p>As part of their business logic, microservices often do not only have to update their own local data store, but they also need to notify other services about data changes that happened. The outbox pattern describes an approach for letting services execute these two tasks in a safe and consistent manner; it provides source services with instant "read your own writes" semantics, while offering reliable, eventually consistent data exchange across service boundaries.</p> </div> </div> </div> <div class="paragraph"> <p>If you&#8217;ve built a couple of microservices, you&#8217;ll probably agree that the <a href="https://blog.christianposta.com/microservices/the-hardest-part-about-microservices-data/">hardest part about them is data</a>: microservices don&#8217;t exist in isolation and very often they need to propagate data and data changes amongst each other.</p> </div> <div class="paragraph"> <p>For instance consider a microservice that manages purchase orders: when a new order is placed, information about that order may have to be relayed to a shipment service (so it can assemble shipments of one or more orders) and a customer service (so it can update things like the customer&#8217;s total credit balance based on the new order).</p> </div> <div class="paragraph"> <p>There are different approaches for letting the order service know the other two about new purchase orders; e.g. it could invoke some <a href="https://en.wikipedia.org/wiki/Representational_state_transfer">REST</a>, <a href="https://grpc.io/">grpc</a> or other (synchronous) API provided by these services. This might create some undesired coupling, though: the sending service must know which other services to invoke and where to find them. It also must be prepared for these services temporarily not being available. Service meshes such as <a href="https://istio.io/">Istio</a> can come in helpful here, by providing capabilities like request routing, retries, circuit breakers and much more.</p> </div> <div class="paragraph"> <p>The general issue of any synchronous approach is that one service cannot really function without the other services which it invokes. While buffering and retrying might help in cases where other services only need to be <em>notified</em> of certain events, this is not the case if a service actually needs to <em>query</em> other services for information. For instance, when a purchase order is placed, the order service might need to obtain the information how many times the purchased item is on stock from an inventory service.</p> </div> <div class="paragraph"> <p>Another downside of such a synchronous approach is that it lacks re-playability, i.e. the possibility for new consumers to arrive after events have been sent and still be able to consume the entire event stream from the beginning.</p> </div> <div class="paragraph"> <p>Both problems can be addressed by using an asynchronous data exchange approach instead: i.e having the order, inventory and other services propagate events through a durable message log such as <a href="http://kafka.apache.org/">Apache Kafka</a>. By subscribing to these event streams, each service will be notified about the data change of other services. It can react to these events and, if needed, create a local representation of that data in its own data store, using a representation tailored towards its own needs. For instance, such view might be denormalized to efficiently support specific access patterns, or it may only contain a subset of the original data that&#8217;s relevant to the consuming service.</p> </div> <div class="paragraph"> <p>Durable logs also support re-playability, i.e. new consumers can be added as needed, enabling use cases you might not have had in mind originally, and without touching the source service. E.g. consider a data warehouse which should keep information about all the orders ever placed, or some full-text search functionality on purchase orders based on <a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a>. Once the purchase order events are in a Kafka topic (Kafka&#8217;s topic&#8217;s retention policy settings can be used to ensure that events remain in a topic as long as its needed for the given use cases and business requirements), new consumers can subscribe, process the topic from the very beginning and materialize a view of all the data in a microservice&#8217;s database, search index, data warehouse etc.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="title">Dealing with Topic Growth</div> <div class="paragraph"> <p>Depending on the amount of data (number and size of records, frequency of changes), it may or may not be feasible to keep events in topics for a long or even indefinite time. Very often, some or even all events pertaining to a given data item (e.g. a specific purchase order) might be eligible for deletion from a business point of view after a given point in time. See the box "Deletion of Events from Kafka Topics" further below for some more thoughts on the deletion of events from Kafka topics in order to keep their size within bounds.</p> </div> </td> </tr> </table> </div> </div> </div> <div class="sect1"> <h2 id="the_issue_of_dual_writes"><a class="anchor" href="#the_issue_of_dual_writes"></a>The Issue of Dual Writes</h2> <div class="sectionbody"> <div class="paragraph"> <p>In order to provide their functionality, microservices will typically have their own local data store. For instance, the order service may use a relational database to persist the information about purchase orders. When a new order is placed, this may result in an <code>INSERT</code> operation in a table <code>PurchaseOrder</code> in the service&#8217;s database. At the same time, the service may wish to send an event about the new order to Apache Kafka, so to propagate that information to other interested services.</p> </div> <div class="paragraph"> <p>Simply issuing these two requests may lead to potential inconsistencies, though. The reason being that we cannot have one shared transaction that would span the service&#8217;s database as well as Apache Kafka, as the latter doesn&#8217;t support to be enlisted in distributed (XA) transactions. So in unfortunate circumstances it might happen that we end up with having the new purchase order persisted in the local database, but not having sent the corresponding message to Kafka (e.g. due to some networking issue). Or, the other way around, we might have sent the message to Kafka but failed to persist the purchase order in the local database. Both situations are undesirable; this may cause no shipment to be created for a seemingly successfully placed order. Or a shipment gets created, but then there&#8217;d be no trace about the corresponding purchase order in the order service itself.</p> </div> <div class="paragraph"> <p>So how can this situation be avoided? The answer is to only modify <em>one</em> of the two resources (the database <em>or</em> Apache Kafka) and drive the update of the second one based on that, in an eventually consistent manner. Let&#8217;s first consider the case of only writing to Apache Kafka.</p> </div> <div class="paragraph"> <p>When receiving a new purchase order, the order service would not do the <code>INSERT</code> into its database synchronously; instead, it would only send an event describing the new order to a Kafka topic. So only one resource gets modified at a time, and if something goes wrong with that, we&#8217;ll find out about it instantly and report back to the caller of the order service that the request failed.</p> </div> <div class="paragraph"> <p>At the same time, the service itself would subscribe to that Kafka topic. That way, it will be notified when a new message arrives in the topic and it can persist the new purchase order in its database. There&#8217;s one subtle challenge here, though, and that is the lack of "read your own write" semantics. E.g. let&#8217;s assume the order service also has an API for searching for all the purchase orders of a given customer. When invoking that API right after placing a new order, due to the asynchronous nature of processing messages from the Kafka topic, it might happen that the purchase order has not yet been persisted in the service&#8217;s database and thus will not be returned by that query. That can lead to a very confusing user experience, as users for instance may miss newly placed orders in their shopping history. There are ways to deal with this situation, e.g. the service could keep newly placed purchase orders in memory and answer subsequent queries based on that. This gets quickly non-trivial though when implementing more complex queries or considering that the order service might also comprise multiple nodes in a clustered set-up, which would require propagation of that data within the cluster.</p> </div> <div class="paragraph"> <p>Now how would things look like when only writing to the database synchronously and driving the export of a message to Apache Kafka based on that? This is where the outbox pattern comes in.</p> </div> </div> </div> <div class="sect1"> <h2 id="the_outbox_pattern"><a class="anchor" href="#the_outbox_pattern"></a>The Outbox Pattern</h2> <div class="sectionbody"> <div class="paragraph"> <p>The idea of this approach is to have an "outbox" table in the service&#8217;s database. When receiving a request for placing a purchase order, not only an <code>INSERT</code> into the <code>PurchaseOrder</code> table is done, but, as part of the same transaction, also a record representing the event to be sent is inserted into that outbox table.</p> </div> <div class="paragraph"> <p>The record describes an event that happened in the service, for instance it could be a JSON structure representing the fact that a new purchase order has been placed, comprising data on the order itself, its order lines as well as contextual information such as a use case identifier. By explicitly emitting events via records in the outbox table, it can be ensured that events are structured in a way suitable for external consumers. This also helps to make sure that event consumers won&#8217;t break when for instance altering the internal domain model or the <code>PurchaseOrder</code> table.</p> </div> <div class="paragraph"> <p>An asynchronous process monitors that table for new entries. If there are any, it propagates the events as messages to Apache Kafka. This gives us a very nice balance of characteristics: By synchronously writing to the <code>PurchaseOrder</code> table, the source service benefits from "read your own writes" semantics. A subsequent query for purchase orders will return the newly persisted order, as soon as that first transaction has been committed. At the same time, we get reliable, asynchronous, eventually consistent data propagation to other services via Apache Kafka.</p> </div> <div class="paragraph"> <p>Now, the outbox pattern isn&#8217;t actually a new idea. It has been in use for quite some time. In fact, even when using JMS-style message brokers, which actually could participate in distributed transactions, it can be a preferable option to avoid any coupling and potential impact by downtimes of remote resources such as a message broker. You can also find a description of the pattern on Chris Richardson&#8217;s excellent <a href="https://microservices.io/patterns/data/application-events.html">microservices.io</a> site.</p> </div> <div class="paragraph"> <p>Nevertheless, the pattern gets much less attention than it deserves and it is especially useful in the context of microservices. As we&#8217;ll see, the outbox pattern can be implemented in a very elegant and efficient way using change data capture and Debezium. In the following, let&#8217;s explore how.</p> </div> </div> </div> <div class="sect1"> <h2 id="an_implementation_based_on_change_data_capture"><a class="anchor" href="#an_implementation_based_on_change_data_capture"></a>An Implementation Based on Change Data Capture</h2> <div class="sectionbody"> <div class="paragraph"> <p><a href="/blog/2018/07/19/advantages-of-log-based-change-data-capture/">Log-based Change Data Capture</a> (CDC) is a great fit for capturing new entries in the outbox table and stream them to Apache Kafka. As opposed to any polling-based approach, event capture happens with a very low overhead in near-realtime. Debezium comes with <a href="/docs/connectors/">CDC connectors</a> for several databases such as MySQL, Postgres and SQL Server. The following example will use the <a href="/docs/connectors/postgresql">Debezium connector for Postgres</a>.</p> </div> <div class="paragraph"> <p>You can find the complete <a href="https://github.com/debezium/debezium-examples/tree/master/outbox">source code of the example</a> on GitHub. Refer to the <a href="https://github.com/debezium/debezium-examples/blob/master/outbox/README.md">README.md</a> for details on building and running the example code. The example is centered around two microservices, <a href="https://github.com/debezium/debezium-examples/tree/master/outbox/order-service">order-service</a> and <a href="https://github.com/debezium/debezium-examples/tree/master/outbox/shipment-service">shipment-service</a>. Both are implemented in Java, using <a href="http://cdi-spec.org/">CDI</a> as the component model and JPA/Hibernate for accessing their respective databases. The order service runs on <a href="http://wildfly.org/">WildFly</a> and exposes a simple REST API for placing purchase orders and canceling specific order lines. It uses a Postgres database as its local data store. The shipment service is based on <a href="http://thorntail.io/">Thorntail</a>; via Apache Kafka, it receives events exported by the order service and creates corresponding shipment entries in its own MySQL database. Debezium tails the transaction log ("write-ahead log", WAL) of the order service&#8217;s Postgres database in order to capture any new events in the outbox table and propagates them to Apache Kafka.</p> </div> <div class="paragraph"> <p>The overall architecture of the solution can be seen in the following picture:</p> </div> <div class="imageblock centered-image"> <img src="/images/outbox_pattern.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Outbox Pattern Overview"> </div> <div class="paragraph"> <p>Note that the pattern is in no way tied to these specific implementation choices. It could equally well be realized using alternative technologies such as Spring Boot (e.g. leveraging Spring Data&#8217;s <a href="https://docs.spring.io/spring-data/commons/docs/current/api/index.html?org/springframework/data/domain/DomainEvents.html">support for domain events</a>), plain JDBC or other programming languages than Java altogether.</p> </div> <div class="paragraph"> <p>Now let&#8217;s take a closer look at some of the relevant components of the solution.</p> </div> <div class="sect2"> <h3 id="the_outbox_table"><a class="anchor" href="#the_outbox_table"></a>The Outbox Table</h3> <div class="paragraph"> <p>The <code>outbox</code> table resides in the database of the order service and has the following structure:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code>Column        |          Type          | Modifiers&#x000A;--------------+------------------------+-----------&#x000A;id            | uuid                   | not null&#x000A;aggregatetype | character varying(255) | not null&#x000A;aggregateid   | character varying(255) | not null&#x000A;type          | character varying(255) | not null&#x000A;payload       | jsonb                  | not null</code></pre> </div> </div> <div class="paragraph"> <p>Its columns are these:</p> </div> <div class="ulist"> <ul> <li> <p><code>id</code>: unique id of each message; can be used by consumers to detect any duplicate events, e.g. when restarting to read messages after a failure. Generated when creating a new event.</p> </li> <li> <p><code>aggregatetype</code>: the type of the <em>aggregate root</em> to which a given event is related; the idea being, leaning on the same concept of domain-driven design, that exported events should refer to an aggregate (<a href="https://martinfowler.com/bliki/DDD_Aggregate.html">"a cluster of domain objects that can be treated as a single unit"</a>), where the aggregate root provides the sole entry point for accessing any of the entities within the aggregate. This could for instance be "purchase order" or "customer".</p> <div class="paragraph"> <p>This value will be used to route events to corresponding topics in Kafka, so there&#8217;d be a topic for all events related to purchase orders, one topic for all customer-related events etc. Note that also events pertaining to a child entity contained within one such aggregate should use that same type. So e.g. an event representing the cancelation of an individual order line (which is part of the purchase order aggregate) should also use the type of its aggregate root, "order", ensuring that also this event will go into the "order" Kafka topic.</p> </div> </li> <li> <p><code>aggregateid</code>: the id of the aggregate root that is affected by a given event; this could for instance be the id of a purchase order or a customer id; Similar to the aggregate type, events pertaining to a sub-entity contained within an aggregate should use the id of the containing aggregate root, e.g. the purchase order id for an order line cancelation event. This id will be used as the key for Kafka messages later on. That way, all events pertaining to one aggregate root or any of its contained sub-entities will go into the same partition of that Kafka topic, which ensures that consumers of that topic will consume all the events related to one and the same aggregate in the exact order as they were produced.</p> </li> <li> <p><code>type</code>: the type of event, e.g. "Order Created" or "Order Line Canceled". Allows consumers to trigger suitable event handlers.</p> </li> <li> <p><code>payload</code>: a JSON structure with the actual event contents, e.g. containing a purchase order, information about the purchaser, contained order lines, their price etc.</p> </li> </ul> </div> </div> <div class="sect2"> <h3 id="sending_events_to_the_outbox"><a class="anchor" href="#sending_events_to_the_outbox"></a>Sending Events to the Outbox</h3> <div class="paragraph"> <p>In order to "send" events to the outbox, code in the order service could in general just do an <code>INSERT</code> into the outbox table. However, it&#8217;s a good idea to go for a slightly more abstract API, allowing to adjust implementation details of the outbox later on more easily, if needed. <a href="https://docs.jboss.org/weld/reference/latest/en-US/html/events.html">CDI events</a> come in very handy for this. They can be raised in the application code and will be processed <em>synchronously</em> by the outbox event sender, which will do the required <code>INSERT</code> into the outbox table.</p> </div> <div class="paragraph"> <p>All outbox event types should implement the following contract, resembling the structure of the outbox table shown before:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">public interface ExportedEvent {&#x000A;&#x000A;    String getAggregateId();&#x000A;    String getAggregateType();&#x000A;    JsonNode getPayload();&#x000A;    String getType();&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>To produce such event, application code uses an injected <code>Event</code> instance, as e.g. here in the <code>OrderService</code> class:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@ApplicationScoped&#x000A;public class OrderService {&#x000A;&#x000A;    @PersistenceContext&#x000A;    private EntityManager entityManager;&#x000A;&#x000A;    @Inject&#x000A;    private Event&lt;ExportedEvent&gt; event;&#x000A;&#x000A;    @Transactional&#x000A;    public PurchaseOrder addOrder(PurchaseOrder order) {&#x000A;        order = entityManager.merge(order);&#x000A;&#x000A;        event.fire(OrderCreatedEvent.of(order));&#x000A;        event.fire(InvoiceCreatedEvent.of(order));&#x000A;&#x000A;        return order;&#x000A;    }&#x000A;&#x000A;    @Transactional&#x000A;    public PurchaseOrder updateOrderLine(long orderId, long orderLineId,&#x000A;            OrderLineStatus newStatus) {&#x000A;        // ...&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>In the <code>addOrder()</code> method, the JPA entity manager is used to persist the incoming order in the database and the injected <code>event</code> is used to fire a corresponding <code>OrderCreatedEvent</code> and an <code>InvoiceCreatedEvent</code>. Again, keep in mind that, despite the notion of "event", these two things happen within one and the same transaction. i.e. within this transaction, three records will be inserted into the database: one in the table with purchase orders and two in the outbox table.</p> </div> <div class="paragraph"> <p>Actual event implementations are straight-forward; as an example, here&#8217;s the <code>OrderCreatedEvent</code> class:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">public class OrderCreatedEvent implements ExportedEvent {&#x000A;&#x000A;    private static ObjectMapper mapper = new ObjectMapper();&#x000A;&#x000A;    private final long id;&#x000A;    private final JsonNode order;&#x000A;&#x000A;    private OrderCreatedEvent(long id, JsonNode order) {&#x000A;        this.id = id;&#x000A;        this.order = order;&#x000A;    }&#x000A;&#x000A;    public static OrderCreatedEvent of(PurchaseOrder order) {&#x000A;        ObjectNode asJson = mapper.createObjectNode()&#x000A;                .put("id", order.getId())&#x000A;                .put("customerId", order.getCustomerId())&#x000A;                .put("orderDate", order.getOrderDate().toString());&#x000A;&#x000A;        ArrayNode items = asJson.putArray("lineItems");&#x000A;&#x000A;        for (OrderLine orderLine : order.getLineItems()) {&#x000A;        items.add(&#x000A;                mapper.createObjectNode()&#x000A;                .put("id", orderLine.getId())&#x000A;                .put("item", orderLine.getItem())&#x000A;                .put("quantity", orderLine.getQuantity())&#x000A;                .put("totalPrice", orderLine.getTotalPrice())&#x000A;                .put("status", orderLine.getStatus().name())&#x000A;            );&#x000A;        }&#x000A;&#x000A;        return new OrderCreatedEvent(order.getId(), asJson);&#x000A;    }&#x000A;&#x000A;    @Override&#x000A;    public String getAggregateId() {&#x000A;        return String.valueOf(id);&#x000A;    }&#x000A;&#x000A;    @Override&#x000A;    public String getAggregateType() {&#x000A;        return "Order";&#x000A;    }&#x000A;&#x000A;    @Override&#x000A;    public String getType() {&#x000A;        return "OrderCreated";&#x000A;    }&#x000A;&#x000A;    @Override&#x000A;    public JsonNode getPayload() {&#x000A;        return order;&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>Note how <a href="https://github.com/FasterXML/jackson">Jackson&#8217;s</a> <code>ObjectMapper</code> is used to create a JSON representation of the event&#8217;s payload.</p> </div> <div class="paragraph"> <p>Now let&#8217;s take a look at the code that consumes any fired <code>ExportedEvent</code> and does the corresponding write to the outbox table:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@ApplicationScoped&#x000A;public class EventSender {&#x000A;&#x000A;    @PersistenceContext&#x000A;    private EntityManager entityManager;&#x000A;&#x000A;    public void onExportedEvent(@Observes ExportedEvent event) {&#x000A;        OutboxEvent outboxEvent = new OutboxEvent(&#x000A;                event.getAggregateType(),&#x000A;                event.getAggregateId(),&#x000A;                event.getType(),&#x000A;                event.getPayload()&#x000A;        );&#x000A;&#x000A;        entityManager.persist(outboxEvent);&#x000A;        entityManager.remove(outboxEvent);&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>It&#8217;s rather simple: for each event the CDI runtime will invoke the <code>onExportedEvent()</code> method. An instance of the <code>OutboxEvent</code> entity is persisted in the database&#8201;&#8212;&#8201;and removed right away!</p> </div> <div class="paragraph"> <p>This might be surprising at first. But it makes sense when remembering how log-based CDC works: it doesn&#8217;t examine the actual contents of the table in the database, but instead it tails the append-only transaction log. The calls to <code>persist()</code> and <code>remove()</code> will create an <code>INSERT</code> and a <code>DELETE</code> entry in the log once the transaction commits. After that, Debezium will process these events: for any <code>INSERT</code>, a message with the event&#8217;s payload will be sent to Apache Kafka. <code>DELETE</code> events on the other hand can be ignored, as the removal from the outbox table is a mere technicality that doesn&#8217;t require any propagation to the message broker. So we are able to capture the event added to the outbox table by means of CDC, but when looking at the contents of the table itself, it will always be empty. This means that no additional disk space is needed for the table (apart from the log file elements which will automatically be discarded at some point) and also no separate house-keeping process is required to stop it from growing indefinitely.</p> </div> </div> <div class="sect2"> <h3 id="registering_the_debezium_connector"><a class="anchor" href="#registering_the_debezium_connector"></a>Registering the Debezium Connector</h3> <div class="paragraph"> <p>With the outbox implementation in place, it&#8217;s time to register the Debezium Postgres connector, so it can capture any new events in the outbox table and relay them to Apache Kafka. That can be done by POST-ing the following JSON request to the REST API of Kafka Connect:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">{&#x000A;    "name": "outbox-connector",&#x000A;    "config": {&#x000A;        "connector.class" : "io.debezium.connector.postgresql.PostgresConnector",&#x000A;        "tasks.max" : "1",&#x000A;        "database.hostname" : "order-db",&#x000A;        "database.port" : "5432",&#x000A;        "database.user" : "postgresuser",&#x000A;        "database.password" : "postgrespw",&#x000A;        "database.dbname" : "orderdb",&#x000A;        "database.server.name" : "dbserver1",&#x000A;        "schema.whitelist" : "inventory",&#x000A;        "table.whitelist" : "inventory.outboxevent",&#x000A;        "tombstones.on.delete" : "false",&#x000A;        "transforms" : "router",&#x000A;        "transforms.router.type" : "io.debezium.examples.outbox.routingsmt.EventRouter"&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>This sets up an instance of <code>io.debezium.connector.postgresql.PostgresConnector</code>, capturing changes from the specified Postgres instance. Note that by means of a table whitelist, only changes from the <code>outboxevent</code> table are captured. It also applies a single message transform (SMT) named <code>EventRouter</code>.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="title">Deletion of Events from Kafka Topics</div> <div class="paragraph"> <p>By setting the <code>tombstones.on.delete</code> to <code>false</code>, no deletion markers ("tombstones") will be emitted by the connector when an event record gets deleted from the outbox table. That makes sense, as the deletion from the outbox table shouldn&#8217;t affect the retention of events in the corresponding Kafka topics. Instead, a specific retention time for the event topics may be configured in Kafka, e.g. to retain all purchase order events for 30 days.</p> </div> <div class="paragraph"> <p>Alternatively, one could work with <a href="https://kafka.apache.org/documentation/#compaction">compacted topics</a>. This would require some changes to the design of events in the outbox table:</p> </div> <div class="ulist"> <ul> <li> <p>they must describe the entire aggregate; so for instance also an event representing the cancelation of a single order line should describe the complete current state of the containing purchase order; that way consumers will be able to obtain the entire state of the purchase order also when only seeing the last event pertaining to a given order, after log compaction ran.</p> </li> <li> <p>they must have one more <code>boolean</code> attribute indicating whether a particular event represents the deletion of the event&#8217;s aggregate root. Such an event (e.g. of type <code>OrderDeleted</code>) could then be used by the event routing SMT described in the next section to produce a deletion marker for that aggregate root. Log compaction would then remove all events pertaining to the given purchase order when its <code>OrderDeleted</code> event has been written to the topic.</p> </li> </ul> </div> <div class="paragraph"> <p>Naturally, when deleting events, the event stream will not be re-playable from its very beginning any longer. Depending on the specific business requirements, it might be sufficient to just keep the final state of a given purchase order, customer etc. This could be achieved using compacted topics and a sufficiently value for the topic&#8217;s <code>delete.retention.ms</code> setting. Another option could be to move historic events to some sort of cold storage (e.g. an Amazon S3 bucket), from where they can be retrieved if needed, followed by reading the latest events from the Kafka topics. Which approach to follow depends on the specific requirements, expected amount of data and expertise in the team developing and operating the solution.</p> </div> </td> </tr> </table> </div> </div> <div class="sect2"> <h3 id="topic_routing"><a class="anchor" href="#topic_routing"></a>Topic Routing</h3> <div class="paragraph"> <p>By default, the Debezium connectors will send all change events originating from one given table to the same topic, i.e. we&#8217;d end up with a single Kafka topic named <code>dbserver1.inventory.outboxevent</code> which would contain all events, be it order events, customer events etc.</p> </div> <div class="paragraph"> <p>To simplify the implementation of consumers which are only interested in specific event types it makes more sense, though, to have multiple topics, e.g. <code>OrderEvents</code>, <code>CustomerEvents</code> and so on. For instance the shipment service might not be interested in any customer events. By only subscribing to the <code>OrderEvents</code> topic, it will be sure to never receive any customer events.</p> </div> <div class="paragraph"> <p>In order to route the change events captured from the outbox table to different topics, that custom SMT <code>EventRouter</code> is used. Here is the code of its <code>apply()</code> method, which will be invoked by Kafka Connect for each record emitted by the Debezium connector:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@Override&#x000A;public R apply(R record) {&#x000A;    // Ignoring tombstones just in case&#x000A;    if (record.value() == null) {&#x000A;        return record;&#x000A;    }&#x000A;&#x000A;    Struct struct = (Struct) record.value();&#x000A;    String op = struct.getString("op");&#x000A;&#x000A;    // ignoring deletions in the outbox table&#x000A;    if (op.equals("d")) {&#x000A;        return null;&#x000A;    }&#x000A;    else if (op.equals("c")) {&#x000A;        Long timestamp = struct.getInt64("ts_ms");&#x000A;        Struct after = struct.getStruct("after");&#x000A;&#x000A;        String key = after.getString("aggregateid");&#x000A;        String topic = after.getString("aggregatetype") + "Events";&#x000A;&#x000A;        String eventId = after.getString("id");&#x000A;        String eventType = after.getString("type");&#x000A;        String payload = after.getString("payload");&#x000A;&#x000A;        Schema valueSchema = SchemaBuilder.struct()&#x000A;            .field("eventType", after.schema().field("type").schema())&#x000A;            .field("ts_ms", struct.schema().field("ts_ms").schema())&#x000A;            .field("payload", after.schema().field("payload").schema())&#x000A;            .build();&#x000A;&#x000A;        Struct value = new Struct(valueSchema)&#x000A;            .put("eventType", eventType)&#x000A;            .put("ts_ms", timestamp)&#x000A;            .put("payload", payload);&#x000A;&#x000A;        Headers headers = record.headers();&#x000A;        headers.addString("eventId", eventId);&#x000A;&#x000A;        return record.newRecord(topic, null, Schema.STRING_SCHEMA, key, valueSchema, value,&#x000A;                record.timestamp(), headers);&#x000A;    }&#x000A;    // not expecting update events, as the outbox table is "append only",&#x000A;    // i.e. event records will never be updated&#x000A;    else {&#x000A;        throw new IllegalArgumentException("Record of unexpected op type: " + record);&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>When receiving a delete event (<code>op</code> = <code>d</code>), it will discard that event, as that deletion of event records from the outbox table is not relevant to downstream consumers. Things get more interesting, when receiving a create event (<code>op</code> = <code>c</code>). Such record will be propagated to Apache Kafka.</p> </div> <div class="paragraph"> <p>Debezium&#8217;s change events have a complex structure, that contain the old (<code>before</code>) and new (<code>after</code>) state of the represented row. The event structure to propagate is obtained from the <code>after</code> state. The <code>aggregatetype</code> value from the captured event record is used to build the name of the topic to send the event to. For instance, events with <code>aggregatetype</code> set to <code>Order</code> will be sent to the <code>OrderEvents</code> topic. <code>aggregateid</code> is used as the message key, making sure all messages of that aggregate will go into the same partition of that topic. The message value is a structure comprising the original event payload (encoded as JSON), the timestamp indicating when the event was produced and the event type. Finally, the event UUID is propagated as a Kafka header field. This allows for efficient duplicate detection by consumers, without having to examine the actual message contents.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="events_in_apache_kafka"><a class="anchor" href="#events_in_apache_kafka"></a>Events in Apache Kafka</h2> <div class="sectionbody"> <div class="paragraph"> <p>Now let&#8217;s take a look into the <code>OrderEvents</code> and <code>CustomerEvents</code> topics.</p> </div> <div class="paragraph"> <p>If you have checked out the example sources and started all the components via Docker Compose (see the <em>README.md</em> file in the example project for more details), you can place purchase orders via the order service&#8217;s REST API like so:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code>cat resources/data/create-order-request.json | http POST http://localhost:8080/order-service/rest/orders</code></pre> </div> </div> <div class="paragraph"> <p>Similarly, specific order lines can be canceled:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code>cat resources/data/cancel-order-line-request.json | http PUT http://localhost:8080/order-service/rest/orders/1/lines/2</code></pre> </div> </div> <div class="paragraph"> <p>When using a tool such as the very practical <a href="https://github.com/edenhill/kafkacat">kafkacat</a> utility, you should now see messages like these in the <code>OrderEvents</code> topic:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code>kafkacat -b kafka:9092 -C -o beginning -f 'Headers: %h\nKey: %k\nValue: %s\n' -q -t OrderEvents</code></pre> </div> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code>Headers: eventId=d03dfb18-8af8-464d-890b-09eb8b2dbbdd&#x000A;Key: "4"&#x000A;Value: {"eventType":"OrderCreated","ts_ms":1550307598558,"payload":"{\"id\": 4, \"lineItems\": [{\"id\": 7, \"item\": \"Debezium in Action\", \"status\": \"ENTERED\", \"quantity\": 2, \"totalPrice\": 39.98}, {\"id\": 8, \"item\": \"Debezium for Dummies\", \"status\": \"ENTERED\", \"quantity\": 1, \"totalPrice\": 29.99}], \"orderDate\": \"2019-01-31T12:13:01\", \"customerId\": 123}"}&#x000A;Headers: eventId=49f89ea0-b344-421f-b66f-c635d212f72c&#x000A;Key: "4"&#x000A;Value: {"eventType":"OrderLineUpdated","ts_ms":1550308226963,"payload":"{\"orderId\": 4, \"newStatus\": \"CANCELLED\", \"oldStatus\": \"ENTERED\", \"orderLineId\": 7}"}</code></pre> </div> </div> <div class="paragraph"> <p>The <code>payload</code> field with the message values is the string-ified JSON representation of the original events. The Debezium Postgres connector emits <code>JSONB</code> columns as a string (using the <code>io.debezium.data.Json</code> logical type name), which is why the quotes are escaped. The <a href="https://stedolan.github.io/jq/">jq</a> utility, and more specifically, its <code>fromjson</code> operator, come in handy for displaying the event payload in a more readable way:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code>kafkacat -b kafka:9092 -C -o beginning -t Order | jq '.payload | fromjson'</code></pre> </div> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">{&#x000A;  "id": 4,&#x000A;  "lineItems": [&#x000A;    {&#x000A;      "id": 7,&#x000A;      "item": "Debezium in Action",&#x000A;      "status": "ENTERED",&#x000A;      "quantity": 2,&#x000A;      "totalPrice": 39.98&#x000A;    },&#x000A;    {&#x000A;      "id": 8,&#x000A;      "item": "Debezium for Dummies",&#x000A;      "status": "ENTERED",&#x000A;      "quantity": 1,&#x000A;      "totalPrice": 29.99&#x000A;    }&#x000A;  ],&#x000A;  "orderDate": "2019-01-31T12:13:01",&#x000A;  "customerId": 123&#x000A;}&#x000A;{&#x000A;  "orderId": 4,&#x000A;  "newStatus": "CANCELLED",&#x000A;  "oldStatus": "ENTERED",&#x000A;  "orderLineId": 7&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>You can also take a look at the <code>CustomerEvents</code> topic to inspect the events representing the creation of an invoice when a purchase order is added.</p> </div> <div class="sect2"> <h3 id="duplicate_detection_in_the_consuming_service"><a class="anchor" href="#duplicate_detection_in_the_consuming_service"></a>Duplicate Detection in the Consuming Service</h3> <div class="paragraph"> <p>At this point, our implementation of the outbox pattern is fully functional; when the order service receives a request to place an order (or cancel an order line), it will persist the corresponding state in the <code>purchaseorder</code> and <code>orderline</code> tables of its database. At the same time, within the same transaction, corresponding event entries will be added to the outbox table in the same database. The Debezium Postgres connector captures any insertions into that table and routes the events into the Kafka topic corresponding to the aggregate type represented by a given event.</p> </div> <div class="paragraph"> <p>To wrap things up, let&#8217;s explore how another microservice such as the shipment service can consume these messages. The entry point into that service is a regular Kafka consumer implementation, which is not too exciting and hence omitted here for the sake of brevity. You can find its <a href="https://github.com/debezium/debezium-examples/blob/master/outbox/shipment-service/src/main/java/io/debezium/examples/outbox/shipment/facade/KafkaEventConsumer.java">source code</a> in the example repository. For each incoming message on the <code>Order</code> topic, the consumer calls the <code>OrderEventHandler</code>:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@ApplicationScoped&#x000A;public class OrderEventHandler {&#x000A;&#x000A;    private static final Logger LOGGER = LoggerFactory.getLogger(OrderEventHandler.class);&#x000A;&#x000A;    @Inject&#x000A;    private MessageLog log;&#x000A;&#x000A;    @Inject&#x000A;    private ShipmentService shipmentService;&#x000A;&#x000A;    @Transactional&#x000A;    public void onOrderEvent(UUID eventId, String key, String event) {&#x000A;        if (log.alreadyProcessed(eventId)) {&#x000A;            LOGGER.info("Event with UUID {} was already retrieved, ignoring it", eventId);&#x000A;            return;&#x000A;        }&#x000A;&#x000A;        JsonObject json = Json.createReader(new StringReader(event)).readObject();&#x000A;        JsonObject payload = json.containsKey("schema") ? json.getJsonObject("payload") :json;&#x000A;&#x000A;        String eventType = payload.getString("eventType");&#x000A;        Long ts = payload.getJsonNumber("ts_ms").longValue();&#x000A;        String eventPayload = payload.getString("payload");&#x000A;&#x000A;        JsonReader payloadReader = Json.createReader(new StringReader(eventPayload));&#x000A;        JsonObject payloadObject = payloadReader.readObject();&#x000A;&#x000A;        if (eventType.equals("OrderCreated")) {&#x000A;            shipmentService.orderCreated(payloadObject);&#x000A;        }&#x000A;        else if (eventType.equals("OrderLineUpdated")) {&#x000A;            shipmentService.orderLineUpdated(payloadObject);&#x000A;        }&#x000A;        else {&#x000A;            LOGGER.warn("Unkown event type");&#x000A;        }&#x000A;&#x000A;        log.processed(eventId);&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>The first thing done by <code>onOrderEvent()</code> is to check whether the event with the given UUID has been processed before. If so, any further calls for that same event will be ignored. This is to prevent any duplicate processing of events caused by the "at least once" semantics of this data pipeline. For instance it could happen that the Debezium connector or the consuming service fail before acknowledging the retrieval of a specific event with the source database or the messaging broker, respectively. In that case, after a restart of Debezium or the consuming service, a few events may be processed a second time. Propagating the event UUID as a Kafka message header allows for an efficient detection and exclusion of duplicates in the consumer.</p> </div> <div class="paragraph"> <p>If a message is received for the first time, the message value is parsed and the business method of the <code>ShippingService</code> method corresponding to the specific event type is invoked with the event payload. Finally, the message is marked as processed with the message log.</p> </div> <div class="paragraph"> <p>This <code>MessageLog</code> simply keeps track of all consumed events in a table within the service&#8217;s local database:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@ApplicationScoped&#x000A;public class MessageLog {&#x000A;&#x000A;    @PersistenceContext&#x000A;    private EntityManager entityManager;&#x000A;&#x000A;    @Transactional(value=TxType.MANDATORY)&#x000A;    public void processed(UUID eventId) {&#x000A;        entityManager.persist(new ConsumedMessage(eventId, Instant.now()));&#x000A;    }&#x000A;&#x000A;    @Transactional(value=TxType.MANDATORY)&#x000A;    public boolean alreadyProcessed(UUID eventId) {&#x000A;        return entityManager.find(ConsumedMessage.class, eventId) != null;&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>That way, should the transaction be rolled back for some reason, also the original message will not be marked as processed and an exception would bubble up to the Kafka event consumer loop. This allows for re-trying to process the message later on.</p> </div> <div class="paragraph"> <p>Note that a more complete implementation should take care of re-trying given messages only for a certain number of times, before re-routing any unprocessable messages to a dead-letter queue or similar. Also there should be some house-keeping on the message log table; periodically, all events older than the consumer&#8217;s current offset committed with the broker may be deleted, as it&#8217;s ensured that such messages won&#8217;t be propagated to the consumer another time.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="summary"><a class="anchor" href="#summary"></a>Summary</h2> <div class="sectionbody"> <div class="paragraph"> <p>The outbox pattern is a great way for propagating data amongst different microservices.</p> </div> <div class="paragraph"> <p>By only modifying a single resource - the source service&#8217;s own database - it avoids any potential inconsistencies of altering multiple resources at the same time which don&#8217;t share one common transactional context (the database and Apache Kafka). By writing to the database first, the source service has instant "read your own writes" semantics, which is important for a consistent user experience, allowing query methods invoked following to a write to instantly reflect any data changes.</p> </div> <div class="paragraph"> <p>At the same time, the pattern enables asynchronous event propagation to other microservices. Apache Kafka acts as a highly scalable and reliable backbone for the messaging amongst the services. Given the right topic retention settings, new consumers may come up long after an event has been originally produced, and build up their own local state based on the event history.</p> </div> <div class="paragraph"> <p>Putting Apache Kafka into the center of the overall architecture also ensures a decoupling of involved services. If for instance single components of the solution fail or are not available for some time, e.g. during an update, events will simply be processed later on: after a restart, the Debezium connector will continue to tail the outbox table from the point where it left off before. Similarly, any consumer will continue to process topics from its previous offset. By keeping track of already successfully processed messages, duplicates can be detected and excluded from repeated handling.</p> </div> <div class="paragraph"> <p>Naturally, such event pipeline between different services is eventually consistent, i.e. consumers such as the shipping service may lag a bit behind producers such as the order service. Usually, that&#8217;s just fine, though, and can be handled in terms of the application&#8217;s business logic. For instance there&#8217;ll typically be no need to create a shipment within the very same second as an order has been placed. Also, end-to-end delays of the overall solution are typically low (seconds or even sub-second range), thanks to log-based change data capture which allows for emission of events in near-realtime.</p> </div> <div class="paragraph"> <p>One last thing to keep in mind is that the structure of the events exposed via the outbox should be considered a part of the emitting service&#8217;s API. I.e. when needed, their structure should be adjusted carefully and with compatibility considerations in mind. This is to ensure to not accidentally break any consumers when upgrading the producing service. At the same time, consumers should be lenient when handling messages and for instance not fail when encountering unknown attributes within received events.</p> </div> <div class="paragraph"> <p><em>Many thanks to Hans-Peter Grahsl, Jiri Pechanec, Justin Holmes and René Kerner for their feedback while writing this post!</em></p> </div> </div> </div> <div class="sect1"> <h2 id="about_debezium"><a class="anchor" href="#about_debezium"></a>About Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of <a href="http://kafka.apache.org/">Kafka</a> and provides <a href="http://kafka.apache.org/documentation.html#connect">Kafka Connect</a> compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is <a href="/license/">open source</a> under the <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, Version 2.0</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="get_involved"><a class="anchor" href="#get_involved"></a>Get involved</h2> <div class="sectionbody"> <div class="paragraph"> <p>We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter <a href="https://twitter.com/debezium">@debezium</a>, <a href="https://gitter.im/debezium/user">chat with us on Gitter</a>, or join our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> to talk with the community. All of the code is open source <a href="https://github.com/debezium/">on GitHub</a>, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or <a href="https://issues.jboss.org/projects/DBZ/issues/">log an issue</a>.</p> </div> </div> </div> </div> <hr> <div class="post"> <h1 class="title"> <a href="/blog/2018/12/05/automating-cache-invalidation-with-change-data-capture/">Automating Cache Invalidation With Change Data Capture</a> </h1> <div class="byline"> <p> <em> December 05, 2018 by Gunnar Morling </em> <em> under&nbsp; </em> <a class="label label-info" href="/blog/tags/discussion/">discussion</a> <a class="label label-info" href="/blog/tags/examples/">examples</a> </p> </div> <div id="preamble"> <div class="sectionbody"> <div class="paragraph"> <p>The <a href="https://docs.jboss.org/hibernate/stable/orm/userguide/html_single/Hibernate_User_Guide.html#caching-config">second-level cache</a> of Hibernate ORM / JPA is a proven and efficient way to increase application performance: caching read-only or rarely modified entities avoids roundtrips to the database, resulting in improved response times of the application.</p> </div> <div class="paragraph"> <p>Unlike the first-level cache, the second-level cache is associated with the session factory (or entity manager factory in JPA terms), so its contents are shared across transactions and concurrent sessions. Naturally, if a cached entity gets modified, the corresponding cache entry must be updated (or purged from the cache), too. As long as the data changes are done through Hibernate ORM, this is nothing to worry about: the ORM will update the cache automatically.</p> </div> <div class="paragraph"> <p>Things get tricky, though, when bypassing the application, e.g. when modifying records directly in the database. Hibernate ORM then has no way of knowing that the cached data has become stale, and it&#8217;s necessary to invalidate the affected items explicitly. A common way for doing so is to foresee some admin functionality that allows to clear an application&#8217;s caches. For this to work, it&#8217;s vital to not forget about calling that invalidation functionality, or the application will keep working with outdated cached data.</p> </div> <div class="paragraph"> <p>In the following we&#8217;re going to explore an alternative approach for cache invalidation, which works in a reliable and fully automated way: by employing Debezium and its <a href="/blog/2018/07/19/advantages-of-log-based-change-data-capture/">change data capture</a> (CDC) capabilities, you can track data changes in the database itself and react to any applied change. This allows to invalidate affected cache entries in near-realtime, without the risk of stale data due to missed changes. If an entry has been evicted from the cache, Hibernate ORM will load the latest version of the entity from the database the next time is requested.</p> </div> </div> </div> <div class="sect1"> <h2 id="the_example_application"><a class="anchor" href="#the_example_application"></a>The Example Application</h2> <div class="sectionbody"> <div class="paragraph"> <p>As an example, consider this simple model of two entities, <code>PurchaseOrder</code> and <code>Item</code>:</p> </div> <div class="imageblock centered-image"> <img src="/images/cache_invalidation_class_diagram.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Example domain model"> </div> <div class="paragraph"> <p>A purchase order represents the order of an item, where its total price is the ordered quantity times the item&#8217;s base price.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="title">Source Code</div> <div class="paragraph"> <p>The <a href="https://github.com/debezium/debezium-examples/tree/master/cache-invalidation/">source code</a> of this example is provided on GitHub. If you want to follow along and try out all the steps described in the following, clone the repo and follow the instructions in <a href="https://github.com/debezium/debezium-examples/tree/master/cache-invalidation/_README.md">README.md</a> for building the project.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>Modelling order and item as JPA entities is straight-forward:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@Entity&#x000A;public class PurchaseOrder {&#x000A;&#x000A;    @Id&#x000A;    @GeneratedValue(generator = "sequence")&#x000A;    @SequenceGenerator(&#x000A;        name = "sequence", sequenceName = "seq_po", initialValue = 1001, allocationSize = 50&#x000A;    )&#x000A;    private long id;&#x000A;    private String customer;&#x000A;    @ManyToOne private Item item;&#x000A;    private int quantity;&#x000A;    private BigDecimal totalPrice;&#x000A;&#x000A;    // ...&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>As changes to items are rare, the <code>Item</code> entity should be cached. This can be done by simply specifying JPA&#8217;s <a href="https://docs.oracle.com/javaee/7/api/javax/persistence/Cacheable.html">@Cacheable</a> annotation:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@Entity&#x000A;@Cacheable&#x000A;public class Item {&#x000A;&#x000A;    @Id&#x000A;    private long id;&#x000A;    private String description;&#x000A;    private BigDecimal price;&#x000A;&#x000A;    // ...&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>You also need to enable the second-level cache in the <em>META-INF/persistence.xml</em> file. The property <code>hibernate.cache.use_second_level_cache</code> activates the cache itself, and the <code>ENABLE_SELECTIVE</code> cache mode causes only those entities to be put into the cache which are annotated with <code>@Cacheable</code>. It&#8217;s also a good idea to enable SQL query logging and cache access statistics. That way you&#8217;ll be able to verify whether things work as expected by examining the application log:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-xml" data-lang="xml">&lt;?xml version="1.0" encoding="utf-8"?&gt;&#x000A;&lt;persistence xmlns="http://xmlns.jcp.org/xml/ns/persistence"&#x000A;    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"&#x000A;    xsi:schemaLocation="..."&#x000A;    version="2.2"&gt;&#x000A;&#x000A;    &lt;persistence-unit name="orders-PU-JTA" transaction-type="JTA"&gt;&#x000A;        &lt;jta-data-source&gt;java:jboss/datasources/OrderDS&lt;/jta-data-source&gt;&#x000A;        &lt;shared-cache-mode&gt;ENABLE_SELECTIVE&lt;/shared-cache-mode&gt;&#x000A;        &lt;properties&gt;&#x000A;            &lt;property name="hibernate.cache.use_second_level_cache" value="true" /&gt;&#x000A;&#x000A;            &lt;property name="hibernate.show_sql" value="true" /&gt;&#x000A;            &lt;property name="hibernate.format_sql" value="true" /&gt;&#x000A;            &lt;property name="hibernate.generate_statistics" value="true" /&gt;&#x000A;&#x000A;            &lt;!-- dialect etc. ... --&gt;&#x000A;        &lt;/properties&gt;&#x000A;    &lt;/persistence-unit&gt;&#x000A;&lt;/persistence&gt;</code></pre> </div> </div> <div class="paragraph"> <p>When running on a <a href="https://www.oracle.com/technetwork/java/javaee/overview/index.html">Java EE</a> application server (or <a href="https://jakarta.ee/">Jakarta EE</a> how the stack is called after it has been donated to the Eclipse Foundation), that&#8217;s all you need to enable second-level caching. In the case of <a href="http://wildfly.org/">WildFly</a> (which is what&#8217;s used in the example project), the <a href="http://infinispan.org/">Infinispan</a> key/value store is used as the cache provider by default.</p> </div> <div class="paragraph"> <p>Now try and see what happens when modifying an item&#8217;s price by running some SQL in the database, bypassing the application layer. If you&#8217;ve checked out the example source code, comment out the <code>DatabaseChangeEventListener</code> class and start the application as described in the <em>README.md</em>. You then can place purchase orders using curl like this (a couple of example items have been persisted at application start-up):</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">&gt; curl -H "Content-Type: application/json" \&#x000A;  -X POST \&#x000A;  --data '{ "customer" : "Billy-Bob", "itemId" : 10003, "quantity" : 2 }' \&#x000A;  http://localhost:8080/cache-invalidation/rest/orders</code></pre> </div> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">{&#x000A;    "id" : 1002,&#x000A;    "customer" : "Billy-Bob",&#x000A;    "item" : {&#x000A;        "id" :10003,&#x000A;        "description" : "North By Northwest",&#x000A;        "price" : 14.99&#x000A;    },&#x000A;    "quantity" : 2,&#x000A;    "totalPrice" : 29.98&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>The response is the expected one, as the item price is 14.99. Now update the item&#8217;s price directly in the database. The example uses Postgres, so you can use the <em>psql</em> CLI utility to do so:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">docker-compose exec postgres bash -c 'psql -U $POSTGRES_USER $POSTGRES_DB -c "UPDATE item SET price = 20.99 where id = 10003"'</code></pre> </div> </div> <div class="paragraph"> <p>Placing another purchase order for the same item using curl, you&#8217;ll see that the calculated total price doesn&#8217;t reflect the update. Not good! But it&#8217;s not too surprising, given that the price update was applied completely bypassing the application layer and Hibernate ORM.</p> </div> </div> </div> <div class="sect1"> <h2 id="the_change_event_handler"><a class="anchor" href="#the_change_event_handler"></a>The Change Event Handler</h2> <div class="sectionbody"> <div class="paragraph"> <p>Now let&#8217;s explore how to use Debezium and CDC to react to changes in the <code>item</code> table and invalidate corresponding cache entries.</p> </div> <div class="paragraph"> <p>While Debezium most of the times is deployed into <a href="https://kafka.apache.org/documentation/#connect">Kafka Connect</a> (thus streaming change events into Apache Kafka topics), it has another mode of operation that comes in very handy for the use case at hand. Using the <a href="/docs/embedded/">embedded engine</a>, you can run the Debezium connectors as a library directly within your application. For each change event received from the database, a configured callback method will be invoked, which in the case at hand will evict the affected item from the second-level cache.</p> </div> <div class="paragraph"> <p>The following picture shows the design of this approach:</p> </div> <div class="imageblock centered-image"> <img src="/images/cache_invalidation_architecture.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Architecture Overview"> </div> <div class="paragraph"> <p>While this doesn&#8217;t come with the scalability and fault tolerance provided by Apache Kafka, it nicely fits the given requirements. As the second-level cache is bound to the application lifecycle, there is for instance no need for the offset management and restarting capabilities provided by the Kafka Connect framework. For the given use case it is enough to receive data change events while the application is running, and using the embedded engine enables exactly that.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="title">Clustered Applications</div> <div class="paragraph"> <p>Note that it still might make sense to use Apache Kafka and the regular deployment of Debezium into Kafka Connect when running a clustered application where each node has a local cache. Instead of registering a connector on each node, Kafka and Connect would allow you to deploy a single connector instance and have the application nodes listen to the topic(s) with the change events. This would result in less resource utilization in the database.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>Having added the dependencies of the Debezium embedded engine (<em>io.debezium:debezium-embedded:0.9.0.Beta1</em>) and the Debezium Postgres connector (<em>io.debezium:debezium-connector-postgres:0.9.0.Beta1</em>) to your project, a class <code>DatabaseChangeEventListener</code> for listening to any changes in the database can be implemented like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@ApplicationScoped&#x000A;public class DatabaseChangeEventListener {&#x000A;&#x000A;    @Resource&#x000A;    private ManagedExecutorService executorService;&#x000A;&#x000A;    @PersistenceUnit private EntityManagerFactory emf;&#x000A;&#x000A;    @PersistenceContext&#x000A;    private EntityManager em;&#x000A;&#x000A;    private EmbeddedEngine engine;&#x000A;&#x000A;    public void startEmbeddedEngine(@Observes @Initialized(ApplicationScoped.class) Object init) {&#x000A;        Configuration config = Configuration.empty()&#x000A;                .withSystemProperties(Function.identity()).edit()&#x000A;                .with(EmbeddedEngine.CONNECTOR_CLASS, PostgresConnector.class)&#x000A;                .with(EmbeddedEngine.ENGINE_NAME, "cache-invalidation-engine")&#x000A;                .with(EmbeddedEngine.OFFSET_STORAGE, MemoryOffsetBackingStore.class)&#x000A;                .with("name", "cache-invalidation-connector")&#x000A;                .with("database.hostname", "postgres")&#x000A;                .with("database.port", 5432)&#x000A;                .with("database.user", "postgresuser")&#x000A;                .with("database.password", "postgrespw")&#x000A;                .with("database.server.name", "dbserver1")&#x000A;                .with("database.dbname", "inventory")&#x000A;                .with("database.whitelist", "public")&#x000A;                .with("snapshot.mode", "never")&#x000A;                .build();&#x000A;&#x000A;        this.engine = EmbeddedEngine.create()&#x000A;                .using(config)&#x000A;                .notifying(this::handleDbChangeEvent)&#x000A;                .build();&#x000A;&#x000A;        executorService.execute(engine);&#x000A;    }&#x000A;&#x000A;    @PreDestroy&#x000A;    public void shutdownEngine() {&#x000A;        engine.stop();&#x000A;    }&#x000A;&#x000A;    private void handleDbChangeEvent(SourceRecord record) {&#x000A;        if (record.topic().equals("dbserver1.public.item")) {&#x000A;            Long itemId = ((Struct) record.key()).getInt64("id");&#x000A;            Struct payload = (Struct) record.value();&#x000A;            Operation op = Operation.forCode(payload.getString("op"));&#x000A;&#x000A;            if (op == Operation.UPDATE || op == Operation.DELETE) {&#x000A;                emf.getCache().evict(Item.class, itemId);&#x000A;            }&#x000A;        }&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>Upon application start-up, this configures an instance of the <a href="/docs/connectors/postgresql/">Debezium Postgres connector</a> and sets up the embedded engine for running the connector. The <a href="/docs/connectors/postgresql/#connector-properties">connector options</a> (host name, credentials etc.) are mostly the same as when deploying the connector into Kafka Connect. There is no need for doing an initial snapshot of the existing data, hence the <a href="/docs/connectors/postgresql/#snapshots">snapshot mode</a> is set to "never".</p> </div> <div class="paragraph"> <p>The offset storage option is used for controlling how connector offsets should be persisted. As it&#8217;s not necessary to process any change events occurring while the connector is not running (instead you&#8217;d just begin to read the log from the current location after the restart), the in-memory implementation provided by Kafka Connect is used.</p> </div> <div class="paragraph"> <p>Once configured, the embedded engine must be run via an <code>Executor</code> instance. As the example runs in WildFly, a managed executor can simply be obtained through <code>@Resource</code> injection for that purpose (see <a href="https://www.jcp.org/en/jsr/detail?id=236">JSR 236</a>).</p> </div> <div class="paragraph"> <p>The embedded engine is configured to invoke the <code>handleDbChangeEvent()</code> method for each received data change event. In this method it first is checked whether the incoming event originates from the <code>item</code> table. If that&#8217;s the case, and if the change event represents an <code>UPDATE</code> or <code>DELETE</code> statement, the affected <code>Item</code> instance is evicted from the second-level cache. JPA 2.0 provides a <a href="https://javaee.github.io/javaee-spec/javadocs/index.html?javax/persistence/Cache.html">simple API</a> for this purpose which is accessible via the <code>EntityManagerFactory</code>.</p> </div> <div class="paragraph"> <p>With the <code>DatabaseChangeEventListener</code> class in place, the cache entry will now automatically be evicted when doing another item update via <em>psql</em>. When placing the first purchase order for that item after the update, you&#8217;ll see in the application log how Hibernate ORM executes a query <code>SELECT ... FROM item ...</code> in order to load the item referenced by the order. Also the cache statistics will report one "L2C miss". Upon subsequent orders of that same item it will be obtained from the cache again.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="title">Eventual Consistency</div> <div class="paragraph"> <p>While the event handling happens in near-realtime, it&#8217;s important to point out that it still applies eventual consistency semantics. This means that there is a very short time window between the point in time where a transaction is committed and the point in time where the change event is streamed from the log to the event handler and the cache entry is invalidated.</p> </div> </td> </tr> </table> </div> </div> </div> <div class="sect1"> <h2 id="avoiding_cache_invalidations_after_application_triggered_data_changes"><a class="anchor" href="#avoiding_cache_invalidations_after_application_triggered_data_changes"></a>Avoiding Cache Invalidations After Application-triggered Data Changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>The change event listener shown above satisfies the requirement of invalidating cached items after external data changes. But in its current form it is evicting cache items a bit too aggressively: cached items will also be purged when updating an <code>Item</code> instance through the application itself. This is not only not needed (as the cached item already is the current version), but it&#8217;s even counter-productive: the superfluous cache evictions will cause additional database roundtrips, resulting in longer response times.</p> </div> <div class="paragraph"> <p>It is therefore necessary to distinguish between data changes performed by the application itself and external data changes. Only in the latter case the affected items should be evicted from the cache. In order to do so, you can leverage the fact that each Debezium data change event contains the id of the originating transaction. Keeping track of all transactions run by the application itself allows to trigger the cache eviction only for those items altered by external transactions.</p> </div> <div class="paragraph"> <p>Accounting for this change, the overall architecture looks like so:</p> </div> <div class="imageblock centered-image"> <img src="/images/cache_invalidation_architecture_tx_registry.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Architecture Overview with Transaction Registry"> </div> <div class="paragraph"> <p>The first thing to implement is the transaction registry, i.e. a class for the transaction book keeping:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@ApplicationScoped&#x000A;public class KnownTransactions {&#x000A;&#x000A;    private final DefaultCacheManager cacheManager;&#x000A;    private final Cache&lt;Long, Boolean&gt; applicationTransactions;&#x000A;&#x000A;    public KnownTransactions() {&#x000A;        cacheManager = new DefaultCacheManager();&#x000A;        cacheManager.defineConfiguration(&#x000A;                "tx-id-cache",&#x000A;                new ConfigurationBuilder()&#x000A;                    .expiration()&#x000A;                        .lifespan(60, TimeUnit.SECONDS)&#x000A;                    .build()&#x000A;                );&#x000A;&#x000A;        applicationTransactions = cacheManager.getCache("tx-id-cache");&#x000A;    }&#x000A;&#x000A;    @PreDestroy&#x000A;    public void stopCacheManager() {&#x000A;        cacheManager.stop();&#x000A;    }&#x000A;&#x000A;    public void register(long txId) {&#x000A;        applicationTransactions.put(txId, true);&#x000A;    }&#x000A;&#x000A;    public boolean isKnown(long txId) {&#x000A;        return Boolean.TRUE.equals(applicationTransactions.get(txId));&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>This uses the Infinispan <code>DefaultCacheManager</code> for creating and maintaining an in-memory cache of transaction ids encountered by the application. As data change events arrive in near-realtime, the TTL of the cache entries can be rather short (in fact, the value of one minute shown in the example is chosen very conservatively, usually events should be received within seconds).</p> </div> <div class="paragraph"> <p>The next step is to retrieve the current transaction id whenever a request is processed by the application and register it within <code>KnownTransactions</code>. This should happen once per transaction. There are multiple ways for implementing this logic; in the following a Hibernate ORM <code>FlushEventListener</code> is used for this purpose:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">class TransactionRegistrationListener implements FlushEventListener {&#x000A;&#x000A;    private volatile KnownTransactions knownTransactions;&#x000A;&#x000A;    public TransactionRegistrationListener() {&#x000A;    }&#x000A;&#x000A;    @Override&#x000A;    public void onFlush(FlushEvent event) throws HibernateException {&#x000A;        event.getSession().getActionQueue().registerProcess( session -&gt; {&#x000A;            Number txId = (Number) event.getSession().createNativeQuery("SELECT txid_current()")&#x000A;                    .setFlushMode(FlushMode.MANUAL)&#x000A;                    .getSingleResult();&#x000A;&#x000A;            getKnownTransactions().register(txId.longValue());&#x000A;        } );&#x000A;    }&#x000A;&#x000A;    private  KnownTransactions getKnownTransactions() {&#x000A;        KnownTransactions value = knownTransactions;&#x000A;&#x000A;        if (value == null) {&#x000A;            knownTransactions = value = CDI.current().select(KnownTransactions.class).get();&#x000A;        }&#x000A;&#x000A;        return value;&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>As there&#8217;s no portable way to obtain the transaction id, this is done using a native SQL query. In the case of Postgres, the <code>txid_current()</code> function can be called for that. Hibernate ORM event listeners are not subject to dependency injection via CDI. Hence the static <code>current()</code> method is used to obtain a handle to the application&#8217;s CDI container and get a reference to the <code>KnownTransactions</code> bean.</p> </div> <div class="paragraph"> <p>This listener will be invoked whenever Hibernate ORM is synchronizing its persistence context with the database ("flushing"), which usually happens exactly once when the transaction is committed.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="title">Manual Flushes</div> <div class="paragraph"> <p>The session / entity manager can also be flushed manually, in which case the <code>txid_current()</code> function would be invoked multiple times. That&#8217;s neglected here for the sake of simplicity. The actual code in the example repo contains a slightly extended version of this class which makes sure that the transaction id is obtained only once.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>To register the flush listener with Hibernate ORM, an <code>Integrator</code> implementation must be created and declared in the <em>META-INF/services/org.hibernate.integrator.spi.Integrator</em> file:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">public class TransactionRegistrationIntegrator implements Integrator {&#x000A;&#x000A;    @Override&#x000A;    public void integrate(Metadata metadata, SessionFactoryImplementor sessionFactory,&#x000A;            SessionFactoryServiceRegistry serviceRegistry) {&#x000A;        serviceRegistry.getService(EventListenerRegistry.class)&#x000A;            .appendListeners(EventType.FLUSH, new TransactionRegistrationListener());&#x000A;    }&#x000A;&#x000A;    @Override&#x000A;    public void disintegrate(SessionFactoryImplementor sessionFactory,&#x000A;            SessionFactoryServiceRegistry serviceRegistry) {&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code>io.debezium.examples.cacheinvalidation.persistence.TransactionRegistrationIntegrator</code></pre> </div> </div> <div class="paragraph"> <p>During bootstrap, Hibernate ORM will detect the integrator class (by means of the <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/ServiceLoader.html">Java service loader</a>), invoke its <code>integrate()</code> method which in turn will register the listener class for the <code>FLUSH</code> event.</p> </div> <div class="paragraph"> <p>The last step is to exclude any events stemming from transactions run by the application itself in the database change event handler:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@ApplicationScoped&#x000A;public class DatabaseChangeEventListener {&#x000A;&#x000A;    // ...&#x000A;&#x000A;    @Inject&#x000A;    private KnownTransactions knownTransactions;&#x000A;&#x000A;    private void handleDbChangeEvent(SourceRecord record) {&#x000A;        if (record.topic().equals("dbserver1.public.item")) {&#x000A;            Long itemId = ((Struct) record.key()).getInt64("id");&#x000A;            Struct payload = (Struct) record.value();&#x000A;            Operation op = Operation.forCode(payload.getString("op"));&#x000A;            Long txId = ((Struct) payload.get("source")).getInt64("txId");&#x000A;&#x000A;            if (!knownTransactions.isKnown(txId) &amp;&amp;&#x000A;                    (op == Operation.UPDATE || op == Operation.DELETE)) {&#x000A;                emf.getCache().evict(Item.class, itemId);&#x000A;            }&#x000A;        }&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>And with that, you got all the pieces in place: cached <code>Item</code>s will only be evicted after external data changes, but not after changes done by the application itself. To confirm, you can invoke the example&#8217;s <code>items</code> resource using curl:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">&gt; curl -H "Content-Type: application/json" \&#x000A;  -X PUT \&#x000A;  --data '{ "description" : "North by Northwest", "price" : 20.99}' \&#x000A;  http://localhost:8080/cache-invalidation/rest/items/10003</code></pre> </div> </div> <div class="paragraph"> <p>When placing the next order for the item after this update, you should see that the <code>Item</code> entity is obtained from the cache, i.e. the change event will not have caused the item&#8217;s cache entry to be evicted. In contrast, if you update the item&#8217;s price via <em>psql</em> another time, the item should be removed from the cache and the order request will produce a cache miss, followed by a <code>SELECT</code> against the <code>item</code> table in the database.</p> </div> </div> </div> <div class="sect1"> <h2 id="summary"><a class="anchor" href="#summary"></a>Summary</h2> <div class="sectionbody"> <div class="paragraph"> <p>In this blog post we&#8217;ve explored how Debezium and change data capture can be employed to invalidate application-level caches after external data changes. Compared to manual cache invalidation, this approach works very reliably (by capturing changes directly from the database log, no events will be missed) and fast (cache eviction happens in near-realtime after the data changes).</p> </div> <div class="paragraph"> <p>As you have seen, not too much glue code is needed in order to implement this. While the shown implementation is somewhat specific to the entities of the example, it should be possible to implement the change event handler in a more generic fashion, so that it can handle a set of configured entity types (essentially, the database change listener would have to convert the primary key field(s) from the change events into the primary key type of the corresponding entities in a generic way). Also such generic implementation would have to provide the logic for obtaining the current transaction id for the most commonly used databases.</p> </div> <div class="paragraph"> <p>Please let us know whether you think this would be an interesting extension to have for Debezium and Hibernate ORM. For instance this could be a new module under the Debezium umbrella, and it could also be a very great project to work on, should you be interested in contributing to Debezium. If you got any thoughts on this idea, please post a comment below or come to our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a>.</p> </div> <div class="paragraph"> <p>Many thanks to Guillaume Smet, Hans-Peter Grahsl and Jiri Pechanec for their feedback while writing this post!</p> </div> </div> </div> <div class="sect1"> <h2 id="about_debezium"><a class="anchor" href="#about_debezium"></a>About Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of <a href="http://kafka.apache.org/">Kafka</a> and provides <a href="http://kafka.apache.org/documentation.html#connect">Kafka Connect</a> compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is <a href="/license/">open source</a> under the <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, Version 2.0</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="get_involved"><a class="anchor" href="#get_involved"></a>Get involved</h2> <div class="sectionbody"> <div class="paragraph"> <p>We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter <a href="https://twitter.com/debezium">@debezium</a>, <a href="https://gitter.im/debezium/user">chat with us on Gitter</a>, or join our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> to talk with the community. All of the code is open source <a href="https://github.com/debezium/">on GitHub</a>, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or <a href="https://issues.jboss.org/projects/DBZ/issues/">log an issue</a>.</p> </div> </div> </div> </div> <hr> <div class="post"> <h1 class="title"> <a href="/blog/2018/09/20/materializing-aggregate-views-with-hibernate-and-debezium/">Materializing Aggregate Views With Hibernate and Debezium</a> </h1> <div class="byline"> <p> <em> September 20, 2018 by Gunnar Morling </em> <em> under&nbsp; </em> <a class="label label-info" href="/blog/tags/discussion/">discussion</a> <a class="label label-info" href="/blog/tags/examples/">examples</a> </p> </div> <div id="preamble"> <div class="sectionbody"> <div class="paragraph"> <p>Updating external full text search indexes (e.g. <a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a>) after data changes is a very popular use case for change data capture (CDC).</p> </div> <div class="paragraph"> <p>As we&#8217;ve discussed in a <a href="/blog/2018/01/17/streaming-to-elasticsearch/">blog post</a> a while ago, the combination of Debezium&#8217;s CDC source connectors and Confluent&#8217;s <a href="https://docs.confluent.io/current/connect/connect-elasticsearch/docs/index.html">sink connector for Elasticsearch</a> makes it straight forward to capture data changes in MySQL, Postgres etc. and push them towards Elastisearch in near real-time. This results in a 1:1 relationship between tables in the source database and a corresponding search index in Elasticsearch, which is perfectly fine for many use cases.</p> </div> <div class="paragraph"> <p>It gets more challenging though if you&#8217;d like to put entire aggregates into a single index. An example could be a customer and all their addresses; those would typically be stored in two separate tables in an RDBMS, linked by a foreign key, whereas you&#8217;d like to have just one index in Elasticsearch, containing documents of customers with their addresses embedded, allowing you to efficiently search for customers based on their address.</p> </div> <div class="paragraph"> <p>Following up to the <a href="/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/">KStreams-based solution</a> to this we described recently, we&#8217;d like to present in this post an alternative for materializing such aggregate views driven by the application layer.</p> </div> </div> </div> <div class="sect1"> <h2 id="overview"><a class="anchor" href="#overview"></a>Overview</h2> <div class="sectionbody"> <div class="paragraph"> <p>The idea is to materialize views in a separate table in the source database, right in the moment the original data is altered.</p> </div> <div class="paragraph"> <p>Aggregates are serialized as JSON structures (which naturally can represent any nested object structure) and stored in a specific table. This is done within the actual transaction altering the data, which means the aggregate view is always consistent with the primary data. In particular this approach isn&#8217;t prone to exposing intermediary aggregations as the KStreams-based solution discussed in the post linked above.</p> </div> <div class="paragraph"> <p>The following picture shows the overall architecture:</p> </div> <img src="/images/jpa_aggregations.png" style="max-width:100%; margin-bottom:10px;" class="responsive-image" alt="Streaming Materialized Aggregate Views to Elastisearch"> <div class="paragraph"> <p>Here the aggregate views are materialized by means of a small extension to <a href="http://hibernate.org/orm/">Hibernate ORM</a>, which stores the JSON aggregates within the source database (note "aggregate views" can be considered conceptually the same as "materialized views" as known from different RDBMS, as in that they materialize the result of a "join" operation, but technically we&#8217;re not using the latter to store aggregate views, but a regular table). Changes to that aggregate table are then captured by Debezium and streamed to one topic per aggregate type. The Elasticsearch sink connector can subscribe to these topics and update corresponding full-text indexes.</p> </div> <div class="paragraph"> <p>You can find a proof-of-concept implementation (said Hibernate extension and related code) of this idea in our <a href="https://github.com/debezium/debezium-examples/tree/master/jpa-aggregations">examples repository</a>. Of course the general idea isn&#8217;t limited to Hibernate ORM or JPA, you could implement something similar with any other API you&#8217;re using to access your data.</p> </div> </div> </div> <div class="sect1"> <h2 id="creating_aggregate_views_via_hibernate_orm"><a class="anchor" href="#creating_aggregate_views_via_hibernate_orm"></a>Creating Aggregate Views via Hibernate ORM</h2> <div class="sectionbody"> <div class="paragraph"> <p>For the following let&#8217;s assume we&#8217;re persisting a simple domain model (comprising a <code>Customer</code> entity and a few related ones like <code>Address</code>, (customer) <code>Category</code> etc.) in a database. Using Hibernate for that allows us to make the creation of aggregates fully transparent to the actual application code using a <a href="http://docs.jboss.org/hibernate/orm/current/userguide/html_single/Hibernate_User_Guide.html#events-events">Hibernate event listener</a>. Thanks to its extensible architecture, we can plug such listener into Hibernate just by adding it to the classpath, from where it will be picked up automatically when bootstrapping the entity manager / session factory.</p> </div> <div class="paragraph"> <p>Our example listener reacts to an annotation, <code>@MaterializeAggregate</code>, which marks those entity types that should be the roots of materialized aggregates.</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@Entity&#x000A;@MaterializeAggregate(aggregateName="customers-complete")&#x000A;public class Customer {&#x000A;&#x000A;    @Id&#x000A;    private long id;&#x000A;&#x000A;    private String firstName;&#x000A;&#x000A;    @OneToMany(mappedBy = "customer", fetch = FetchType.EAGER, cascade = CascadeType.ALL)&#x000A;    private Set&lt;Address&gt; addresses;&#x000A;&#x000A;    @ManyToOne&#x000A;    private Category category;&#x000A;&#x000A;    ...&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>Now if any entity annotated with <code>@MaterializeAggregate</code> is inserted, updated or deleted via Hibernate, the listener will kick in and materialize a JSON view of the aggregate root (customer) and its associated entities (addresses, category).</p> </div> <div class="paragraph"> <p>Under the hood the <a href="https://github.com/FasterXML/jackson">Jackson API</a> is used for serializing the model into JSON. This means you can use any of its annotations to customize the JSON output, e.g. <code>@JsonIgnore</code> to exclude the inverse relationship from <code>Address</code> to <code>Customer</code>:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@Entity&#x000A;public class Address {&#x000A;&#x000A;    @Id&#x000A;    private long id;&#x000A;&#x000A;    @ManyToOne&#x000A;    @JoinColumn(name = "customer_id")&#x000A;    @JsonIgnore&#x000A;    private Customer customer;&#x000A;&#x000A;    private String street;&#x000A;&#x000A;    private String city;&#x000A;&#x000A;    ...&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>Note that <code>Address</code> itself isn&#8217;t marked with <code>@MaterializeAggregate</code>, i.e. it won&#8217;t be materialized into an aggregate view by itself.</p> </div> <div class="paragraph"> <p>After using JPA&#8217;s <code>EntityManager</code> to insert or update a few customers, let&#8217;s take a look at the <code>aggregates</code> table which has been populated by the listener (value schema omitted for the sake of brevity):</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-sql" data-lang="sql">&gt; select * from aggregates;&#x000A;&#x000A;| rootType | keySchema | rootId | materialization | valueSchema |&#x000A;&#x000A;| customers-complete&#x000A;&#x000A;| {&#x000A;  "schema" : {&#x000A;    "type" : "struct",&#x000A;    "fields" : [ {&#x000A;      "type" : "int64",&#x000A;      "optional" : false,&#x000A;      "field" : "id"&#x000A;    } ],&#x000A;    "optional" : false,&#x000A;    "name" : "customers-complete.Key"&#x000A;  }&#x000A;}&#x000A;&#x000A;| { "id" : 1004 }&#x000A;&#x000A;| { "schema" : { ... } }&#x000A;&#x000A;| {&#x000A;  "id" : 1004,&#x000A;  "firstName" : "Anne",&#x000A;  "lastName" : "Kretchmar",&#x000A;  "email" : "annek@noanswer.org",&#x000A;  "tags" : [ "long-term", "vip" ],&#x000A;  "birthday" : 5098,&#x000A;  "category" : {&#x000A;    "id" : 100001,&#x000A;    "name" : "Retail"&#x000A;  },&#x000A;  "addresses" : [ {&#x000A;    "id" : 16,&#x000A;    "street" : "1289 University Hill Road",&#x000A;    "city" : "Canehill",&#x000A;    "state" : "Arkansas",&#x000A;    "zip" : "72717",&#x000A;    "type" : "SHIPPING"&#x000A;  } ]&#x000A;} |</code></pre> </div> </div> <div class="paragraph"> <p>The table contains these columns:</p> </div> <div class="ulist"> <ul> <li> <p><code>rootType</code>: The name of the aggregate as given in the <code>@MaterializeAggregate</code> annotation</p> </li> <li> <p><code>rootId</code>: The aggregate&#8217;s id as serialized JSON</p> </li> <li> <p><code>materialization</code>: The aggregate itself as serialized JSON; in this case a customer and their addresses, category etc.</p> </li> <li> <p><code>keySchema</code>: The Kafka Connect schema of the row&#8217;s key</p> </li> <li> <p><code>valueSchema</code>: The Kafka Connect schema of the materialization</p> </li> </ul> </div> <div class="paragraph"> <p>Let&#8217;s talk about the two schema columns for a bit. JSON itself is quite limited as far as its supported data types are concerned. So for instance we&#8217;d loose information about a numeric field&#8217;s value range (int vs. long etc.) without any additional information. Therefore the listener derives the corresponding schema information for key and aggregate view from the entity model and stores it within the aggregate records.</p> </div> <div class="paragraph"> <p>Now Jackson itself only supports JSON Schema, which would be a bit too limited for our purposes. Hence the example implementation provides custom serializers for Jackson&#8217;s schema system, which allow us to emit Kafka Connect&#8217;s schema representation (with more precise type information) instead of plain JSON Schema. This will come in handy in the following when we&#8217;d like to expand the string-based JSON representations of key and value into properly typed Kafka Connect records.</p> </div> </div> </div> <div class="sect1"> <h2 id="capturing_changes_to_the_aggregate_table"><a class="anchor" href="#capturing_changes_to_the_aggregate_table"></a>Capturing Changes to the Aggregate Table</h2> <div class="sectionbody"> <div class="paragraph"> <p>We now have a mechanism in place which transparently persists aggregates into a separate table within the source database, whenever the application data is changed through Hibernate. Note that this happens within the boundaries of the source transaction, so if the same would be rolled back for some reason, also the aggregate view would not be updated.</p> </div> <div class="paragraph"> <p>The Hibernate listener uses insert-or-update semantics when writing an aggregate view, i.e. for a given aggregate root there&#8217;ll always be exactly one corresponding entry in the aggregate table which reflects its current state. If an aggregate root entity is deleted, the listener will also drop the entry from the aggregate table.</p> </div> <div class="paragraph"> <p>So let&#8217;s set up Debezium now to capture any changes to the <code>aggregates</code> table:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">curl -i -X POST \&#x000A;  -H "Accept:application/json" \&#x000A;  -H "Content-Type:application/json" \&#x000A;  http://localhost:8083/connectors/ -d @- &lt;&lt;-EOF&#x000A;  {&#x000A;      "name": "inventory-connector",&#x000A;      "config": {&#x000A;          "connector.class": "io.debezium.connector.mysql.MySqlConnector",&#x000A;          "tasks.max": "1",&#x000A;          "database.hostname": "mysql",&#x000A;          "database.port": "3306",&#x000A;          "database.user": "debezium",&#x000A;          "database.password": "dbz",&#x000A;          "database.server.id": "184054",&#x000A;          "database.server.name": "dbserver1",&#x000A;          "database.whitelist": "inventory",&#x000A;          "table.whitelist": ".*aggregates",&#x000A;          "database.history.kafka.bootstrap.servers": "kafka:9092",&#x000A;          "database.history.kafka.topic": "schema-changes.inventory"&#x000A;      }&#x000A;  }&#x000A;EOF</code></pre> </div> </div> <div class="paragraph"> <p>This registers the MySQL connector with the "inventory" database (we&#8217;re using an expanded version of the schema from the <a href="/docs/tutorial/">Debezium tutorial</a>), capturing any changes to the "aggregates" table.</p> </div> </div> </div> <div class="sect1"> <h2 id="expanding_json"><a class="anchor" href="#expanding_json"></a>Expanding JSON</h2> <div class="sectionbody"> <div class="paragraph"> <p>If we now were to browse the corresponding Kafka topic, we&#8217;d see data change events in the known Debezium format for all the changes to the <code>aggregates</code> table.</p> </div> <div class="paragraph"> <p>The "materialization" field with the records' "after" state still is a single field containing a JSON string, though. What we&#8217;d rather like to have is a strongly typed Kafka Connect record, whose schema exactly describes the aggregate structure and the types of its fields. For that purpose the example project provides an SMT (single message transform) which takes the JSON materialization and the corresponding <code>valueSchema</code> and converts this into a full-blown Kafka Connect record. The same is done for keys. DELETE events are rewritten into tombstone events. Finally, the SMT re-routes every record to a topic named after the aggregate root, allowing consumers to subscribe just to changes to specific aggregate types.</p> </div> <div class="paragraph"> <p>So let&#8217;s add that SMT when registering the Debezium CDC connector:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">...&#x000A;"transforms":"expandjson",&#x000A;"transforms.expandjson.type":"io.debezium.aggregation.smt.ExpandJsonSmt",&#x000A;...</code></pre> </div> </div> <div class="paragraph"> <p>When now browsing the "customers-complete" topic, we&#8217;ll see the strongly typed Kafka Connect records we&#8217;d expect:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">{&#x000A;    "schema": {&#x000A;        "type": "struct",&#x000A;        "fields": [&#x000A;            {&#x000A;                "type": "int64",&#x000A;                "optional": false,&#x000A;                "field": "id"&#x000A;            }&#x000A;        ],&#x000A;        "optional": false,&#x000A;        "name": "customers-complete.Key"&#x000A;    },&#x000A;    "payload": {&#x000A;        "id": 1004&#x000A;    }&#x000A;}&#x000A;{&#x000A;    "schema": {&#x000A;        "type": "struct",&#x000A;        "fields": [ ... ],&#x000A;        "optional": true,&#x000A;        "name": "urn:jsonschema:com:example:domain:Customer"&#x000A;    },&#x000A;    "payload": {&#x000A;        "id": 1004,&#x000A;        "firstName": "Anne",&#x000A;        "lastName": "Kretchmar",&#x000A;        "email": "annek@noanswer.org",&#x000A;        "active": true,&#x000A;        "tags" : [ "long-term", "vip" ],&#x000A;        "birthday" : 5098,&#x000A;        "category": {&#x000A;            "id": 100001,&#x000A;            "name": "Retail"&#x000A;        },&#x000A;        "addresses": [&#x000A;            {&#x000A;                "id": 16,&#x000A;                "street": "1289 University Hill Road",&#x000A;                "city": "Canehill",&#x000A;                "state": "Arkansas",&#x000A;                "zip": "72717",&#x000A;                "type": "LIVING"&#x000A;            }&#x000A;        ]&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>To confirm that these are actual typed Kafka Connect records and not just a single JSON string field, you could for instance use the <a href="/docs/configuration/avro/">Avro message converter</a> and examine the message schemas in the schema registry.</p> </div> </div> </div> <div class="sect1"> <h2 id="sinking_aggregate_messages_into_elasticsearch"><a class="anchor" href="#sinking_aggregate_messages_into_elasticsearch"></a>Sinking Aggregate Messages Into Elasticsearch</h2> <div class="sectionbody"> <div class="paragraph"> <p>The last missing step is to register the Confluent Elasticsearch sink connector, hooking it up with the "customers-complete" topic and letting it push any changes to the corresponding index:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">curl -i -X POST \&#x000A;  -H "Accept:application/json" \&#x000A;  -H "Content-Type:application/json" \&#x000A;  http://localhost:8083/connectors/ -d @- &lt;&lt;-EOF&#x000A;  {&#x000A;      "name": "es-customers",&#x000A;      "config": {&#x000A;          "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",&#x000A;          "tasks.max": "1",&#x000A;          "topics": "customers-complete",&#x000A;          "connection.url": "http://elastic:9200",&#x000A;          "key.ignore": "false",&#x000A;          "schema.ignore" : "false",&#x000A;          "behavior.on.null.values" : "delete",&#x000A;          "type.name": "customer-with-addresses",&#x000A;          "transforms" : "key",&#x000A;          "transforms.key.type": "org.apache.kafka.connect.transforms.ExtractField$Key",&#x000A;          "transforms.key.field": "id"&#x000A;      }&#x000A;  }&#x000A;EOF</code></pre> </div> </div> <div class="paragraph"> <p>This uses Connect&#8217;s <code>ExtractField</code> transformation to obtain just the actual id value from the key struct and use it as key for the corresponding Elasticsearch documents. Specifying the "behavior.on.null.values" option will let the connector delete the corresponding document from the index when encountering a tombstone message (i.e. a message with a key but without value).</p> </div> <div class="paragraph"> <p>Finally, we can use the Elasticsearch REST API to browse the index and of course use its powerful full-text query language to find customers by the address or any other property embedded into the aggregate structure:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">&gt; curl -X GET -H "Accept:application/json" \&#x000A;  http://localhost:9200/customers-complete/_search?pretty&#x000A;&#x000A;  {&#x000A;      "_shards": {&#x000A;          "failed": 0,&#x000A;          "successful": 5,&#x000A;          "total": 5&#x000A;      },&#x000A;      "hits": {&#x000A;          "hits": [&#x000A;              {&#x000A;                  "_id": "1004",&#x000A;                  "_index": "customers-complete",&#x000A;                  "_score": 1.0,&#x000A;                  "_source": {&#x000A;                      "active": true,&#x000A;                      "addresses": [&#x000A;                          {&#x000A;                              "city": "Canehill",&#x000A;                              "id": 16,&#x000A;                              "state": "Arkansas",&#x000A;                              "street": "1289 University Hill Road",&#x000A;                              "type": "LIVING",&#x000A;                              "zip": "72717"&#x000A;                          }&#x000A;                      ],&#x000A;                      "tags" : [ "long-term", "vip" ],&#x000A;                      "birthday" : 5098,&#x000A;                      "category": {&#x000A;                          "id": 100001,&#x000A;                          "name": "Retail"&#x000A;                      },&#x000A;                      "email": "annek@noanswer.org",&#x000A;                      "firstName": "Anne",&#x000A;                      "id": 1004,&#x000A;                      "lastName": "Kretchmar",&#x000A;                      "scores": [],&#x000A;                      "someBlob": null,&#x000A;                      "tags": []&#x000A;                  },&#x000A;                  "_type": "customer-with-addresses"&#x000A;              }&#x000A;          ],&#x000A;          "max_score": 1.0,&#x000A;          "total": 1&#x000A;      },&#x000A;      "timed_out": false,&#x000A;      "took": 11&#x000A;  }</code></pre> </div> </div> <div class="paragraph"> <p>And there you have it: a customer&#8217;s complete data, including their addresses, categories, tags etc., materialized into a single document within Elasticsearch. If you&#8217;re using JPA to update the customer, you&#8217;ll see the data in the index being updated accordingly in near-realtime.</p> </div> </div> </div> <div class="sect1"> <h2 id="pros_and_cons"><a class="anchor" href="#pros_and_cons"></a>Pros and Cons</h2> <div class="sectionbody"> <div class="paragraph"> <p>So what are the advantages and disadvantages of this approach for materializing aggregates from multiple source tables compared to the <a href="/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/">KStreams-based approach</a>?</p> </div> <div class="paragraph"> <p>The big advantage is consistency and awareness of transactional boundaries, whereas the KStreams-based solution in its suggested form was prone to exposing intermediary aggregates. For instance, if you&#8217;re storing a customer and three addresses, it might happen that the streaming query first creates an aggregation of the customer and the two addresses inserted first, and shortly thereafter the complete aggregate with all three addresses. This not the case for the approach discussed here, as you&#8217;ll only ever stream complete aggregates to Kafka. Also this approach feels a bit more "light-weight", i.e. a simple marker annotation (together with some Jackson annotations for fine-tuning the emitted JSON structures) is enough in order to materialize aggregates from your domain model, whereas some more effort was needed to set up the required streams, temporary tables etc. with the KStreams solution.</p> </div> <div class="paragraph"> <p>The downside of driving aggregations through the application layer is that it&#8217;s not fully agnostic to the way you access the primary data. If you bypass the application, e.g. by patching data directly in the database, naturally these updates would be missed, requiring a refresh of affected aggregates. Although this again could be done through change data capture and Debezium: change events to source tables could be captured and consumed by the application itself, allowing it to re-materialize aggregates after external data changes. You also might argue that running JSON serializations within source transactions and storing aggregates within the source database represents some overhead. This often may be acceptable, though.</p> </div> <div class="paragraph"> <p>Another question to ask is what&#8217;s the advantage of using change data capture on an intermediary aggregate table over simply posting REST requests to Elasticsearch. The answer is the highly increased robustness and fault tolerance. If the Elasticsearch cluster can&#8217;t be accessed for some reason, the machinery of Kafka and Kafka Connect will ensure that any change events will be propagated eventually, once the sink is up again. Also other consumers than Elasticsearch can subscribe to the aggregate topic, the log can be replayed from the beginning etc.</p> </div> <div class="paragraph"> <p>Note that while we&#8217;ve been talking primarily about using Elasticsearch as a data sink, there are also other datastores and connectors that support complexly structured records. One example would be MongoDB and the <a href="https://github.com/hpgrahsl/kafka-connect-mongodb">sink connector</a> maintained by Hans-Peter Grahsl, which one could use to sink customer aggregates into MongoDB, for instance enabling efficient retrieval of a customer and all their associated data with a single primary key look-up.</p> </div> </div> </div> <div class="sect1"> <h2 id="outlook"><a class="anchor" href="#outlook"></a>Outlook</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Hibernate ORM extension as well as the SMT discussed in this post can be found in our <a href="https://github.com/debezium/debezium-examples/tree/master/jpa-aggregations">examples repository</a>. They should be considered to be at "proof-of-concept" level currently.</p> </div> <div class="paragraph"> <p>That being said, we&#8217;re considering to make this a Debezium component proper, allowing you to employ this aggregation approach within your Hibernate-based applications just by pulling in this new component. For that we&#8217;d have to improve a few things first, though. Most importantly, an API is needed which will let you (re-)create aggregates on demand, e.g. for existing data or for data updated by bulk updates via the Criteria API / JPQL (which will be missed by listeners). Also aggregates should be re-created automatically, if any of the referenced entities change (with the current PoC, only a change to the customer instance itself will trigger its aggregate view to be rebuilt, but not a change to one of its addresses).</p> </div> <div class="paragraph"> <p>If you like this idea, then let us know about it, so we can gauge the general interest in this. Also, this would be a great item to work on, if you&#8217;re interested in contributing to the Debezium project. Looking forward to hearing from you, e.g. in the comment section below or on our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a>.</p> </div> <div class="paragraph"> <p>Thanks a lot to Hans-Peter Grahsl for his feedback on an earlier version of this post!</p> </div> </div> </div> <div class="sect1"> <h2 id="about_debezium"><a class="anchor" href="#about_debezium"></a>About Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of <a href="http://kafka.apache.org/">Kafka</a> and provides <a href="http://kafka.apache.org/documentation.html#connect">Kafka Connect</a> compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is <a href="/license/">open source</a> under the <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, Version 2.0</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="get_involved"><a class="anchor" href="#get_involved"></a>Get involved</h2> <div class="sectionbody"> <div class="paragraph"> <p>We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter <a href="https://twitter.com/debezium">@debezium</a>, <a href="https://gitter.im/debezium/user">chat with us on Gitter</a>, or join our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> to talk with the community. All of the code is open source <a href="https://github.com/debezium/">on GitHub</a>, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or <a href="https://issues.jboss.org/projects/DBZ/issues/">log an issue</a>.</p> </div> </div> </div> </div> <hr> <ul class="pager"> <li class="previous"> <a href="/blog/tags/examples/page/2/">&laquo; Older</a> </li> <li class="pages">Page 1 of 2</li> <li class="disabled next"> <a href="#">Newer &raquo;</a> </li> </ul> </div> </div> </div> </div> <footer class="container"> <div class="row"> <div class="col-md-5 col-md-offset-1"> <h4>Debezium</h4> <p> &#169; 2019 Debezium Community <br> <br> <i class="icon-fire"></i> Mixed with <a href="http://twitter.github.com/bootstrap">Bootstrap</a>, baked by <a href="http://awestruct.org">Awestruct</a>. <br> <i class="icon-flag"></i> Website and docs licensed under <a href="http://creativecommons.org/licenses/by/3.0/">CC BY 3.0</a>. <br> <i class="icon-flag-alt"></i> Code released under <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, v2.0</a>. <br> <i class="icon-file-alt"></i> <a href="https://www.redhat.com/legal/legal_statement.html" title="Terms">Terms</a> | <a href="https://www.redhat.com/legal/privacy_statement.html" title="Privacy Policy">Privacy</a> </p> </div> <div class="col-md-3"> <h4>Documentation</h4> <ul class="list-unstyled"> <li> <a href="/docs/features/" title="Features">Features</a> </li> <li> <a href="/docs/install/" title="Install">Install</a> </li> <li> <a href="/docs/manage/" title="Manage">Manage</a> </li> <li> <a href="/docs/architecture/" title="Architecture">Architecture</a> </li> <li> <a href="/docs/faq/" title="FAQ">FAQ</a> </li> <li> <a href="/docs/contribute/" title="Contribute">Contribute</a> </li> </ul> </div> <div class="col-md-3"> <h4>Connect</h4> <ul class="list-unstyled"> <li> <a href="/blog" title="Blog">Blog</a> </li> <li> <a href="http://twitter.com/debezium" title="Twitter">Twitter</a> </li> <li> <a href="http://github.com/debezium" title="GitHub">GitHub</a> </li> <li> <a href="https://gitter.im/debezium/dev" title="Chat">Chat</a> </li> <li> <a href="https://groups.google.com/forum/#!forum/debezium" title="Google Groups">Google Groups</a> </li> <li> <a href="http://stackoverflow.com/questions/tagged/debezium" title="StackOverflow">StackOverflow</a> </li> </ul> </div> </div> </footer> <div class="container" id="companyfooter"> <div class="redhatlogo"> <div id="logospacer"></div> <a href="https://www.redhat.com/"><img src="/images/Logo-Red_Hat-Sponsored_By-B-Standard-RGB.svg"></a> </div> </div> <span class="backToTop"> <a href="#top">back to top</a> </span> <script src="https://static.jboss.org/theme/js/libs/bootstrap-community/3.2.0.2/bootstrap-community.min.js"></script> <script type='text/javascript' language='JavaScript' src='https://www.redhat.com/j/elqNow/elqCfg.js'></script> <script type='text/javascript' language='JavaScript' src='https://www.redhat.com/j/elqNow/elqImg.js'></script> <div id="oTags"> <script type="text/javascript" src="//www.redhat.com/j/s_code.js"></script> <script type="text/javascript"><!--
        var coreUrl = encodeURI(document.URL.split("?")[0]).replace(/-/g," ");
        var urlSplit = coreUrl.toLowerCase().split(/\//);
        var urlLast = urlSplit[urlSplit.length-1];
        var pageNameString = "";
        var siteName = "";
        var minorSectionIndex = 3
        if (urlLast == "") {
            urlSplit.splice(-1,1);
        }
        if (urlLast.search(/\./) >= 0) {
            if (urlLast == "index.html") {
                urlSplit.splice(-1,1);
            }
            else {
                urlSplit[urlSplit.length-1] = urlLast.split(".").splice(0,1);
            }
        }
        siteName = urlSplit[2].split(".")[1];
        s.prop14 = s.eVar27 = siteName || "";
        s.prop15 = s.eVar28 = urlSplit[minorSectionIndex] || "";
        s.prop16 = s.eVar29 = urlSplit[minorSectionIndex+1] || "";
        pageNameString = urlSplit.splice(3).join(" | ");
        s.pageName = "jboss | community | " + siteName + " | " + pageNameString;
        s.server = "jboss";
        s.channel = "jboss | community";
        s.prop4 = s.eVar23 = encodeURI(document.URL);
        s.prop21 = s.eVar18 = coreUrl;
        s.prop2 = s.eVar22 = "en";
        s.prop3 = s.eVar19 = "us";
        //--></script> <script type="text/javascript" src="//www.redhat.com/j/rh_omni_footer.js"></script> <script language="JavaScript" type="text/javascript"><!--
        if(navigator.appVersion.indexOf('MSIE')>=0)document.write(unescape('%3C')+'\!-'+'-')
        //--></script> <noscript><a href="http://www.omniture.com" title="Web Analytics"><img src="https://smtrcs.redhat.com/b/ss/redhatcom,redhatglobal/1/H.25.4--NS/0?[AQB]&cdp=3&[AQE]" height="1" width="1" border="0" alt=""/></a></noscript> </div> <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
      document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
      </script> <script type="text/javascript">
      try {
      var pageTracker = _gat._getTracker("UA-10656779-1");
      pageTracker._trackPageview();
      } catch(err) {}</script> <script>
       (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
       
      ga('create', 'UA-76464546-1', 'auto');
      ga('send', 'pageview');
      ga('set', 'anonymizeIp', true);
      ga('require', 'linkid', 'linkid.js');
      
      </script> </div> </body> </html>