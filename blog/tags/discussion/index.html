<!DOCTYPE html> <html lang="en"> <head> <title>Debezium Blog</title> <meta charset="utf-8"> <meta content="width=device-width, initial-scale=1.0" name="viewport"> <meta content="" name="description"> <meta content="" name="author"> <link href="https://static.jboss.org/theme/css/bootstrap-community/3.2.0.2/bootstrap-community.min.css" media="screen" rel="stylesheet"> <!--[if lt IE 9]><script src="https://static.jboss.org/theme/js/libs/html5/pre3.6/html5.min.js"></script><![endif]--> <link href="https://static.jboss.org/example/images/favicon.ico" rel="shortcut icon"> <link href="https://static.jboss.org/example/images/apple-touch-icon-144x144-precomposed.png" rel="apple-touch-icon-precomposed" sizes="144x144"> <link href="https://static.jboss.org/example/images/apple-touch-icon-114x114-precomposed.png" rel="apple-touch-icon-precomposed" sizes="114x114"> <link href="https://static.jboss.org/example/images/apple-touch-icon-72x72-precomposed.png" rel="apple-touch-icon-precomposed" sizes="72x72"> <link href="https://static.jboss.org/example/images/apple-touch-icon-precomposed.png" rel="apple-touch-icon-precomposed"> <link href="/stylesheets/debezium.css" rel="stylesheet" type="text/css"> <style>
      @media (min-width: 980px) {
        .banner { background-image: url(https://static.jboss.org/example/images/debezium-banner-1180px.png); height: 110px;  }
      }
      @media (max-width: 979px) {
        .banner { background-image: url(https://static.jboss.org/example/images/debezium-logo.png); background-repeat:no-repeat; background-position: center bottom; height: 60px; }
      }
      @media (max-width: 650px) {
        .banner { width: 100%; margin: 0px auto; }
      }
      @media (max-width: 450px) {
        .banner { height: 90px; }
      }
    </style> <link href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/styles/default.min.css" rel="stylesheet"> <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/highlight.min.js"></script> <script>
      hljs.initHighlightingOnLoad();
    </script> <script src="https://static.jboss.org/theme/js/libs/jquery/jquery-1.9.1.min.js"></script> <style>
      /* adjusting the vertical spacing for when a stickynav is engaged */
      .breadcrumb-fixed > .active {
        color: #8c8f91;
      }
      .breadcrumb-fixed {
        margin: 70px 0 10px;
        padding: 8px 15px;
        margin-bottom: 20px;
        list-style: none;
        background-color: #f5f5f5;
        border-radius: 4px;
      }
      
      .breadcrumb-fixed > li {
        display: inline-block;
      }
    </style> </head> <body> <div id="rhbar"> <a class="jbdevlogo" href="https://www.jboss.org/projects/about"></a> <a class="rhlogo" href="https://www.redhat.com/"></a> </div> <div id=""> <ul class="visuallyhidden" id="top"> <li> <a accesskey="n" href="#nav" title="Skip to navigation">Skip to navigation</a> </li> <li> <a accesskey="c" href="#page" title="Skip to content">Skip to content</a> </li> </ul> <div class="container" id="content"> <div class="navbar navbar-inverse navbar-fix"> <div class="container-fluid"> <div class="navbar-header"> <button class="navbar-toggle collapsed" data-target="#navbar-1" data-toggle="collapse"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button> <a class="navbar-brand" href="/"> Debezium </a> </div> <div class="collapse navbar-collapse" id="navbar-1"> <ul class="nav navbar-nav pull-right"> <li class=""><a href="/docs/faq">FAQ</a></li> <li class=""><a href="/docs">DOCS</a></li> <li class=""><a href="/community">COMMUNITY</a></li> <li class="active"><a href="/blog">BLOG</a></li> </ul> </div> </div> </div> <div id="equalHeightsLayout"> <div class="row post-text-padding row-no-expand"> <div class="hidden-xs col-sm-3 no-right-padding" id="leftdocnav"> <div class="panel-docnav"> <div class="panel-heading"> <h3 class="panel-title"> Latest posts </h3> </div> <div class="panel-body"> <ul class="list-group"> <li class="list-group-item"> <a href="/blog/2018/10/04/debezium-0-9-0-alpha2-released/" rel="tooltip" title="Click to go to post">Debezium 0.9.0.Alpha2 Released</a> </li> <li class="list-group-item"> <a href="/blog/2018/09/20/materializing-aggregate-views-with-hibernate-and-debezium/" rel="tooltip" title="Click to go to post">Materializing Aggregate Views With Hibernate and Debezium</a> </li> <li class="list-group-item"> <a href="/blog/2018/09/19/debezium-0-8-3-final-released/" rel="tooltip" title="Click to go to post">Debezium 0.8.3.Final Released</a> </li> <li class="list-group-item"> <a href="/blog/2018/08/30/streaming-mysql-data-changes-into-kinesis/" rel="tooltip" title="Click to go to post">Streaming MySQL Data Changes to Amazon Kinesis</a> </li> <li class="list-group-item"> <a href="/blog/2018/08/30/debezium-0-8-2-released/" rel="tooltip" title="Click to go to post">Debezium 0.8.2 Released</a> </li> <li class="list-group-item"> <a href="/blog/2018/07/26/debezium-0-9-0-alpha1-released/" rel="tooltip" title="Click to go to post">Debezium 0.9 Alpha1 and 0.8.1 Released</a> </li> <li class="list-group-item"> <a href="/blog/2018/07/19/advantages-of-log-based-change-data-capture/" rel="tooltip" title="Click to go to post">Five Advantages of Log-Based Change Data Capture</a> </li> <li class="list-group-item"> <a href="/blog/2018/07/12/debezium-0-8-0-final-released/" rel="tooltip" title="Click to go to post">Debezium 0.8 Final Is Released</a> </li> </ul> </div> </div> </div> <div class="col-xs-12 col-sm-9" id="maincol"> <div class="text-right"> <h3> Subscribe <a class="rss" href="/blog.atom"> <i class="icon-rss"></i> </a> </h3> </div> <hr> <div class="post"> <h1 class="title"> <a href="/blog/2018/09/20/materializing-aggregate-views-with-hibernate-and-debezium/">Materializing Aggregate Views With Hibernate and Debezium</a> </h1> <div class="byline"> <p> <em> September 20, 2018 by Gunnar Morling </em> <em> under&nbsp; </em> <a class="label label-info" href="/blog/tags/discussion/">discussion</a> <a class="label label-info" href="/blog/tags/examples/">examples</a> </p> </div> <div id="preamble"> <div class="sectionbody"> <div class="paragraph"> <p>Updating external full text search indexes (e.g. <a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a>) after data changes is a very popular use case for change data capture (CDC).</p> </div> <div class="paragraph"> <p>As we&#8217;ve discussed in a <a href="/blog/2018/01/17/streaming-to-elasticsearch/">blog post</a> a while ago, the combination of Debezium&#8217;s CDC source connectors and Confluent&#8217;s <a href="https://docs.confluent.io/current/connect/connect-elasticsearch/docs/index.html">sink connector for Elasticsearch</a> makes it straight forward to capture data changes in MySQL, Postgres etc. and push them towards Elastisearch in near real-time. This results in a 1:1 relationship between tables in the source database and a corresponding search index in Elasticsearch, which is perfectly fine for many use cases.</p> </div> <div class="paragraph"> <p>It gets more challenging though if you&#8217;d like to put entire aggregates into a single index. An example could be a customer and all their addresses; those would typically be stored in two separate tables in an RDBMS, linked by a foreign key, whereas you&#8217;d like to have just one index in Elasticsearch, containing documents of customers with their addresses embedded, allowing you to efficiently search for customers based on their address.</p> </div> <div class="paragraph"> <p>Following up to the <a href="/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/">KStreams-based solution</a> to this we described recently, we&#8217;d like to present in this post an alternative for materializing such aggregate views driven by the application layer.</p> </div> </div> </div> <div class="sect1"> <h2 id="overview"><a class="anchor" href="#overview"></a>Overview</h2> <div class="sectionbody"> <div class="paragraph"> <p>The idea is to materialize views in a separate table in the source database, right in the moment the original data is altered.</p> </div> <div class="paragraph"> <p>Aggregates are serialized as JSON structures (which naturally can represent any nested object structure) and stored in a specific table. This is done within the actual transaction altering the data, which means the aggregate view is always consistent with the primary data. In particular this approach isn&#8217;t prone to exposing intermediary aggregations as the KStreams-based solution discussed in the post linked above.</p> </div> <div class="paragraph"> <p>The following picture shows the overall architecture:</p> </div> <img src="/images/jpa_aggregations.png" style="max-width:100%; margin-bottom:10px;" class="responsive-image" alt="Streaming Materialized Aggregate Views to Elastisearch"> <div class="paragraph"> <p>Here the aggregate views are materialized by means of a small extension to <a href="http://hibernate.org/orm/">Hibernate ORM</a>, which stores the JSON aggregates within the source database (note "aggregate views" can be considered conceptually the same as "materialized views" as known from different RDBMS, as in that they materialize the result of a "join" operation, but technically we&#8217;re not using the latter to store aggregate views, but a regular table). Changes to that aggregate table are then captured by Debezium and streamed to one topic per aggregate type. The Elasticsearch sink connector can subscribe to these topics and update corresponding full-text indexes.</p> </div> <div class="paragraph"> <p>You can find a proof-of-concept implementation (said Hibernate extension and related code) of this idea in our <a href="https://github.com/debezium/debezium-examples/tree/master/jpa-aggregations">examples repository</a>. Of course the general idea isn&#8217;t limited to Hibernate ORM or JPA, you could implement something similar with any other API you&#8217;re using to access your data.</p> </div> </div> </div> <div class="sect1"> <h2 id="creating_aggregate_views_via_hibernate_orm"><a class="anchor" href="#creating_aggregate_views_via_hibernate_orm"></a>Creating Aggregate Views via Hibernate ORM</h2> <div class="sectionbody"> <div class="paragraph"> <p>For the following let&#8217;s assume we&#8217;re persisting a simple domain model (comprising a <code>Customer</code> entity and a few related ones like <code>Address</code>, (customer) <code>Category</code> etc.) in a database. Using Hibernate for that allows us to make the creation of aggregates fully transparent to the actual application code using a <a href="http://docs.jboss.org/hibernate/orm/current/userguide/html_single/Hibernate_User_Guide.html#events-events">Hibernate event listener</a>. Thanks to its extensible architecture, we can plug such listener into Hibernate just by adding it to the classpath, from where it will be picked up automatically when bootstrapping the entity manager / session factory.</p> </div> <div class="paragraph"> <p>Our example listener reacts to an annotation, <code>@MaterializeAggregate</code>, which marks those entity types that should be the roots of materialized aggregates.</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@Entity&#x000A;@MaterializeAggregate(aggregateName="customers-complete")&#x000A;public class Customer {&#x000A;&#x000A;    @Id&#x000A;    private long id;&#x000A;&#x000A;    private String firstName;&#x000A;&#x000A;    @OneToMany(mappedBy = "customer", fetch = FetchType.EAGER, cascade = CascadeType.ALL)&#x000A;    private Set&lt;Address&gt; addresses;&#x000A;&#x000A;    @ManyToOne&#x000A;    private Category category;&#x000A;&#x000A;    ...&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>Now if any entity annotated with <code>@MaterializeAggregate</code> is inserted, updated or deleted via Hibernate, the listener will kick in and materialize a JSON view of the aggregate root (customer) and its associated entities (addresses, category).</p> </div> <div class="paragraph"> <p>Under the hood the <a href="https://github.com/FasterXML/jackson">Jackson API</a> is used for serializing the model into JSON. This means you can use any of its annotations to customize the JSON output, e.g. <code>@JsonIgnore</code> to exclude the inverse relationship from <code>Address</code> to <code>Customer</code>:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@Entity&#x000A;public class Address {&#x000A;&#x000A;    @Id&#x000A;    private long id;&#x000A;&#x000A;    @ManyToOne&#x000A;    @JoinColumn(name = "customer_id")&#x000A;    @JsonIgnore&#x000A;    private Customer customer;&#x000A;&#x000A;    private String street;&#x000A;&#x000A;    private String city;&#x000A;&#x000A;    ...&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>Note that <code>Address</code> itself isn&#8217;t marked with <code>@MaterializeAggregate</code>, i.e. it won&#8217;t be materialized into an aggregate view by itself.</p> </div> <div class="paragraph"> <p>After using JPA&#8217;s <code>EntityManager</code> to insert or update a few customers, let&#8217;s take a look at the <code>aggregates</code> table which has been populated by the listener (value schema omitted for the sake of brevity):</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-sql" data-lang="sql">&gt; select * from aggregates;&#x000A;&#x000A;| rootType | keySchema | rootId | materialization | valueSchema |&#x000A;&#x000A;| customers-complete&#x000A;&#x000A;| {&#x000A;  "schema" : {&#x000A;    "type" : "struct",&#x000A;    "fields" : [ {&#x000A;      "type" : "int64",&#x000A;      "optional" : false,&#x000A;      "field" : "id"&#x000A;    } ],&#x000A;    "optional" : false,&#x000A;    "name" : "customers-complete.Key"&#x000A;  }&#x000A;}&#x000A;&#x000A;| { "id" : 1004 }&#x000A;&#x000A;| { "schema" : { ... } }&#x000A;&#x000A;| {&#x000A;  "id" : 1004,&#x000A;  "firstName" : "Anne",&#x000A;  "lastName" : "Kretchmar",&#x000A;  "email" : "annek@noanswer.org",&#x000A;  "tags" : [ "long-term", "vip" ],&#x000A;  "birthday" : 5098,&#x000A;  "category" : {&#x000A;    "id" : 100001,&#x000A;    "name" : "Retail"&#x000A;  },&#x000A;  "addresses" : [ {&#x000A;    "id" : 16,&#x000A;    "street" : "1289 University Hill Road",&#x000A;    "city" : "Canehill",&#x000A;    "state" : "Arkansas",&#x000A;    "zip" : "72717",&#x000A;    "type" : "SHIPPING"&#x000A;  } ]&#x000A;} |</code></pre> </div> </div> <div class="paragraph"> <p>The table contains these columns:</p> </div> <div class="ulist"> <ul> <li> <p><code>rootType</code>: The name of the aggregate as given in the <code>@MaterializeAggregate</code> annotation</p> </li> <li> <p><code>rootId</code>: The aggregate&#8217;s id as serialized JSON</p> </li> <li> <p><code>materialization</code>: The aggregate itself as serialized JSON; in this case a customer and their addresses, category etc.</p> </li> <li> <p><code>keySchema</code>: The Kafka Connect schema of the row&#8217;s key</p> </li> <li> <p><code>valueSchema</code>: The Kafka Connect schema of the materialization</p> </li> </ul> </div> <div class="paragraph"> <p>Let&#8217;s talk about the two schema columns for a bit. JSON itself is quite limited as far as its supported data types are concerned. So for instance we&#8217;d loose information about a numeric field&#8217;s value range (int vs. long etc.) without any additional information. Therefore the listener derives the corresponding schema information for key and aggregate view from the entity model and stores it within the aggregate records.</p> </div> <div class="paragraph"> <p>Now Jackson itself only supports JSON Schema, which would be a bit too limited for our purposes. Hence the example implementation provides custom serializers for Jackson&#8217;s schema system, which allow us to emit Kafka Connect&#8217;s schema representation (with more precise type information) instead of plain JSON Schema. This will come in handy in the following when we&#8217;d like to expand the string-based JSON representations of key and value into properly typed Kafka Connect records.</p> </div> </div> </div> <div class="sect1"> <h2 id="capturing_changes_to_the_aggregate_table"><a class="anchor" href="#capturing_changes_to_the_aggregate_table"></a>Capturing Changes to the Aggregate Table</h2> <div class="sectionbody"> <div class="paragraph"> <p>We now have a mechanism in place which transparently persists aggregates into a separate table within the source database, whenever the application data is changed through Hibernate. Note that this happens within the boundaries of the source transaction, so if the same would be rolled back for some reason, also the aggregate view would not be updated.</p> </div> <div class="paragraph"> <p>The Hibernate listener uses insert-or-update semantics when writing an aggregate view, i.e. for a given aggregate root there&#8217;ll always be exactly one corresponding entry in the aggregate table which reflects its current state. If an aggregate root entity is deleted, the listener will also drop the entry from the aggregate table.</p> </div> <div class="paragraph"> <p>So let&#8217;s set up Debezium now to capture any changes to the <code>aggregates</code> table:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">curl -i -X POST \&#x000A;  -H "Accept:application/json" \&#x000A;  -H "Content-Type:application/json" \&#x000A;  http://localhost:8083/connectors/ -d @- &lt;&lt;-EOF&#x000A;  {&#x000A;      "name": "inventory-connector",&#x000A;      "config": {&#x000A;          "connector.class": "io.debezium.connector.mysql.MySqlConnector",&#x000A;          "tasks.max": "1",&#x000A;          "database.hostname": "mysql",&#x000A;          "database.port": "3306",&#x000A;          "database.user": "debezium",&#x000A;          "database.password": "dbz",&#x000A;          "database.server.id": "184054",&#x000A;          "database.server.name": "dbserver1",&#x000A;          "database.whitelist": "inventory",&#x000A;          "table.whitelist": ".*aggregates",&#x000A;          "database.history.kafka.bootstrap.servers": "kafka:9092",&#x000A;          "database.history.kafka.topic": "schema-changes.inventory"&#x000A;      }&#x000A;  }&#x000A;EOF</code></pre> </div> </div> <div class="paragraph"> <p>This registers the MySQL connector with the "inventory" database (we&#8217;re using an expanded version of the schema from the <a href="/docs/tutorial">Debezium tutorial</a>), capturing any changes to the "aggregates" table.</p> </div> </div> </div> <div class="sect1"> <h2 id="expanding_json"><a class="anchor" href="#expanding_json"></a>Expanding JSON</h2> <div class="sectionbody"> <div class="paragraph"> <p>If we now were to browse the corresponding Kafka topic, we&#8217;d see data change events in the known Debezium format for all the changes to the <code>aggregates</code> table.</p> </div> <div class="paragraph"> <p>The "materialization" field with the records' "after" state still is a single field containing a JSON string, though. What we&#8217;d rather like to have is a strongly typed Kafka Connect record, whose schema exactly describes the aggregate structure and the types of its fields. For that purpose the example project provides an SMT (single message transform) which takes the JSON materialization and the corresponding <code>valueSchema</code> and converts this into a full-blown Kafka Connect record. The same is done for keys. DELETE events are rewritten into tombstone events. Finally, the SMT re-routes every record to a topic named after the aggregate root, allowing consumers to subscribe just to changes to specific aggregate types.</p> </div> <div class="paragraph"> <p>So let&#8217;s add that SMT when registering the Debezium CDC connector:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">...&#x000A;"transforms":"expandjson",&#x000A;"transforms.expandjson.type":"io.debezium.aggregation.smt.ExpandJsonSmt",&#x000A;...</code></pre> </div> </div> <div class="paragraph"> <p>When now browsing the "customers-complete" topic, we&#8217;ll see the strongly typed Kafka Connect records we&#8217;d expect:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">{&#x000A;    "schema": {&#x000A;        "type": "struct",&#x000A;        "fields": [&#x000A;            {&#x000A;                "type": "int64",&#x000A;                "optional": false,&#x000A;                "field": "id"&#x000A;            }&#x000A;        ],&#x000A;        "optional": false,&#x000A;        "name": "customers-complete.Key"&#x000A;    },&#x000A;    "payload": {&#x000A;        "id": 1004&#x000A;    }&#x000A;}&#x000A;{&#x000A;    "schema": {&#x000A;        "type": "struct",&#x000A;        "fields": [ ... ],&#x000A;        "optional": true,&#x000A;        "name": "urn:jsonschema:com:example:domain:Customer"&#x000A;    },&#x000A;    "payload": {&#x000A;        "id": 1004,&#x000A;        "firstName": "Anne",&#x000A;        "lastName": "Kretchmar",&#x000A;        "email": "annek@noanswer.org",&#x000A;        "active": true,&#x000A;        "tags" : [ "long-term", "vip" ],&#x000A;        "birthday" : 5098,&#x000A;        "category": {&#x000A;            "id": 100001,&#x000A;            "name": "Retail"&#x000A;        },&#x000A;        "addresses": [&#x000A;            {&#x000A;                "id": 16,&#x000A;                "street": "1289 University Hill Road",&#x000A;                "city": "Canehill",&#x000A;                "state": "Arkansas",&#x000A;                "zip": "72717",&#x000A;                "type": "LIVING"&#x000A;            }&#x000A;        ]&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>To confirm that these are actual typed Kafka Connect records and not just a single JSON string field, you could for instance use the <a href="/docs/configuration/avro/">Avro message converter</a> and examine the message schemas in the schema registry.</p> </div> </div> </div> <div class="sect1"> <h2 id="sinking_aggregate_messages_into_elasticsearch"><a class="anchor" href="#sinking_aggregate_messages_into_elasticsearch"></a>Sinking Aggregate Messages Into Elasticsearch</h2> <div class="sectionbody"> <div class="paragraph"> <p>The last missing step is to register the Confluent Elasticsearch sink connector, hooking it up with the "customers-complete" topic and letting it push any changes to the corresponding index:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">curl -i -X POST \&#x000A;  -H "Accept:application/json" \&#x000A;  -H "Content-Type:application/json" \&#x000A;  http://localhost:8083/connectors/ -d @- &lt;&lt;-EOF&#x000A;  {&#x000A;      "name": "es-customers",&#x000A;      "config": {&#x000A;          "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",&#x000A;          "tasks.max": "1",&#x000A;          "topics": "customers-complete",&#x000A;          "connection.url": "http://elastic:9200",&#x000A;          "key.ignore": "false",&#x000A;          "schema.ignore" : "false",&#x000A;          "behavior.on.null.values" : "delete",&#x000A;          "type.name": "customer-with-addresses",&#x000A;          "transforms" : "key",&#x000A;          "transforms.key.type": "org.apache.kafka.connect.transforms.ExtractField$Key",&#x000A;          "transforms.key.field": "id"&#x000A;      }&#x000A;  }&#x000A;EOF</code></pre> </div> </div> <div class="paragraph"> <p>This uses Connect&#8217;s <code>ExtractField</code> transformation to obtain just the actual id value from the key struct and use it as key for the corresponding Elasticsearch documents. Specifying the "behavior.on.null.values" option will let the connector delete the corresponding document from the index when encountering a tombstone message (i.e. a message with a key but without value).</p> </div> <div class="paragraph"> <p>Finally, we can use the Elasticsearch REST API to browse the index and of course use its powerful full-text query language to find customers by the address or any other property embedded into the aggregate structure:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">&gt; curl -X GET -H "Accept:application/json" \&#x000A;  http://localhost:9200/customers-complete/_search?pretty&#x000A;&#x000A;  {&#x000A;      "_shards": {&#x000A;          "failed": 0,&#x000A;          "successful": 5,&#x000A;          "total": 5&#x000A;      },&#x000A;      "hits": {&#x000A;          "hits": [&#x000A;              {&#x000A;                  "_id": "1004",&#x000A;                  "_index": "customers-complete",&#x000A;                  "_score": 1.0,&#x000A;                  "_source": {&#x000A;                      "active": true,&#x000A;                      "addresses": [&#x000A;                          {&#x000A;                              "city": "Canehill",&#x000A;                              "id": 16,&#x000A;                              "state": "Arkansas",&#x000A;                              "street": "1289 University Hill Road",&#x000A;                              "type": "LIVING",&#x000A;                              "zip": "72717"&#x000A;                          }&#x000A;                      ],&#x000A;                      "tags" : [ "long-term", "vip" ],&#x000A;                      "birthday" : 5098,&#x000A;                      "category": {&#x000A;                          "id": 100001,&#x000A;                          "name": "Retail"&#x000A;                      },&#x000A;                      "email": "annek@noanswer.org",&#x000A;                      "firstName": "Anne",&#x000A;                      "id": 1004,&#x000A;                      "lastName": "Kretchmar",&#x000A;                      "scores": [],&#x000A;                      "someBlob": null,&#x000A;                      "tags": []&#x000A;                  },&#x000A;                  "_type": "customer-with-addresses"&#x000A;              }&#x000A;          ],&#x000A;          "max_score": 1.0,&#x000A;          "total": 1&#x000A;      },&#x000A;      "timed_out": false,&#x000A;      "took": 11&#x000A;  }</code></pre> </div> </div> <div class="paragraph"> <p>And there you have it: a customer&#8217;s complete data, including their addresses, categories, tags etc., materialized into a single document within Elasticsearch. If you&#8217;re using JPA to update the customer, you&#8217;ll see the data in the index being updated accordingly in near-realtime.</p> </div> </div> </div> <div class="sect1"> <h2 id="pros_and_cons"><a class="anchor" href="#pros_and_cons"></a>Pros and Cons</h2> <div class="sectionbody"> <div class="paragraph"> <p>So what are the advantages and disadvantages of this approach for materializing aggregates from multiple source tables compared to the <a href="/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/">KStreams-based approach</a>?</p> </div> <div class="paragraph"> <p>The big advantage is consistency and awareness of transactional boundaries, whereas the KStreams-based solution in its suggested form was prone to exposing intermediary aggregates. For instance, if you&#8217;re storing a customer and three addresses, it might happen that the streaming query first creates an aggregation of the customer and the two addresses inserted first, and shortly thereafter the complete aggregate with all three addresses. This not the case for the approach discussed here, as you&#8217;ll only ever stream complete aggregates to Kafka. Also this approach feels a bit more "light-weight", i.e. a simple marker annotation (together with some Jackson annotations for fine-tuning the emitted JSON structures) is enough in order to materialize aggregates from your domain model, whereas some more effort was needed to set up the required streams, temporary tables etc. with the KStreams solution.</p> </div> <div class="paragraph"> <p>The downside of driving aggregations through the application layer is that it&#8217;s not fully agnostic to the way you access the primary data. If you bypass the application, e.g. by patching data directly in the database, naturally these updates would be missed, requiring a refresh of affected aggregates. Although this again could be done through change data capture and Debezium: change events to source tables could be captured and consumed by the application itself, allowing it to re-materialize aggregates after external data changes. You also might argue that running JSON serializations within source transactions and storing aggregates within the source database represents some overhead. This often may be acceptable, though.</p> </div> <div class="paragraph"> <p>Another question to ask is what&#8217;s the advantage of using change data capture on an intermediary aggregate table over simply posting REST requests to Elasticsearch. The answer is the highly increased robustness and fault tolerance. If the Elasticsearch cluster can&#8217;t be accessed for some reason, the machinery of Kafka and Kafka Connect will ensure that any change events will be propagated eventually, once the sink is up again. Also other consumers than Elasticsearch can subscribe to the aggregate topic, the log can be replayed from the beginning etc.</p> </div> <div class="paragraph"> <p>Note that while we&#8217;ve been talking primarily about using Elasticsearch as a data sink, there are also other datastores and connectors that support complexly structured records. One example would be MongoDB and the <a href="https://github.com/hpgrahsl/kafka-connect-mongodb">sink connector</a> maintained by Hans-Peter Grahsl, which one could use to sink customer aggregates into MongoDB, for instance enabling efficient retrieval of a customer and all their associated data with a single primary key look-up.</p> </div> </div> </div> <div class="sect1"> <h2 id="outlook"><a class="anchor" href="#outlook"></a>Outlook</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Hibernate ORM extension as well as the SMT discussed in this post can be found in our <a href="https://github.com/debezium/debezium-examples/tree/master/jpa-aggregations">examples repository</a>. They should be considered to be at "proof-of-concept" level currently.</p> </div> <div class="paragraph"> <p>That being said, we&#8217;re considering to make this a Debezium component proper, allowing you to employ this aggregation approach within your Hibernate-based applications just by pulling in this new component. For that we&#8217;d have to improve a few things first, though. Most importantly, an API is needed which will let you (re-)create aggregates on demand, e.g. for existing data or for data updated by bulk updates via the Criteria API / JPQL (which will be missed by listeners). Also aggregates should be re-created automatically, if any of the referenced entities change (with the current PoC, only a change to the customer instance itself will trigger its aggregate view to be rebuilt, but not a change to one of its addresses).</p> </div> <div class="paragraph"> <p>If you like this idea, then let us know about it, so we can gauge the general interest in this. Also, this would be a great item to work on, if you&#8217;re interested in contributing to the Debezium project. Looking forward to hearing from you, e.g. in the comment section below or on our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a>.</p> </div> <div class="paragraph"> <p>Thanks a lot to Hans-Peter Grahsl for his feedback on an earlier version of this post!</p> </div> </div> </div> <div class="sect1"> <h2 id="about_debezium"><a class="anchor" href="#about_debezium"></a>About Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of <a href="http://kafka.apache.org/">Kafka</a> and provides <a href="http://kafka.apache.org/documentation.html#connect">Kafka Connect</a> compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is <a href="/license">open source</a> under the <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, Version 2.0</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="get_involved"><a class="anchor" href="#get_involved"></a>Get involved</h2> <div class="sectionbody"> <div class="paragraph"> <p>We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter <a href="https://twitter.com/debezium">@debezium</a>, <a href="https://gitter.im/debezium/user">chat with us on Gitter</a>, or join our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> to talk with the community. All of the code is open source <a href="https://github.com/debezium/">on GitHub</a>, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or <a href="https://issues.jboss.org/projects/DBZ/issues/">log an issue</a>.</p> </div> </div> </div> </div> <hr> <div class="post"> <h1 class="title"> <a href="/blog/2018/08/30/streaming-mysql-data-changes-into-kinesis/">Streaming MySQL Data Changes to Amazon Kinesis</a> </h1> <div class="byline"> <p> <em> August 30, 2018 by Gunnar Morling </em> <em> under&nbsp; </em> <a class="label label-info" href="/blog/tags/discussion/">discussion</a> <a class="label label-info" href="/blog/tags/examples/">examples</a> </p> </div> <div id="preamble"> <div class="sectionbody"> <div class="paragraph"> <p>Most of the times Debezium is used to stream data changes into <a href="http://kafka.apache.org/">Apache Kafka</a>. What though if you&#8217;re using another streaming platform such as <a href="https://pulsar.incubator.apache.org/">Apache Pulsar</a> or a cloud-based solution such as <a href="https://aws.amazon.com/kinesis/">Amazon Kinesis</a>, <a href="https://azure.microsoft.com/services/event-hubs/">Azure Event Hubs</a> and the like? Can you still benefit from Debezium&#8217;s powerful change data capture (CDC) capabilities and ingest changes from databases such as MySQL, Postgres, SQL Server etc.?</p> </div> <div class="paragraph"> <p>Turns out, with just a bit of glue code, you can! In the following we&#8217;ll discuss how to use Debezium to capture changes in a MySQL database and stream the change events into Kinesis, a fully-managed data streaming service available on the Amazon cloud.</p> </div> </div> </div> <div class="sect1"> <h2 id="introducing_the_debezium_embedded_engine"><a class="anchor" href="#introducing_the_debezium_embedded_engine"></a>Introducing the Debezium Embedded Engine</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is implemented as a set of connectors for Kafka and thus usually is run via <a href="https://kafka.apache.org/documentation/#connect">Kafka Connect</a>. But there&#8217;s one little gem in Debezium which isn&#8217;t as widely known yet, which is the <a href="/docs/embedded/">embedded engine</a>.</p> </div> <div class="paragraph"> <p>When using this engine, the Debezium connectors are not executed within Kafka Connect, but as a library embedded into your own Java application. For this purpose, the <em>debezium-embedded</em> module provides a small runtime environment which performs the tasks that&#8217;d otherwise be handled by the Kafka Connect framework: requesting change records from the connector, committing offsets etc. Each change record produced by the connector is passed to a configured event handler method, which in our case will convert the record into its JSON representation and submit it to a Kinesis stream, using the Kinesis Java API.</p> </div> <div class="paragraph"> <p>The overall architecture looks like so:</p> </div> <img src="/images/debezium-embedded.png" style="max-width:100%; margin-bottom:10px;" class="responsive-image" alt="Debezium Embedded Engine Streaming to Amazon Kinesis"> <div class="paragraph"> <p>Now let&#8217;s walk through the relevant parts of the code required for that. A complete executable example can be found in the <a href="https://github.com/debezium/debezium-examples/tree/master/kinesis">debezium-examples</a> repo on GitHub.</p> </div> </div> </div> <div class="sect1"> <h2 id="set_up"><a class="anchor" href="#set_up"></a>Set-Up</h2> <div class="sectionbody"> <div class="paragraph"> <p>In order to use Debezium&#8217;s embedded engine, add the <em>debezium-embedded</em> dependency as well as the Debezium connector of your choice to your project&#8217;s <em>pom.xml</em>. In the following we&#8217;re going to use the connector for MySQL. We also need to add a dependency to the <a href="https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/kinesis/package-summary.html">Kinesis Client API</a>, so these are the dependencies needed:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-xml" data-lang="xml">...&#x000A;&lt;dependency&gt;&#x000A;    &lt;groupId&gt;io.debezium&lt;/groupId&gt;&#x000A;    &lt;artifactId&gt;debezium-embedded&lt;/artifactId&gt;&#x000A;    &lt;version&gt;0.8.3.Final&lt;/version&gt;&#x000A;&lt;/dependency&gt;&#x000A;&lt;dependency&gt;&#x000A;    &lt;groupId&gt;io.debezium&lt;/groupId&gt;&#x000A;    &lt;artifactId&gt;debezium-connector-mysql&lt;/artifactId&gt;&#x000A;    &lt;version&gt;0.8.3.Final&lt;/version&gt;&#x000A;&lt;/dependency&gt;&#x000A;&lt;dependency&gt;&#x000A;    &lt;groupId&gt;com.amazonaws&lt;/groupId&gt;&#x000A;    &lt;artifactId&gt;amazon-kinesis-client&lt;/artifactId&gt;&#x000A;    &lt;version&gt;1.9.0&lt;/version&gt;&#x000A;&lt;/dependency&gt;&#x000A;...</code></pre> </div> </div> </div> </div> <div class="sect1"> <h2 id="configuring_the_embedded_engine"><a class="anchor" href="#configuring_the_embedded_engine"></a>Configuring the Embedded Engine</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Debezium embedded engine is configured through an instance of <code>io.debezium.config.Configuration</code>. This class can obtain values from system properties or from a given config file, but for the sake of the example we&#8217;ll simply pass all required values via its fluent builder API:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">Configuration config = Configuration.create()&#x000A;    .with(EmbeddedEngine.CONNECTOR_CLASS, "io.debezium.connector.mysql.MySqlConnector")&#x000A;    .with(EmbeddedEngine.ENGINE_NAME, "kinesis")&#x000A;    .with(MySqlConnectorConfig.SERVER_NAME, "kinesis")&#x000A;    .with(MySqlConnectorConfig.SERVER_ID, 8192)&#x000A;    .with(MySqlConnectorConfig.HOSTNAME, "localhost")&#x000A;    .with(MySqlConnectorConfig.PORT, 3306)&#x000A;    .with(MySqlConnectorConfig.USER, "debezium")&#x000A;    .with(MySqlConnectorConfig.PASSWORD, "dbz")&#x000A;    .with(MySqlConnectorConfig.DATABASE_WHITELIST, "inventory")&#x000A;    .with(MySqlConnectorConfig.TABLE_WHITELIST, "inventory.customers")&#x000A;    .with(EmbeddedEngine.OFFSET_STORAGE,&#x000A;        "org.apache.kafka.connect.storage.MemoryOffsetBackingStore")&#x000A;    .with(MySqlConnectorConfig.DATABASE_HISTORY,&#x000A;        MemoryDatabaseHistory.class.getName())&#x000A;    .with("schemas.enable", false)&#x000A;    .build();</code></pre> </div> </div> <div class="paragraph"> <p>If you&#8217;ve ever set up the Debezium MySQL connector in Kafka Connect, most of the properties will look familiar to you.</p> </div> <div class="paragraph"> <p>But let&#8217;s talk about the <code>OFFSET_STORAGE</code> and <code>DATABASE_HISTORY</code> options in a bit more detail. They deal with how connector offsets and the database history should be persisted. When running the connector via Kafka Connect, both would typically be stored in specific Kafka topics. But that&#8217;s not an option here, so an alternative is needed. For this example we&#8217;re simply going to keep the offsets and database history in memory. I.e. if the engine is restarted, this information will be lost and the connector will start from scratch, e.g. with a new initial snapshot.</p> </div> <div class="paragraph"> <p>While out of scope for this blog post, it wouldn&#8217;t be too difficult to create alternative implementations of the <code>OffsetBackingStore</code> and <code>DatabaseHistory</code> contracts, respectively. For instance if you&#8217;re fully committed into the AWS cloud services, you could think of storing offsets and database history in the DynamoDB NoSQL store. Note that, different from Kafka, a Kinesis stream wouldn&#8217;t be suitable for storing the database history. The reason being, that the maximum retention period for Kinesis data streams is seven days, whereas the database history must be kept for the entire lifetime of the connector. Another alternative could be to use the existing filesystem based implementations <code>FileOffsetBackingStore</code> and <code>FileDatabaseHistory</code>, respectively.</p> </div> <div class="paragraph"> <p>The next step is to build an <code>EmbeddedEngine</code> instance from the configuration. Again this is done using a fluent API:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">EmbeddedEngine engine = EmbeddedEngine.create()&#x000A;    .using(config)&#x000A;    .using(this.getClass().getClassLoader())&#x000A;    .using(Clock.SYSTEM)&#x000A;    .notifying(this::sendRecord)&#x000A;    .build();</code></pre> </div> </div> <div class="paragraph"> <p>The most interesting part here is the <code>notifying</code> call. The method passed here is the one which will be invoked by the engine for each emitted data change record. So let&#8217;s take a look at the implementation of this method.</p> </div> </div> </div> <div class="sect1"> <h2 id="sending_change_records_to_kinesis"><a class="anchor" href="#sending_change_records_to_kinesis"></a>Sending Change Records to Kinesis</h2> <div class="sectionbody"> <div class="paragraph"> <p>The <code>sendRecord()</code> method is where the magic happens. We&#8217;ll convert the incoming <code>SourceRecord</code> into an equivalent JSON representation and propagate it to a Kinesis stream.</p> </div> <div class="paragraph"> <p>For that, it&#8217;s important to understand some conceptual differences between Apache Kafka and Kinesis. Specifically, messages in Kafka have a <em>key</em> and a <em>value</em> (which both are arbitrary byte arrays). In case of Debezium, the key of data change events represents the primary key of the affected record and the value is a structure comprising of old and new row state as well as some additional metadata.</p> </div> <div class="paragraph"> <p>In Kinesis on the other hand a message contains a <em>data blob</em> (again an arbitrary byte sequence) and a <em>partition key</em>. Kinesis streams can be split up into multiple shards and the partition key is used to determine into which shard a given message should go.</p> </div> <div class="paragraph"> <p>Now one could think of mapping the key from Debezium&#8217;s change data events to the Kinesis partition key, but partition keys are limited to a length of 256 bytes. Depending on the length of primary key column(s) in the captured tables, this might not be enough. So a safer option is to create a hash value from the change message key and use that as the partition key. This in turn means that the change message key structure should be added next to the actual value to the Kinesis message&#8217;s data blob. While the key column values themselves are part of the value structure, too, a consumer otherwise wouldn&#8217;t know which column(s) make up the primary key.</p> </div> <div class="paragraph"> <p>With that in mind, let&#8217;s take a look at the <code>sendRecord()</code> implementation:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">private void sendRecord(SourceRecord record) {&#x000A;    // We are interested only in data events not schema change events&#x000A;    if (record.topic().equals("kinesis")) {&#x000A;        return;&#x000A;    }&#x000A;&#x000A;    // create schema for container with key *and* value&#x000A;    Schema schema = SchemaBuilder.struct()&#x000A;        .field("key", record.keySchema())&#x000A;        .field("value", record.valueSchema())&#x000A;        .build();&#x000A;&#x000A;    Struct message = new Struct(schema);&#x000A;    message.put("key", record.key());&#x000A;    message.put("value", record.value());&#x000A;&#x000A;    // create partition key by hashing the record's key&#x000A;    String partitionKey = String.valueOf(&#x000A;        record.key() != null ? record.key().hashCode() : -1);&#x000A;&#x000A;    // create data blob representing the container by using Kafka Connect's&#x000A;    // JSON converter&#x000A;    final byte[] payload = valueConverter.fromConnectData(&#x000A;        "dummy", schema, message);&#x000A;&#x000A;    // Assemble the put-record request ...&#x000A;    PutRecordRequest putRecord = new PutRecordRequest();&#x000A;&#x000A;    putRecord.setStreamName(record.topic());&#x000A;    putRecord.setPartitionKey(partitionKey);&#x000A;    putRecord.setData(ByteBuffer.wrap(payload));&#x000A;&#x000A;    // ... and execute it&#x000A;    kinesisClient.putRecord(putRecord);&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>The code is quite straight-forward; as discussed above it&#8217;s first creating a container structure containing key <em>and</em> value of the incoming source record. This structure then is converted into a binary representation using the JSON converter provided by Kafka Connect (an instance of <code>JsonConverter</code>). Then a <code>PutRecordRequest</code> is assembled from that blob, the partition key and the change record&#8217;s topic name, which finally is sent to Kinesis.</p> </div> <div class="paragraph"> <p>The Kinesis client object can be re-used and is set up once like so:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">// Uses the credentials from the local "default" AWS profile&#x000A;AWSCredentialsProvider credentialsProvider =&#x000A;    new ProfileCredentialsProvider("default");&#x000A;&#x000A;this.kinesisClient = AmazonKinesisClientBuilder.standard()&#x000A;    .withCredentials(credentialsProvider)&#x000A;    .withRegion("eu-central-1") // use your AWS region here&#x000A;    .build();</code></pre> </div> </div> <div class="paragraph"> <p>With that, we&#8217;ve set up an instance of Debezium&#8217;s <code>EmbeddedEngine</code> which runs the configured MySQL connector and passes each emitted change event to Amazon Kinesis. The last missing step is to actually run the engine. This is done on a separate thread using an <code>Executor</code>, e.g. like so:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">ExecutorService executor = Executors.newSingleThreadExecutor();&#x000A;executor.execute(engine);</code></pre> </div> </div> <div class="paragraph"> <p>Note you also should make sure to properly shut down the engine eventually. How that can be done <a href="https://github.com/debezium/debezium-examples/blob/master/kinesis/src/main/java/io/debezium/examples/kinesis/ChangeDataSender.java#L83-L88">is shown</a> in the accompanying example in the <em>debezium-examples</em> repo.</p> </div> </div> </div> <div class="sect1"> <h2 id="running_the_example"><a class="anchor" href="#running_the_example"></a>Running the Example</h2> <div class="sectionbody"> <div class="paragraph"> <p>Finally let&#8217;s take a look at running the complete example and consuming the Debezium CDC events from the Kinesis stream. Start by cloning the examples repository and go to the <em>kinesis</em> directory:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">git clone https://github.com/debezium/debezium-examples.git&#x000A;cd debezium-examples/kinesis</code></pre> </div> </div> <div class="paragraph"> <p>Make sure you&#8217;ve met the <a href="https://github.com/debezium/debezium-examples/tree/master/kinesis#prerequisites">prerequisites</a> described in the example&#8217;s <em>README.md</em>; most notably you should have a local Docker installation and you&#8217;ll need to have set up an AWS account as well as have the AWS client tools installed. Note that Kinesis isn&#8217;t part of the free tier when registering with AWS, i.e. you&#8217;ll pay a (small) amount of money when executing the example. Don&#8217;t forget to delete the streams you&#8217;ve set up once done, we won&#8217;t pay your AWS bills :)</p> </div> <div class="paragraph"> <p>Now run Debezium&#8217;s MySQL example database to have some data to play with:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">docker run -it --rm --name mysql -p 3306:3306 \&#x000A;  -e MYSQL_ROOT_PASSWORD=debezium \&#x000A;  -e MYSQL_USER=mysqluser \&#x000A;  -e MYSQL_PASSWORD=mysqlpw \&#x000A;  debezium/example-mysql:0.8</code></pre> </div> </div> <div class="paragraph"> <p>Create a Kinesis stream for change events from the <code>customers</code> table:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">aws kinesis create-stream --stream-name kinesis.inventory.customers \&#x000A;  --shard-count 1</code></pre> </div> </div> <div class="paragraph"> <p>Execute the Java application that runs the Debezium embedded engine (if needed, adjust the value of the <code>kinesis.region</code> property in <em>pom.xml</em> to your own region first):</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">mvn exec:java</code></pre> </div> </div> <div class="paragraph"> <p>This will start up the engine and the MySQL connector, which takes an initial snapshot of the captured database.</p> </div> <div class="paragraph"> <p>In order to take a look at the CDC events in the Kinesis stream, the AWS CLI can be used (usually, you&#8217;d implement a Kinesis Streams application for consuming the events). To do so, set up a <a href="https://docs.aws.amazon.com/streams/latest/dev/developing-consumers-with-sdk.html#kinesis-using-sdk-java-get-data-shard-iterators">shard iterator</a> first:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">ITERATOR=$(aws kinesis get-shard-iterator --stream-name kinesis.inventory.customers --shard-id 0 --shard-iterator-type TRIM_HORIZON | jq '.ShardIterator')</code></pre> </div> </div> <div class="paragraph"> <p>Note how the <a href="https://stedolan.github.io/jq/">jq</a> utility is used to obtain the generated id of the iterator from the JSON structure returned by the Kinesis API. Next that iterator can be used to examine the stream:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">aws kinesis get-records --shard-iterator $ITERATOR</code></pre> </div> </div> <div class="paragraph"> <p>You should receive an array of records like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">{&#x000A;    "Records": [&#x000A;        {&#x000A;            "SequenceNumber":&#x000A;                "49587760482547027816046765529422807492446419903410339842",&#x000A;            "ApproximateArrivalTimestamp": 1535551896.475,&#x000A;            "Data": "eyJiZWZvcm...4OTI3MzN9",&#x000A;            "PartitionKey": "eyJpZCI6MTAwMX0="&#x000A;        },&#x000A;        ...&#x000A;    ]&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>The <code>Data</code> element is a Base64-encoded representation of the message&#8217;s data blob. Again <em>jq</em> comes in handy: we can use it to just extract the <code>Data</code> part of each record and decode the Base64 representation (make sure to use jq 1.6 or newer):</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">aws kinesis get-records --shard-iterator $ITERATOR | \&#x000A;  jq -r '.Records[].Data | @base64d' | jq .</code></pre> </div> </div> <div class="paragraph"> <p>Now you should see the change events as JSON, each one with key and value:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">{&#x000A;  "key": {&#x000A;    "id": 1001&#x000A;  },&#x000A;  "value": {&#x000A;    "before": null,&#x000A;    "after": {&#x000A;      "id": 1001,&#x000A;      "first_name": "Sally",&#x000A;      "last_name": "Thomas",&#x000A;      "email": "sally.thomas@acme.com"&#x000A;    },&#x000A;    "source": {&#x000A;      "version": "0.8.1.Final",&#x000A;      "name": "kinesis",&#x000A;      "server_id": 0,&#x000A;      "ts_sec": 0,&#x000A;      "gtid": null,&#x000A;      "file": "mysql-bin.000003",&#x000A;      "pos": 154,&#x000A;      "row": 0,&#x000A;      "snapshot": true,&#x000A;      "thread": null,&#x000A;      "db": "inventory",&#x000A;      "table": "customers",&#x000A;      "query": null&#x000A;    },&#x000A;    "op": "c",&#x000A;    "ts_ms": 1535555325628&#x000A;  }&#x000A;}&#x000A;...</code></pre> </div> </div> <div class="paragraph"> <p>Next let&#8217;s try and update a record in MySQL:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell"># Start MySQL CLI client&#x000A;docker run -it --rm --name mysqlterm --link mysql --rm mysql:5.7 \&#x000A;  sh -c 'exec mysql -h"$MYSQL_PORT_3306_TCP_ADDR" \&#x000A;  -P"$MYSQL_PORT_3306_TCP_PORT" -uroot -p"$MYSQL_ENV_MYSQL_ROOT_PASSWORD"'&#x000A;&#x000A;# In the MySQL client&#x000A;use inventory;&#x000A;update customers set first_name = 'Trudy' where id = 1001;</code></pre> </div> </div> <div class="paragraph"> <p>If you now fetch the iterator again, you should see one more data change event representing that update:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">...&#x000A;&#x000A;{&#x000A;  "key": {&#x000A;    "id": 1001&#x000A;  },&#x000A;  "value": {&#x000A;    "before": {&#x000A;      "id": 1001,&#x000A;      "first_name": "Sally",&#x000A;      "last_name": "Thomas",&#x000A;      "email": "sally.thomas@acme.com"&#x000A;    },&#x000A;    "after": {&#x000A;      "id": 1001,&#x000A;      "first_name": "Trudy",&#x000A;      "last_name": "Thomas",&#x000A;      "email": "sally.thomas@acme.com"&#x000A;    },&#x000A;    "source": {&#x000A;      "version": "0.8.1.Final",&#x000A;      "name": "kinesis",&#x000A;      "server_id": 223344,&#x000A;      "ts_sec": 1535627629,&#x000A;      "gtid": null,&#x000A;      "file": "mysql-bin.000003",&#x000A;      "pos": 364,&#x000A;      "row": 0,&#x000A;      "snapshot": false,&#x000A;      "thread": 10,&#x000A;      "db": "inventory",&#x000A;      "table": "customers",&#x000A;      "query": null&#x000A;    },&#x000A;    "op": "u",&#x000A;    "ts_ms": 1535627622546&#x000A;  }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>Once you&#8217;re done, stop the embedded engine application by hitting Ctrl + C, stop the MySQL server by running <code>docker stop mysql</code> and delete the <em>kinesis.inventory.customers</em> stream in Kinesis.</p> </div> </div> </div> <div class="sect1"> <h2 id="summary_and_outlook"><a class="anchor" href="#summary_and_outlook"></a>Summary and Outlook</h2> <div class="sectionbody"> <div class="paragraph"> <p>In this blog post we&#8217;ve demonstrated that Debezium cannot only be used to stream data changes into Apache Kafka, but also into other streaming platforms such as Amazon Kinesis. Leveraging its embedded engine and by implementing a bit of glue code, you can benefit from <a href="/docs/connectors/">all the CDC connectors</a> provided by Debezium and their capabilities and connect them to the streaming solution of your choice.</p> </div> <div class="paragraph"> <p>And we&#8217;re thinking about even further simplifying this usage of Debezium. Instead of requiring you to implement your own application that invokes the embedded engine API, we&#8217;re considering to provide a small self-contained Debezium runtime which you can simply execute. It&#8217;d be configured with the source connector to run and make use of an outbound plug-in SPI with ready-to-use implementations for Kinesis, Apache Pulsar and others. Of course such runtime would also provide suitable implementations for safely persisting offsets and database history, and it&#8217;d offer means of monitoring, health checks etc. Meaning you could connect the Debezium source connectors with your preferred streaming platform in a robust and reliable way, without any manual coding required!</p> </div> <div class="paragraph"> <p>If you like this idea, then please check out JIRA issue <a href="https://issues.jboss.org/browse/DBZ-651">DBZ-651</a> and let us know about your thoughts, e.g. by leaving a comment on the issue, in the comment section below or on our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="about_debezium"><a class="anchor" href="#about_debezium"></a>About Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of <a href="http://kafka.apache.org/">Kafka</a> and provides <a href="http://kafka.apache.org/documentation.html#connect">Kafka Connect</a> compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is <a href="/license">open source</a> under the <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, Version 2.0</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="get_involved"><a class="anchor" href="#get_involved"></a>Get involved</h2> <div class="sectionbody"> <div class="paragraph"> <p>We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter <a href="https://twitter.com/debezium">@debezium</a>, <a href="https://gitter.im/debezium/user">chat with us on Gitter</a>, or join our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> to talk with the community. All of the code is open source <a href="https://github.com/debezium/">on GitHub</a>, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or <a href="https://issues.jboss.org/projects/DBZ/issues/">log an issue</a>.</p> </div> </div> </div> </div> <hr> <div class="post"> <h1 class="title"> <a href="/blog/2018/07/19/advantages-of-log-based-change-data-capture/">Five Advantages of Log-Based Change Data Capture</a> </h1> <div class="byline"> <p> <em> July 19, 2018 by Gunnar Morling </em> <em> under&nbsp; </em> <a class="label label-info" href="/blog/tags/discussion/">discussion</a> </p> </div> <div id="preamble"> <div class="sectionbody"> <div class="paragraph"> <p>Yesterday I had the opportunity to present Debezium and the idea of change data capture (CDC) to the <a href="https://twitter.com/JUG_DA/status/1019634941020332032">Darmstadt Java User Group</a>. It was a great evening with lots of interesting discussions and questions. One of the questions being the following: what is the advantage of using a log-based change data capturing tool such as Debezium over simply polling for updated records?</p> </div> <div class="paragraph"> <p>So first of all, what&#8217;s the difference between the two approaches? With polling-based (or query-based) CDC you repeatedly run queries (e.g. via JDBC) for retrieving any newly inserted or updated rows from the tables to be captured. Log-based CDC in contrast works by reacting to any changes to the database&#8217;s log files (e.g. MySQL&#8217;s binlog or MongoDB&#8217;s op log).</p> </div> <div class="paragraph"> <p>As this wasn&#8217;t the first time this question came up, I thought I could provide a more extensive answer also here on the blog. That way I&#8217;ll be able to refer to this post in the future, should the question come up again :)</p> </div> <div class="paragraph"> <p>So without further ado, here&#8217;s my list of five advantages of log-based CDC over polling-based approaches.</p> </div> <div class="dlist"> <dl> <dt class="hdlist1">All Data Changes Are Captured</dt> <dd> <p>By reading the database&#8217;s log, you get the complete list of all data changes in their exact order of application. This is vital for many use cases where you are interested in the complete history of record changes. In contrast, with a polling-based approach you might miss intermediary data changes that happen between two runs of the poll loop. For instance it could happen that a record is inserted and deleted between two polls, in which case this record would never be captured by poll-based CDC.</p> <div class="paragraph"> <p>Related to this is the aspect of downtimes, e.g. when updating the CDC tool. With poll-based CDC, only the latest state of a given record would be captured once the CDC tool is back online, missing any earlier changes to the record that occurred during the downtime. A log-based CDC tool will be able to resume reading the database log from the point where it left off before it was shut down, causing the complete history of data changes to be captured.</p> </div> </dd> <dt class="hdlist1">Low Delays of Events While Avoiding Increased CPU Load</dt> <dd> <p>With polling, you might be tempted to increase the frequency of polling attempts in order to reduce the chances of missing intermediary updates. While this works to some degree, polling too frequently may cause performance issues (as the queries used for polling cause load on the source database). On the other hand, expanding the polling interval will reduce the CPU load but may not only result in missed change events but also in a longer delay for propagating data changes. Log-based CDC allows you to react to data changes in near real-time without paying the price of spending CPU time on running polling queries repeatedly.</p> </dd> <dt class="hdlist1">No Impact on Data Model</dt> <dd> <p>Polling requires some indicator to identify those records that have been changed since the last poll. So all the captured tables need to have some column like <code>LAST_UPDATE_TIMESTAMP</code> which can be used to find changed rows. This can be fine in some cases, but in others such requirement might not be desirable. Specifically, you&#8217;ll need to make sure that the update timestamps are maintained correctly on all tables to be captured by the writing applications or e.g. through triggers.</p> </dd> <dt class="hdlist1">Can Capture Deletes</dt> <dd> <p>Naturally, polling will not allow you to identify any records that have been deleted since the last poll. Often times that&#8217;s a problem for replication-like use cases where you&#8217;d like to have an identical data set on the source database and the replication targets, meaning you&#8217;d also like to delete records on the sink side if they have been removed in the source database.</p> </dd> <dt class="hdlist1">Can Capture Old Record State And Further Meta Data</dt> <dd> <p>Depending on the source database&#8217;s capabilities, log-based CDC can provide the old record state for update and delete events. Whereas with polling, you&#8217;ll only get the current row state. Having the old row state handy in a single change event can be interesting for many use cases, e.g. if you&#8217;d like to display the complete data change with old and new column values to an application user for auditing purposes.</p> <div class="paragraph"> <p>In addition, log-based approaches often can provide streams of schema changes (e.g. in form of applied DDL statements) and expose additional metadata such as transaction ids or the user applying a certain change. These things may generally be doable with query-based approaches, too (depending on the capabilities of the database), I haven&#8217;t really seen it being done in practice, though.</p> </div> </dd> </dl> </div> </div> </div> <div class="sect1"> <h2 id="summary"><a class="anchor" href="#summary"></a>Summary</h2> <div class="sectionbody"> <div class="paragraph"> <p>And that&#8217;s it, five advantages of log-based change data capture. Note that this is not to say that polling-based CDC doesn&#8217;t have its applications. If for instance your use case can be satisfied by propagating changes once per hour and it&#8217;s not a problem to miss intermediary versions of records that were valid in between, it can be perfectly fine.</p> </div> <div class="paragraph"> <p>But if you&#8217;re interested in capturing data changes in near real-time, making sure you don&#8217;t miss any change events (including deletions), then I&#8217;d recommend very much to explore the possibilities of log-based CDC as enabled by Debezium. The Debezium connectors do all the heavy-lifting for you, i.e. you don&#8217;t have to deal with all the low-level specifics of the individual databases and the means of getting changes from their logs. Instead, you can consume the generic and largely unified change data events produced by Debezium.</p> </div> </div> </div> <div class="sect1"> <h2 id="about_debezium"><a class="anchor" href="#about_debezium"></a>About Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of <a href="http://kafka.apache.org/">Kafka</a> and provides <a href="http://kafka.apache.org/documentation.html#connect">Kafka Connect</a> compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is <a href="/license">open source</a> under the <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, Version 2.0</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="get_involved"><a class="anchor" href="#get_involved"></a>Get involved</h2> <div class="sectionbody"> <div class="paragraph"> <p>We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter <a href="https://twitter.com/debezium">@debezium</a>, <a href="https://gitter.im/debezium/user">chat with us on Gitter</a>, or join our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> to talk with the community. All of the code is open source <a href="https://github.com/debezium/">on GitHub</a>, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or <a href="https://issues.jboss.org/projects/DBZ/issues/">log an issue</a>.</p> </div> </div> </div> </div> <hr> <div class="post"> <h1 class="title"> <a href="/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/">Creating DDD aggregates with Debezium and Kafka Streams</a> </h1> <div class="byline"> <p> <em> March 08, 2018 by Hans-Peter Grahsl, Gunnar Morling </em> <em> under&nbsp; </em> <a class="label label-info" href="/blog/tags/discussion/">discussion</a> <a class="label label-info" href="/blog/tags/examples/">examples</a> </p> </div> <div id="preamble"> <div class="sectionbody"> <div class="paragraph"> <p>Microservice-based architectures can be considered an industry trend and are thus often found in enterprise applications lately. One possible way to keep data synchronized across multiple services and their backing data stores is to make us of an approach called <a href="https://vladmihalcea.com/a-beginners-guide-to-cdc-change-data-capture/">change data capture</a>, or CDC for short.</p> </div> <div class="paragraph"> <p>Essentially CDC allows to listen to any modifications which are occurring at one end of a data flow (i.e. the data source) and communicate them as change events to other interested parties or storing them into a data sink. Instead of doing this in a point-to-point fashion, it&#8217;s advisable to decouple this flow of events between data sources and data sinks. Such a scenario can be implemented based on <a href="http://debezium.io/">Debezium</a> and <a href="https://kafka.apache.org/">Apache Kafka</a> with relative ease and effectively no coding.</p> </div> <div class="paragraph"> <p>As an example, consider the following microservice-based architecture of an order management system:</p> </div> <div class="imageblock centered-image"> <div class="content"> <img src="/images/msa_streaming.png" alt="Microservice-based architecture of an order management system"> </div> </div> <div class="paragraph"> <p>This system comprises three services, <em>Order</em>, <em>Item</em> and <em>Stock</em>. If the <em>Order</em> service receives an order request, it will need information from the other two, such as item definitions or the stock count for specific items. Instead of making synchronous calls to these services to obtain this information, CDC can be used to set up change event streams for the data managed by the <em>Item</em> and <em>Stock</em> services. The <em>Order</em> service can subscribe to these event streams and keep a local copy of the relevant item and stock data in its own database. This approach helps to decouple the services (e.g. no direct impact by service outages) and can also be beneficial for overall performance, as each service can hold optimized views just of those data items owned by other services which it is interested in.</p> </div> </div> </div> <div class="sect1"> <h2 id="how_to_handle_aggregate_objects"><a class="anchor" href="#how_to_handle_aggregate_objects"></a>How to Handle Aggregate Objects?</h2> <div class="sectionbody"> <div class="paragraph"> <p>There are use cases however, where things are a bit more tricky. It is sometimes useful to share information across services and data stores by means of so-called aggregates, which are a concept/pattern defined by domain-driven design (DDD). In general, a <a href="https://martinfowler.com/bliki/DDD_Aggregate.html">DDD aggregate</a> is used to transfer state which can be comprised of multiple different domain objects that are together treated as a single unit of information.</p> </div> <div class="paragraph"> <p>Concrete examples are:</p> </div> <div class="ulist"> <ul> <li> <p><strong>customers and their addresses</strong> which are represented as a customer record <em>aggregate</em> storing a customer and a list of addresses</p> </li> <li> <p><strong>orders and corresponding line items</strong> which are represented as an order record <em>aggregate</em> storing an order and all its line items</p> </li> </ul> </div> <div class="paragraph"> <p>Chances are that the data of the involved domain objects backing these DDD aggregates are stored in separate relations of an RDBMS. When making use of the CDC capabilities currently found in Debezium, all changes to domain objects will be independently captured and by default eventually reflected in separate Kafka topics, one per RDBMS relation. While this behaviour is tremendously helpful for a lot of use cases it can be pretty limiting to others, like the DDD aggregate scenario described above. Therefore, this blog post explores how DDD aggregates can be built based on Debezium CDC events, using the <a href="https://kafka.apache.org/documentation/streams/">Kafka Streams API</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="capturing_change_events_from_a_data_source"><a class="anchor" href="#capturing_change_events_from_a_data_source"></a>Capturing Change Events from a Data Source</h2> <div class="sectionbody"> <div class="paragraph"> <p>The complete source code for this blog post is provided in the Debezium <a href="https://github.com/debezium/debezium-examples/tree/master/kstreams">examples repository</a> on GitHub. Begin by cloning this repository and changing into the <em>kstreams</em> directory:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">git clone https://github.com/debezium/debezium-examples.git&#x000A;cd kstreams</code></pre> </div> </div> <div class="paragraph"> <p>The project provides a Docker Compose file with services for all the components you may already know from the <a href="/docs/tutorial/">Debezium tutorial</a>:</p> </div> <div class="ulist"> <ul> <li> <p><a href="https://zookeeper.apache.org/">Apache ZooKeeper</a></p> </li> <li> <p><a href="https://kafka.apache.org/">Apache Kafka</a></p> </li> <li> <p>A <a href="https://kafka.apache.org/documentation/#connect">Kafka Connect</a> instance with the Debezium CDC connectors</p> </li> <li> <p><a href="http://www.mysql.com/">MySQL</a> (populated with some test data)</p> </li> </ul> </div> <div class="paragraph"> <p>In addition it declares the following services:</p> </div> <div class="ulist"> <ul> <li> <p><a href="http://www.mongodb.com/">MongoDB</a> which will be used as a data sink</p> </li> <li> <p>Another Kafka Connect instance which will host the MongoDB sink connector</p> </li> <li> <p>A service for running the DDD aggregation process we&#8217;re going to build in the following</p> </li> </ul> </div> <div class="paragraph"> <p>We&#8217;ll get to those three in a bit, for now let&#8217;s prepare the source side of our pipeline:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">export DEBEZIUM_VERSION=0.7&#x000A;docker-compose up mysql zookeeper kafka connect_source</code></pre> </div> </div> <div class="paragraph"> <p>Once all services have been started, register an instance of the Debezium MySQL connector by submitting the following JSON document:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">{&#x000A;    "name": "mysql-source",&#x000A;    "config": {&#x000A;        "connector.class": "io.debezium.connector.mysql.MySqlConnector",&#x000A;        "tasks.max": "1",&#x000A;        "database.hostname": "mysql",&#x000A;        "database.port": "3306",&#x000A;        "database.user": "debezium",&#x000A;        "database.password": "dbz",&#x000A;        "database.server.id": "184054",&#x000A;        "database.server.name": "dbserver1",&#x000A;        "table.whitelist": "inventory.customers,inventory.addresses",&#x000A;        "database.history.kafka.bootstrap.servers": "kafka:9092",&#x000A;        "database.history.kafka.topic": "schema-changes.inventory",&#x000A;        "transforms": "unwrap",&#x000A;        "transforms.unwrap.type":"io.debezium.transforms.UnwrapFromEnvelope",&#x000A;        "transforms.unwrap.drop.tombstones":"false"&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>To do so, run the following curl command:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">curl -i -X POST -H "Accept:application/json" -H  "Content-Type:application/json" http://localhost:8083/connectors/ -d @mysql-source.json</code></pre> </div> </div> <div class="paragraph"> <p>This sets up the connector for the specified database, using the given credentials. For our purposes we&#8217;re only interested in changes to the <code>customers</code> and <code>addresses</code> tables, hence the <code>table.whitelist</code> property is given to just select these two tables. Another noteworthy thing is the "unwrap" transform that is applied. By default, Debezium&#8217;s CDC events would contain the old and new state of changed rows and some additional metadata on the source of the change. By applying the <a href="/docs/configuration/event-flattening/">UnwrapFromEnvelope</a> SMT (single message transformation), only the new state will be propagated into the corresponding Kafka topics.</p> </div> <div class="paragraph"> <p>We can take a look at them once the connector has been deployed and finished its initial snapshot of the two captured tables:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">docker-compose exec kafka /kafka/bin/kafka-console-consumer.sh \&#x000A;    --bootstrap-server kafka:9092 \&#x000A;    --from-beginning \&#x000A;    --property print.key=true \&#x000A;    --topic dbserver1.inventory.customers # or dbserver1.inventory.addresses</code></pre> </div> </div> <div class="paragraph"> <p>E.g. you should see the following output</p> </div> <div class="paragraph"> <p>(formatted and omitting the schema information for the sake of readability) for the topic with customer changes:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">{&#x000A;    "schema": { ... },&#x000A;    "payload": {&#x000A;        "id": 1001&#x000A;    }&#x000A;}&#x000A;{&#x000A;    "schema": { ... },&#x000A;    "payload": {&#x000A;        "id": 1001,&#x000A;        "first_name": "Sally",&#x000A;        "last_name": "Thomas",&#x000A;        "email": "sally.thomas@acme.com"&#x000A;    }&#x000A;}&#x000A;...</code></pre> </div> </div> </div> </div> <div class="sect1"> <h2 id="building_ddd_aggregates"><a class="anchor" href="#building_ddd_aggregates"></a>Building DDD Aggregates</h2> <div class="sectionbody"> <div class="paragraph"> <p>The KStreams application is going to process data from the two Kafka topics. These topics receive CDC events based on the customers and addresses relations found in MySQL, each of which has its corresponding Jackson-annotated POJO (Customer and Address), enriched by a field holding the CDC event type (i.e. UPSERT/DELETE).</p> </div> <div class="paragraph"> <p>Since the Kafka topic records are in Debezium JSON format with unwrapped envelopes, a special <strong>SerDe</strong> has been written in order to be able to read/write these records using their POJO or Debezium event representation respectively. While the serializer simply converts the POJOs into JSON using Jackson, the deserializer is a "hybrid" one, being able to deserialize from either Debezium CDC events or jsonified POJOs.</p> </div> <div class="paragraph"> <p>With that in place, the KStreams topology to create and maintain DDD aggregates on-the-fly can be built as follows:</p> </div> <div class="sect2"> <h3 id="customers_topic_parent"><a class="anchor" href="#customers_topic_parent"></a>Customers Topic ("parent")</h3> <div class="paragraph"> <p>All the customer records are simply read from the customer topic into a <strong>KTable</strong> which will automatically maintain the latest state per customer according to the record key (i.e. the customer&#8217;s PK)</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">KTable&lt;DefaultId, Customer&gt; customerTable =&#x000A;        builder.table(parentTopic, Consumed.with(defaultIdSerde,customerSerde));</code></pre> </div> </div> </div> <div class="sect2"> <h3 id="addresses_topic_children"><a class="anchor" href="#addresses_topic_children"></a>Addresses Topic ("children")</h3> <div class="paragraph"> <p>For the address records the processing is a bit more involved and needs several steps. First, all the address records are read into a <strong>KStream</strong>.</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">KStream&lt;DefaultId, Address&gt; addressStream = builder.stream(childrenTopic,&#x000A;        Consumed.with(defaultIdSerde, addressSerde));</code></pre> </div> </div> <div class="paragraph"> <p>Second, a 'pseudo' grouping of these address records is done based on their keys (the original primary key in the relation), During this step the relationships towards the corresponding customer records are maintained. This effectively allows to keep track which address record belongs to which customer record, even in the light of address record deletions. To achieve this an additional <em>LatestAddress</em> POJO is introduced which allows to store the latest known PK &lt;&#8594; FK relation in addition to the <em>Address</em> record itself.</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">KTable&lt;DefaultId,LatestAddress&gt; tempTable = addressStream&#x000A;        .groupByKey(Serialized.with(defaultIdSerde, addressSerde))&#x000A;        .aggregate(&#x000A;                () -&gt; new LatestAddress(),&#x000A;                (DefaultId addressId, Address address, LatestAddress latest) -&gt; {&#x000A;                    latest.update(&#x000A;                        address, addressId, new DefaultId(address.getCustomer_id()));&#x000A;                    return latest;&#x000A;                },&#x000A;                Materialized.&lt;DefaultId,LatestAddress,KeyValueStore&lt;Bytes, byte[]&gt;&gt;&#x000A;                        as(childrenTopic+"_table_temp")&#x000A;                            .withKeySerde(defaultIdSerde)&#x000A;                                .withValueSerde(latestAddressSerde)&#x000A;        );</code></pre> </div> </div> <div class="paragraph"> <p>Third, the intermediate <strong>KTable</strong> is again converted to a <strong>KStream</strong>. The <em>LatestAddress</em> records are transformed to have the customer id (FK relationship) as their new key in order to group them per customer. During the grouping step, customer specific addresses are updated which can result in an address record being added or deleted. For this purpose, another POJO called <em>Addresses</em> is introduced, which holds a map of address records that gets updated accordingly. The result is a <strong>KTable</strong> holding the most recent <em>Addresses</em> per customer id.</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">KTable&lt;DefaultId, Addresses&gt; addressTable = tempTable.toStream()&#x000A;        .map((addressId, latestAddress) -&gt;&#x000A;            new KeyValue&lt;&gt;(latestAddress.getCustomerId(),latestAddress))&#x000A;        .groupByKey(Serialized.with(defaultIdSerde,latestAddressSerde))&#x000A;        .aggregate(&#x000A;                () -&gt; new Addresses(),&#x000A;                (customerId, latestAddress, addresses) -&gt; {&#x000A;                    addresses.update(latestAddress);&#x000A;                    return addresses;&#x000A;                },&#x000A;                Materialized.&lt;DefaultId,Addresses,KeyValueStore&lt;Bytes, byte[]&gt;&gt;&#x000A;                        as(childrenTopic+"_table_aggregate")&#x000A;                            .withKeySerde(defaultIdSerde)&#x000A;                                .withValueSerde(addressesSerde)&#x000A;        );</code></pre> </div> </div> </div> <div class="sect2"> <h3 id="combining_customers_with_addresses"><a class="anchor" href="#combining_customers_with_addresses"></a>Combining Customers With Addresses</h3> <div class="paragraph"> <p>Finally, it&#8217;s easy to bring customers and addresses together by <strong>joining the customers KTable with the addresses KTable</strong> and thereby building the DDD aggregates which are represented by the <em>CustomerAddressAggregate</em> POJO. At the end, the KTable changes are written to a KStream, which in turn gets saved into a kafka topic. This allows to make use of the resulting DDD aggregates in manifold ways.</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">KTable&lt;DefaultId,CustomerAddressAggregate&gt; dddAggregate =&#x000A;          customerTable.join(addressTable, (customer, addresses) -&gt;&#x000A;              customer.get_eventType() == EventType.DELETE ?&#x000A;                      null :&#x000A;                      new CustomerAddressAggregate(customer,addresses.getEntries())&#x000A;          );&#x000A;&#x000A;  dddAggregate.toStream().to("final_ddd_aggregates",&#x000A;                              Produced.with(defaultIdSerde,(Serde)aggregateSerde));</code></pre> </div> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>Records in the customers KTable might receive a CDC delete event. If so, this can be detected by checking the event type field of the customer POJO and e.g. return 'null' instead of a DDD aggregate. Such a convention can be helpful whenever consuming parties also need to act to deletions accordingly._</p> </div> </td> </tr> </table> </div> </div> </div> </div> <div class="sect1"> <h2 id="running_the_aggregation_pipeline"><a class="anchor" href="#running_the_aggregation_pipeline"></a>Running the Aggregation Pipeline</h2> <div class="sectionbody"> <div class="paragraph"> <p>Having implemented the aggregation pipeline, it&#8217;s time to give it a test run. To do so, build the <em>poc-ddd-aggregates</em> Maven project which contains the complete implementation:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">mvn clean package -f poc-ddd-aggregates/pom.xml</code></pre> </div> </div> <div class="paragraph"> <p>Then run the <code>aggregator</code> service from the Compose file which takes the JAR built by this project and launches it using the <a href="https://hub.docker.com/r/fabric8/java-jboss-openjdk8-jdk/">java-jboss-openjdk8-jdk</a> base image:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">docker-compose up -d aggregator</code></pre> </div> </div> <div class="paragraph"> <p>Once the aggregation pipeline is running, we can take a look at the aggregated events using the console consumer:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">docker-compose exec kafka /kafka/bin/kafka-console-consumer.sh \&#x000A;    --bootstrap-server kafka:9092 \&#x000A;    --from-beginning \&#x000A;    --property print.key=true \&#x000A;    --topic final_ddd_aggregates</code></pre> </div> </div> </div> </div> <div class="sect1"> <h2 id="transferring_ddd_aggregates_to_data_sinks"><a class="anchor" href="#transferring_ddd_aggregates_to_data_sinks"></a>Transferring DDD Aggregates to Data Sinks</h2> <div class="sectionbody"> <div class="paragraph"> <p>We originally set out to build these DDD aggregates in order to transfer data and synchronize changes between a data source (MySQL tables in this case) and a convenient data sink. By definition, DDD aggregates are typically complex data structures and therefore it makes perfect sense to write them to data stores which offer flexible ways and means to query and/or index them. Talking about NoSQL databases, a document store seems the most natural choice with <a href="https://www.mongodb.com/">MongoDB</a> being the leading database for such use cases.</p> </div> <div class="paragraph"> <p>Thanks to <a href="https://kafka.apache.org/documentation/#connect">Kafka Connect</a> and numerous turn-key ready <a href="https://www.confluent.io/product/connectors/">connectors</a> it is almost effortless to get this done. Using a <a href="https://github.com/hpgrahsl/kafka-connect-mongodb">MongoDB sink connector</a> from the open-source community, it is easy to have the DDD aggregates written into MongoDB. All it needs is a proper configuration which can be posted to the <a href="https://docs.confluent.io/current/connect/restapi.html">REST API</a> of Kafka Connect in order to run the connector.</p> </div> <div class="paragraph"> <p>So let&#8217;s start MongoDb and another Kafka Connect instance for hosting the sink connector:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">docker-compose up -d mongodb connect_sink</code></pre> </div> </div> <div class="paragraph"> <p>In case the DDD aggregates should get written unmodified into MongoDB, a configuration may look as simple as follows:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">{&#x000A;    "name": "mongodb-sink",&#x000A;    "config": {&#x000A;        "connector.class": "at.grahsl.kafka.connect.mongodb.MongoDbSinkConnector",&#x000A;        "tasks.max": "1",&#x000A;        "topics": "final_ddd_aggregates",&#x000A;        "mongodb.connection.uri": "mongodb://mongodb:27017/inventory?w=1&amp;journal=true",&#x000A;        "mongodb.collection": "customers_with_addresses",&#x000A;        "mongodb.document.id.strategy": "at.grahsl.kafka.connect.mongodb.processor.id.strategy.FullKeyStrategy",&#x000A;        "mongodb.delete.on.null.values": true&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>As with the source connector, deploy the connector using curl:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">curl -i -X POST -H "Accept:application/json" -H  "Content-Type:application/json" http://localhost:8084/connectors/ -d @mongodb-sink.json</code></pre> </div> </div> <div class="paragraph"> <p>This connector will consume messages from the "final_ddd_aggregates" Kafka topic and write them as <strong>MongoDB documents</strong> into the "customers_with_addresses" collection.</p> </div> <div class="paragraph"> <p>You can take a look by firing up a Mongo shell and querying the collection&#8217;s contents:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">docker-compose exec mongodb bash -c 'mongo inventory'&#x000A;&#x000A;&gt; db.customers_with_addresses.find().pretty()</code></pre> </div> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">{&#x000A;    "_id": {&#x000A;        "id": "1001"&#x000A;    },&#x000A;    "addresses": [&#x000A;        {&#x000A;            "zip": "76036",&#x000A;            "_eventType": "UPSERT",&#x000A;            "city": "Euless",&#x000A;            "street": "3183 Moore Avenue",&#x000A;            "id": "10",&#x000A;            "state": "Texas",&#x000A;            "customer_id": "1001",&#x000A;            "type": "SHIPPING"&#x000A;        },&#x000A;        {&#x000A;            "zip": "17116",&#x000A;            "_eventType": "UPSERT",&#x000A;            "city": "Harrisburg",&#x000A;            "street": "2389 Hidden Valley Road",&#x000A;            "id": "11",&#x000A;            "state": "Pennsylvania",&#x000A;            "customer_id": "1001",&#x000A;            "type": "BILLING"&#x000A;        }&#x000A;    ],&#x000A;    "customer": {&#x000A;        "_eventType": "UPSERT",&#x000A;        "last_name": "Thomas",&#x000A;        "id": "1001",&#x000A;        "first_name": "Sally",&#x000A;        "email": "sally.thomas@acme.com"&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>Due to the combination of the data in a single document some parts aren&#8217;t needed or redundant. To get rid of any unwanted data (e.g. _eventType, customer_id of each address sub-document) it would also be possible to adapt the configuration in order to blacklist said fields.</p> </div> <div class="paragraph"> <p>Finally, you update some customer or address data in the MySQL source database:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER -p$MYSQL_PASSWORD inventory'&#x000A;&#x000A;mysql&gt; update customers set first_name= "Sarah" where id = 1001;</code></pre> </div> </div> <div class="paragraph"> <p>Shortly thereafter, you should see that the corresponding aggregate document in MongoDB has been updated accordingly.</p> </div> </div> </div> <div class="sect1"> <h2 id="drawbacks_and_limitations"><a class="anchor" href="#drawbacks_and_limitations"></a>Drawbacks and Limitations</h2> <div class="sectionbody"> <div class="paragraph"> <p>While this first version for creating DDD aggregates from table-based CDC events basically works, it is very important to understand its current limitations:</p> </div> <div class="ulist"> <ul> <li> <p>not generically applicable thus needs custom code for POJOs and intermediate types</p> </li> <li> <p>cannot be scaled across multiple instances as is due to missing but necessary data repartitioning prior to processing</p> </li> <li> <p>limited to building aggregates based on a single JOIN between 1:N relationships</p> </li> <li> <p>resulting DDD aggregates are eventually consistent, meaning that it is possible for them to temporarily exhibit intermediate state before converging</p> </li> </ul> </div> <div class="paragraph"> <p>The first few can be addressed with a reasonable amount of work on the KStreams application. The last one, dealing with the eventually consistent nature of resulting DDD aggregates is much harder to correct and will require some efforts at Debezium&#8217;s own CDC mechanism.</p> </div> </div> </div> <div class="sect1"> <h2 id="outlook"><a class="anchor" href="#outlook"></a>Outlook</h2> <div class="sectionbody"> <div class="paragraph"> <p>In this post we described an approach for creating aggregated events from Debezium&#8217;s CDC events. In a follow-up blog post we may dive a bit more into the topic of how to be able to horizontally scale the DDD creation by running multiple KStreams aggregator instances. For that purpose, the data needs proper re-partitioning before running the topology. In addition, it could be interesting to look into a somewhat more generic version which only needs custom classes to the describe the two main POJOs involved.</p> </div> <div class="paragraph"> <p>We also thought about providing a ready-to-use component which would work in a generic way (based on Connect records, i.e. not tied to a specific serialization format such as JSON) and could be set up as a configurable stand-alone process running given aggregations.</p> </div> <div class="paragraph"> <p>Also on the topic of dealing with eventual consistency we got some ideas, but those will need some more exploration and investigation for sure. Stay tuned!</p> </div> <div class="paragraph"> <p>We&#8217;d love to hear about your feedback on the topic of event aggreation. If you got any ideas or thoughts on the subject, please get in touch by posting a comment below or sending a message to our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a>.</p> </div> </div> </div> </div> <hr> <ul class="pager"> <li class="disabled previous"> <a href="#">&laquo; Older</a> </li> <li class="pages">Page 1 of 1</li> <li class="disabled next"> <a href="#">Newer &raquo;</a> </li> </ul> </div> </div> </div> </div> <footer class="container"> <div class="row"> <div class="col-md-5 col-md-offset-1"> <h4>Debezium</h4> <p> &#169; 2018 Debezium Community <br> <br> <i class="icon-fire"></i> Mixed with <a href="http://twitter.github.com/bootstrap">Bootstrap</a>, baked by <a href="http://awestruct.org">Awestruct</a>. <br> <i class="icon-flag"></i> Website and docs licensed under <a href="http://creativecommons.org/licenses/by/3.0/">CC BY 3.0</a>. <br> <i class="icon-flag-alt"></i> Code released under <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, v2.0</a>. <br> <i class="icon-file-alt"></i> <a href="https://www.redhat.com/legal/legal_statement.html" title="Terms">Terms</a> | <a href="https://www.redhat.com/legal/privacy_statement.html" title="Privacy Policy">Privacy</a> </p> </div> <div class="col-md-3"> <h4>Documentation</h4> <ul class="list-unstyled"> <li> <a href="/docs/features" title="Features">Features</a> </li> <li> <a href="/docs/install" title="Install">Install</a> </li> <li> <a href="/docs/manage" title="Manage">Manage</a> </li> <li> <a href="/docs/architecture" title="Architecture">Architecture</a> </li> <li> <a href="/docs/faq" title="FAQ">FAQ</a> </li> <li> <a href="/docs/contribute" title="Contribute">Contribute</a> </li> </ul> </div> <div class="col-md-3"> <h4>Connect</h4> <ul class="list-unstyled"> <li> <a href="/blog" title="Blog">Blog</a> </li> <li> <a href="http://twitter.com/debezium" title="Twitter">Twitter</a> </li> <li> <a href="http://github.com/debezium" title="GitHub">GitHub</a> </li> <li> <a href="https://gitter.im/debezium/dev" title="Chat">Chat</a> </li> <li> <a href="https://groups.google.com/forum/#!forum/debezium" title="Google Groups">Google Groups</a> </li> <li> <a href="http://stackoverflow.com/questions/tagged/debezium" title="StackOverflow">StackOverflow</a> </li> </ul> </div> </div> </footer> <div class="container" id="companyfooter"> <div class="redhatlogo"> <div id="logospacer"></div> <a href="https://www.redhat.com/"><img src="https://static.jboss.org/theme/images/common/redhat_logo.png"></a> </div> </div> <span class="backToTop"> <a href="#top">back to top</a> </span> <script src="https://static.jboss.org/theme/js/libs/bootstrap-community/3.2.0.2/bootstrap-community.min.js"></script> <script type='text/javascript' language='JavaScript' src='https://www.redhat.com/j/elqNow/elqCfg.js'></script> <script type='text/javascript' language='JavaScript' src='https://www.redhat.com/j/elqNow/elqImg.js'></script> <div id="oTags"> <script type="text/javascript" src="//www.redhat.com/j/s_code.js"></script> <script type="text/javascript"><!--
        var coreUrl = encodeURI(document.URL.split("?")[0]).replace(/-/g," ");
        var urlSplit = coreUrl.toLowerCase().split(/\//);
        var urlLast = urlSplit[urlSplit.length-1];
        var pageNameString = "";
        var siteName = "";
        var minorSectionIndex = 3
        if (urlLast == "") {
            urlSplit.splice(-1,1);
        }
        if (urlLast.search(/\./) >= 0) {
            if (urlLast == "index.html") {
                urlSplit.splice(-1,1);
            }
            else {
                urlSplit[urlSplit.length-1] = urlLast.split(".").splice(0,1);
            }
        }
        siteName = urlSplit[2].split(".")[1];
        s.prop14 = s.eVar27 = siteName || "";
        s.prop15 = s.eVar28 = urlSplit[minorSectionIndex] || "";
        s.prop16 = s.eVar29 = urlSplit[minorSectionIndex+1] || "";
        pageNameString = urlSplit.splice(3).join(" | ");
        s.pageName = "jboss | community | " + siteName + " | " + pageNameString;
        s.server = "jboss";
        s.channel = "jboss | community";
        s.prop4 = s.eVar23 = encodeURI(document.URL);
        s.prop21 = s.eVar18 = coreUrl;
        s.prop2 = s.eVar22 = "en";
        s.prop3 = s.eVar19 = "us";
        //--></script> <script type="text/javascript" src="//www.redhat.com/j/rh_omni_footer.js"></script> <script language="JavaScript" type="text/javascript"><!--
        if(navigator.appVersion.indexOf('MSIE')>=0)document.write(unescape('%3C')+'\!-'+'-')
        //--></script> <noscript><a href="http://www.omniture.com" title="Web Analytics"><img src="https://smtrcs.redhat.com/b/ss/redhatcom,redhatglobal/1/H.25.4--NS/0?[AQB]&cdp=3&[AQE]" height="1" width="1" border="0" alt=""/></a></noscript> </div> <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
      document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
      </script> <script type="text/javascript">
      try {
      var pageTracker = _gat._getTracker("UA-10656779-1");
      pageTracker._trackPageview();
      } catch(err) {}</script> <script>
       (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
       
      ga('create', 'UA-76464546-1', 'auto');
      ga('send', 'pageview');
      ga('set', 'anonymizeIp', true);
      ga('require', 'linkid', 'linkid.js');
      
      </script> </div> </body> </html>