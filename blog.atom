<?xml version="1.0" encoding="utf-8" ?>
<feed xml:lang="en-US" xmlns="http://www.w3.org/2005/Atom">
<id>https://debezium.io/</id>
<title>Debezium Blog</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<link href="https://debezium.io/blog.atom" rel="self" type="application/atom+xml" />
<link href="https://debezium.io/" rel="alternate" type="text/html" />
<entry>
<id>https://debezium.io/blog/2020/04/16/debezium-1-2-alpha1-released/</id>
<title>Debezium 1.2.0.Alpha1 Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2020-04-16T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2020/04/16/debezium-1-2-alpha1-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="sqlserver"></category>
<category term="cassandra"></category>
<summary>



I&#8217;m very happy to announce the release of Debezium 1.2.0.Alpha1!


This first drop of the 1.2 release line provides a number of useful new features:




Support for message transformations (SMTs) and converters in the Debezium embedded engine API


A new SMT for filtering out change events using scripting languages


Automatic reconnects for the SQL Server connector


A new column masking mode using consistent hash values




Overall, the community fixed not less than 41 issues for this release.
Let&#8217;s take a closer look at some of them in the remainder of this post.




Embedded Engine Improvements


Debezium&#8217;s embedded engine is a very useful tool for handling change events in cases...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I’m very happy to announce the release of Debezium &lt;strong&gt;1.2.0.Alpha1&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This first drop of the 1.2 release line provides a number of useful new features:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Support for message transformations (SMTs) and converters in the Debezium embedded engine API&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A new SMT for filtering out change events using scripting languages&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Automatic reconnects for the SQL Server connector&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A new column masking mode using consistent hash values&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Overall, the community fixed not less than &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%201.2.0.Alpha1%20ORDER%20BY%20issuetype%20DESC&quot;&gt;41 issues&lt;/a&gt; for this release.
Let’s take a closer look at some of them in the remainder of this post.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;embedded_engine_improvements&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#embedded_engine_improvements&quot; /&gt;Embedded Engine Improvements&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium’s &lt;a href=&quot;https://debezium.io/documentation/reference/1.2/development/engine.html&quot;&gt;embedded engine&lt;/a&gt; is a very useful tool for handling change events in cases where Apache Kafka and Kafka Connect are not available.
For instance it allows to use Debezium’s CDC capabilities and stream change events to alternative messaging infrastructure such as Amazon Kinesis or Google Pub/Sub.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To further improve the experience when working with this API, it supports the serialization of change events into different formats now
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1807&quot;&gt;DBZ-1807&lt;/a&gt;): JSON, Avro and CloudEvents.
This spares developers from having to deal with record serialization themselves.
As an example, here is how to use JSON as serialization format:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;Properties props = new Properties();

// don&#39;t include schema in message
props.setProperty(&quot;converter.schemas.enable&quot;, &quot;false&quot;); &lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;(1)&lt;/b&gt;
// further properties as needed...

DebeziumEngine&amp;lt;ChangeEvent&amp;lt;String&amp;gt;&amp;gt; engine = DebeziumEngine.create(Json.class) &lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;(2)&lt;/b&gt;
    .using(props)
    .notifying((records, committer) -&amp;gt; { &lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;(3)&lt;/b&gt;
        for (ChangeEvent&amp;lt;String&amp;gt; r : records) {
            System.out.println(&quot;Key = &#39;&quot; + key + &quot;&#39; value = &#39;&quot; + value + &quot;&#39;&quot;);
            committer.markProcessed(r);
        }
    })
    .build();&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;colist arabic&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;All the options of the underlying converter can be used&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;Json.class&lt;/code&gt; is a type token requesting serialization into JSON&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;records&lt;/code&gt; is a batch of change events, represented as JSON strings&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The embedded engine now also supports the usage of Kafka Connect SMTs
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1930&quot;&gt;DBZ-1930&lt;/a&gt;).
They can be simply configured via the properties passed to the engine builder:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;Properties props = new Properties();

props.setProperty(&quot;transforms&quot;, &quot;router&quot;);
props.setProperty(&quot;transforms.router.type&quot;, &quot;org.apache.kafka.connect.transforms.RegexRouter&quot;);
props.setProperty(&quot;transforms.router.regex&quot;, &quot;(.*)&quot;);
props.setProperty(&quot;transforms.router.replacement&quot;, &quot;trf$1&quot;);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This allows to use any existing Kafka Connect SMT, such as the ones coming with Kafka Connect itself, or Debezium’s SMTs,
e.g. for &lt;a href=&quot;https://debezium.io/documentation/reference/1.2/configuration/topic-routing.html&quot;&gt;topic routing&lt;/a&gt;,
&lt;a href=&quot;https://debezium.io/documentation/reference/1.2/configuration/event-flattening.html&quot;&gt;new record state extraction&lt;/a&gt; and &lt;a href=&quot;https://debezium.io/documentation/reference/1.2/configuration/outbox-event-router.html&quot;&gt;outbox event routing&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;These improvements lay the foundation to the upcoming stand-alone Debezium runtime,
which will be based on the embedded engine and make its functionality available as a ready-to-use service.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;content_based_event_filtering&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#content_based_event_filtering&quot; /&gt;Content-based Event Filtering&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This release also adds another very versatile transformation to Debezium:
the &lt;a href=&quot;https://debezium.io/documentation/reference/1.2/configuration/filtering.html&quot;&gt;message filter SMT&lt;/a&gt;.
Applied to the Debezium connectors on the source side of a Kafka Connect data streaming pipeline,
it allows to filter out specific change events based on their field values.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;E.g. you could use this to filter out any change events of a specific customer type or product category.
Filters are given as script expressions,
using any language compatible with the javax.scripting API
(&lt;a href=&quot;https://jcp.org/en/jsr/detail?id=223&quot;&gt;JSR 223&lt;/a&gt;).
Note Debezium doesn’t provide any such scripting language implementation itself;
instead you can choose from a wide range of available options such as Groovy, MVEL or graal.js (JavaScript via GraalVM) and add it to the Kafka Connect plug-in path yourself.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here’s an example using Groovy:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;...
transforms=filter
transforms.filter.type=io.debezium.transforms.Filter
transforms.filter.language=jsr223.groovy
transforms.filter.condition=value.after.customerType != 42
...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;code&gt;value&lt;/code&gt; is the change event’s value; you could also refer to the event’s key and even the corresponding schema objects.
Groovy automatically resolves property paths such as &lt;code&gt;value.after.customerType&lt;/code&gt; to look-ups in map-like data structures such as Kafka Connect’s &lt;code&gt;Struct&lt;/code&gt; type.
This allows for very concise filtering conditions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note this SMT is incubating state for now, i.e. details around its API and configuration surface may still change.
Please give it a try and share your experiences.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;other_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#other_features&quot; /&gt;Other Features&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Besides these key features, there’s a number of other new functionalities coming with the 1.2.0.Alpha1 release:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;New metrics &lt;code&gt;NumberOfDisconnects&lt;/code&gt; and &lt;code&gt;NumberOfPrimaryElections&lt;/code&gt; for the MongoDB connector (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1859&quot;&gt;DBZ-1859&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support for automatic reconnects after connection losses in the SQL Server connector (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1882&quot;&gt;DBZ-1882&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;New column masking mode &quot;consistent hashing&quot; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1692&quot;&gt;DBZ-1692&lt;/a&gt;):
Debezium allows to mask specific column values,
e.g. to satisfy concerns around data privacy and protection.
Using the new &quot;consistent hashing&quot; mode it’s now possible to not only use asterisks as masking characters,
but also hash values based on the masked data contents.
Quoting the original issue reporter, this &quot;will be useful for [anonymizing] data but in this case it still needs to be relatable between topics. It’s a typical requirement for warehouses where you want to anonymize sensitive data but still need to keep referential integrity of your data&quot;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Allowing to link update change events in case of primary key updates (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1531&quot;&gt;DBZ-1531&lt;/a&gt;): most relational Debezium connectors represent an update to the primary key of a record by a delete event using the old key and a subsequent insert event using the updated key; using the new record headers &lt;code&gt;__debezium.newkey&lt;/code&gt; and &lt;code&gt;__debezium.oldkey&lt;/code&gt;,
it is now possible for consumers to link these change events together when working with change data from the MySQL and Postgres connectors&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Upgrade of Debezium’s container images to Apache Kafka 2.4.1 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1925&quot;&gt;DBZ-1925&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;bugfixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bugfixes&quot; /&gt;Bugfixes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Also a number of bugs were fixed, e.g.:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;High CPU usage when the Postgres connector is idle (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1960&quot;&gt;DBZ-1960&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Empty wal2json empty change event could cause NPE (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1922&quot;&gt;DBZ-1922&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cassandra Connector: unable to deserialize column mutation with reversed type
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1967&quot;&gt;DBZ-1967&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Outbox Quarkus Extension throws NPE in quarkus:dev mode (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1966&quot;&gt;DBZ-1966&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Validation of binlog_row_image is not compatible with MySQL 5.5 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1950&quot;&gt;DBZ-1950&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please refer to the &lt;a href=&quot;https://debezium.io/releases/1.2/release-notes/&quot;&gt;release notes&lt;/a&gt; for the complete list of resolved issues as well as procedures for upgrading from earlier Debezium versions.
We’ve also backported the critical bugfixes to the 1.1 branch and will release Debezium 1.1.1 tomorrow.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A big thank you to all the contributors from the community who worked on this release:
&lt;a href=&quot;https://github.com/Iskuskov&quot;&gt;Alexander Iskuskov&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ahus1&quot;&gt;Alexander Schwartz&lt;/a&gt;,
&lt;a href=&quot;https://github.com/bingqinzhou&quot;&gt;Bingqin Zhou&lt;/a&gt;,
&lt;a href=&quot;https://github.com/fgakk&quot;&gt;Fatih Güçlü Akkaya&lt;/a&gt;,
&lt;a href=&quot;https://github.com/grantcooksey&quot;&gt;Grant Cooksey&lt;/a&gt;,
&lt;a href=&quot;https://github.com/JanHendrikDolling&quot;&gt;Jan-Hendrik Dolling&lt;/a&gt;,
&lt;a href=&quot;https://github.com/lga-zurich&quot;&gt;Luis Garcés-Erice&lt;/a&gt;,
&lt;a href=&quot;https://github.com/devzer01&quot;&gt;Nayana Hettiarachchi&lt;/a&gt; and
&lt;a href=&quot;https://github.com/rk3rn3r&quot;&gt;René Kerner&lt;/a&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2020/04/09/using-debezium-wit-apicurio-api-schema-registry/</id>
<title>Using Debezium With the Apicurio API and Schema Registry</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2020-04-09T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2020/04/09/using-debezium-wit-apicurio-api-schema-registry/" rel="alternate" type="text/html" />
<author>
<name>Jiri Pechanec</name>
</author>
<category term="schema"></category>
<category term="avro"></category>
<category term="apicurio"></category>
<summary>



Change events streamed from a database by Debezium are (in developer parlance) strongly typed.
This means that event consumers should be aware of the types of data conveyed in the events.
This problem of passing along message type data can be solved in multiple ways:




the message structure is passed out-of-band to the consumer, which is able to process the data stored in it


the message contains metadata (the schema) that is embedded within the message


the message contains a reference to a registry which contains the associated metadata




An example of the first case is Apache Kafka&#8217;s well known JsonConverter.
It can operate in two modes...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Change events streamed from a database by Debezium are (in developer parlance) strongly typed.
This means that event consumers should be aware of the types of data conveyed in the events.
This problem of passing along message type data can be solved in multiple ways:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;the message structure is passed out-of-band to the consumer, which is able to process the data stored in it&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the message contains metadata (the &lt;em&gt;schema&lt;/em&gt;) that is embedded within the message&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the message contains a reference to a registry which contains the associated metadata&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;An example of the first case is Apache Kafka’s well known &lt;code&gt;JsonConverter&lt;/code&gt;.
It can operate in two modes - with and without schemas.
When configured to work without schemas, it generates a plain JSON message where the consumer either needs to know the types of each field beforehand, or it needs to execute heuristic rules to &quot;guess&quot; and map values to datatypes.
While this approach is quite flexible it can fail for more advanced cases, e.g. temporal or other semantic types encoded as strings.
Also, constraints associated with the types are usually lost.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here’s an example of such a message:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
  &quot;before&quot;: null,
  &quot;after&quot;: {
    &quot;id&quot;: 1001,
    &quot;first_name&quot;: &quot;Sally&quot;,
    &quot;last_name&quot;: &quot;Thomas&quot;,
    &quot;email&quot;: &quot;sally.thomas@acme.com&quot;
  },
  &quot;source&quot;: {
    &quot;version&quot;: &quot;1.1.0.Final&quot;,
    &quot;connector&quot;: &quot;mysql&quot;,
    &quot;name&quot;: &quot;dbserver1&quot;,
    &quot;ts_ms&quot;: 0,
    &quot;snapshot&quot;: &quot;true&quot;,
    &quot;db&quot;: &quot;inventory&quot;,
    &quot;table&quot;: &quot;customers&quot;,
    &quot;server_id&quot;: 0,
    &quot;gtid&quot;: null,
    &quot;file&quot;: &quot;mysql-bin.000003&quot;,
    &quot;pos&quot;: 154,
    &quot;row&quot;: 0,
    &quot;thread&quot;: null,
    &quot;query&quot;: null
  },
  &quot;op&quot;: &quot;c&quot;,
  &quot;ts_ms&quot;: 1586331101491,
  &quot;transaction&quot;: null
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note how no type information beyond JSON’s basic type system is present.
E.g. a consumer cannot conclude from the event itself, which length the numeric &lt;code&gt;id&lt;/code&gt; field has.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;An example of the second case is again &lt;code&gt;JsonConverter&lt;/code&gt;.
By means of its &lt;code&gt;schemas.enable&lt;/code&gt; option, the JSON message will consist of two parts - &lt;code&gt;schema&lt;/code&gt; and &lt;code&gt;payload&lt;/code&gt;.
The &lt;code&gt;payload&lt;/code&gt; part is exactly the same as in the previous case; the &lt;code&gt;schema&lt;/code&gt; part contains a description of the message, its fields, field types and associated type constraints.
This enables the consumer to process the message in a type-safe way.
The drawback of this approach is that the message size has increased significantly, as the schema is quite a large object.
As schemas tend to be changed rarely (how often do you change the definitions of the columns of your database tables?),
adding the schema to each and every event poses a signficant overhead.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The following example of a message with a schema clearly shows that the schema itself can be significantly larger than the payload and is not very economical to use:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
  &quot;schema&quot;: {
    &quot;type&quot;: &quot;struct&quot;,
    &quot;fields&quot;: [
      {
        &quot;type&quot;: &quot;struct&quot;,
        &quot;fields&quot;: [
          {
            &quot;type&quot;: &quot;int32&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;id&quot;
          },
          {
            &quot;type&quot;: &quot;string&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;first_name&quot;
          },
          {
            &quot;type&quot;: &quot;string&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;last_name&quot;
          },
          {
            &quot;type&quot;: &quot;string&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;email&quot;
          }
        ],
        &quot;optional&quot;: true,
        &quot;name&quot;: &quot;dbserver1.inventory.customers.Value&quot;,
        &quot;field&quot;: &quot;before&quot;
      },
      {
        &quot;type&quot;: &quot;struct&quot;,
        &quot;fields&quot;: [
          {
            &quot;type&quot;: &quot;int32&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;id&quot;
          },
          {
            &quot;type&quot;: &quot;string&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;first_name&quot;
          },
          {
            &quot;type&quot;: &quot;string&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;last_name&quot;
          },
          {
            &quot;type&quot;: &quot;string&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;email&quot;
          }
        ],
        &quot;optional&quot;: true,
        &quot;name&quot;: &quot;dbserver1.inventory.customers.Value&quot;,
        &quot;field&quot;: &quot;after&quot;
      },
      {
        &quot;type&quot;: &quot;struct&quot;,
        &quot;fields&quot;: [
          {
            &quot;type&quot;: &quot;string&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;version&quot;
          },
          {
            &quot;type&quot;: &quot;string&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;connector&quot;
          },
          {
            &quot;type&quot;: &quot;string&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;name&quot;
          },
          {
            &quot;type&quot;: &quot;int64&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;ts_ms&quot;
          },
          {
            &quot;type&quot;: &quot;string&quot;,
            &quot;optional&quot;: true,
            &quot;name&quot;: &quot;io.debezium.data.Enum&quot;,
            &quot;version&quot;: 1,
            &quot;parameters&quot;: {
              &quot;allowed&quot;: &quot;true,last,false&quot;
            },
            &quot;default&quot;: &quot;false&quot;,
            &quot;field&quot;: &quot;snapshot&quot;
          },
          {
            &quot;type&quot;: &quot;string&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;db&quot;
          },
          {
            &quot;type&quot;: &quot;string&quot;,
            &quot;optional&quot;: true,
            &quot;field&quot;: &quot;table&quot;
          },
          {
            &quot;type&quot;: &quot;int64&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;server_id&quot;
          },
          {
            &quot;type&quot;: &quot;string&quot;,
            &quot;optional&quot;: true,
            &quot;field&quot;: &quot;gtid&quot;
          },
          {
            &quot;type&quot;: &quot;string&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;file&quot;
          },
          {
            &quot;type&quot;: &quot;int64&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;pos&quot;
          },
          {
            &quot;type&quot;: &quot;int32&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;row&quot;
          },
          {
            &quot;type&quot;: &quot;int64&quot;,
            &quot;optional&quot;: true,
            &quot;field&quot;: &quot;thread&quot;
          },
          {
            &quot;type&quot;: &quot;string&quot;,
            &quot;optional&quot;: true,
            &quot;field&quot;: &quot;query&quot;
          }
        ],
        &quot;optional&quot;: false,
        &quot;name&quot;: &quot;io.debezium.connector.mysql.Source&quot;,
        &quot;field&quot;: &quot;source&quot;
      },
      {
        &quot;type&quot;: &quot;string&quot;,
        &quot;optional&quot;: false,
        &quot;field&quot;: &quot;op&quot;
      },
      {
        &quot;type&quot;: &quot;int64&quot;,
        &quot;optional&quot;: true,
        &quot;field&quot;: &quot;ts_ms&quot;
      },
      {
        &quot;type&quot;: &quot;struct&quot;,
        &quot;fields&quot;: [
          {
            &quot;type&quot;: &quot;string&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;id&quot;
          },
          {
            &quot;type&quot;: &quot;int64&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;total_order&quot;
          },
          {
            &quot;type&quot;: &quot;int64&quot;,
            &quot;optional&quot;: false,
            &quot;field&quot;: &quot;data_collection_order&quot;
          }
        ],
        &quot;optional&quot;: true,
        &quot;field&quot;: &quot;transaction&quot;
      }
    ],
    &quot;optional&quot;: false,
    &quot;name&quot;: &quot;dbserver1.inventory.customers.Envelope&quot;
  },
  &quot;payload&quot;: {
    &quot;before&quot;: null,
    &quot;after&quot;: {
      &quot;id&quot;: 1001,
      &quot;first_name&quot;: &quot;Sally&quot;,
      &quot;last_name&quot;: &quot;Thomas&quot;,
      &quot;email&quot;: &quot;sally.thomas@acme.com&quot;
    },
    &quot;source&quot;: {
      &quot;version&quot;: &quot;1.1.0.Final&quot;,
      &quot;connector&quot;: &quot;mysql&quot;,
      &quot;name&quot;: &quot;dbserver1&quot;,
      &quot;ts_ms&quot;: 0,
      &quot;snapshot&quot;: &quot;true&quot;,
      &quot;db&quot;: &quot;inventory&quot;,
      &quot;table&quot;: &quot;customers&quot;,
      &quot;server_id&quot;: 0,
      &quot;gtid&quot;: null,
      &quot;file&quot;: &quot;mysql-bin.000003&quot;,
      &quot;pos&quot;: 154,
      &quot;row&quot;: 0,
      &quot;thread&quot;: null,
      &quot;query&quot;: null
    },
    &quot;op&quot;: &quot;c&quot;,
    &quot;ts_ms&quot;: 1586331101491,
    &quot;transaction&quot;: null
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;registry&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#registry&quot; /&gt;Registry&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Then there is the third approach that combines strong points of the first two, while it removes their drawbacks at the cost of introducing a new component - a registry - that stores and versions message schemas.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There are multiple schema registry implementations available;
in the following we’re going to focus on the &lt;a href=&quot;https://github.com/Apicurio/apicurio-registry&quot;&gt;Apicurio Registry&lt;/a&gt;,
which is an open-source (Apache license 2.0) API and schema registry.
The project provides not only the registry itself, but also client libraries and tight integration with Apache Kafka and Kafka Connect in form of serializers and converters.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Apicurio enables Debezium and consumers to exchange messages whose schema is stored in the registry and pass only a reference to the schema in the messages themselves.
A the structure of captured source tables and thus message schemas evolve, the registry creates new versions of the schemas, too, so not only current but also historical schemas are available.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Apicurio provides multiple serialization formats out-of-the-box:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;JSON with externalized schema support&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://avro.apache.org/&quot;&gt;Apache Avro&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://developers.google.com/protocol-buffers&quot;&gt;Protocol Buffers&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Every serializer and deserializer knows how to automatically interact with the Apicurio API so the consumer is isolated from it as an implementation detail.
The only information necessary is the location of the registry.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Apicurio also provides API compatibility layers for schema registries from IBM and Confluent.
This is a very useful feature, as it enables the use of 3rd-party tools like &lt;a href=&quot;https://github.com/edenhill/kafkacat&quot;&gt;kafkacat&lt;/a&gt;, even if they are not aware of Apicurio’s native API.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;json_converter&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#json_converter&quot; /&gt;JSON Converter&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the Debezium examples repository, there is a &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/tutorial/docker-compose-mysql-apicurio.yaml&quot;&gt;Docker Compose&lt;/a&gt; based example, that deploys the Apicurio registry side-by-side with the standard Debezium tutorial example setup.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;https://debezium.io/images/2020-04-09-debezium-apicurio-registry/topology.png&quot; style=&quot;max-width:100%;&quot; class=&quot;responsive-image&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Figure 1. The Deployment Topology&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To follow the example you need to clone the Debezium &lt;a href=&quot;https://github.com/debezium/debezium-examples/&quot;&gt;example repository&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd tutorial
$ export DEBZIUM_VERSION=1.1

# Start the deployment
$ docker-compose -f docker-compose-mysql-apicurio.yaml up -d --build

# Start the connector
curl -i -X POST -H &quot;Accept:application/json&quot; \
    -H  &quot;Content-Type:application/json&quot; \
    http://localhost:8083/connectors/ -d @register-mysql-apicurio-converter-json.json

# Read content of the first message
$ docker run --rm --tty \
    --network tutorial_default debezium/tooling bash \
    -c &#39;kafkacat -b kafka:9092 -C -o beginning -q -t dbserver1.inventory.customers -c 1 | jq .&#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The resulting message should look like:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
  &quot;schemaId&quot;: 48,
  &quot;payload&quot;: {
    &quot;before&quot;: null,
    &quot;after&quot;: {
      &quot;id&quot;: 1001,
      &quot;first_name&quot;: &quot;Sally&quot;,
      &quot;last_name&quot;: &quot;Thomas&quot;,
      &quot;email&quot;: &quot;sally.thomas@acme.com&quot;
    },
    &quot;source&quot;: {
      &quot;version&quot;: &quot;1.1.0.Final&quot;,
      &quot;connector&quot;: &quot;mysql&quot;,
      &quot;name&quot;: &quot;dbserver1&quot;,
      &quot;ts_ms&quot;: 0,
      &quot;snapshot&quot;: &quot;true&quot;,
      &quot;db&quot;: &quot;inventory&quot;,
      &quot;table&quot;: &quot;customers&quot;,
      &quot;server_id&quot;: 0,
      &quot;gtid&quot;: null,
      &quot;file&quot;: &quot;mysql-bin.000003&quot;,
      &quot;pos&quot;: 154,
      &quot;row&quot;: 0,
      &quot;thread&quot;: null,
      &quot;query&quot;: null
    },
    &quot;op&quot;: &quot;c&quot;,
    &quot;ts_ms&quot;: 1586334283147,
    &quot;transaction&quot;: null
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The JSON message contains the full payload and at the same time a reference to a schema with id &lt;code&gt;48&lt;/code&gt;.
It is possible to query the schema from the registry either using &lt;code&gt;id&lt;/code&gt; or using a schema symbolic name as defined by Debezium documentation.
In this case both commands&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ docker run --rm --tty \
    --network tutorial_default \
    debezium/tooling bash -c &#39;http http://apicurio:8080/ids/64 | jq .&#39;

$ docker run --rm --tty \
    --network tutorial_default \
    debezium/tooling bash -c &#39;http http://apicurio:8080/artifacts/dbserver1.inventory.customers-value | jq .&#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;result in the same schema description:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
  &quot;type&quot;: &quot;struct&quot;,
  &quot;fields&quot;: [
    {
      &quot;type&quot;: &quot;struct&quot;,
      &quot;fields&quot;: [
        {
          &quot;type&quot;: &quot;int32&quot;,
          &quot;optional&quot;: false,
          &quot;field&quot;: &quot;id&quot;
        },
        {
          &quot;type&quot;: &quot;string&quot;,
          &quot;optional&quot;: false,
          &quot;field&quot;: &quot;first_name&quot;
        },
        {
          &quot;type&quot;: &quot;string&quot;,
          &quot;optional&quot;: false,
          &quot;field&quot;: &quot;last_name&quot;
        },
        {
          &quot;type&quot;: &quot;string&quot;,
          &quot;optional&quot;: false,
          &quot;field&quot;: &quot;email&quot;
        }
      ],
      &quot;optional&quot;: true,
      &quot;name&quot;: &quot;dbserver1.inventory.customers.Value&quot;,
      &quot;field&quot;: &quot;before&quot;
    },
...
  ],
  &quot;optional&quot;: false,
  &quot;name&quot;: &quot;dbserver1.inventory.customers.Envelope&quot;
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Which is the same as we have seen in the &quot;JSON with schema&quot; example before.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The connector registration request differs in a few lines from the previous one:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;...
&quot;key.converter&quot;: &quot;io.apicurio.registry.utils.converter.ExtJsonConverter&quot;, &lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;(1)&lt;/b&gt;
&quot;key.converter.apicurio.registry.url&quot;: &quot;http://apicurio:8080&quot;, &lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;(2)&lt;/b&gt;
&quot;key.converter.apicurio.registry.global-id&quot;:
    &quot;io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy&quot;, &lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;(3)&lt;/b&gt;

&quot;value.converter&quot;: &quot;io.apicurio.registry.utils.converter.ExtJsonConverter&quot;, &lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;(1)&lt;/b&gt;
&quot;value.converter.apicurio.registry.url&quot;: &quot;http://apicurio:8080&quot;, &lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;(2)&lt;/b&gt;
&quot;value.converter.apicurio.registry.global-id&quot;:
    &quot;io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy&quot; &lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;(3)&lt;/b&gt;
...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;colist arabic&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The Apicurio JSON converter is used as both key and value converter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The Apicurio registry endpoint&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;This setting ensures that it is posible to automatically register the schema id which is the typical setting in Debezium deployment&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;avro_converter&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#avro_converter&quot; /&gt;Avro Converter&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So far we have demonstrated serialization of messages into the JSON format only.
While using the JSON format with the registry has a lot of advantages, like easy human readability, it’s still not very space-efficient.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To transfer really only the data without any significant overhead, it is useful to use binary format serialization like Avro format.
In this case, we would pack the data only without any field names and other ceremony, and again the message will contain a reference to a schema stored in the registry.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s look at how easily the Avro serialization can be used with Apicurio’s Avro converter.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;# Tear down the previous deployment
$ docker-compose -f docker-compose-mysql-apicurio.yaml down

# Start the deployment
$ docker-compose -f docker-compose-mysql-apicurio.yaml up -d --build

# Start the connector
curl -i -X POST -H &quot;Accept:application/json&quot; \
    -H  &quot;Content-Type:application/json&quot; \
    http://localhost:8083/connectors/ \
    -d @register-mysql-apicurio-converter-avro.json&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We can query the registry using schema name:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ docker run --rm --tty \
    --network tutorial_default \
    debezium/tooling \
    bash -c &#39;http http://apicurio:8080/artifacts/dbserver1.inventory.customers-value | jq .&#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The resulting schema description is slightly different for the previous ones as it has an Avro flavour:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
  &quot;type&quot;: &quot;record&quot;,
  &quot;name&quot;: &quot;Envelope&quot;,
  &quot;namespace&quot;: &quot;dbserver1.inventory.customers&quot;,
  &quot;fields&quot;: [
    {
      &quot;name&quot;: &quot;before&quot;,
      &quot;type&quot;: [
        &quot;null&quot;,
        {
          &quot;type&quot;: &quot;record&quot;,
          &quot;name&quot;: &quot;Value&quot;,
          &quot;fields&quot;: [
            {
              &quot;name&quot;: &quot;id&quot;,
              &quot;type&quot;: &quot;int&quot;
            },
            {
              &quot;name&quot;: &quot;first_name&quot;,
              &quot;type&quot;: &quot;string&quot;
            },
            {
              &quot;name&quot;: &quot;last_name&quot;,
              &quot;type&quot;: &quot;string&quot;
            },
            {
              &quot;name&quot;: &quot;email&quot;,
              &quot;type&quot;: &quot;string&quot;
            }
          ],
          &quot;connect.name&quot;: &quot;dbserver1.inventory.customers.Value&quot;
        }
      ],
      &quot;default&quot;: null
    },
    {
      &quot;name&quot;: &quot;after&quot;,
      &quot;type&quot;: [
        &quot;null&quot;,
        &quot;Value&quot;
      ],
      &quot;default&quot;: null
    },
...
  ],
  &quot;connect.name&quot;: &quot;dbserver1.inventory.customers.Envelope&quot;
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The connector registration request also differs from the standard one in a handful of lines:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;...
&quot;key.converter&quot;: &quot;io.apicurio.registry.utils.converter.AvroConverter&quot;, &lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;(1)&lt;/b&gt;
&quot;key.converter.apicurio.registry.url&quot;: &quot;http://apicurio:8080&quot;, &lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;(2)&lt;/b&gt;
&quot;key.converter.apicurio.registry.converter.serializer&quot;:
    &quot;io.apicurio.registry.utils.serde.AvroKafkaSerializer&quot;, &lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;(3)&lt;/b&gt;
&quot;key.converter.apicurio.registry.converter.deserializer&quot;:
    &quot;io.apicurio.registry.utils.serde.AvroKafkaDeserializer&quot;, &lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;(3)&lt;/b&gt;
&quot;key.converter.apicurio.registry.global-id&quot;:
    &quot;io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy&quot;, &lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;b&gt;(4)&lt;/b&gt;

&quot;value.converter&quot;: &quot;io.apicurio.registry.utils.converter.AvroConverter&quot;, &lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;(1)&lt;/b&gt;
&quot;value.converter.apicurio.registry.url&quot;: &quot;http://apicurio:8080&quot;, &lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;(2)&lt;/b&gt;
&quot;value.converter.apicurio.registry.converter.serializer&quot;:
    &quot;io.apicurio.registry.utils.serde.AvroKafkaSerializer&quot;, &lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;(3)&lt;/b&gt;
&quot;value.converter.apicurio.registry.converter.deserializer&quot;:
    &quot;io.apicurio.registry.utils.serde.AvroKafkaDeserializer&quot;, &lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;(3)&lt;/b&gt;
&quot;value.converter.apicurio.registry.global-id&quot;:
    &quot;io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy&quot;, &lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;b&gt;(4)&lt;/b&gt;
...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;colist arabic&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The Apicurio Avro converter is used as both key and value converter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The Apicurio registry endpoint&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Prescribes which serializer and deserializer should be used by the converter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;This setting ensures that it is posible to automatically register the schema id which is the typical setting in Debezium deployment&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To demonstrate consumption of the messages on the sink side we can, for example, use the &lt;a href=&quot;https://github.com/confluentinc/kafka-connect-elasticsearch&quot;&gt;Kafka Connect Elasticsearch connector&lt;/a&gt;. The sink configuration will be again extended only with converter configuration, and the sink connector can consume Avro-enabled topics, without any other changes needed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
  &quot;name&quot;: &quot;elastic-sink&quot;,
  &quot;config&quot;: {
    &quot;connector.class&quot;: &quot;io.confluent.connect.elasticsearch.ElasticsearchSinkConnector&quot;,
    &quot;tasks.max&quot;: &quot;1&quot;,
    &quot;topics&quot;: &quot;customers&quot;,
    &quot;connection.url&quot;: &quot;http://elastic:9200&quot;,
    &quot;transforms&quot;: &quot;unwrap,key&quot;,
    &quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.ExtractNewRecordState&quot;,
    &quot;transforms.unwrap.drop.tombstones&quot;: &quot;false&quot;,
    &quot;transforms.key.type&quot;: &quot;org.apache.kafka.connect.transforms.ExtractField$Key&quot;,
    &quot;transforms.key.field&quot;: &quot;id&quot;,
    &quot;key.ignore&quot;: &quot;false&quot;,
    &quot;type.name&quot;: &quot;customer&quot;,
    &quot;behavior.on.null.values&quot;: &quot;delete&quot;,

    &quot;key.converter&quot;: &quot;io.apicurio.registry.utils.converter.AvroConverter&quot;,
    &quot;key.converter.apicurio.registry.url&quot;: &quot;http://apicurio:8080&quot;,
    &quot;key.converter.apicurio.registry.converter.serializer&quot;:
        &quot;io.apicurio.registry.utils.serde.AvroKafkaSerializer&quot;,
    &quot;key.converter.apicurio.registry.converter.deserializer&quot;:
        &quot;io.apicurio.registry.utils.serde.AvroKafkaDeserializer&quot;,
    &quot;key.converter.apicurio.registry.global-id&quot;:
        &quot;io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy&quot;,

    &quot;value.converter&quot;: &quot;io.apicurio.registry.utils.converter.AvroConverter&quot;,
    &quot;value.converter.apicurio.registry.url&quot;: &quot;http://apicurio:8080&quot;,
    &quot;value.converter.apicurio.registry.converter.serializer&quot;:
        &quot;io.apicurio.registry.utils.serde.AvroKafkaSerializer&quot;,
    &quot;value.converter.apicurio.registry.converter.deserializer&quot;:
        &quot;io.apicurio.registry.utils.serde.AvroKafkaDeserializer&quot;,
    &quot;value.converter.apicurio.registry.global-id&quot;:
        &quot;io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy&quot;,
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#conclusion&quot; /&gt;Conclusion&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In this article we discussed multiple approaches to message/schema association.
The Apicurio registry was presented as a solution for schema sotrage and versioning and we have demonstrated how Apicurio can be integrated with Debezium connectors to efficiently deliver messages with schema to the consumer.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can find a complete example for using the Debezium connectors together with the Apicurio registry in the &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/tutorial#using-mysql-and-apicurio-registry&quot;&gt;tutorial&lt;/a&gt; project of the Debezium examples repository on GitHub.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open-source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open-source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve our existing connectors and add even more connectors.
If you find problems or have an idea on how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2020/03/31/debezium-newsletter-01-2020/</id>
<title>Debezium&#8217;s Newsletter 01/2020</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2020-03-31T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2020/03/31/debezium-newsletter-01-2020/" rel="alternate" type="text/html" />
<author>
<name>Chris Cranford</name>
</author>
<category term="community"></category>
<category term="news"></category>
<category term="newsletter"></category>
<summary>



Welcome to the latest edition of the Debezium community newsletter, in which we share all things CDC related including blog posts, group discussions, as well as StackOverflow
questions that are relevant to our user community.


In case you missed our last edition, you can check it out here.




Upcoming Events


Due to the corona virus situation, many conferences the Debezium team had planned to attend, have been postponed or even cancelled.
E.g. JavaDay Istanbul has been moved to September, and QCon Sao Paulo to December.
We hope the situation will have improved by then and look forward to meeting again with the Debezium community in person...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Welcome to the latest edition of the Debezium community newsletter, in which we share all things CDC related including blog posts, group discussions, as well as StackOverflow
questions that are relevant to our user community.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In case you missed our last edition, you can check it out &lt;a href=&quot;https://debezium.io/blog/2019/10/17/debezium-newsletter-02-2019/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;upcoming_events&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#upcoming_events&quot; /&gt;Upcoming Events&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Due to the corona virus situation, many conferences the Debezium team had planned to attend, have been postponed or even cancelled.
E.g. JavaDay Istanbul has been moved to September, and QCon Sao Paulo to December.
We hope the situation will have improved by then and look forward to meeting again with the Debezium community in person eventually.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Until then, there’s a few virtual events you can enjoy;
there’ll be a Debezium session at the &lt;a href=&quot;https://www.redhat.com/en/summit&quot;&gt;Red Hat Summit 2020 - Virtual Experience&lt;/a&gt;.
We’re also planning to do another episode on Debezium at &lt;a href=&quot;https://developers.redhat.com/devnation/&quot;&gt;DevNation Live&lt;/a&gt;.
If you’d like to have a session on Debezium at your virtual meetup or conference, please get in touch!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;articles&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#articles&quot; /&gt;Articles&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There have been a number of blog posts about Debezium lately; here are some of the latest ones that you should not miss:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A two-part series discussing tailing a database transaction log using Debezium by Abdullah Yildirim:
&lt;a href=&quot;https://medium.com/trendyol-tech/transaction-log-tailing-with-debezium-part-1-aeb968d72220&quot;&gt;Part 1&lt;/a&gt;,
&lt;a href=&quot;https://medium.com/trendyol-tech/transaction-log-tailing-with-debezium-part-2-9ecaebf063b9&quot;&gt;Part 2&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://medium.com/everything-full-stack/streaming-data-changes-to-a-data-lake-with-debezium-and-delta-lake-pipeline-299821053dc3&quot;&gt;Streaming data changes to a Data Lake with Debezium and Delta Lake Pipeline&lt;/a&gt; by Yinon D. Nahamu.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://thoughts-on-java.org/outbox-pattern-with-cdc-and-debezium/&quot;&gt;Implementing the Outbox Pattern with CDC using Debezium&lt;/a&gt; by Thorben Janssen.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Debezium and Apache Camel integration scenario:
&lt;a href=&quot;https://debezium.io/blog/2020/02/19/debezium-camel-integration/&quot;&gt;Original blog&lt;/a&gt; by Jiri Pechanec (English),
&lt;a href=&quot;https://rheb.hatenablog.com/entry/2020/02/19/debezium-camel-integration/&quot;&gt;republished blog&lt;/a&gt; (Japanese)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/presentations/data-streaming-kafka-debezium/&quot;&gt;Recording&lt;/a&gt; and &lt;a href=&quot;https://speakerdeck.com/gunnarmorling/practical-change-data-streaming-use-cases-with-apache-kafka-and-debezium-qcon-san-francisco-2019&quot;&gt;slides&lt;/a&gt; from QCon and JokerConf where Gunnar Morling discusses practical CDC streaming use cases with Apache Kafka and Debezium.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://debezium.io/blog/2020/03/05/db2-cdc-approaches/&quot;&gt;Approaches to running Change Data Capture for Db2&lt;/a&gt; by Luis Garcés-Erice, Sean Rooney, and Peter Urbanetz.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://debezium.io/blog/2020/02/25/lessons-learned-running-debezium-with-postgresql-on-rds/&quot;&gt;Lessons learned from running Debezium with PostgreSQL on Amazon RDS&lt;/a&gt; by Ashhar Hasan.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://shekhargulati.com/2019/12/07/the-5-minute-introduction-to-log-based-change-data-capture-with-debezium/&quot;&gt;The 5 minute introduction to Log-based Change Data Capture with Debezium&lt;/a&gt; by Shekhar Gulati.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Distributed Data for Microservices — Event Sourcing vs. Change Data Capture: A &lt;a href=&quot;https://debezium.io/blog/2020/02/10/event-sourcing-vs-cdc/&quot;&gt;Original post&lt;/a&gt; by Eric Murphy, &lt;a href=&quot;https://rheb.hatenablog.com/entry/2020/02/10/event-sourcing-vs-cdc/&quot;&gt;Japanese translation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Series of blog posts about Debezium by Bhuvanesh &quot;The Data Guy&quot;:&lt;/p&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://thedataguy.in/debezium-mysql-snapshot-for-aws-rds-aurora-from-backup-snaphot/&quot;&gt;Debezium MySQL Snapshot for AWS RDS Aurora From Backup Snapshot&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-and-resume-from-master/&quot;&gt;Debezium MySQL Snapshot From Read Replica and Resume From Master&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://thedataguy.in/debezium-mysql-snapshot-from-read-replica-with-gtid/&quot;&gt;Debezium MySQL Snapshot From Read Replica with GTID&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://thedataguy.in/monitor-debezium-mysql-connector-with-prometheus-and-grafana/&quot;&gt;Monitor Debezium MySQL Connector with Prometheus and Grafana&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://thedataguy.in/build-production-grade-debezium-with-confluent-kafka-cluster/&quot;&gt;Build Production Grade Debezium Cluster with Confluent Kafka&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/br/presentations/postgresql-ao-datalake-utilizando-kafkadebezium/&quot;&gt;From PostgreSQL to Data Lake using Kafka and Debezium&lt;/a&gt; (Portuguese)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Google Cloud Platform recently published &lt;a href=&quot;https://github.com/GoogleCloudPlatform/DataflowTemplates/tree/master/v2/cdc-parent&quot;&gt;this repository&lt;/a&gt; illustrating an example of how to capture data from a MySQL database and sync it with BigQuery using Cloud Dataflow and Debezium.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A recent &lt;a href=&quot;https://twitter.com/gunnarmorling/status/1242130486173949952&quot;&gt;spike of interest&lt;/a&gt; in being able to use Debezium with GCPcloud’s managed PostgreSQL service.
We recommend if you’re interested in seeing CloudSQL support for Debezium, give the &lt;a href=&quot;https://issuetracker.google.com/issues/70756171&quot;&gt;issue&lt;/a&gt; an up-vote.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A &lt;a href=&quot;https://www.dataengineeringpodcast.com/debezium-change-data-capture-episode-114/&quot;&gt;very special episode&lt;/a&gt; of the Data Engineering Podcast by Tobias Macey, together with Debezium project founder Randall Hauch and Gunnar Morling&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please also check out our compiled list of &lt;a href=&quot;https://debezium.io/documentation/online-resources/&quot;&gt;resources around Debezium&lt;/a&gt; for even more related posts, articles and presentations.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;examples&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#examples&quot; /&gt;Examples&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;An example is an excellent way to get a better understanding of how or why something behaves as it does.
Debezium’s &lt;a href=&quot;https://github.com/debezium/debezium-examples&quot;&gt;examples repository&lt;/a&gt; has undergone several changes recently we’d like to highlight:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[NEW] &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/camel-component&quot;&gt;Integration of Debezium with Apache Camel&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[NEW] &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/camel-kafka-connect&quot;&gt;Integration of Apache Kafka Connect and Apache Camel&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[NEW] &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/cloudevents&quot;&gt;Demonstrates how Debezium can publish CloudEvents&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[NEW] &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/testcontainers&quot;&gt;Integration of Testcontainers to perform integration tests for CDC&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[UPDATED] &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/outbox&quot;&gt;Debezium Outbox using the new Debezium Quarkus Outbox extension&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We also discovered a &lt;a href=&quot;https://github.com/pmsipilot/docker-compose-viz&quot;&gt;very helpful tool&lt;/a&gt; for visualizing the contents of Docker Compose files.
So we’ve begun to add diagrams like this one for the &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/kstreams-live-update&quot;&gt;kstreams-live-update&lt;/a&gt; demo to the examples,
helping to familiarize with the examples more easily:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;https://debezium.io/images/newsletter_2020_01_docker-compose.png&quot; style=&quot;max-width:90%;&quot; class=&quot;responsive-image&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;KStreams Live Update Example Topology&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;time_to_upgrade&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#time_to_upgrade&quot; /&gt;Time to Upgrade&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium version &lt;a href=&quot;https://debezium.io/blog/2020/03/24/debezium-1-1-0-final-released/&quot;&gt;1.1.0.Final&lt;/a&gt; was released last week.
If you are using an older version, we urge you to check out the latest major release.
For details on the bug fixes, enhancements, and improvements that spanned 5 releases, check out the &lt;a href=&quot;https://debezium.io/releases/1.1/release-notes/&quot;&gt;release-notes&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium team has also begun active development on the next major version, 1.2.
The major focus in 1.2 is implementing a standalone container to run Debezium without Apache Kafka and Connect, enabling users to send change events to Kinesis and other platforms more easily.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Keep an eye on our &lt;a href=&quot;https://debezium.io/releases/&quot;&gt;releases page&lt;/a&gt; to get a jump start on what bug fixes, enhancements, and changes will be coming in 1.2 as they become available.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;questions_and_answers&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#questions_and_answers&quot; /&gt;Questions and Answers&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/60730628/how-to-configure-debezium-to-use-specific-column-as-kafka-message-key&quot;&gt;How to configure Debezium to use specific columns as Kafka message key&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/60506859/database-connection-failed-when-reading-from-copy&quot;&gt;Database connection failed when reading from copy&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/60140741/does-debezium-provide-delivery-and-ordering-guarantees&quot;&gt;Does Debezium provide delivery and ordering guarantees?&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/59943376/configuring-debezium-mysql-connector-via-env-vars&quot;&gt;Configuring Debezium MySQL connector via env vars&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/59754337/postgres-debezium-cdc-does-not-publish-changes-to-kafka&quot;&gt;Postgres Debezium CDC does not publish changes to Kafka&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;using_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#using_debezium&quot; /&gt;Using Debezium?&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Our &lt;a href=&quot;https://www.debezium.io/community/users&quot;&gt;community users&lt;/a&gt; page includes a variety of organizations that are currently using Debezium.
If you are a user of Debezium and would like to be included, please send us a GitHub pull request or reach out to us directly through our community channels found &lt;a href=&quot;https://debezium.io/community&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And if you haven’t yet done so,
please consider &lt;a href=&quot;https://github.com/debezium/debezium/stargazers&quot;&gt;adding a ⭐&lt;/a&gt; for the GitHub repo;
keep them coming, we’re almost at 3,000 stars!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;getting_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#getting_involved&quot; /&gt;Getting Involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It can often be overwhelming when starting to work on an existing code base.
We welcome community contributions and we want to make the process of getting started extremely easy.
Below is a list of open issues that are currently labeled with &lt;code&gt;easy-starter&lt;/code&gt; if you want to dive in quick.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Configure Avro serialization automatically when detecting link to schema registry (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-59&quot;&gt;DBZ-59&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support CREATE TABLE …​ LIKE syntax for blacklisted source table (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1496&quot;&gt;DBZ-1496&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Explore SMT for Externalizing large column values (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1541&quot;&gt;DBZ-1541&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update the tutorial to use the Debezium tooling container image (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1572&quot;&gt;DBZ-1572&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Debezium for SQL Server does not support reconnecting after the connection is broken (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1882&quot;&gt;DBZ-1882&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;feedback&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#feedback&quot; /&gt;Feedback&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We intend to publish new additions to this newsletter periodically.
Should anyone have any suggestions on changes or what could be highlighted here, we welcome that feedback.
You can reach out to us via any of our community channels found &lt;a href=&quot;https://debezium.io/community&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And most importantly, stay safe and healthy wherever you are!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2020/03/24/debezium-1-1-final-released/</id>
<title>Debezium 1.1.0.Final Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2020-03-24T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2020/03/24/debezium-1-1-final-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="oracle"></category>
<category term="sqlserver"></category>
<category term="db2"></category>
<category term="cassandra"></category>
<summary>



It&#8217;s with great excitement that I&#8217;m announcing the release of Debezium 1.1.0.Final!


About three months after the 1.0 release, this new version comes with many exciting new features such as:




a Quarkus extension facilitating the outbox pattern


support for the CloudEvents specification


an incubating connector for the IBM Db2 database


transaction marker events


support for CDC integration testing via Testcontainers


a brand-new API Debezium module containing a reworked embedded engine API as well as an SPI for customizing schema and values of change events




Besides these key features, there&#8217;s many other smaller improvements such as reconnects for the Postgres connector, more flexibility when extracting the after state from...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It’s with great excitement that I’m announcing the release of Debezium &lt;strong&gt;1.1.0.Final&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;About three months after the 1.0 release, this new version comes with many exciting new features such as:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;a &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/integrations/outbox.html&quot;&gt;Quarkus&lt;/a&gt; extension facilitating the outbox pattern&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;support for the &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/integrations/cloudevents.html&quot;&gt;CloudEvents&lt;/a&gt; specification&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;an incubating &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/connectors/db2.html&quot;&gt;connector for the IBM Db2 database&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;transaction marker events&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;support for &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/integrations/testcontainers.html&quot;&gt;CDC integration testing&lt;/a&gt; via Testcontainers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;a brand-new API Debezium module containing a reworked &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/development/engine.html&quot;&gt;embedded engine API&lt;/a&gt; as well as an SPI for &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/development/converters.html&quot;&gt;customizing schema and values of change events&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Besides these key features, there’s many other smaller improvements such as reconnects for the Postgres connector, more flexibility when extracting the &lt;code&gt;after&lt;/code&gt; state from change events,
and more powerful options for propagating metadata on the source types of captured columns.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Since the CR1 release, &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/versions/12344981&quot;&gt;22 more issues&lt;/a&gt; were resolved: mostly bug fixes, documentation improvements and related to the stabilization of some flaky tests that’d fail intermittently on our CI server.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Besides these, there’s one very useful improvement for the Postgres connector:
as part of its heartbeat functionality, it can now regularly execute DML operations in the source database (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1815&quot;&gt;DBZ-1815&lt;/a&gt;).
This helps to prevent situations with a combination of multiple databases on the same database host,
receiving writes with different frequencies,
where Debezium couldn’t acknowledge processed WAL offsets with the source database otherwise.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Overall, &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20in%20(1.1.0.Alpha1%2C%201.1.0.Beta1%2C%201.1.0.Beta2%2C%201.1.0.CR1%2C%201.1.0.Final)&quot;&gt;123 issues&lt;/a&gt; have been resolved for Debezium 1.1 and its preview releases.
Please refer to the original announcements
(&lt;a href=&quot;https://debezium.io/blog/2020/01/16/debezium-1-1-alpha1-released/&quot;&gt;Alpha1&lt;/a&gt;,
&lt;a href=&quot;https://debezium.io/blog/2020/02/11/debezium-1-1-beta1-released/&quot;&gt;Beta1&lt;/a&gt;,
&lt;a href=&quot;https://debezium.io/blog/2020/02/13/debezium-1-1-beta2-released/&quot;&gt;Beta2&lt;/a&gt;,
&lt;a href=&quot;https://debezium.io/blog/2020/03/13/debezium-1-1-c1-released/&quot;&gt;CR1&lt;/a&gt;)
to learn more about the details.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The most important part of any open-source project is its community.
The following people have contributed to Debezium 1.1,
bumping the total number of community members contributing to Debezium to more than 175:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://github.com/Alan-zhangzf&quot;&gt;Alan Zhangzf&lt;/a&gt;,
&lt;a href=&quot;https://github.com/oscerd&quot;&gt;Andrea Cosentino&lt;/a&gt;,
&lt;a href=&quot;https://github.com/lordofthejars&quot;&gt;Alex Soto&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ahus1&quot;&gt;Alexander Schwartz&lt;/a&gt;,
&lt;a href=&quot;https://github.com/bingqinzhou&quot;&gt;Bingqin Zhou&lt;/a&gt;,
&lt;a href=&quot;https://github.com/daanroosen-DS&quot;&gt;Daan Roosen&lt;/a&gt;,
&lt;a href=&quot;https://github.com/mozinator&quot;&gt;Fabian Aussems&lt;/a&gt;,
&lt;a href=&quot;https://github.com/FabioCantarini&quot;&gt;Fabio Cantarini&lt;/a&gt;,
&lt;a href=&quot;https://github.com/FrankMormino&quot;&gt;Frank Mormino&lt;/a&gt;,
&lt;a href=&quot;https://github.com/blcksrx&quot;&gt;Hossein Torabi&lt;/a&gt;,
&lt;a href=&quot;https://github.com/igabaydulin&quot;&gt;Igor Gabaydulin&lt;/a&gt;,
&lt;a href=&quot;https://github.com/juyttenh&quot;&gt;Jan Uyttenhove&lt;/a&gt;,
&lt;a href=&quot;https://github.com/JanHendrikDolling&quot;&gt;Jan-Hendrik Dolling&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jgraf50&quot;&gt;John Graf&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jpsoroulas&quot;&gt;John Psoroulas&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jhuiting&quot;&gt;Jos Huiting&lt;/a&gt;,
&lt;a href=&quot;https://github.com/lga-zurich&quot;&gt;Luis Garcés-Erice&lt;/a&gt;,
&lt;a href=&quot;https://github.com/mzbyszynski&quot;&gt;Marc Zbyszynski&lt;/a&gt;,
&lt;a href=&quot;https://github.com/matzew&quot;&gt;Matthias Wessendorf&lt;/a&gt;,
&lt;a href=&quot;https://github.com/mwinstanley&quot;&gt;Melissa Winstanley&lt;/a&gt;,
&lt;a href=&quot;https://github.com/zrlurb&quot;&gt;Peter Urbanetz&lt;/a&gt;,
&lt;a href=&quot;https://github.com/raultov&quot;&gt;Raúl Tovar&lt;/a&gt;,
&lt;a href=&quot;https://github.com/rgibaiev&quot;&gt;Ruslan Gibaiev&lt;/a&gt;,
&lt;a href=&quot;https://github.com/sahandilshan&quot;&gt;Sahan Dilshan&lt;/a&gt;,
&lt;a href=&quot;https://github.com/SeanRooooney&quot;&gt;Sean Rooney&lt;/a&gt;,
&lt;a href=&quot;https://github.com/taylor-rolison&quot;&gt;Taylor Rolison&lt;/a&gt;,
&lt;a href=&quot;https://github.com/vasilyulianko-visma&quot;&gt;Vasily Ulianko&lt;/a&gt;,
&lt;a href=&quot;https://github.com/vedit&quot;&gt;Vedit Firat Arig&lt;/a&gt;,
&lt;a href=&quot;https://github.com/liulangwa&quot;&gt;Yongjun Du&lt;/a&gt; and
&lt;a href=&quot;https://github.com/Wang-Yu-Chao&quot;&gt;Yuchao Wang&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A big, big &quot;thank you&quot; to each and every one of you!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Having shipped Debezium 1.1, the team is focusing on 1.2 now.
Adhering to a quarterly release cadence,
Debezium 1.2 is scheduled for end of June.
You can expect preview releases roughly every three weeks.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;What’s coming up in 1.2?
Check out the &lt;a href=&quot;https://debezium.io/roadmap/&quot;&gt;roadmap&lt;/a&gt; to learn more;
one key feature will be a &quot;standalone mode&quot; for running Debezium independently of Kafka Connect,
allowing to feed change events to other messaging infrastructure such as Amazon Kinesis.
Also make sure to let us know about your suggestions, requirements and feature requests!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2020/03/19/integration-testing-for-change-data-capture-with-testcontainers/</id>
<title>Integration Testing for Change Data Capture with Testcontainers</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2020-03-19T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2020/03/19/integration-testing-for-change-data-capture-with-testcontainers/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="discussion"></category>
<category term="testcontainers"></category>
<category term="postgres"></category>
<summary>



Setting up change data capture (CDC) pipelines with Debezium typically is a matter of configuration,
without any programming being involved.
It&#8217;s still a very good idea to have automated tests for your CDC set-up,
making sure that everything is configured correctly
and that your Debezium connectors are set up as intended.


There&#8217;s two main components involved whose configuration need consideration:




The source database: it must be set up so that Debezium can connect to it and retrieve change events; details depend on the specific database, e.g. for MySQL the binlog must be in "row" mode,
for Postgres, one of the supported logical decoding plug-ins must be installed,...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Setting up change data capture (CDC) pipelines with Debezium typically is a matter of configuration,
without any programming being involved.
It’s still a very good idea to have automated tests for your CDC set-up,
making sure that everything is configured correctly
and that your Debezium connectors are set up as intended.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There’s two main components involved whose configuration need consideration:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The source database:&lt;/strong&gt; it must be set up so that Debezium can connect to it and retrieve change events; details depend on the specific database, e.g. for MySQL the binlog must be in &quot;row&quot; mode,
for Postgres, one of the supported logical decoding plug-ins must be installed, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;The Debezium connector:&lt;/strong&gt; it must be configured using the right database host and credentials,
possibly using SSL, applying table and column filters, potentially one or more single message transformations (SMTs), etc.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;!-- more --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is where the newly added Debezium &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/integrations/testcontainers.html&quot;&gt;support for integration tests&lt;/a&gt; with &lt;a href=&quot;https://www.testcontainers.org/&quot;&gt;Testcontainers&lt;/a&gt; comes in.
It allows to set up all the required components (Apache Kafka, Kafka Connect etc.) using Linux container images, configure and deploy a Debezium connector and run assertions against produced change data events.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s take a look at how it’s done.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;project_set_up&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#project_set_up&quot; /&gt;Project Set-Up&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Assuming you’re working with Apache Maven for dependency management,
add the following dependencies to your &lt;em&gt;pom.xml&lt;/em&gt;,
pulling in the Debezium Testcontainers integration and the Testcontainers &lt;a href=&quot;https://www.testcontainers.org/modules/kafka/&quot;&gt;module for Apache Kafka&lt;/a&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;io.debezium&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;debezium-testing-testcontainers&amp;lt;/artifactId&amp;gt;
  &amp;lt;version&amp;gt;1.1.0.CR1&amp;lt;/version&amp;gt;
  &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;
&amp;lt;/dependency&amp;gt;
&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;org.testcontainers&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;kafka&amp;lt;/artifactId&amp;gt;
  &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;
&amp;lt;/dependency&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Also add the Testcontainers dependency for your database, e.g. Postgres:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;org.testcontainers&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;postgresql&amp;lt;/artifactId&amp;gt;
  &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;
&amp;lt;/dependency&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can find an example project with the complete configuration in the &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/testcontainers&quot;&gt;debezium-examples&lt;/a&gt; repo on GitHub.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;initializing_testcontainers&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#initializing_testcontainers&quot; /&gt;Initializing Testcontainers&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Having declared all the required dependencies, it’s time to write a CDC integration test.
With Testcontainers, integration tests are implemented using Linux containers and Docker.
It provides a Java API for starting and managing the resources needed by a test.
We can use this to fire up Apache Kafka, Kafka Connect and a Postgres database:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public class CdcTest {

  private static Network network = Network.newNetwork(); &lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;(1)&lt;/b&gt;

  private static KafkaContainer kafkaContainer = new KafkaContainer()
      .withNetwork(network); &lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;(2)&lt;/b&gt;

  public static PostgreSQLContainer&amp;lt;?&amp;gt; postgresContainer =
      new PostgreSQLContainer&amp;lt;&amp;gt;(&quot;debezium/postgres:11&quot;)
          .withNetwork(network)
          .withNetworkAliases(&quot;postgres&quot;); &lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;(3)&lt;/b&gt;

  public static DebeziumContainer debeziumContainer =
      new DebeziumContainer(&quot;1.1.0.CR1&quot;)
          .withNetwork(network)
          .withKafka(kafkaContainer)
          .dependsOn(kafkaContainer); &lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;b&gt;(4)&lt;/b&gt;

  @BeforeClass
  public static void startContainers() { &lt;i class=&quot;conum&quot; data-value=&quot;5&quot; /&gt;&lt;b&gt;(5)&lt;/b&gt;
    Startables.deepStart(Stream.of(
        kafkaContainer, postgresContainer, debeziumContainer))
            .join();
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;colist arabic&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Define a Docker network to be used by all the services&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Set up a container for Apache Kafka&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Set up a container for Postgres 11 (using Debezium’s Postgres container image)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Set up a container for Kafka Connect with Debezium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;5&quot; /&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Start all three containers in a &lt;code&gt;@BeforeClass&lt;/code&gt; method&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note that you need to have Docker installed in order to use Testcontainers.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;the_test_implementation&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_test_implementation&quot; /&gt;The Test Implementation&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With the needed infrastructure in place, we can write a test for our CDC set-up.
The overall flow of the test is this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Configure a Debezium connector for the Postgres database&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Execute a few SQL statements to change some data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Retrieve the resulting change data events from the corresponding Kafka topic using a Kafka consumer&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run some assertions against these events&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here’s the shell for the test:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Test
public void canObtainChangeEventsFromPostgres() throws Exception {
  try (Connection connection = getConnection(postgresContainer);
      Statement statement = connection.createStatement();
      KafkaConsumer&amp;lt;String, String&amp;gt; consumer =
          getConsumer(kafkaContainer)) {

      // TODO ...
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The credentials for the database connection can be obtained from the Postgres container started via Testcontainers,
nicely avoiding any redundancies:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;private Connection getConnection(PostgreSQLContainer&amp;lt;?&amp;gt; postgresContainer)
    throws SQLException {

  return DriverManager.getConnection(postgresContainer.getJdbcUrl(),
      postgresContainer.getUsername(),
      postgresContainer.getPassword());
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The same goes for the Kafka consumer:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;private KafkaConsumer&amp;lt;String, String&amp;gt; getConsumer(
    KafkaContainer kafkaContainer) {

  return new KafkaConsumer&amp;lt;&amp;gt;(
      ImmutableMap.of(
          ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,
          kafkaContainer.getBootstrapServers(),

          ConsumerConfig.GROUP_ID_CONFIG,
          &quot;tc-&quot; + UUID.randomUUID(),

          ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,
          &quot;earliest&quot;),
      new StringDeserializer(),
      new StringDeserializer());
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now let’s implement the actual test logic:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;statement.execute(&quot;create schema todo&quot;); &lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;(1)&lt;/b&gt;
statement.execute(&quot;create table todo.Todo (&quot; +
                    &quot;id int8 not null, &quot; +
                    &quot;title varchar(255), &quot; +
                    &quot;primary key (id))&quot;);
statement.execute(&quot;alter table todo.Todo replica identity full&quot;);
statement.execute(&quot;insert into todo.Todo values (1, &#39;Learn CDC&#39;)&quot;);
statement.execute(&quot;insert into todo.Todo values (2, &#39;Learn Debezium&#39;)&quot;);

ConnectorConfiguration connector = ConnectorConfiguration
        .forJdbcContainer(postgresContainer)
        .with(&quot;database.server.name&quot;, &quot;dbserver1&quot;);

debeziumContainer.registerConnector(&quot;my-connector&quot;,
        connector); &lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;(2)&lt;/b&gt;

consumer.subscribe(Arrays.asList(&quot;dbserver1.todo.todo&quot;));

List&amp;lt;ConsumerRecord&amp;lt;String, String&amp;gt;&amp;gt; changeEvents =
        drain(consumer, 2); &lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;(3)&lt;/b&gt;

ConsumerRecord&amp;lt;String, String&amp;gt; changeEvent = changeEvents.get(0);
assertThat(JsonPath.&amp;lt;Integer&amp;gt; read(changeEvent.key(), &quot;$.id&quot;))
  .isEqualTo(1);
assertThat(JsonPath.&amp;lt;String&amp;gt; read(changeEvent.value(), &quot;$.op&quot;))
  .isEqualTo(&quot;r&quot;);
assertThat(JsonPath.&amp;lt;String&amp;gt; read(changeEvent.value(), &quot;$.after.title&quot;))
  .isEqualTo(&quot;Learn CDC&quot;);

changeEvent = changeEvents.get(1);
assertThat(JsonPath.&amp;lt;Integer&amp;gt; read(changeEvent.key(), &quot;$.id&quot;))
  .isEqualTo(2);
assertThat(JsonPath.&amp;lt;String&amp;gt; read(changeEvent.value(), &quot;$.op&quot;))
  .isEqualTo(&quot;r&quot;);
assertThat(JsonPath.&amp;lt;String&amp;gt; read(changeEvent.value(), &quot;$.after.title&quot;))
  .isEqualTo(&quot;Learn Debezium&quot;);

consumer.unsubscribe();&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;colist arabic&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Create a table in the Postgres database and insert two records&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Register an instance of the Debezium Postgres connector&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Read two records from the change event topic in Kafka and assert their attributes&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note how Debezium’s Testcontainers support allows to seed the connector configuration from the database container,
avoiding the need to give the database connection properties explicitly.
Only the unique &lt;code&gt;database.server.name&lt;/code&gt; must be given,
and of course you could apply other configuration options such as table or column filters, SMTs and more.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The source code for the &lt;code&gt;drain()&lt;/code&gt; method for reading a given number of records from a Kafka topic is omitted for the sake of brevity.
You can &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/testcontainers/src/test/java/io/debezium/examples/testcontainers/DebeziumContainerTest.java#L125-L138&quot;&gt;find it&lt;/a&gt; in the full example on GitHub.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/json-path/JsonPath&quot;&gt;JsonPath-based&lt;/a&gt; assertions come in handy for asserting the attributes of the expecting data change events,
but of course you could also use any other JSON API for the job.
When using Apache Avro instead of JSON as a serialization format, you’d have to use the Avro APIs instead.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;wrap_up&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#wrap_up&quot; /&gt;Wrap-Up&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Testcontainers and Debezium’s support for it make it fairly easy to write automated integration tests for your CDC set-up.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The testing approach discussed in this post could be expanded in multiple ways.
E.g. it might be desirable to put your connector configuration under revision control
(so you can manage and track any configuration changes)
and drive the test using these configuration files.
You also might take things one step further and test your entire data streaming pipeline.
To do so, you’d have to deploy not only the Debezium connector(s),
but also a sink connector, e.g. for your data warehouse or search server.
You could then run assertions against the data in those sink systems,
ensuring the correctness of your data pipeline end-to-end.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;What’s your take on testing CDC set-ups and pipelines?
Let us know in the comments below!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2020/03/13/debezium-1-1-c1-released/</id>
<title>Debezium 1.1.0.CR1 Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2020-03-13T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2020/03/13/debezium-1-1-c1-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="oracle"></category>
<category term="sqlserver"></category>
<category term="db2"></category>
<summary>



It&#8217;s my pleasure to announce the release of Debezium 1.1.0.CR1!


This release brings a brand-new API module, including a facility for overriding the schema and value conversion of specific columns.
The Postgres connector gained the ability to reconnect to the database after a connection loss, and the MongoDB connector supports the metrics known from other connectors now.




The Debezium API Module


In most cases Debezium users don&#8217;t interact with Debezium programmatically,
but rather configure and deploy the different connectors via Kafka Connect.
There are some exceptions, though, most notably the Debezium embedded engine.
Also we&#8217;re expanding Debezium by adding SPIs like the one for customizing schemas and...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;1.1.0.CR1&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This release brings a brand-new API module, including a facility for overriding the schema and value conversion of specific columns.
The Postgres connector gained the ability to reconnect to the database after a connection loss, and the MongoDB connector supports the metrics known from other connectors now.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;the_debezium_api_module&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_debezium_api_module&quot; /&gt;The Debezium API Module&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In most cases Debezium users don’t interact with Debezium programmatically,
but rather configure and deploy the different connectors via Kafka Connect.
There are some exceptions, though, most notably the Debezium &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/development/engine.html&quot;&gt;embedded engine&lt;/a&gt;.
Also we’re expanding Debezium by adding SPIs like the one for customizing schemas and values (see below).
This raises the question, which packages and classes within Debezium are meant for public access by users, and which ones not.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To clarify that split between Debezium’s public Java API and its internal implementation packages, a new API module has been introduced.
For compilation, user code should only depend on this module,
whereas the &lt;em&gt;debezium-embedded&lt;/em&gt; and &lt;em&gt;debezium-core&lt;/em&gt; artifacts should be runtime dependencies.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;E.g. the following set-up should be used with Apache Maven:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;io.debezium&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;debezium-api&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;${version.debezium}&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;io.debezium&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;debezium-embedded&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;${version.debezium}&amp;lt;/version&amp;gt;
    &amp;lt;scope&amp;gt;runtime&amp;lt;/scope&amp;gt;
&amp;lt;/dependency&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While backwards compatible evolution will be ensured for the API module,
breaking changes in the implementation modules may be done at any time.
So you should make sure your application code doesn’t rely on these.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With the introduction of the API module, also the bootstrap API for the embedded engine has been reworked.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;DebeziumEngine&amp;lt;SourceRecord&amp;gt; engine = DebeziumEngine.create(Connect.class)
    .using(config)
    .notifying(record -&amp;gt; {
        System.out.println(record);
    })
    .build();&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Event handlers still receive the Kafka Connect &lt;code&gt;SourceRecord&lt;/code&gt; type for now,
but going forward we’ll also support event handlers working with other representations such as JSON or Avro.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;schema_and_value_customization_spi&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#schema_and_value_customization_spi&quot; /&gt;Schema and Value Customization SPI&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A question that has come repeatedly in the chat and on the mailing list is how to customize the schema and value of specific change event fields.
For instance you might want to export temporal column values as a specifically formatted string instead of the default representation (a long value representing milli-seconds since epoch).
The answer to this has been to implement a custom message transformation (SMT) for Kafka Connect.
As they can add some overhead in terms of through-put though
(in particular when dealing with more complex schemas),
we’ve added a new extension point to Debezium itself,
which allows you to override the schema and value conversion for specific columns of captured tables.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As an example, the following converter could be implemented to export Postgres &lt;code&gt;ISBN&lt;/code&gt; column values with a specific logical field type &quot;isbn&quot; instead of the default &quot;string&quot; schema:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public class IsbnConverter implements CustomConverter&amp;lt;SchemaBuilder, RelationalColumn&amp;gt; {

    private SchemaBuilder isbnSchema;

    // could evaluate config here
    @Override
    public void configure(Properties props) {
        isbnSchema = SchemaBuilder.string().name(&quot;isbn&quot;);
    }

    @Override
    public void converterFor(RelationalColumn column,
            ConverterRegistration&amp;lt;SchemaBuilder&amp;gt; registration) {

        // register custom schema and value conversion for &quot;isbn&quot; columns
        if (&quot;isbn&quot;.equals(column.typeName())) {
            registration.register(isbnSchema, x -&amp;gt; x.toString());
        }
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In order to use such custom converter, compile it and add the JAR to the plug-in directory of the connector.
Configure it like so:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;converters=isbn
isbn.type=com.example.IsbnConverter
# custom options as needed
isbn.foo=bar&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;other_features_and_changes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#other_features_and_changes&quot; /&gt;Other Features and Changes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The MongoDB connector has been migrated to the common CDC connector framework we’ve been started to develop for the SQL Server and Oracle connectors.
This is going to significantly simplify the maintenance of the code base for us,
as many cross-cutting features can be implemented in one central place.
As an example, the MongoDB connector now supports the metrics you already can use with the relational connectors
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-845&quot;&gt;DBZ-845&lt;/a&gt;),
allowing you to monitor that connector in production.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Postgres connector supports automatic reconnects now in case the database connection has been lost
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1723&quot;&gt;DBZ-1723&lt;/a&gt;).
Again this has been largely implemented as a generic facility, the Postgres connector is the first one to make use of this.
We’ve seen increased reports of Postgres disconnects in specific environments,
which is why we decided this to be the first connector to support reconnects.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When streaming change events to other relational databases,
the &lt;code&gt;column.propagate.source.type&lt;/code&gt; connector option comes in handy for propagating the exact column definition as a schema header.
So far, this was to be configured per each individual column;
as of this release, this option can be set globally for column types,
drastically reducing the need for configuration when working with many columns of a specific type whose schema information should be exported
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1830&quot;&gt;DBZ-1830&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another improvement for MongoDB users is the new support for exporting information about the sharding key in update and delete events
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1781&quot;&gt;DBZ-1781&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As always, a good number of bugs was fixed, too.
Overall, &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%201.1.0.CR1%20ORDER%20BY%20issuetype%20DESC&amp;amp;startIndex=20&quot;&gt;44 issues&lt;/a&gt; were addressed for Debezium 1.1.0.CR1.
Please refer to the &lt;a href=&quot;https://debezium.io/releases/1.1/release-notes/#release-1.1.0-cr1&quot;&gt;release notes&lt;/a&gt; for more details.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;An open-source project like Debezium would be nothing without its community of contributors.
The following people have contributed to this release:
&lt;a href=&quot;https://github.com/Alan-zhangzf&quot;&gt;Alan Zhangzf&lt;/a&gt;,
&lt;a href=&quot;https://github.com/FabioCantarini&quot;&gt;Fabio Cantarini&lt;/a&gt;,
&lt;a href=&quot;https://github.com/blcksrx&quot;&gt;Hossein Torabi&lt;/a&gt;,
&lt;a href=&quot;https://github.com/JanHendrikDolling&quot;&gt;JanHendrikDolling&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jgraf50&quot;&gt;John Graf&lt;/a&gt;,
&lt;a href=&quot;https://github.com/raultov&quot;&gt;Raúl Tovar&lt;/a&gt; and
&lt;a href=&quot;https://github.com/rgibaiev&quot;&gt;Ruslan Gibaiev&lt;/a&gt;.
Thank you to each and eveyone of you!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With the CR1 release being done,
it won’t be much longer until Debezium 1.1 Final.
Depending on issues found with this release candidate,
we might do a CR2 release, followed by the Final shortly thereafter.
For plans for future versions please refer to the &lt;a href=&quot;https://debezium.io/roadmap/&quot;&gt;roadmap&lt;/a&gt; and let us know about your requirements.
Our general plan is to adopt a cadence of quarterly minor releases,
i.e. you can expect Debezium 1.2 in about three months from now,
1.3 in Q3 and so on.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2020/03/05/db2-cdc-approaches/</id>
<title>Approaches to Running Change Data Capture for Db2</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2020-03-05T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2020/03/05/db2-cdc-approaches/" rel="alternate" type="text/html" />
<author>
<name>Luis Garcés-Erice, Sean Rooney, Peter Urbanetz</name>
</author>
<category term="db2"></category>
<category term="discussion"></category>
<summary>



We have developed a Debezium connector for usage with Db2 which
is now available as part of the Debezium incubator.
Here we describe the use case we have for Change Data Capture (CDC),
the various approaches that already exist in the Db2 ecology,
and how we came to Debezium. In addition, we motivate the approach
we took to implementing the Db2 Debezium connector.




Background: Bringing Data to a Datalake


In 2016  IBM started  an effort to  build a
single  platform on  which IBM&#8217;s  Enterprise Data  could be  ingested,
managed            ...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We have developed a Debezium connector for usage with &lt;a href=&quot;https://www.ibm.com/analytics/db2&quot;&gt;Db2&lt;/a&gt; which
is now available as part of the Debezium incubator.
Here we describe the use case we have for Change Data Capture (CDC),
the various approaches that already exist in the Db2 ecology,
and how we came to Debezium. In addition, we motivate the approach
we took to implementing the Db2 Debezium connector.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;background_bringing_data_to_a_datalake&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#background_bringing_data_to_a_datalake&quot; /&gt;Background: Bringing Data to a Datalake&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In 2016  IBM started  an effort to  build a
single  platform on  which IBM’s  Enterprise Data  could be  ingested,
managed                                                            and
processed: the Cognitive Enterprise Data Platform (&lt;a href=&quot;https://www.slideshare.net/Chief_Data_Officer_Forum/ibm-chief-data-officer-summit-spring-2018-seth-dobrin-ed-walsh&quot;&gt;CEDP&lt;/a&gt;).
IBM Research was one of the major contributors to this project. One of
the  fundamental  activities  was bringing  data  from  geographically
distributed       data       centers       to       the       platform.
The &lt;a href=&quot;https://ieeexplore.ieee.org/document/8998484&quot;&gt;ingestion into the Datalake&lt;/a&gt; used a wide variety of technologies.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;https://debezium.io/images/2020-03-05-db2-cdc-approaches_diagram_1.png&quot; style=&quot;max-width:100%;&quot; class=&quot;responsive-image&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Figure 1. CEDP Logical Architecture&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A  significant  fraction  of  this  enterprise  data  is  gathered  in
relational   databases  present   in   existing  data-warehouses   and
datamarts.  These are generally  production systems whose primary usage
is as &quot;systems  of record&quot; for marketing, sales,  human resources etc.
As these are systems run by IBM for IBM unsurprisingly they are mainly
some variant of IBM’s Db2.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;getting_data_from_db2_efficiently&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#getting_data_from_db2_efficiently&quot; /&gt;Getting Data From Db2 Efficiently&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Data is ingested  into an immutable Landing Zone  within the Datalake.
This Landing Zone is implemented  as a HDFS instance.  Streaming data,
e.g. news, is moved from the  source using Kafka and then written into
HDFS using the appropriate connector.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One of  our key design objective  is automation. There are  over 5,000
relational database  tables from  over 200 different  sources ingested
every day. In  order to scale  the data processing platform  - aside from
the governance processes  that allow data owners to bring  data to the
platform - the ingestion itself must be self-service.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Initially relational data was always bulk loaded from the source using
&lt;a href=&quot;http://sqoop.apache.org/&quot;&gt;Sqoop&lt;/a&gt;.  A REST  Interface is made available such that  the data owners
can  configure when  the  data should  be  moved, e.g.   periodically,
trigger on  event etc.   A Sqoop  ingestion is  a distributed  set of
tasks each of which uses a JDBC connection to read part of a relational
database  table, generate  a file  based representation  of the  data,
e.g. &lt;a href=&quot;https://parquet.apache.org/&quot;&gt;Parquet&lt;/a&gt;, and  then store it on HDFS. With  Sqoop we can completely
refresh the  data, or append  to it, However we &lt;strong&gt;cannot&lt;/strong&gt;  modify
the data incrementally.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;From a practical point of view this limits the periodicity with which
data can  be updated. Some of  the larger tables represent tens of
GBytes of compressed Parquet. While Sqoop  allows many tasks to be run
in  parallel for  the  same  table the  bottleneck is typically  the
network  across  the WAN  and/or rate  controlling at the  source
database system itself.  Often only a  small fraction of the table is
modified on any particular day, meaning that a huge amount of data is
sent unnecessarily.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To address these issues we introduced the use of Change Data Capture (CDC) for the movement
of data  across the WAN.  Ingestion in CDC mode into a storage system  designed for files  that are
never modified is  problematic. While
some           recent          work           like
&lt;a href=&quot;https://databricks.com/product/delta-lake-on-databricks&quot;&gt;Deltalakes&lt;/a&gt;
or
&lt;a href=&quot;https://www.slideshare.net/Hadoop_Summit/what-is-new-in-apache-hive-30&quot;&gt;Hive 3.0&lt;/a&gt;
have  started introducing  delta  changes into  the Hadoop  ecosystem,
these were not mature enough for our needs.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As an alternative we use the concept of a Relational Database Drop Zone in which
data owners can instantiate shadows of their database and from which
we then ingest  into HDFS. As the  Drop Zone and Landing  Zone are in
the  same  data  center  and  the   ingesting  of  data  is  a  highly
parallelizable  task,  the  actual  ingestion of  large  tables  was
typically orders of magnitude faster  than the transferring of the data
from the source.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Data owners  could move data  using whatever tool they  preferred into
their Drop Zone. In particular they could transfer &lt;em&gt;changes&lt;/em&gt; to data obtained through CDC.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;CDC  systems are  almost  as old  as  relational databases  themselves.
Typically  these were  designed  for purposes  of  back-up or  failure
recovery and were designed for use by a database administrator.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Db2 has a long pedigree being over  40 years old and running on a wide
set of operating systems including zOS, AIX, Linux and Windows. It has
evolved a  large set of distinct  tools for CDC for  uses in different
contexts.     We    started    exploring     the    use    of    IBM’s
&lt;a href=&quot;https://www.ibm.com/support/pages/q-replication-and-sql-replication-product-documentation-pdf-format-version-101-linux-unix-and-windows&quot;&gt;SQL
Replication&lt;/a&gt;.  Once tables are put into CDC mode by the admin, a
capture agent is started that reads changes made to those tables from
the transaction log. The changes are  stored in dedicated CDC Tables. At
the remote database an apply agent periodically reads the changes from
these CDC tables and updates the shadow tables.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While conceptually this is quite simple in practice it is difficult to
automate  for the  following reasons:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Source  and sink  are tightly coupled  and therefore the same  table cannot  easily be replicated  to multiple  different target  database systems.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If  the source  system  was already  using replication on a  table e.g. for back-up purposes, we  cannot use this method to replicate to the Datalake.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Elevated privileges are required on the source. Data owners give read access to their system for Sqoop, but  giving administrator poses compliance problems.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Elevated privileges  are  required  on the  sink.  For simplicity  our  Drop Zone  is  a  single  Db2 system  with  database instances for each  of the data sources.  Allowing the  data owners to set up  SQL Replication to  the Drop Zone  would allow them  access to each other’s instances, which is  a compliance violation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The  tools are designed for system admins and as a result  there are a large number  of gotchas  for  the  unwary. For example, care must be taken in selecting a wide range of parameters such as: the mode  that  the transaction log has  to be in to  allow CDC, the time  the last backup was taken, whether the  database is row or column oriented etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is a Db2-specific  solution; although the  majority of the  relational data
sources were Db2, we also had Netezza, MySQL and SQL Server sources.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We found in  practice that the combination of the  above meant that it
was impractical to  allow data owners to use IBM  SQL Replication as a
CDC mechanism for the Datalake.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;IBM  offers another  set  of  tools for  data  replication called  IBM
InfoSphere Data Replication (&lt;a href=&quot;https://www.ibm.com/support/knowledgecenter/SSTRGZ_11.4.0/com.ibm.idr.frontend.doc/pv_welcome.html&quot;&gt;IIDR&lt;/a&gt;). This is sold as a product distinct
from Db2.   IIDR is not  a Db2 specific  solution, working for  a wide
range of relational  databases as well as  non-relational data storage
systems, e.g. file systems.  In essence IIDR has source agents and sink
agents.   The source  and sink  agent run  at or  close to  the target
system. Source agents  read the changes and propagate them  via a wide
range of protocols  including TCP sockets, MQ, Shared Files  etc. to the
sink agent.  The  source and sink agents are configured  via an entity
called the Access Server through which sources are  connected to sinks
and the tables  to capture are specified. The Access Server is itself
typically  controlled via  a  Graphical User  Interface  by a  system
administrator.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Thus for example we can have a Db2 source agent and an IIDR Kafka sink agent
that behaves like a standard Apache Kafka Connect source connector, i.e. it writes change events into a Kafka topic. The initial records are Upsert messages (REFRESH phase) and subsequent changes are propagated as a sequence of Upsert/Delete
Messages (MIRROR phase).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;IIDR   makes   the  system   more   loosely   coupled  and   less   Db2
specific. However, it is still not simple to automate. In essence we
need to be able  to allow a data owner to  specify the source database
system and the  tables to replicate via a REST  call and automatically
configure and  deploy the  necessary agents and  Access Server  on our
Kubernetes cluster. As we can not run on the source system itself we
catalog the remote Db2 system to look  like it was local and ran the
agent on that.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;IIDR assumes the  agent runs on the same hardware  architecture as the
relational database system. The IIDR agent uses a low level &lt;a href=&quot;https://www.ibm.com/support/knowledgecenter/SSEPGG_11.5.0/com.ibm.db2.luw.apdv.api.doc/doc/r0001673.html&quot;&gt;Db2 API&lt;/a&gt; to
read            the           transaction            log.
Many  of our  sources systems  are  running on  AIX/PowerPC while the
Kubernetes platform on which the agents are deployed runs on Linux/Intel. This leads to endianness compatibility problems.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There  are two limitations to this approach:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;IIDR  is designed to be monitored and managed by  a system administrator. Trying to capture
the actions and responses of an administrator via scripts that parse these
logs and attempt  to react to failure in IIDR can only be brittle.  As long as nothing
misbehaves  the  system  runs  fine, but  if  something  fails  (network
interruption, Kubernetes proxy failure, LDAP  being down, etc.)  it
is almost  impossible to  automate the  appropriate response.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;While touching  the source  system as  little as  possible was  an admirable
objective, from a practical point of  view it is almost impossible on a
production system to run the CDC  system independently of the source system.
If a system admin reloads from back-up  an older version of a table or
radically changes the  DDL of that table the CDC  system must be aware
that this has occurred and take the appropriate action. In the case of
changing the  DDL, a  new version  of the table  is created and
consequently a new version of the KTable in turn must be created.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We  saw these  and many  more  problems when  trying to  use the  above
approach for using  CDC against real production  systems.  We concluded
that the administration of the CDC system and the source system cannot
be done independently and that to a large extent our problems came from
trying to use IIDR for a use-case for which it was not intended.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;approaches_to_implementing_a_debezium_db2_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#approaches_to_implementing_a_debezium_db2_connector&quot; /&gt;Approaches to Implementing a Debezium Db2 Connector&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When  Debezium  became available  we  started  evaluating it  for  our
purposes.  As it works with a wide range of relational database systems and
is open source we could imagine that database administrators would allow
it  to  be  used  to  generate a  representation  of  their  data  for
downstream  applications.   Essentially,  the  Debezium  system  would
become an  extension of  the database source  system. Debezium  is not
required to produce an &lt;strong&gt;identical&lt;/strong&gt;  copy of the database  tables (unlike
IIDR or SQL Replication). Typically the downstream application are for
auxiliary tasks, i.e.  analytics, not for fail  over, meaning problems
such as preserving precise types are less pressing.  For example, if
a time-stamp  field is represented  as a  string in Elasticsearch it
is not the end of the world.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The only concern we had  with Debezium was  that it didn’t  have a
connector for Db2.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Two approaches  presented themselves:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Use  the low level Db2  API to read directly the transaction log like  IIDR does.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the SQL Replication CDC capture tables to read capture tables using SQL.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;An investigation  of the code concluded that  the model used  by the
already &lt;a href=&quot;https://debezium.io/documentation/reference/1.0/connectors/sqlserver.html&quot;&gt;existing  connector&lt;/a&gt;
for  Microsoft  SQL  Server  could
be largely reused for Db2. In essence:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The SQL queries to poll the changes are different&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The structure and nature of  the Logical Sequence Number (LSN) are  different&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The fact  that Db2 distinguishes between a  database system and  a database  while SQL Server does not needs to be accounted for.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Otherwise everything else could be reused. Thus we adapted the existing SQL Server code base
to implement the Db2 connector.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;future_workextensions&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#future_workextensions&quot; /&gt;Future Work/Extensions&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;benchmarking&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#benchmarking&quot; /&gt;Benchmarking&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Connectors  for Db2 and SQL Server use a polling model i.e. the connectors
periodically query the CDC table to determine what has changed since
the last time they polled.
A natural question is what is the &quot;optimal&quot; polling frequency given the fact that polling itself has a cost, i.e. what are the trade-offs between latency and load ?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We are interested in building a general purpose framework for benchmarking
systems in order to get a better understanding of where the trade-offs
are in terms of latency, throughput of the CDC system and load on the
source system.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;db2_notification_system&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#db2_notification_system&quot; /&gt;Db2 Notification System&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Rather than building a polling connector for Db2 it would also be possible to create a notification
system. We considered this, but decided the polling connector was simpler for a first implementation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One way to build a notification connector for Db2 would be to:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Identifying change events by the usage of OS file system watchers (Linux or Windows).
This can monitor the transaction log directory of the Db2 database and send events when files are modified or created.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Determining the exact nature of the event by reading the actual table changes with the &lt;a href=&quot;https://www.ibm.com/support/knowledgecenter/en/SSEPGG_11.1.0/com.ibm.db2.luw.apdv.api.doc/doc/r0001673.html&quot;&gt;db2ReadLog API&lt;/a&gt;. In principle
this API can be invoked remotely as a service.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Determining the related Db2 data structure via SQL connection, e.g. table DDL.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium event-driven Db2 connector would wait on notifications and then read the actual changes via db2ReadLog and SQL.
This would require the watcher agent to run locally on the database system, similarly to the capture server.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;dml_v_ddl_changes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#dml_v_ddl_changes&quot; /&gt;DML v DDL Changes&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Change Data Capture (CDC) systems propagate modifications at the source tables made via Data Manipulation Language (DML) operations such as INSERT, DELETE etc. They do not explicitly handle changes to the source table made via Data Definition Language (DDL) operations such as TRUNCATE, ALTER etc. It is not really clear what the behavior of Debezium should be made when a DDL change occurs. We are
looking at exploring what the Debezium model should be for changes of this sort.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#conclusion&quot; /&gt;Conclusion&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While it is attractive to assume new enterprise data systems are built completely from scratch it will almost certainly be necessary to interact with existing relational database systems for some considerable time.
Debezium is a promising framework for connecting existing enterprise data systems into data processing platforms such as Datalakes.
Our work currently at IBM Research is focusing on building hybrid-cloud data-orchestration systems with Kafka and Debezium being central components.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2020/02/25/lessons-learned-running-debezium-with-postgresql-on-rds/</id>
<title>Lessons Learned from Running Debezium with PostgreSQL on Amazon RDS</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2020-02-25T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2020/02/25/lessons-learned-running-debezium-with-postgresql-on-rds/" rel="alternate" type="text/html" />
<author>
<name>Ashhar Hasan</name>
</author>
<category term="aws"></category>
<category term="postgres"></category>
<category term="rds"></category>
<summary>



In this blog post, we are going to discuss how Delhivery, the leading supply chain services company in India, is using Debezium to power a lot of different business use-cases ranging from driving event driven microservices, providing data integration and moving operational data to a data warehouse for real-time analytics and reporting. We will also take a look at the early mistakes we made when integrating Debezium and how we solved them so that any future users can avoid them, discuss one of the more challenging production incidents we faced and how Debezium helped ensure we could recover without any...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In this blog post, we are going to discuss how &lt;a href=&quot;https://www.delhivery.com/&quot;&gt;Delhivery&lt;/a&gt;, the leading supply chain services company in India, is using Debezium to power a lot of different business use-cases ranging from driving event driven microservices, providing data integration and moving operational data to a data warehouse for real-time analytics and reporting. We will also take a look at the early mistakes we made when integrating Debezium and how we solved them so that any future users can avoid them, discuss one of the more challenging production incidents we faced and how Debezium helped ensure we could recover without any data loss. In closing, we discuss what value Debezium has provided us, areas where we believe there is a scope for improvement and how Debezium fits into our future goals.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;debezium_at_delhivery&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#debezium_at_delhivery&quot; /&gt;Debezium at Delhivery&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We work in the logistics landscape and hence most of the software we write is focused around state - status changes of a shipment, tracking location updates, collecting real time data and reacting to it. The most common place where you might find &quot;state&quot; in any software architecture is the database. We maintain all of our transactional data primarily in a document database like MongoDB and in relational database systems (specifically PostgreSQL) for different services within the organisation. There is a need to allow efficient and near-real time analysis of the transactional data across all the different data sources to allow surfacing insights and looking at the big picture of how the organisation is doing and to make data driven decisions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To solve the above goal, we are using Debezium to perform Change Data Capture on our transactional data to make it available in Kafka, our choice of message broker. Once that data is available in Kafka we can do one or all of the following:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Perform streaming joins or data enrichment across change streams of different relational tables (maybe even from different databases or services altogether. eg. Enriching shipments with trip and vehicle data)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Creating domain events from change streams for consumption by downstream services (eg. Creating an aggregate message with order, shipment and product information from three different change streams)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Moving the change stream data into a data lake to allow for disaster recovery or replaying part of the data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Complex event processing to generate real time metrics and power dashboards (eg. Live count of items in transit, average trip time within each region etc.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium makes all of those above use-cases possible and very easy to build by providing a common platform and framework to connect our existing data sources like MongoDB, PostgreSQL or MySQL.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This article is going to share our learnings with using Debezium on AWS RDS (AWS’s managed database service) and hopefully help transfer some knowledge we’ve gained in that process and also document how to skip unparseable records from PostgreSQL’s WAL until &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1760&quot;&gt;DBZ-1760&lt;/a&gt; is fixed (already implemented and scheduled for the next Debezium 1.1 preview release).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here’s a brief architecture overview that shows a few of the use-cases that Debezium is powering and the general data platform.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;https://debezium.io/images/2020-02-25-debezium-on-rds/figure01.png&quot; style=&quot;max-width:90%;&quot; class=&quot;responsive-image&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Figure 1. Current Architecture&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But to get to the above was an iterative process which took a lot of experimentation and trial and error.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;debezium_with_postgresql_on_aws_rds&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#debezium_with_postgresql_on_aws_rds&quot; /&gt;Debezium with PostgreSQL on AWS RDS&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now we are going to discuss some of the learnings from running Debezium with PostgreSQL on AWS RDS. We are not going to focus on how to get started with Debezium with PostgreSQL on RDS since it’s documented in detail at &lt;a href=&quot;https://debezium.io/documentation/reference/1.0/connectors/postgresql.html#amazon-rds&quot;&gt;the documentation for the PostgreSQL connector&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;lessons_learnt&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#lessons_learnt&quot; /&gt;Lessons Learnt&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We started by creating a proof-of-concept whose goal was to listen to changes from 3 different tables within a single PostgreSQL database and create two views downstream, one as the join of the three tables and another view which includes aggregated metrics tracked as a time-series. Both the join and the aggregations were implemented using &lt;a href=&quot;https://kafka.apache.org/documentation/streams/&quot;&gt;Kafka Streams&lt;/a&gt; since it was easier to setup and learn compared to other stream processing frameworks. Since Debezium already provides a very feature rich &lt;a href=&quot;https://hub.docker.com/r/debezium/connect&quot;&gt;Docker container image&lt;/a&gt; we extended that slightly and decided to run the service as containers on AWS’s Elastic Container Service which is a container orchestration service.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There were a few mistakes we made when we were starting out. All of the solutions to our mistakes are now documented in the Debezium documentation but they are all listed together here to make it easier to avoid them.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We initially used the &lt;strong&gt;wal2json&lt;/strong&gt; plugin which caused the connector to encounter &lt;strong&gt;OutOfMemoryError&lt;/strong&gt; when committing large transactions (transactions whose serialized form uses more memory than available Java Heap space). Hence our recommendation is:&lt;/p&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;On PostgreSQL &amp;lt; 10, use the &lt;strong&gt;wal2json_streaming&lt;/strong&gt; plugin to avoid &lt;strong&gt;OutOfMemoryError&lt;/strong&gt; on large transactions.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;On PostgreSQL &amp;gt;= 10, use the &lt;strong&gt;pgoutput&lt;/strong&gt; plugin.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We were producing JSON messages with schemas enabled; this creates larger Kafka records than needed, in particular if schema changes are rare. Hence we decided to disable message schemas by setting &lt;code&gt;key.converter.schemas.enabled&lt;/code&gt; and &lt;code&gt;value.converter.schemas.enabled&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt; to reduce the size of each payload considerably hence saving on network bandwidth and serialization/deserialization costs. The only downside is that we now need to maintain the schema of those messages in an external schema registry.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We were observing a few data-types with base64 encoded data. As &lt;a href=&quot;https://debezium.io/documentation/reference/1.0/connectors/postgresql.html#decimal-values&quot;&gt;documented&lt;/a&gt;, that’s the default for NUMERIC columns, but can be difficult to handle for consumers. To convert to a format that’s easier to parse at the expense of some accuracy loss, we configured the data type specific properties &lt;a href=&quot;https://debezium.io/documentation/reference/1.0/connectors/postgresql.html#data-types&quot;&gt;as documented&lt;/a&gt;. Specifically, set &lt;code&gt;decimal.handling.mode&lt;/code&gt; to &lt;code&gt;string&lt;/code&gt; to receive NUMERIC, DECIMAL and equivalent types as a string (eg. &quot;3.14&quot;) and set &lt;code&gt;hstore.handling.mode&lt;/code&gt; to &lt;code&gt;json&lt;/code&gt; to receive HSTORE columns as a JSON string.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Always ensure basic hygiene checks like database disk usage, transaction logs disk usage and network and disk bandwidth being used for read and write operations. No alarms were set up on the transaction logs disk usage on the database. We added alarms on the RDS metric &lt;strong&gt;TransactionLogsDiskUsage&lt;/strong&gt; and &lt;strong&gt;OldestReplicationSlotLag&lt;/strong&gt; to alert us when the transaction logs disk usage increased above a threshold or when a replication slot started lagging - meaning that Debezium might have died.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We had not enabled heartbeats in Debezium. Heartbeats are needed to control the &lt;a href=&quot;https://debezium.io/documentation/reference/1.0/connectors/postgresql.html#wal-disk-space&quot;&gt;WAL disk space consumption&lt;/a&gt; in the following cases:&lt;/p&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;There are many updates in a monitored database but only a minuscule amount relates to the monitored table(s) and/or schema(s). We handled this case by enabling heartbeats by setting &lt;code&gt;heartbeat.interval.ms&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The PostgreSQL instance contains multiple databases where the monitored database is low-traffic in comparison to the others. Since the WAL is shared by all databases in an instance it keeps accumulating data which cannot be removed until Debezium reads it. But since the high traffic databases are not monitored Debezium is not able to communicate to the database that the WAL files can be removed to reclaim disk space. To solve this scenario we triggered &quot;heartbeat&quot; events by periodically updating a single row in a table created in the monitored database using the below query:&lt;/p&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;CREATE TABLE IF NOT EXISTS heartbeat (id SERIAL PRIMARY KEY, ts TIMESTAMP WITH TIME ZONE);
INSERT INTO heartbeat (id, ts) VALUES (1, NOW()) ON CONFLICT(id) DO UPDATE SET ts=EXCLUDED.ts;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Since this is a common use-case that comes up when using Debezium with PostgreSQL, an issue has been created to track this at &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1815&quot;&gt;DBZ-1815&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We got severely reduced throughput on tables with JSONB columns. After debugging we were able to confirm the reason as frequent schema refresh by Debezium due to TOASTed columns not being present in the replication message. This was fixed by changing &lt;code&gt;schema.refresh.mode&lt;/code&gt; to &lt;code&gt;columns_diff_exclude_unchanged_toast&lt;/code&gt; and has since been &lt;a href=&quot;https://debezium.io/documentation/reference/1.0/connectors/postgresql.html#discrepance-between-plugins&quot;&gt;documented&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We observed frequent EOF errors on the database connection on a few RDS instance sizes. We are still not sure of the cause but initial investigations point to the issue happening only on instances that have PgBouncer attached (even if not connecting through PgBouncer) or instances with smaller sizes (AWS t2/t3 series).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We initially used a single Debezium connector per PostgreSQL database (instead of per host) but then moved to using a single connector for each team. The main reasons for not running a single connector per PostgreSQL instance were regarding workload isolation. Any team performing bulk data updates or deletions or unplanned schema migrations will only impact their own Debezium connector instead of the entire PostgreSQL instance since Debezium filters events at its end according to the database and/or schema whitelist and blacklist configured. We are trying to identify possible issues in this configuration but haven’t found any yet. Moving to a single connector per team setup also eased a lot of management overhead regarding configuration changes since we no longer need to co-ordinate between multiple teams when creating a release plan for any changes. Although multiple replication slots on a single database do add overhead, we are able to run fine with around 6 to 10 slots per database host without any noticeable performance impact.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;production_incidents&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#production_incidents&quot; /&gt;Production Incidents&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As is common with every software development project we did hit a few issues and here we discuss one of the more difficult ones in detail. But thanks to Debezium being focused on ensuring data consistency we were able to recover without &lt;strong&gt;ANY&lt;/strong&gt; data loss.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;The issue we discuss below is already fixed in Debezium 1.0 and you should update as soon as possible&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A new feature for skipping such unprocessable events in general has been merged as &lt;a href=&quot;https://github.com/debezium/debezium/pull/1271&quot;&gt;PR#1271&lt;/a&gt; in the core Debezium framework and will be part of the next Debezium 1.1 preview release.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Two of the common things developers often fail to do are proper date-time handling and software version upgrades. Both of these can lead to issues on their own but makes things difficult when both occur together. We recently faced such an issue and provide a way to handle it. We’ll start with some background on why this issue came up in the first place.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;PostgreSQL’s &lt;a href=&quot;https://www.postgresql.org/docs/current/datatype-datetime.html&quot;&gt;date/time types documentation&lt;/a&gt; states that the TIMESTAMP types can range from &lt;strong&gt;4713 BC&lt;/strong&gt; to &lt;strong&gt;294276 AD&lt;/strong&gt;. Before Debezium 0.10, there were serveral issues regarding datetime overflow for dates too far into the future like &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1255&quot;&gt;DBZ-1255&lt;/a&gt; and &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1205&quot;&gt;DBZ-1205&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;the_bug_and_dealing_with_it&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_bug_and_dealing_with_it&quot; /&gt;The Bug and Dealing With It&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To hit the above issue you need to have a date sufficiently far into the future. You can get one if you are not using ISO8601 or epoch time and have a bug in your custom datetime formatter.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So, the bug was triggered by the application writing a datetime value containing the year &lt;strong&gt;20200&lt;/strong&gt; into one of the tables monitored by Debezium which caused Debezium to throw an exception since we were still running on 0.9 in production.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Unfortunately our log pattern alerts did not work that day and the error silently skipped past us until the high replication lag alarms went off. Upon inspecting the logs we did figure out where the issue was coming from and for which value. Unfortunately the log did not tell what table the issue was in (&lt;em&gt;hint - can become a valuable contribution&lt;/em&gt;) and which column contained the offending value. Luckily only four tables were monitored and each of them had two TIMESTAMPTZ columns and it was easy to query for the offending value in those to find the actual record.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A quick read of the source code showed us that this was happening for any year &amp;gt; 9999 and hence we queried the database to check if any other such values existed. Thankfully no other values existed. By now we had a clear plan in mind:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;Stop Debezium&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Correct the data for the record&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Somehow get Debezium to skip the unparseable record&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add validations to database to ensure such values don’t skip through for the time being&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Upgrade Debezium to 1.0&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But we were stuck at the 3rd step above since we could not find an equivalent option to MySQL’s &lt;code&gt;event.deserialization.failure.handling.mode&lt;/code&gt; for the PostgreSQL connector.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;how_debezium_and_postgresql_track_offsets&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#how_debezium_and_postgresql_track_offsets&quot; /&gt;How Debezium and PostgreSQL track offsets&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Each change record in PostgreSQL has a position which is tracked using a value known as a log sequence number (LSN). PostgreSQL represents it as two hexadecimal numbers - logical &lt;strong&gt;xLog&lt;/strong&gt; and &lt;strong&gt;segment&lt;/strong&gt;. Debezium represents it as the decimal representation of that value. The actual conversion implementation can be seen in PostgreSQL’s JDBC driver &lt;a href=&quot;https://github.com/pgjdbc/pgjdbc/blob/1970c4a3fb8ebf4cc52f5d8b0d4977388ee713e7/pgjdbc/src/main/java/org/postgresql/replication/LogSequenceNumber.java#L42&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Periodically Debezium writes the last processed LSN and transaction id to the Kafka Connect offsets topic and advances the replication slot to match that. On startup, Debezium uses the last record from the Kafka Connect offsets topic to rewind the replication slot to the position as described before continuing streaming changes. This means that to change the position in the WAL where Debezium picks up from requires a change in both Debezium’s tracked information in the Kafka Connect offsets topic as well as server side in PostgreSQL.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;skipping_unparseable_events&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#skipping_unparseable_events&quot; /&gt;Skipping Unparseable Events&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We were able to use the above information to make Debezium skip the unparseable event by performing the following steps:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;Stop Debezium to make the replication slot inactive.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Check Debezium has stopped listening on the replication slot by running &lt;code&gt;SELECT * FROM pg_replication_slots WHERE slot_name = &#39;&amp;lt;your-slot-name&amp;gt;&#39;;&lt;/code&gt;. The &lt;code&gt;active&lt;/code&gt; column should be &lt;code&gt;f&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Check the last message in Debezium’s offsets topic and note down the value for the &lt;code&gt;lsn&lt;/code&gt; key. eg. &lt;code&gt;1516427642656&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Convert that long representation of LSN into the hexadecimal format using PosgtreSQL’s Java driver using the below Java code:&lt;/p&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;import org.postgresql.replication.LogSequenceNumber;

class Scratch {
  public static void main(String[] args) {
      LogSequenceNumber a = LogSequenceNumber.valueOf(1516427642656L);
      System.out.println(a.asString());
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Peek changes from the WAL upto the LSN above using &lt;code&gt;SELECT pg_logical_slot_peek_changes(&#39;&amp;lt;your-slot-name&amp;gt;&#39;, &#39;&amp;lt;lsn-from-above&amp;gt;&#39;, 1)&lt;/code&gt;. This is the replication change that we are going to skip, so please make sure that this is the record that you want to skip. Once confirmed, proceed to next step.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Advance the replication slot by skipping 1 change using &lt;code&gt;SELECT pg_logical_slot_get_changes(&#39;&amp;lt;your-slot-name&amp;gt;&#39;, NULL, 1)&lt;/code&gt;. This will consume 1 change from the replication slot.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Publish a message to Debezium’s offset topic with the next LSN and TxId. We were able to successfully get it working by adding 1 to both the &lt;code&gt;lsn&lt;/code&gt; and the &lt;code&gt;txId&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Deploy Debezium again and it should have skipped the record.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#conclusion&quot; /&gt;Conclusion&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;why_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#why_debezium&quot; /&gt;Why Debezium?&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In closing we would like to highlight the issues Debezium has solved for us.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One of the biggest concerns when handling any data is regarding data consistency and Debezium helps us avoid dual writes and maintains data consistency between our RDBMS and Kafka which makes it easier to ensure data consistency in all further layers.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium enables low overhead change data capture and now we have ended up defaulting to enabling Debezium for all new data sources being created.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium’s support for a wide variety of data sources, PostgreSQL, MySQL and MongoDB specifically, helps us provide a standard technology and platform to perform data integration on. No more having to write custom code to connect each data source.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium being open source proved to be immensely useful in the early days to make sure we were able to send in patches for a few bugs ourselves without having to ask someone to prioritise the issue. And since it’s open source there is a growing community around it which can help you figure out your issues and provide general guidance. Check out &lt;a href=&quot;https://debezium.io/community/&quot;&gt;this page&lt;/a&gt; on the Debezium website for a lot of awesome community contributed content.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;challenges&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#challenges&quot; /&gt;Challenges&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Having said the above Debezium is still quite a young project and has a few areas in which improvement will be welcome (and your contributions too in the form of code, design, ideas, documentation and even blog posts):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Zero-downtime high availability. Debezium relies on the Kafka Connect framework to provide high availability but it does not provide something similar to a hot standby instance. It takes time for an existing connector to shut down and a new instance to come up - which might be acceptable for a few use-cases but unacceptable in others. See &lt;a href=&quot;https://medium.com/blablacar-tech/streaming-data-out-of-the-monolith-building-a-highly-reliable-cdc-stack-d71599131acb&quot;&gt;this blog post by BlaBlaCar&lt;/a&gt; for a discussion and their solution around it.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support for other data sinks besides Kafka. In a few scenarios you might want to directly move the events from your database to an API, a different data store or maybe a different message broker. But since Debezium is currently written on top of Kafka Connect it can only write the data into Kafka. Debezium does provide an embedded engine which you can use as a library to consume change events in your Java applications.  See &lt;a href=&quot;https://debezium.io/documentation/reference/operations/embedded.html&quot;&gt;the documentation around embedding Debezium&lt;/a&gt;. In case you do end up writing a different adapter around Debezium to move data into a different destination, consider making it open source so that both you benefit by additional maintainers and the community benefits by getting new use cases solved.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Common framework to write any new CDC implementation. We particularly have a use case of performing CDC on top of AWS DynamoDB. Instead of writing a custom Kafka Connector from scratch, we can reuse the Debezium core framework and write only the DynamoDB specific parts. This will help prevent bugs since a lot of the existing flows and edge cases might have already been handled. There is ongoing work around this theme to refactor all existing Debezium connectors to use the common framework to make it easier to write new custom connectors. For an example of how to implement one, take a look at the &lt;a href=&quot;https://github.com/debezium/debezium-incubator&quot;&gt;Debezium incubator repository&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A few minor annoyances which are already tracked on the project’s issue tracker - specifically &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1760&quot;&gt;DBZ-1760 (skipping unparseable records)&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1263&quot;&gt;DBZ-1263 (update table whitelist for existing connector)&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/DBZ-1723&quot;&gt;DBZ-1723 (Reconnect to DB on failure)&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/DBZ-823&quot;&gt;DBZ-823 (Parallel snapshots)&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;future_scope&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#future_scope&quot; /&gt;Future Scope&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We do have a few tasks planned for the future to improve our existing workflow regarding Debezium and Kafka Connect.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Upgrading to Debezium v1.0. Debezium recently released the first 1.0 release with a number of new features including &lt;a href=&quot;https://debezium.io/documentation/reference/integrations/cloudevents.html&quot;&gt;support for the CloudEvents format&lt;/a&gt; which we are looking towards to provide a unified message format for all data across the organisation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Trying out the Outbox design pattern as documented at &lt;a href=&quot;https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/&quot;&gt;Reliable Microservices Data Exchange With the Outbox Pattern&lt;/a&gt; to unify application events and data change events. The outbox pattern also provides transactional guarantees across service boundaries in a microservices system - something everybody wants in an event based microservices architecture.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Setting up an &lt;a href=&quot;https://atlas.apache.org/&quot;&gt;Apache Atlas&lt;/a&gt; integration to automate the creation of data sources and tracking data lineage in Atlas to help with data governance and discoverability.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Writing and open sourcing an AWS DynamoDB CDC connector as a Debezium connector. Since we are using AWS DynamoDB too we need to provide the same capabilities that the other data sources are using in terms of CDC. For that we are writing a DynamoDB CDC connector using Debezium as a framework. The work is still in its early stages and is planned to be released as an open source connector.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So overall, we started the post by sharing our business use-case and discussed how Debezium has helped us solve them. We then detailed how we have been running Debezium in production for performing CDC on PostgreSQL on AWS RDS and talked about the mistakes we made when starting out and how to solve them. And as is common in software engineering, we did face production incidents along the way and are sharing our learnings from that incident in the hopes that they might be useful for the wider community.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;em&gt;Also a lot of thanks to the people who reviewed this post including &lt;a href=&quot;https://twitter.com/gunnarmorling&quot;&gt;Gunnar Morling&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/kbhara&quot;&gt;Kapil Bharati&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/akashdeep1&quot;&gt;Akash Deep Verma&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;further_reading&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#further_reading&quot; /&gt;Further Reading&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;debezium_documentation_and_repositories&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#debezium_documentation_and_repositories&quot; /&gt;Debezium Documentation and Repositories&lt;/h3&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://debezium.io/documentation/reference/1.0/connectors/postgresql.html&quot;&gt;Debezium PostgreSQL Connector Documentation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://debezium.io/documentation/reference/1.0/connectors/postgresql.html#amazon-rds&quot;&gt;Debezium with PostgreSQL on
Amazon RDS&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://debezium.io/documentation/reference/operations/embedded.html&quot;&gt;Debezium Embedded Engine&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/debezium/debezium-incubator&quot;&gt;Debezium Incubator Connectors - Cassandra, IBM DB2&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;external_documentation&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#external_documentation&quot; /&gt;External Documentation&lt;/h3&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://kafka.apache.org/documentation/streams/&quot;&gt;Kafka Streams&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.postgresql.org/docs/current/datatype-datetime.html&quot;&gt;PostgreSQL date/time data types&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/pgjdbc/pgjdbc/blob/1970c4a3fb8ebf4cc52f5d8b0d4977388ee713e7/pgjdbc/src/main/java/org/postgresql/replication/LogSequenceNumber.java#L42&quot;&gt;PostgreSQL LSN conversion in JDBC driver&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;blogs_and_articles&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#blogs_and_articles&quot; /&gt;Blogs and Articles&lt;/h3&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://medium.com/blablacar-tech/streaming-data-out-of-the-monolith-building-a-highly-reliable-cdc-stack-d71599131acb&quot;&gt;Streaming Data out of the Monolith: Building a Highly Reliable CDC Stack&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/&quot;&gt;Reliable Microservices Data Exchange With the Outbox Pattern&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;relevant_issues&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#relevant_issues&quot; /&gt;Relevant Issues&lt;/h3&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;open_issues&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#open_issues&quot; /&gt;Open Issues&lt;/h4&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1760&quot;&gt;DBZ-1760 - Add option to skip unprocesseable event&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1263&quot;&gt;DBZ-1263 - Allow table.whitelist to be updated after a connector is created&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1815&quot;&gt;DBZ-1815 - The Postgres connector heartbeat should optionally write back a heartbeat change to the DB&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;solved_issues&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#solved_issues&quot; /&gt;Solved Issues&lt;/h4&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1255&quot;&gt;DBZ-1255 - Debezium does not expect a year larger than 9999&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1205&quot;&gt;DBZ-1205 - Overflowed Timestamp in Postgres Connection&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2020/02/19/debezium-camel-integration/</id>
<title>Integration Scenarios with Debezium and Apache Camel</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2020-02-19T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2020/02/19/debezium-camel-integration/" rel="alternate" type="text/html" />
<author>
<name>Jiri Pechanec</name>
</author>
<category term="camel"></category>
<category term="integration"></category>
<category term="quarkus"></category>
<summary>



One of the typical Debezium uses cases is to use change data capture to integrate a legacy system with other systems in the organization.
There are multiple ways how to achieve this goal




Write data to Kafka using Debezium and follow with a combination of Kafka Streams pipelines and Kafka Connect connectors to deliver the changes to other systems


Use Debezium Embedded engine in a Java standalone application and write the integration code using plain Java; that&#8217;s often used to send change events to alternative messaging infrastructure such as Amazon Kinesis, Google Pub/Sub etc.


Use an existing integration framework or service bus to express...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One of the typical Debezium uses cases is to use change data capture to integrate a legacy system with other systems in the organization.
There are multiple ways how to achieve this goal&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Write data to Kafka using Debezium and follow with a combination of Kafka Streams pipelines and Kafka Connect connectors to deliver the changes to other systems&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/operations/embedded.html&quot;&gt;Debezium Embedded engine&lt;/a&gt; in a Java standalone application and write the integration code using plain Java; that’s often used to send change events to alternative messaging infrastructure such as Amazon Kinesis, Google Pub/Sub etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use an existing integration framework or service bus to express the pipeline logic&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This article is focusing on the third option - a dedicated integration framework.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;apache_camel&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#apache_camel&quot; /&gt;Apache Camel&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://camel.apache.org/&quot;&gt;Camel&lt;/a&gt; is an open-source integration framework that enables developers to read, transform, route and write data from and to disparate systems and services.
It provides a large amount of ready-made &lt;a href=&quot;https://camel.apache.org/components/latest/&quot;&gt;components&lt;/a&gt; that either provide interfaces to 3rd party systems or offers an implementation of &lt;a href=&quot;https://en.wikipedia.org/wiki/Enterprise_Integration_Patterns&quot;&gt;enterprise integration patterns&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This combination allows the developer to easily connect to target systems and express integration pipelines using a declarative DSL.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;camel_and_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#camel_and_debezium&quot; /&gt;Camel and Debezium&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://camel.apache.org/blog/Camel3-Whatsnew/&quot;&gt;Camel 3&lt;/a&gt; has been released by the end of the year 2019 and in addition to the major re-architecture new Debezium components have been added to the codebase.
It also enables Camel to be used as &lt;a href=&quot;https://camel.apache.org/camel-kafka-connector/latest/index.html&quot;&gt;a connector&lt;/a&gt; in Kafka Connect runtime.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This post focuses solely on Debezium components use and the latter option will be covered in a future post.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://camel.apache.org/components/latest/debezium-mysql-component.html&quot;&gt;Debezium MySQL Connector Component&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://camel.apache.org/components/latest/debezium-postgres-component.html&quot;&gt;Debezium PostgreSQL Connector Component&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://camel.apache.org/components/latest/debezium-sqlserver-component.html&quot;&gt;Debezium SQL Server Connector Component&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://camel.apache.org/components/latest/debezium-mongodb-component.html&quot;&gt;Debezium MongoDB Connector Component&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As you can see every &lt;em&gt;non-incubating&lt;/em&gt; Debezium connector is represented by its dedicated component.
The advantage of this solution is the complete isolation of dependencies and the type-safe configuration of connector instances.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Internally the component exposes a Debezium &lt;a href=&quot;https://camel.apache.org/manual/latest/endpoint.html&quot;&gt;endpoint&lt;/a&gt; with an event-driven &lt;a href=&quot;https://www.javadoc.io/doc/org.apache.camel/camel-api/latest/org/apache/camel/Consumer.html&quot;&gt;Camel consumer&lt;/a&gt; that encapsulates an instance of the &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/operations/embedded.html&quot;&gt;Debezium embedded engine&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;an_example&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#an_example&quot; /&gt;An Example&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As an example we’ve built a simple Question &amp;amp; Answer (Q &amp;amp; A) application,
loosely inspired by StackOverflow and the likes.
A REST API allows to post new questions as well as answers to existing questions,
which are stored in a database.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Any data changes generated by the application (e.g. if a new question or answer got created) are captured via Debezium and passed to the Camel pipeline,
which sends emails via an SMTP server and posts a corresponding tweet on a provided Twitter account.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can find the complete &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/camel-component&quot;&gt;source code&lt;/a&gt; of the example on GitHub.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;topology&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#topology&quot; /&gt;Topology&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There are multiple components in the solution topology:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;https://debezium.io/images/camel-component-topology.svg&quot; style=&quot;max-width:100%;&quot; class=&quot;responsive-image&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Figure 1. The Deployment Topology&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The Q &amp;amp; A application is implemented using the &lt;a href=&quot;https://quarkus.io/&quot;&gt;Quarkus&lt;/a&gt; stack and exposes a REST API to create questions and answers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The application stores its data in the PostgreSQL database&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Camel route runs as a plain Java application that uses an embedded &lt;a href=&quot;https://infinispan.org/&quot;&gt;Infinispan&lt;/a&gt; store to persist its state (used to build an aggregate object linking questions to their answers) and sends messages to about answered questions via e-mail and to an associated Twitter account&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A &lt;a href=&quot;https://github.com/mailhog/MailHog&quot;&gt;MailHog&lt;/a&gt; SMTP server running in a container for sending e-mails&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;question_answer_application&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#question_answer_application&quot; /&gt;Question &amp;amp; Answer Application&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The source application is a simple REST service based on Quarkus.
It manages two entities, &lt;code&gt;Question&lt;/code&gt; and &lt;code&gt;Answer&lt;/code&gt;, with a &lt;code&gt;1:n&lt;/code&gt; relation stored in the PostgreSQL database.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;https://debezium.io/images/camel-component-erd.svg&quot; style=&quot;max-width:100%;&quot; class=&quot;responsive-image&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Figure 2. The Q&amp;amp;A Backend Service Entity Relationship Diagram&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The entities are created using REST API and the association is automatically established between them.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;the_camel_pipeline&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_camel_pipeline&quot; /&gt;The Camel Pipeline&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Camel pipeline is an expression of the following business rules:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For every question created or updated send an email to the question creator&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For every answer created or updated send an email to both question and answer creator&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When a question achieves three answers post a tweet on a dedicated Twitter account about it&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The business requirements are transformed into a pipeline described by this EIP chart:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;https://debezium.io/images/camel-component-pipeline.svg&quot; style=&quot;max-width:90%;&quot; class=&quot;responsive-image&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Figure 4. The Camel Pipeline&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;code_walkthrough&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#code_walkthrough&quot; /&gt;Code Walkthrough&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To use Debezium Camel component we need to add at least following dependencies into a &lt;code&gt;pom.xml&lt;/code&gt; file&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;&amp;lt;dependencyManagement&amp;gt;
  &amp;lt;dependencies&amp;gt;
    &amp;lt;dependency&amp;gt;
      &amp;lt;groupId&amp;gt;org.apache.camel&amp;lt;/groupId&amp;gt;
      &amp;lt;artifactId&amp;gt;camel-bom&amp;lt;/artifactId&amp;gt;
      &amp;lt;version&amp;gt;${version.camel}&amp;lt;/version&amp;gt;
      &amp;lt;type&amp;gt;pom&amp;lt;/type&amp;gt;
      &amp;lt;scope&amp;gt;import&amp;lt;/scope&amp;gt;
    &amp;lt;/dependency&amp;gt;

    &amp;lt;!-- Use required Debezium version --&amp;gt;
    &amp;lt;dependency&amp;gt;
      &amp;lt;groupId&amp;gt;io.debezium&amp;lt;/groupId&amp;gt;
      &amp;lt;artifactId&amp;gt;debezium-connector-postgres&amp;lt;/artifactId&amp;gt;
      &amp;lt;version&amp;gt;${version.debezium}&amp;lt;/version&amp;gt;
    &amp;lt;/dependency&amp;gt;
    &amp;lt;dependency&amp;gt;
      &amp;lt;groupId&amp;gt;io.debezium&amp;lt;/groupId&amp;gt;
      &amp;lt;artifactId&amp;gt;debezium-embedded&amp;lt;/artifactId&amp;gt;
      &amp;lt;version&amp;gt;${version.debezium}&amp;lt;/version&amp;gt;
    &amp;lt;/dependency&amp;gt;
    &amp;lt;dependency&amp;gt;
      &amp;lt;groupId&amp;gt;io.debezium&amp;lt;/groupId&amp;gt;
      &amp;lt;artifactId&amp;gt;debezium-core&amp;lt;/artifactId&amp;gt;
      &amp;lt;version&amp;gt;${version.debezium}&amp;lt;/version&amp;gt;
    &amp;lt;/dependency&amp;gt;

  &amp;lt;/dependencies&amp;gt;
&amp;lt;/dependencyManagement&amp;gt;
&amp;lt;dependencies&amp;gt;
  &amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;org.apache.camel&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;camel-core&amp;lt;/artifactId&amp;gt;
  &amp;lt;/dependency&amp;gt;
  &amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;org.apache.camel&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;camel-debezium-postgres&amp;lt;/artifactId&amp;gt;
  &amp;lt;/dependency&amp;gt;
&amp;lt;/dependencies&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The pipeline logic itself is defined in &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/camel-component/qa-camel/src/main/java/io/debezium/examples/camel/pipeline/QaDatabaseUserNotifier.java&quot;&gt;QaDatabaseUserNotifier&lt;/a&gt; class.
Its main route looks like tis:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public class QaDatabaseUserNotifier extends RouteBuilder {

  @Override
  public void configure() throws Exception {
    from(&quot;debezium-postgres:localhost?&quot;
        + &quot;databaseHostname={{database.hostname}}&quot;
        + &quot;&amp;amp;databasePort={{database.port}}&quot;
        + &quot;&amp;amp;databaseUser={{database.user}}&quot;
        + &quot;&amp;amp;databasePassword={{database.password}}&quot;
        + &quot;&amp;amp;databaseDbname=postgres&quot;
        + &quot;&amp;amp;databaseServerName=qa&quot;
        + &quot;&amp;amp;schemaWhitelist={{database.schema}}&quot;
        + &quot;&amp;amp;tableWhitelist={{database.schema}}.question,{{database.schema}}.answer&quot;
        + &quot;&amp;amp;offsetStorage=org.apache.kafka.connect.storage.MemoryOffsetBackingStore&quot;)
        .routeId(QaDatabaseUserNotifier.class.getName() + &quot;.DatabaseReader&quot;)     &lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;(1)&lt;/b&gt;
        .log(LoggingLevel.DEBUG, &quot;Incoming message ${body} with headers ${headers}&quot;)
        .choice()                                                                &lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;(2)&lt;/b&gt;
          .when(isQuestionEvent)
            .filter(isCreateOrUpdateEvent)                                       &lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;(3)&lt;/b&gt;
              .convertBodyTo(Question.class)                                     &lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;b&gt;(4)&lt;/b&gt;
              .log(LoggingLevel.TRACE, &quot;Converted to logical class ${body}&quot;)
              .bean(store, &quot;readFromStoreAndUpdateIfNeeded&quot;)                     &lt;i class=&quot;conum&quot; data-value=&quot;5&quot; /&gt;&lt;b&gt;(5)&lt;/b&gt;
              .to(ROUTE_MAIL_QUESTION_CREATE)                                    &lt;i class=&quot;conum&quot; data-value=&quot;6&quot; /&gt;&lt;b&gt;(6)&lt;/b&gt;
            .endChoice()
          .when(isAnswerEvent)
            .filter(isCreateOrUpdateEvent)
              .convertBodyTo(Answer.class)
              .log(LoggingLevel.TRACE, &quot;Converted to logical class ${body}&quot;)
              .bean(store, &quot;readFromStoreAndAddAnswer&quot;)
              .to(ROUTE_MAIL_ANSWER_CHANGE)
              .filter(hasManyAnswers)                                            &lt;i class=&quot;conum&quot; data-value=&quot;7&quot; /&gt;&lt;b&gt;(7)&lt;/b&gt;
                .setBody().simple(&quot;Question &#39;${exchangeProperty[aggregate].text}&#39; has &quot; +
                    &quot;many answers (generated at &quot; + Instant.now() + &quot;)&quot;)
                .to(TWITTER_SERVER)
              .end()
            .endChoice()
          .otherwise()
            .log(LoggingLevel.WARN, &quot;Unknown type ${headers[&quot; +
                DebeziumConstants.HEADER_IDENTIFIER + &quot;]}&quot;)
        .endParent();

    from(ROUTE_MAIL_QUESTION_CREATE)                                             &lt;i class=&quot;conum&quot; data-value=&quot;6&quot; /&gt;&lt;b&gt;(6)&lt;/b&gt;
      .routeId(QaDatabaseUserNotifier.class.getName() + &quot;.QuestionNotifier&quot;)
      .setHeader(&quot;To&quot;).simple(&quot;${body.email}&quot;)
      .setHeader(&quot;Subject&quot;).simple(&quot;Question created/edited&quot;)
      .setBody().simple(&quot;Question &#39;${body.text}&#39; was created or edited&quot;)
      .to(SMTP_SERVER);
  }

  @Converter
  public static class Converters {

    @Converter
    public static Question questionFromStruct(Struct struct) {                   &lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;b&gt;(4)&lt;/b&gt;
      return new Question(struct.getInt64(&quot;id&quot;), struct.getString(&quot;text&quot;),
          struct.getString(&quot;email&quot;));
    }

    @Converter
    public static Answer answerFromStruct(Struct struct) {                       &lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;b&gt;(4)&lt;/b&gt;
      return new Answer(struct.getInt64(&quot;id&quot;), struct.getString(&quot;text&quot;),
          struct.getString(&quot;email&quot;), struct.getInt64(&quot;question_id&quot;));
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;colist arabic&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;from&lt;/code&gt; is the Debezium source endpoint. The URI parts map directly to connector configuration options.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The pipeline logic is split depending on the change event type.
The recognition is based on &lt;code&gt;CamelDebeziumIdentifier&lt;/code&gt; header which contains the identifier (&lt;code&gt;&amp;lt;server_name&amp;gt;.&amp;lt;schema_name&amp;gt;.&amp;lt;table_name&amp;gt;&lt;/code&gt;) of the source table.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The pipeline is now able to process only updates and deletes.
The recognition is based on &lt;code&gt;CamelDebeziumOperation&lt;/code&gt; header that contains &lt;code&gt;op&lt;/code&gt; field of the message &lt;code&gt;Envelope&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The Kafka Connect’s &lt;code&gt;Struct&lt;/code&gt; type is converted into a logical type used in the pipeline.
The conversion is performed by a custom Camel converter.
It is possible to use out-of-the-box &lt;code&gt;DebeziumTypeConverter&lt;/code&gt; that converts &lt;code&gt;Struct&lt;/code&gt; into a &lt;code&gt;Map&lt;/code&gt; but this tightly couples pipeline logic into the table structure.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;5&quot; /&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;A supplementary route is invoked that communicates with a &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/camel-component/qa-camel/src/main/java/io/debezium/examples/camel/pipeline/AggregateStore.java&quot;&gt;message store&lt;/a&gt; based on an Infinispan cache to build a message aggregate.
The message store checks if it has the question already stored.
If not a new aggregate is created and stored otherwise the stored aggregate is updated with new data.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;6&quot; /&gt;&lt;b&gt;6&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;A supplementary route is invoked that formats a mail message and delivers it to the question creator via the SMTP endpoint.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;7&quot; /&gt;&lt;b&gt;7&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The route part related to the answer message type is very similar (answers are added to question aggregate).
The main difference is the posting of a Twitter message when the aggregate contains three answers.&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;On a side note, for the sake of simplicitiy, the example currently uses volatile memory to store the Debezium offsets.
For persistent storage you could either use a file-based offset store or create a custom offset store implementation based on Infinispan, delegating the storage of offsets to the underlying cache.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;demo&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#demo&quot; /&gt;Demo&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In order to run the demo, you need to have a Twitter &lt;a href=&quot;https://developer.twitter.com/en/docs/basics/getting-started&quot;&gt;developer account&lt;/a&gt; with appropriate API keys and secrets.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Go to the application directory and build all components:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;$ mvn clean install&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Start the services (provide your own Twitter API credentials):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;$ env TWITTER_CONSUMER_KEY=&amp;lt;...&amp;gt; TWITTER_CONSUMER_SECRET=&amp;lt;...&amp;gt; TWITTER_ACCESS_TOKEN=&amp;lt;...&amp;gt; TWITTER_ACCESS_TOKEN_SECRET=&amp;lt;...&amp;gt; docker-compose up&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In another terminal create a question and three answers to it:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;$ curl -v -X POST -H &#39;Content-Type: application/json&#39; http://0.0.0.0:8080/question/ -d @src/test/resources/messages/create-question.json
$ curl -v -X POST -H &#39;Content-Type: application/json&#39; http://0.0.0.0:8080/question/1/answer -d @src/test/resources/messages/create-answer1.json
$ curl -v -X POST -H &#39;Content-Type: application/json&#39; http://0.0.0.0:8080/question/1/answer -d @src/test/resources/messages/create-answer2.json
$ curl -v -X POST -H &#39;Content-Type: application/json&#39; http://0.0.0.0:8080/question/1/answer -d @src/test/resources/messages/create-answer3.json&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Twitter account should contain a new tweet with a text like &quot;Question &#39;How many legs does a dog have?&#39; has many answers (generated at 2020-02-17T08:02:33.744Z)&quot;.
Also the &lt;a href=&quot;http://localhost:8025/&quot;&gt;MailHog server UI&lt;/a&gt; should display messages like these:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;https://debezium.io/images/camel-component-mailhog.png&quot; style=&quot;max-width:100%;&quot; class=&quot;responsive-image&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Figure 4. The MailHog Messages&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#conclusion&quot; /&gt;Conclusion&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Apache Camel is a very interesting option for implementing system integration scenarios.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Without the need for any external messaging infrastructure, it is very easy to deploy a standalone Camel route with the Debezium component, enabling the capture of data changes and execution of complex routing and transformation operations on them.
Camel equips the developer with a full arsenal of enterprise integration pattern implementations, as well as more than hundred connectors for different systems that could be included in a complex service orchestration.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The source code of the full example is available &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/camel-component&quot;&gt;on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open-source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open-source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve our existing connectors and add even more connectors.
If you find problems or have an idea on how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2020/02/13/debezium-1-1-beta2-released/</id>
<title>Debezium 1.1.0.Beta2 Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2020-02-13T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2020/02/13/debezium-1-1-beta2-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="oracle"></category>
<category term="sqlserver"></category>
<category term="db2"></category>
<category term="outbox"></category>
<category term="quarkus"></category>
<summary>



Release early, release often!
After the 1.1 Beta1 and 1.0.1 Final releases earlier this week, I&#8217;m today happy to share the news about the release of Debezium 1.1.0.Beta2!


The main addition in Beta2 is support for integration tests of your change data capture (CDC) set-up using Testcontainers.
In addition, the Quarkus extension for implementing the outbox pattern as well as
the SMT for extracting the after state of change events have been re-worked and offer more configuration flexibility now.




Testcontainers Support


When setting up CDC pipelines, you should test your configuration thoroughly:
the source database must be correctly configured (e.g. think binlog mode of MySQL),
connectors must use...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Release early, release often!
After the 1.1 Beta1 and 1.0.1 Final releases earlier this week, I’m today happy to share the news about the release of Debezium &lt;strong&gt;1.1.0.Beta2&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The main addition in Beta2 is support for integration tests of your change data capture (CDC) set-up using Testcontainers.
In addition, the Quarkus extension for implementing the outbox pattern as well as
the SMT for extracting the &lt;code&gt;after&lt;/code&gt; state of change events have been re-worked and offer more configuration flexibility now.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;testcontainers_support&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#testcontainers_support&quot; /&gt;Testcontainers Support&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When setting up CDC pipelines, you should test your configuration thoroughly:
the source database must be correctly configured (e.g. think binlog mode of MySQL),
connectors must use the right credentials, filters and more.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Automation is king, and thus we’re very excited about the new support for writing CDC integration tests using &lt;a href=&quot;https://www.testcontainers.org/&quot;&gt;Testcontainers&lt;/a&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1722&quot;&gt;DBZ-1722&lt;/a&gt;).
With just a few lines of code, you can set up all the required services using Linux containers,
deploy a Debezium connector and run assertions against the emitted change data events.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’re planning to publish a blog post dedicated to this topic very soon,
in the mean time refer to to the &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/integrations/testcontainers.html&quot;&gt;documentation&lt;/a&gt; to learn more.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;more_configuration_options_for_the_quarkus_outbox_pattern_extension&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#more_configuration_options_for_the_quarkus_outbox_pattern_extension&quot; /&gt;More Configuration Options for the Quarkus Outbox Pattern Extension&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/integrations/outbox.html&quot;&gt;Quarkus extension&lt;/a&gt; for implementing the &lt;a href=&quot;https://debezium.io//blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/&quot;&gt;outbox pattern&lt;/a&gt; comes in handy if a service needs to update its own database as well as send events to external consumers:
by writing events into an &quot;outbox&quot; table and capturing them from there using Debezium,
unsafe &quot;dual writes&quot; to a database and Apache Kafka are avoided.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To offer more flexibility, the Quarkus extension allows now for fully flexible customization of the outbox table’s column types &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1711&quot;&gt;DBZ-1711&lt;/a&gt;.
E.g. you can set the option &lt;code&gt;quarkus.debezium-outbox.payload.column-definition&lt;/code&gt; to &lt;code&gt;JSONB NOT NULL&lt;/code&gt;,
in order to use a Postgres &lt;code&gt;JSONB&lt;/code&gt; column for the outbox table’s payload column.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;more_flexible_after_state_extraction&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#more_flexible_after_state_extraction&quot; /&gt;More Flexible &lt;code&gt;After&lt;/code&gt; State Extraction&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The SMT for &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/configuration/event-flattening.html&quot;&gt;extracting the &lt;code&gt;after&lt;/code&gt; state of change events&lt;/a&gt; allows for a more flexible propagation of specific event attributes now: using the new &lt;code&gt;add.fields&lt;/code&gt; and &lt;code&gt;add.headers&lt;/code&gt; options, any top-level attribute (&lt;code&gt;op&lt;/code&gt;, &lt;code&gt;ts_ms&lt;/code&gt;) as well as &lt;code&gt;source&lt;/code&gt; and &lt;code&gt;transaction&lt;/code&gt; attributes can be propagated into the outgoing record and/or as header of the Kafka record:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;transforms=unwrap,...
transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState
transforms.unwrap.add.fields=table,lsn
transforms.unwrap.add.headers=op,source.ts_ms&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The existing &lt;code&gt;operation.header&lt;/code&gt; and &lt;code&gt;add.source.fields&lt;/code&gt; options have been deprecated and will be removed in a future Debezium version.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;further_changes_and_bugfixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#further_changes_and_bugfixes&quot; /&gt;Further Changes and Bugfixes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Overall &lt;a href=&quot;https://debezium.io/releases/1.1/release-notes/#release-1.1.0-beta2&quot;&gt;13 issues&lt;/a&gt; have been addressed for the Debezium 1.1 Beta2 release.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Most notably, a known Kafka Connect issue has been mitigated which may cause missed change events when the Postgres or Oracle connectors are stopped (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1766&quot;&gt;DBZ-1766&lt;/a&gt;), and the MongoDB SMT for &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/configuration/mongodb-event-flattening.html&quot;&gt;extracting the new document state&lt;/a&gt; handles identifiers with characters un-supported by Apache Avro, e.g. &lt;code&gt;$ref&lt;/code&gt;, gracefully (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1767&quot;&gt;DBZ-1767&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A big shout-out to all the contributors of this release:
&lt;a href=&quot;https://github.com/lordofthejars&quot;&gt;Alex Soto&lt;/a&gt;,
&lt;a href=&quot;https://github.com/daanroosen-DS&quot;&gt;Daan Roosen&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jpsoroulas&quot;&gt;John Psoroulas&lt;/a&gt;,
&lt;a href=&quot;https://github.com/matzew&quot;&gt;Matthias Wessendorf&lt;/a&gt;,
&lt;a href=&quot;https://github.com/mwinstanley&quot;&gt;Melissa Winstanley&lt;/a&gt;,
as well as &lt;a href=&quot;https://github.com/bsideup/&quot;&gt;Sergei Egorov&lt;/a&gt; for his advice on the Testcontainers work.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2020/02/11/debezium-1-1-beta1-released/</id>
<title>Debezium 1.1.0.Beta1 Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2020-02-11T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2020/02/11/debezium-1-1-beta1-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="cassandra"></category>
<category term="db2"></category>
<summary>



It&#8217;s my pleasure to announce the release of Debezium 1.1.0.Beta1!


This release adds support for transaction marker events,
an incubating connector for the IBM Db2 database as well as a wide range of bug fixes.
As the 1.1 release still is under active development,
we&#8217;ve backported an asorted set of bug fixes to the 1.0 branch and released Debezium 1.0.1.Final, too.


At the time of writing this, not all connector archives have been synched to Maven Central yet;
this should be the case within the next few others.




Transaction Markers


A common requirements for Debezium users is to materialize some kind of "aggregate" made up of change events...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;1.1.0.Beta1&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This release adds support for transaction marker events,
an incubating connector for the IBM Db2 database as well as a wide range of bug fixes.
As the 1.1 release still is under active development,
we’ve backported an asorted set of bug fixes to the 1.0 branch and released Debezium &lt;strong&gt;1.0.1.Final&lt;/strong&gt;, too.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;At the time of writing this, not all connector archives have been synched to Maven Central yet;
this should be the case within the next few others.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;transaction_markers&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#transaction_markers&quot; /&gt;Transaction Markers&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A common requirements for Debezium users is to materialize some kind of &quot;aggregate&quot; made up of change events from multiple change data topics.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Consider for instance an e-commerce application which manages purchase orders.
In a relational database, the information representing such order will typically be spread across multiple tables, e.g. &lt;code&gt;PURCHASE_ORDER&lt;/code&gt;, &lt;code&gt;ORDER_LINE&lt;/code&gt; and &lt;code&gt;SHIPMENT_ADDRESS&lt;/code&gt;.
To examine the complete state of a purchase order in the database itself, you’d run a JOIN query against the three tables.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Things get more challenging when looking at the change event topics produced by CDC connectors such as Debezium.
For instance &lt;a href=&quot;https://kafka.apache.org/documentation/streams/&quot;&gt;Kafka Streams&lt;/a&gt; could be used to join the change event streams for the three tables and materialize an aggregate view.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’ve &lt;a href=&quot;https://debezium.io/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/&quot;&gt;explored a possible implementation&lt;/a&gt; of this some time ago here on this blog.
This approach was prone to exposing intermediary aggregates, though.
The reason being that the joining logic had no insight into the transaction boundaries that applied in the source database.
So when for instance inserting a purchase order with 10 order lines,
it could happen that an aggregate gets materialized which contains the order and the first five lines,
shortly followed thereafter by the complete aggregate view with all 10 order lines.
Depending on your use case exposing such intermediary aggregates to downstream consumers
(e.g. a search index) might not be desirable.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In Debezium 1.1 this situation can be addressed by leveraging the new transaction metadata topic supported by most connectors.
When enabled,
a separate topic will be published which contains events indicating the begin and end of transactions:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
  &quot;status&quot;: &quot;BEGIN&quot;,
  &quot;id&quot;: &quot;571&quot;,
  &quot;event_count&quot;: null,
  &quot;data_collections&quot;: null
}
...
{
  &quot;status&quot;: &quot;END&quot;,
  &quot;id&quot;: &quot;571&quot;,
  &quot;event_count&quot;: &quot;11&quot;,
  &quot;data_collections&quot;: [
    {
      &quot;data_collection&quot;: &quot;inventory.purchaseorder&quot;,
      &quot;event_count&quot;: &quot;1&quot;
    },
    {
      &quot;data_collection&quot;: &quot;inventory.orderline&quot;,
      &quot;event_count&quot;: &quot;10&quot;
    }
  ]
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;code&gt;END&lt;/code&gt; events contain the total number of change events originating from this transaction as well as the number of event per affected table.
Actual data change events in the topics for the purchase order and order line tables contain the transaction id, too.
These two things together enable a stream processing application to buffer all the change events originating from one transaction.
Only once it has received all the events of a transaction, it may produce the final aggregate view and publish it to downstream consumers,
avoiding the issue of exposing intermediary aggregate views.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Stay tuned for an in-depth example of such an implementation coming soon!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;ibm_db2_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#ibm_db2_connector&quot; /&gt;IBM Db2 Connector&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Support for the &lt;a href=&quot;https://www.ibm.com/products/db2-database&quot;&gt;IBM Db2&lt;/a&gt; database has been on
&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-695&quot;&gt;the wishlist&lt;/a&gt; for many Debezium users for quite some time.
That’s why we were very excited when a group of IBM engineers reached out to us a while ago,
offering to implement this connector under the Debezium umbrella.
This connector is released in &quot;incubating&quot; state in Debezium 1.1.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note that at this point a license for the IBM IIDR product is required in order to use the &quot;ASN Capture&quot; API leveraged by the connector.
A post with more details around this connector should follow soon;
in the mean time please refer to the connector &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/connectors/db2.html&quot;&gt;reference documentation&lt;/a&gt; to learn more.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;further_changes_and_bugfixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#further_changes_and_bugfixes&quot; /&gt;Further Changes and Bugfixes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Besides the transaction metadata topic and the Db2 connector, a few more improvements and fixes have been completed for Debezium 1.1 Beta1:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The message transformation for extracting the &lt;code&gt;after&lt;/code&gt; state from change events allows to route change events to a specific topic based on a configurable record field
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1715&quot;&gt;DBZ-1715&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;ExtractNewDocumentState&lt;/code&gt; SMT to be used with the Debezium MongoDB connector will convert &lt;code&gt;Date&lt;/code&gt; and &lt;code&gt;Timestamp&lt;/code&gt; fields now into the &lt;code&gt;org.apache.kafka.connect.data.Timestamp&lt;/code&gt; logical type, clarifying its semantics (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1717&quot;&gt;DBZ-1717&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The MySQL connector won’t log the dabase password in DEBUG level any longer (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1748&quot;&gt;DBZ-1748&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Antlr DDL parser of the MySQL connector handles the &lt;code&gt;TRANSACTIONAL&lt;/code&gt; keyword of MariaDB (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1733&quot;&gt;DBZ-1733&lt;/a&gt;) as well as the &lt;code&gt;GET DIAGNOSTICS&lt;/code&gt; statement
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1740&quot;&gt;DBZ-1740&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Postgres connector can be used with proxied connections (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1738&quot;&gt;DBZ-1738&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Overall, &lt;a href=&quot;https://debezium.io/releases/1.1/release-notes/#release-1.1.0-beta1&quot;&gt;27 issues&lt;/a&gt; were fixed for this release.
&lt;a href=&quot;https://debezium.io/releases/1.0/release-notes/#release-1.0.1-final&quot;&gt;16&lt;/a&gt; bugfixes from 1.1 Alpha1 and Beta1 were backported to 1.0.1.Final.
Please make sure to read the upgrade notes when upgrading the Postgres connector and the accompanying decoderbufs logical decoding plugin to 1.1 Beta1, as a specific order of upgrading the two is needed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Many thanks to
&lt;a href=&quot;https://github.com/oscerd&quot;&gt;Andrea Cosentino&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ahus1&quot;&gt;Alexander Schwartz&lt;/a&gt;,
&lt;a href=&quot;https://github.com/bingqinzhou&quot;&gt;Bingqin Zhou&lt;/a&gt;,
&lt;a href=&quot;https://github.com/igabaydulin&quot;&gt;Igor Gabaydulin&lt;/a&gt;,
&lt;a href=&quot;https://github.com/juyttenh&quot;&gt;Jan Uyttenhove&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jhuiting&quot;&gt;Jos Huiting&lt;/a&gt;,
&lt;a href=&quot;https://github.com/lga-zurich&quot;&gt;Luis Garcés-Erice&lt;/a&gt;,
&lt;a href=&quot;https://github.com/mzbyszynski&quot;&gt;Marc Zbyszynski&lt;/a&gt;,
&lt;a href=&quot;https://github.com/zrlurb&quot;&gt;Peter Urbanetz&lt;/a&gt; and
&lt;a href=&quot;https://github.com/SeanRooooney&quot;&gt;Sean Rooney&lt;/a&gt;
for their contributions to this release!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;On our road towards Debezium 1.1, we’ll likely do another Beta release before going to the candidate release phase in a few weeks from now.
To see what’s coming, take a look at the &lt;a href=&quot;https://debezium.io/roadmap/&quot;&gt;roadmap&lt;/a&gt;, or get in touch to tell us about your specific feature requirements!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2020/02/10/event-sourcing-vs-cdc/</id>
<title>Distributed Data for Microservices — Event Sourcing vs. Change Data Capture</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2020-02-10T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2020/02/10/event-sourcing-vs-cdc/" rel="alternate" type="text/html" />
<author>
<name>Eric Murphy</name>
</author>
<category term="discussion"></category>
<category term="event-sourcing"></category>
<category term="cqrs"></category>
<category term="outbox"></category>
<category term="quarkus"></category>
<summary>



This article is a dive into the realms of Event Sourcing, Command Query Responsibility Segregation (CQRS), Change Data Capture (CDC), and the Outbox Pattern. Much needed clarity on the value of these solutions will be presented. Additionally, two differing designs will be explained in detail with the pros/cons of each.


So why do all these solutions even matter? They matter because many teams are building microservices and distributing data across multiple data stores. One system of microservices might involve relational databases, object stores, in-memory caches, and even searchable indexes of data. Data can quickly become lost, out of sync, or even...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This article is a dive into the realms of Event Sourcing, Command Query Responsibility Segregation (CQRS), Change Data Capture (CDC), and the Outbox Pattern. Much needed clarity on the value of these solutions will be presented. Additionally, two differing designs will be explained in detail with the pros/cons of each.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So why do all these solutions even matter? They matter because many teams are building microservices and distributing data across multiple data stores. One system of microservices might involve relational databases, object stores, in-memory caches, and even searchable indexes of data. Data can quickly become lost, out of sync, or even corrupted therefore resulting in disastrous consequences for mission critical systems.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Solutions that help avoid these serious problems are of paramount importance for many organizations. Unfortunately, many vital solutions are somewhat difficult to understand; Event Sourcing, CQRS, CDC, and Outbox are no exception. Please look at these solutions as an opportunity to learn and understand how they could apply to your specific use cases.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As you will find out at the end of this article, I will propose that three of these four solutions have high value, while the other should be discouraged except for the rarest of circumstances. The advice given in this article should be evaluated against your specific needs, because, in some cases, none of these four solutions would be a good fit.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;reactive_systems&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#reactive_systems&quot; /&gt;Reactive Systems&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Taking a quick step back, Event Sourcing and Change Data Capture are solutions that can be used to build distributed systems (i.e. microservices) that are Reactive. Microservices should react to an ever-changing environment  (i.e. the cloud) by being resilient and elastic. The magic behind these abilities is being message and event driven. To find out more, I advise you to read the &lt;a href=&quot;https://www.reactivemanifesto.org/&quot;&gt;Reactive Manifesto&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;https://debezium.io/images/2020-02-06-event-sourcing-vs-cdc/figure01.png&quot; style=&quot;max-width:100%;&quot; class=&quot;responsive-image&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Figure 1. Attributes of a Reactive System, per the Reactive Manifesto&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;shared_goals_for_event_sourcing_and_change_data_capture&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#shared_goals_for_event_sourcing_and_change_data_capture&quot; /&gt;Shared Goals for Event Sourcing and Change Data Capture&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The two core solutions presented in this article are Event Sourcing and Change Data Capture. Before I formally introduce these two solutions, it can be known that they serve similar goals, which are:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;Designate one datastore as the global source of truth for a specific set of data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provide a representation of past and current application state as a series of events, also called a journal or transaction log&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Offer a journal that can replay events, as needed, for rebuilding or refreshing state&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Event Sourcing uses its own journal as the source of truth, while Change Data Capture depends on the underlying database transaction log as the source of truth. This difference has major implications on the design and implementation of software which will be presented later in this article.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;domain_events_vs_change_events&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#domain_events_vs_change_events&quot; /&gt;Domain Events vs. Change Events&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Before we go deeper, it’s important to make a distinction about the types of events we are concerned about for Event Sourcing and Change Data Capture:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Domain events — An explicit event, part of your business domain, that is generated by your application. These events are usually represented in the past tense, such as OrderPlaced, or ItemShipped. These events are the primary concern for Event Sourcing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Change events — Events that are generated from a database transaction log indicating what state transition has occurred. These events are of concern for Change Data Capture.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Domain events and change events are not related unless a change event happens to contain a domain event, which is a premise for the Outbox Pattern to be introduced later in the article.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now that we have established some commonality on Event Sourcing and Change Data Capture, we can go deeper.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;event_sourcing&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#event_sourcing&quot; /&gt;Event Sourcing&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Event Sourcing is a solution that allows software to maintain its state as a journal of domain events. As such, taking the journal in its entirety represents the current state of the application. Having this journal also gives the ability to easily audit the history and also to time travel and reproduce errors generated by previous state.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Event Sourcing implementations usually have these characteristics:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;Domain events generated from the application business logic will add new state for your application&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;State of the application is updated via an append-only event log (a journal) that is generally immutable&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Journal is considered the source of truth for the lifetime of the application&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Journal is replayable to rebuild the state of the application at any point in time&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Journal groups domain events by an ID to capture the current state of an object (an Aggregate from DDD parlance)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;https://debezium.io/images/2020-02-06-event-sourcing-vs-cdc/figure02.png&quot; style=&quot;max-width:100%;&quot; class=&quot;responsive-image&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Figure 2. Representation of Event Sourcing Materializing an Object&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Additionally, Event Sourcing implementations often have these characteristics:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;Snapshotting mechanism for the journal to speed up recreating the state of an application&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mechanism to remove events from the journal as required (usually for compliance reasons)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;API for event dispatching that may be used for distributing state of the application&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lack of transactional guarantees that are normally present for a strongly consistent system&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Backward compatibility mechanism to cope with changing event formats inside the journal&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mechanism to backup and restore the journal, the source of truth for the application&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Event sourcing mimics how a database works, but at the application-level. Per Figure 2, the figure could be updated to represent a database as shown in Figure 3 with roughly the same design.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;https://debezium.io/images/2020-02-06-event-sourcing-vs-cdc/figure03.png&quot; style=&quot;max-width:100%;&quot; class=&quot;responsive-image&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Figure 3. Representation of Database Transaction Materializing a Table&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The comparison between Figure 2 and Figure 3 will become more relevant as we dive deeper into how Event Sourcing and Change Data Capture compare to each other.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;change_data_capture&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#change_data_capture&quot; /&gt;Change Data Capture&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Change Data Capture (CDC) is a solution that captures change events from a database transaction log (or equivalent mechanism) and forwards those events to downstream consumers. CDC ultimately allows application state to be externalized and synchronized with external stores of data.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Change Data Capture implementations usually have these characteristics:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;External process that reads the transaction log of a database with the goal to materialize change events from those transactions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Change events are forwarded to downstream consumers as messages&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As you can see, CDC is a relatively simple concept with a very narrow scope. It’s simply externalizing the transaction log of the database as a stream of events to interested consumers.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;https://debezium.io/images/2020-02-06-event-sourcing-vs-cdc/figure04.png&quot; style=&quot;max-width:100%;&quot; class=&quot;responsive-image&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Figure 4. Change Data Capture Implementation Options&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;CDC also gives you flexibility on how events are consumed. Per Figure 4:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Option 1 is a standalone CDC process to capture and forward events from the transaction log to a message broker&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Option 2 is an embedded CDC client that sends events directly to an application&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Option A is another connector that persists CDC events directly to a datastore&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Option B forwards events to consuming applications via a message broker&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Finally, a CDC implementation often has these characteristics:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;A durable message broker is used to forward events with at-least-once delivery guarantees to all consumers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The ability to replay events from the datastore transaction log and/or message broker for as long as the events are persisted&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;CDC is very flexible and adaptable for multiple use cases. Early adopters of CDC were choosing Option 1/A, but Option 1/B, and also Option 2 are becoming more popular as CDC gains momentum.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;using_cdc_to_implement_the_outbox_pattern&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#using_cdc_to_implement_the_outbox_pattern&quot; /&gt;Using CDC to Implement the Outbox Pattern&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The primary goal of the Outbox Pattern is to ensure that updates to the application state (stored in tables) and publishing of the respective domain event is done within a single transaction. This involves creating an Outbox table in the database to collect those domain events as part of a transaction. Having transactional guarantees around the domain events and their propagation via the Outbox is important for data consistency across a system.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;After the transaction completes, the domain events are then picked up by a CDC connector and forwarded to interested consumers using a reliable message broker (see Figure 5). Those consumers may then use the domain events to materialize their own aggregates (see above per Event Sourcing).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;https://debezium.io/images/2020-02-06-event-sourcing-vs-cdc/figure05.png&quot; style=&quot;max-width:100%;&quot; class=&quot;responsive-image&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Figure 5. Outbox Pattern implemented with CDC (2 Options)&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Outbox is also meant to be abstracted from the application as it’s only an ephemeral store of outgoing event data, and not meant to be read or queried. In fact, the domain events residing in the Outbox may be deleted immediately after insertion!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;event_sourcing_journal_vs_outbox&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#event_sourcing_journal_vs_outbox&quot; /&gt;Event Sourcing Journal vs. Outbox&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We can now take a closer look at the overlap in design of an Event Sourcing journal and CDC with Outbox. By comparing the attributes of the journal with the Outbox table, the similarities become clear. The Aggregate, again from DDD, is at the heart of how the data is stored and consumed for both Outbox and Event Sourcing.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here are the common attributes that exist between an Event Sourcing journal and an Outbox:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Event ID — Unique identifier for the event itself and can be used for de-duplication for idempotent consumers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Aggregate ID — Unique identifier used to partition related events; these events compose an Aggregate’s state&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Aggregate Type — The type of the Aggregate that can be used for routing of events only to interested consumers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sequence/Timestamp — A way to sort events to provide ordering guarantees&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Message Payload — Contains the event data to be exchanged in a format readable by downstream consumers&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Outbox table and the Event Sourcing journal have essentially the same data format. The major difference is that the Event Sourcing journal is meant to be a permanent and immutable store of domain events, while the Outbox is meant to be highly ephemeral and only be a landing zone for domain events to be captured inside change events and forwarded to downstream consumers.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;command_query_responsibility_segregation&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#command_query_responsibility_segregation&quot; /&gt;Command Query Responsibility Segregation&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Command Query Responsibility Segregation pattern, or CQRS for short, is commonly associated with Event Sourcing. However, Event Sourcing is not required to use CQRS. For example, the CQRS pattern could instead be implemented with the Outbox Pattern.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So what is CQRS anyways? It’s a pattern to create alternative representations of data, known as projections, for the primary purpose of being read-only, queryable views on some set of data. There may be multiple projections for the same set of data of interest to various clients.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Command aspect to CQRS applies to an application processing actions (Commands) and ultimately generating domain events that can be used to create state for a projection. That is one reason why CQRS is so often associated with Event Sourcing.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another reason why CQRS pairs well with Event Sourcing is because the journal is not queryable by the application. The only viable way to query data in an event sourced system is through the projections. Keep in mind, these projections are eventually consistent. This brings flexibility but also complexity and deviation from the norm of strongly consistent views that developers may be familiar with.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;https://debezium.io/images/2020-02-06-event-sourcing-vs-cdc/figure06.png&quot; style=&quot;max-width:100%;&quot; class=&quot;responsive-image&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Figure 6. Representation of Event Sourcing with CQRS&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;https://debezium.io/images/2020-02-06-event-sourcing-vs-cdc/figure07.png&quot; style=&quot;max-width:100%;&quot; class=&quot;responsive-image&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Figure 7. Representation of Event Sourcing with CQRS using a Message Broker&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As you can see in Figure 6 and Figure 7, these are two very different interpretations of the CQRS pattern based on Event Sourcing, but the end result is the same, a queryable projection of data originating only from events.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As stated earlier, CQRS can also be paired with the Outbox Pattern, as shown in Figure 8. An advantage with this design is there is still strong consistency within the application database but eventual consistency with the CQRS projections.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;https://debezium.io/images/2020-02-06-event-sourcing-vs-cdc/figure08.png&quot; style=&quot;max-width:100%;&quot; class=&quot;responsive-image&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Figure 8. Representation of the Outbox Pattern with CQRS&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;processing_domain_events_internally&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#processing_domain_events_internally&quot; /&gt;Processing Domain Events Internally&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While this article is very focused on distributing data across a system, using domain events internally for an application can also be important. Processing domain events internally is necessary for a variety of reasons which includes executing business logic within the same microservice context as the event originated from. This is common practice for building event-driven applications.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With either Event Sourcing or CDC, processing domain events internally requires a dispatcher mechanism to pass the event in memory. Some examples of this would be the Vert.x EventBus, Akka Actor System, or Spring Application Events. In the case of the Outbox pattern, the event would be dispatched only after the initial Outbox transaction completes successfully.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;comparison_of_attributes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#comparison_of_attributes&quot; /&gt;Comparison of Attributes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This article has thrown a lot at you, so a table summarizing what has been presented so far may be beneficial:&lt;/p&gt;
&lt;/div&gt;
&lt;table class=&quot;tableblock frame-all grid-all stretch data&quot;&gt;
&lt;colgroup&gt;
&lt;col style=&quot;width: 20%;&quot;&gt;
&lt;col style=&quot;width: 20%;&quot;&gt;
&lt;col style=&quot;width: 20%;&quot;&gt;
&lt;col style=&quot;width: 20%;&quot;&gt;
&lt;col style=&quot;width: 20%;&quot;&gt;
&lt;/col&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;Attribute&lt;/th&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;Event Sourcing&lt;/th&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;CDC&lt;/th&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;CDC + Outbox&lt;/th&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;CQRS&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;strong&gt;Purpose&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Capture state in a journal containing domain events.&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Export Change Events from transaction log.&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Export domain events from an Outbox via CDC.&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Use domain events to generate projections of data.&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;strong&gt;Event Type&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Domain Event&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Change Event&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Domain Event embedded in Change Event&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Domain Event&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;strong&gt;Source of Truth&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Journal&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Transaction Log&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Transaction Log&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Depends on implementation&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;strong&gt;Boundary&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Application&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;System&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;System (CDC)
Application (Outbox)&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Application or System&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;strong&gt;Consistency Model&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;N/A (only writing to the Journal)&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Strongly Consistent (tables), Eventually Consistent (Change Event capture)&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Strongly Consistent (Outbox), Eventually Consistent (Change Event capture)&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Eventually Consistent&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;strong&gt;Replayability&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Yes&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Yes&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Yes&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;Depends on implementation&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/col&gt;
&lt;/col&gt;
&lt;/col&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;proscons_of_event_sourcing_cqrs&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#proscons_of_event_sourcing_cqrs&quot; /&gt;Pros/Cons of Event Sourcing + CQRS&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now that we have a better handle on Event Sourcing and CQRS, let’s examine some of the pros and cons of Event Sourcing when paired with CQRS. These pros/cons take into consideration the current implementations that are available and also documented experiences from both myself and other professionals building distributed systems.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;pros_for_event_sourcing_with_cqrs&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#pros_for_event_sourcing_with_cqrs&quot; /&gt;Pros for Event Sourcing with CQRS&lt;/h3&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;Journal is easily accessible for auditing purposes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generally performant for a high volume of write operations to the Journal&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Possibility to shard the Journal for a very large amount of data (depending on datastore)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;cons_for_event_sourcing_with_cqrs&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#cons_for_event_sourcing_with_cqrs&quot; /&gt;Cons for Event Sourcing with CQRS&lt;/h3&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;Everything is eventually consistent data; a requirement of strongly consistent data doesn’t fit Event Sourcing and CQRS&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Cannot read your own writes to the journal (from a query perspective)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Long term maintenance concerns around the journal and an event sourced architecture&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Need to write a lot of code for compensating actions for error cases&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;No real transactional guarantees for resolving the dual writes flaw (to be covered next)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Need to consider backward compatibility or migration of legacy data as the formats of events change&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Need to consider snapshotting the journal and the implications associated with it&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Talent pool for developers with experience using Event Sourcing and CQRS is virtually nonexistent&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lack of use cases for Event Sourcing limits applicability&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;dual_writes_risk_for_event_sourcing_and_cqrs&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#dual_writes_risk_for_event_sourcing_and_cqrs&quot; /&gt;Dual Writes Risk for Event Sourcing and CQRS&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One problem with Event Sourcing is that there is a possibility of failure to update the CQRS projections if there is an error with the application. This could result in missing data, and unfortunately, it may be difficult to recover that data without proper compensating actions built into the application itself. That is additional code and complexity that falls onto the developer, and is error prone. For example, one workaround is to track a read offset number that correlates to the event sourced journal, to give replayability upon error for reprocessing the domain events and refresh the CQRS projections.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The underlying reason for this possibility of errors is the lack of transactions for writing to both the Journal and the CQRS projections. This is what is known as “dual writes”, and it greatly increases the risk for errors. This dual writes flaw is represented in Figure 9.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;https://debezium.io/images/2020-02-06-event-sourcing-vs-cdc/figure09.png&quot; style=&quot;max-width:100%;&quot; class=&quot;responsive-image&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Figure 9. Lack of Transactional Integrity with Event Sourcing and CQRS&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Even adding a message broker, as shown in Figure 7 would not resolve the dual writes issue. With that design, you are still writing out the message to a message broker and an error could arise.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The dual writes flaw is just one example of some of the challenges in working with Event Sourcing with CQRS. Additionally, the long term maintenance and Day 2 impact of having the journal as the source of truth increases risk for your application over time. Event sourcing is also a paradigm that is unfamiliar to most engineers and is easy to make wrong assumptions or bad design choices that ultimately may lead to rearchitecting parts of your system.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Given the pros and cons about Event Sourcing paired with CQRS, it’s advisable to seek out alternatives before settling on this design. Your use case may fit Event Sourcing but CDC may also fit the bill.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;debezium_for_cdc_and_outbox&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#debezium_for_cdc_and_outbox&quot; /&gt;Debezium for CDC and Outbox&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source CDC project supported by Red Hat that has gradually gained popularity over the past few years. Recently, Debezium added full support for the Outbox Pattern with an extension to the Quarkus Java microservice runtime.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium, Quarkus, and the Outbox offer a comprehensive solution which avoids the Dual Writes flaw, and is generally a more practical solution for your average developer team as compared to Event Sourcing solutions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;exampleblock centered-image responsive-image&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;https://debezium.io/images/2020-02-06-event-sourcing-vs-cdc/figure10.png&quot; style=&quot;max-width:100%;&quot; class=&quot;responsive-image&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Figure 10. Error Handling of the Outbox Pattern with CQRS&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;pros_for_cdc_outbox_with_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#pros_for_cdc_outbox_with_debezium&quot; /&gt;Pros for CDC + Outbox with Debezium&lt;/h3&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;Source of truth stays within the application database tables and transaction log&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Transactional guarantees and reliable messaging greatly reduce possibility for data loss or corruption&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Flexible solution that fits into a prototypical microservice architecture&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Simpler design is easier to maintain over the long term&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Can read and query your own writes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Opportunity for strong consistency within the application database; eventual consistency across the remainder of the system&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;cons_for_cdc_outbox_with_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#cons_for_cdc_outbox_with_debezium&quot; /&gt;Cons for CDC + Outbox with Debezium&lt;/h3&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;Additional latency may be present by reading the transaction log and also going through a message broker; tuning may be required for minimizing latency&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Quarkus, while great, is the only current option for an off the shelf Outbox API; You could also roll your own implementation if needed&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#conclusion&quot; /&gt;Conclusion&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Building distributed systems, even with microservices, can be very challenging. That is what makes novel solutions like Event Sourcing appealing to consider. However, CDC and Outbox using Debezium is usually a better alternative to Event Sourcing, and is compatible with the CQRS pattern to boot. While Event Sourcing may still have value in some use cases, I encourage you to give Debezium and the Outbox a try first.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;further_reading&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#further_reading&quot; /&gt;Further Reading&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;docs_and_repos&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#docs_and_repos&quot; /&gt;Docs and Repos&lt;/h3&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://debezium.io/documentation/reference/1.0/tutorial.html&quot;&gt;Debezium Tutorial&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://debezium.io/documentation/reference/1.0/configuration/outbox-event-router.html&quot;&gt;Debezium Outbox Event Router&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/outbox&quot;&gt;Debezium Outbox Pattern Sample Application (Quarkus)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://quarkus.io/get-started/&quot;&gt;Quarkus Getting Started&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;blogs_and_articles&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#blogs_and_articles&quot; /&gt;Blogs and Articles&lt;/h3&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/&quot;&gt;Reliable Microservices Data Exchange With the Outbox Pattern&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://microservices.io/patterns/data/transactional-outbox.html&quot;&gt;Transactional Outbox Pattern&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://debezium.io/blog/2020/01/22/outbox-quarkus-extension/&quot;&gt;Outbox Event Router goes Supersonic!&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/previous-versions/msp-n-p/jj591559(v=pandp.10)&quot;&gt;Introducing Event Sourcing&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/previous-versions/msp-n-p/jj554200(v=pandp.10)?redirectedfrom=MSDN&quot;&gt;Exploring CQRS and Event Sourcing&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2019/09/cqrs-event-sourcing-production/&quot;&gt;What they don’t tell you about event sourcing&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/news/2019/09/cqrs-event-sourcing-production/&quot;&gt;Day Two Problems When Using CQRS and Event Sourcing&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.confluent.io/blog/event-sourcing-vs-derivative-event-sourcing-explained/&quot;&gt;Introducing Derivative Event Sourcing&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.kislayverma.com/post/domain-events-versus-change-data-capture&quot;&gt;Domain Events versus Change Data Capture&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.reactivemanifesto.org/&quot;&gt;Reactive Manifesto&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;videos_and_podcasts&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#videos_and_podcasts&quot; /&gt;Videos and Podcasts&lt;/h3&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.infoq.com/podcasts/change-data-capture-debezium/?itm_source=podcasts_about_the-infoq-podcast&amp;amp;itm_medium=link&amp;amp;itm_campaign=the-infoq-podcast&quot;&gt;Gunnar Morling on Change Data Capture and Debezium&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=6nU9i022yeY&amp;amp;feature=youtu.be&quot;&gt;Microservices &amp;amp; Data: Implementing the Outbox Pattern with Debezium&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/col&gt;&lt;/colgroup&gt;&lt;/table&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2020/01/22/outbox-quarkus-extension/</id>
<title>Outbox Event Router goes Supersonic!</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2020-01-22T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2020/01/22/outbox-quarkus-extension/" rel="alternate" type="text/html" />
<author>
<name>Chris Cranford</name>
</author>
<category term="discussion"></category>
<category term="examples"></category>
<category term="outbox"></category>
<category term="quarkus"></category>
<summary>



Outbox as in that folder in my email client?
No, not exactly but there are some similarities!


The term outbox describes a pattern that allows independent components or services to perform read your own write semantics while concurrently providing a reliable, eventually consistent view to those writes across component or service boundaries.


You can read more about the Outbox pattern and how it applies to microservices in our blog post, Reliable Microservices Data Exchange With the Outbox Patttern.


So what exactly is an Outbox Event Router?


In Debezium version 0.9.3.Final, we introduced a ready-to-use Single Message Transform (SMT) that builds on the Outbox pattern to...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Outbox as in that folder in my email client?
No, not exactly but there are some similarities!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The term outbox describes a pattern that allows independent components or services to perform &lt;em&gt;read your own write&lt;/em&gt; semantics while concurrently providing a reliable, eventually consistent view to those writes across component or service boundaries.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can read more about the Outbox pattern and how it applies to microservices in our blog post, &lt;a href=&quot;https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/&quot;&gt;Reliable Microservices Data Exchange With the Outbox Patttern&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So what exactly is an Outbox Event Router?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In Debezium version 0.9.3.Final, we introduced a ready-to-use &lt;a href=&quot;https://kafka.apache.org/documentation/#connect_transforms&quot;&gt;Single Message Transform&lt;/a&gt; (SMT) that builds on the Outbox pattern to propagate data change events using Debezium and Kafka.
Please see the &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/configuration/outbox-event-router.html&quot;&gt;documentation&lt;/a&gt; for details on how to use this transformation.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;going_supersonic_with_quarkus&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#going_supersonic_with_quarkus&quot; /&gt;Going Supersonic with Quarkus!&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;http://www.quarkus.io&quot;&gt;Quarkus&lt;/a&gt; is a Kubernetes Native Java framework that is tailored for GraalVM and HotSpot using the &lt;em&gt;best-of-breed&lt;/em&gt; Java technologies and standards.
Quarkus aims to offer developers a unified reactive and imperative programming model to address a wide range of application architectures.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So what does all this mean exactly in laymen’s terms?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In short, the Debezium community can now leverage the Outbox pattern in a Quarkus-based application using a ready-to-use extension that works in parallel with your Debezium connector to emit change data events.
The Debezium Outbox extension for Quarkus can be used in both JVM or Native image modes in Quarkus.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;how_to_get_it&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#how_to_get_it&quot; /&gt;How to get it?&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Currently the dependency must be manually added to your Quarkus application’s &lt;code&gt;pom.xml&lt;/code&gt; as shown below.
There are plans to make this extension available in the Quarkus extension catalogue as well as via Quarkus&#39; Maven plugin in a future release.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;io.debezium.quarkus&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;debezium-quarkus-outbox&amp;lt;/artifactId&amp;gt;
  &amp;lt;version&amp;gt;1.1.0.Alpha1&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;At the time of this blog, the extension was released as &lt;em&gt;1.1.0.Alpha1&lt;/em&gt;.&lt;br&gt;
A newer version of the extension may be available, see &lt;a href=&quot;https://debezium.io/releases/&quot;&gt;Releases&lt;/a&gt; for details.&lt;/br&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;using_the_extension&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#using_the_extension&quot; /&gt;Using the extension&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium Outbox extension uses the Observer pattern to monitor when the user application emits an object that implements the &lt;code&gt;io.debezium.outbox.quarkus.ExportedEvent&lt;/code&gt; interface.
This allows the Quarkus application behavior to be completely decoupled from that of the extension.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Lets walk through a simple example where a service is responsible for storing newly created orders and then emits an event that could be used to notify other interested services that an order has been created.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So to get started, we’ll begin by first implementing &lt;code&gt;OrderCreatedEvent&lt;/code&gt;, an implementation of &lt;code&gt;ExportedEvent&lt;/code&gt;.
This event is used to signal when an &lt;code&gt;Order&lt;/code&gt; has been saved by the &lt;code&gt;OrderService&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public class OrderCreatedEvent implements ExportedEvent&amp;lt;String, JsonNode&amp;gt; {
    private final long orderId;
    private final JsonNode payload;
    private final Instant created;

    public OrderCreatedEvent(Instant createdAt, Order order) {
        this.orderId = order.getId();
        this.payload = convertOrderToJsonNode(order);
        this.created = createdAt;
    }

    @Override
    public String getAggregateId() {
        return String.valueOf(orderId);
    }

    @Override
    public String getAggregateType() {
        return &quot;Order&quot;;
    }

    @Override
    public JsonNode getPayload() {
        return payload;
    }

    @Override
    public String getType() {
        return &quot;OrderCreated&quot;;
    }

    @Override
    public Instant getTimestamp() {
        return created;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;ExportedEvent&lt;/code&gt; interface is the contract that defines how a Quarkus application is to provide the extension with the data to persist to the outbox database table.
This contract exposes several different values discussed below:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;aggregate_id&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#aggregate_id&quot; /&gt;Aggregate Id&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The aggregate id is used when emitting messages to Kafka as the message key to preserve message order.
In this example, the &lt;code&gt;OrderCreatedEvent&lt;/code&gt; returns the order identifier.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock tip&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-tip&quot; title=&quot;Tip&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;ExportedEvent&lt;/code&gt; interface is parameterized and the first argument of the parameter argument list allows the application to specify the return data type for the aggregate id.
While this example uses a &lt;code&gt;String&lt;/code&gt;, the value returned can be any persistable object type.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;aggregate_type&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#aggregate_type&quot; /&gt;Aggregate Type&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The aggregate type is a string-based value that is used to append to the Kafka topic name and also assists in routing of the given message inside the Outbox Event Router SMT.
In this example, we use &lt;code&gt;Order&lt;/code&gt; and when using the default configuration of the SMT, messages would be found in the &lt;code&gt;outbox.event.Order&lt;/code&gt; topic.
Please see the &lt;code&gt;route.topic.replacement&lt;/code&gt; in the &lt;a href=&quot;documentation/reference/1.1/configuration/outbox-event-router.html#configuration-options&quot;&gt;SMT configuration options&lt;/a&gt; for more details.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;type&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#type&quot; /&gt;Type&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The message type is a string value that is emitted in the Kafka message’s envelope.
In this example, the value in the message envelope would be &lt;code&gt;OrderCreated&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;timestamp&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#timestamp&quot; /&gt;Timestamp&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;By default, the Outbox Event Router SMT emits outbox events using the current timestamp when processing records but this may not always be sufficient for every use case.
This field allows the source application to specify an &lt;code&gt;Instant&lt;/code&gt; that can then be configured through the &lt;a href=&quot;documentation/reference/1.1/configuration/outbox-event-router.html#configuration-options&quot;&gt;SMT configuration options&lt;/a&gt; to be used as the Kafka message timestamp instead.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;payload&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#payload&quot; /&gt;Payload&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The payload is the message content or value and is what is consumed by consumers of the Kafka topic.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock tip&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-tip&quot; title=&quot;Tip&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;ExportedEvent&lt;/code&gt; interface is parameterized and the second argument of the parameter argument list allows the application to specify the return data type for the payload.
While this example uses a &lt;code&gt;JsonNode&lt;/code&gt; to store a JSON representation of the &lt;code&gt;Order&lt;/code&gt;, the payload can be any persistable object type.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If multiple implementations of &lt;code&gt;ExportedEvent&lt;/code&gt; exist in a Quarkus application, they must all use the same signature.
If different signatures are required, the code should be split into different Quarkus applications because all &lt;code&gt;ExportedEvent&lt;/code&gt; implementations will be stored in the same database outbox table for a given Quarkus application.
We are currently investigating alternatives to loosen this restriction in a future release to allow multiple variants within the same application.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;By itself, this &lt;code&gt;OrderCreatedEvent&lt;/code&gt; does nothing on its own.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Next we want to implement an application component that is responsible for persisting the order to the database and then to emit the &lt;code&gt;OrderCreatedEvent&lt;/code&gt; event.
The &lt;code&gt;OrderService&lt;/code&gt; class below uses JPA to persist the &lt;code&gt;Order&lt;/code&gt; entity and then &lt;code&gt;javax.enterprise.event.Event&amp;lt;T&amp;gt;&lt;/code&gt; to notify the outbox extension.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class OrderService {
    @Inject
    EntityManager entityManager;

    @Inject
    Event&amp;lt;ExportedEvent&amp;lt;String, JsonNode&amp;gt;&amp;gt; event;

    @Transactional
    public Order addOrder(Order order) {
        entityManager.persist(order);
        event.fire(new OrderCreatedEvent(Instant.now(), order));
        return order;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Before starting the application, certain configuration settings must be specified in &lt;code&gt;application.properties&lt;/code&gt;.
An example configuration might look like the following where we specify the database to connect to as well as how the persistence provider, Hibernate, is to operate.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-properties&quot; data-lang=&quot;properties&quot;&gt;quarkus.datasource.driver=org.postgresql.Driver
quarkus.datasource.url=jdbc:postgresql://order-db:5432/orderdb?currentSchema=orders
quarkus.datasource.username=user
quarkus.datasource.password=password
quarkus.hibernate-orm.database.generation=update
quarkus.hibernate-orm.dialect=org.hibernate.dialect.PostgreSQLDialect
quarkus.hibernate-orm.log.sql=true&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;By starting the application with this configuration the outbox table &lt;code&gt;OutboxEvent&lt;/code&gt; will be created in the &lt;code&gt;orders&lt;/code&gt; schema of the the &lt;code&gt;order-db&lt;/code&gt; database with the following layout:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;orderdb=# \d orders.outboxevent
                        Table &quot;orders.outboxevent&quot;
    Column     |            Type             | Collation | Nullable | Default
---------------+-----------------------------+-----------+----------+---------
 id            | uuid                        |           | not null |
 aggregatetype | character varying(255)      |           | not null |
 aggregateid   | character varying(255)      |           | not null |
 type          | character varying(255)      |           | not null |
 timestamp     | timestamp without time zone |           | not null |
 payload       | character varying(8000)     |           |          |
Indexes:
    &quot;outboxevent_pkey&quot; PRIMARY KEY, btree (id)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When using &lt;code&gt;JsonNode&lt;/code&gt; as the payload return type, the extension uses a JPA attribute converter to store the contents as a string in the database.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Should the table or column names not fit your naming convention, they can be customized with several &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/integrations/outbox.html#_build_time_configuration_options&quot;&gt;build-time configuration options&lt;/a&gt;.
For example, if you wanted the table to be named &lt;code&gt;outbox&lt;/code&gt; rather than &lt;code&gt;outboxevent&lt;/code&gt; add the following line to the &lt;code&gt;application.properties&lt;/code&gt; file:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-properties&quot; data-lang=&quot;properties&quot;&gt;quarkus.debezium-outbox.table-name=outbox&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you enabled SQL logging or check the row count of the outbox table, you might find it unusual that after saving the order that a record is inserted into the outbox table but then is immediately deleted.
This is the default behavior since rows are not required to be retained for Debezium to pick up the change.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If row retention is required, this can be configured using a &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/integrations/outbox.html#_runtime_configuration_options&quot;&gt;run-time configuration option&lt;/a&gt;.
In order to enable row retention, add the following configuration to the &lt;code&gt;application.properties&lt;/code&gt; file.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-properties&quot; data-lang=&quot;properties&quot;&gt;quarkus.debezium-outbox.remove-after-insert=false&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;setting_up_the_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#setting_up_the_connector&quot; /&gt;Setting up the connector&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Up to this point we’ve covered how to configure and use the extension in a Quarkus application to save events into the outbox database table.
The last step is to configure the Debezium connector to monitor the outbox and emit those records to Kafka.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’re going to use the following connector configuration:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
  &quot;connector.class&quot;: &quot;io.debezium.connector.postgresql.PostgresConnector&quot;,
  &quot;tasks.max&quot;: &quot;1&quot;,
  &quot;database.hostname&quot;: &quot;order-db&quot;,
  &quot;database.port&quot;: &quot;5432&quot;,
  &quot;database.user&quot;: &quot;user&quot;,
  &quot;database.password&quot;: &quot;password&quot;,
  &quot;database.dbname&quot;: &quot;orderdb&quot;,
  &quot;database.server.name&quot;: &quot;dbserver1&quot;,
  &quot;schema.whitelist&quot; : &quot;orders&quot;,
  &quot;table.whitelist&quot;: &quot;orders.outboxevent&quot;,
  &quot;tombstones.on.delete&quot;: &quot;false&quot;,
  &quot;transforms&quot;: &quot;outbox&quot;,
  &quot;transforms.outbox.type&quot; : &quot;io.debezium.transforms.outbox.EventRouter&quot;,
  &quot;transforms.outbox.route.topic.replacement&quot;: &quot;${routedByValue}.events&quot;,
  &quot;transforms.outbox.table.field.event.timestamp&quot;: &quot;timestamp&quot;,
  &quot;transforms.outbox.table.fields.additional.placement&quot;: &quot;type:header:eventType&quot;
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A vast majority of this is standard Debezium connector configuration, but what is important are the last several lines that begin with &lt;strong&gt;transforms&lt;/strong&gt;.
These are configuration options that are used by Kafka Connect to configure and call the Outbox Event Router SMT.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This configuration uses a custom &lt;code&gt;route.topic.replacement&lt;/code&gt; configuration property.
This setting will instead route &lt;code&gt;OrderCreatedEvent&lt;/code&gt; rows from the outbox to the &lt;code&gt;Order.events&lt;/code&gt; topic rather than the default &lt;code&gt;outbox.events.Order&lt;/code&gt; topic.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This configuration also specifies the &lt;code&gt;field.event.timestamp&lt;/code&gt; configuration property.
This setting will instead populate the Kafka message time from the &lt;code&gt;timestamp&lt;/code&gt; field in the outbox database table rather than the current timestamp when processing the row.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please see &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/configuration/outbox-event-router.html#configuration-options&quot;&gt;Outbox Event Router Configuration Options&lt;/a&gt; for details on how to configure the SMT.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once the connector is running, the &lt;code&gt;Order.events&lt;/code&gt; topic will be populated with messages from the outbox table.
The following JSON example represents an &lt;code&gt;Order&lt;/code&gt; which gets saved by the &lt;code&gt;OrderService&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
    &quot;customerId&quot; : &quot;123&quot;,
    &quot;orderDate&quot; : &quot;2019-01-31T12:13:01&quot;,
    &quot;lineItems&quot; : [
        {
            &quot;item&quot; : &quot;Debezium in Action&quot;,
            &quot;quantity&quot; : 2,
            &quot;totalPrice&quot; : 39.98
        },
        {
            &quot;item&quot; : &quot;Debezium for Dummies&quot;,
            &quot;quantity&quot; : 1,
            &quot;totalPrice&quot; : 29.99
        }
    ]
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When examining the &lt;code&gt;Order.events&lt;/code&gt; topic, the event emitted will look like the following:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
  &quot;key&quot;: &quot;1&quot;,
  &quot;headers&quot;: &quot;id=cc74eac7-176b-44e7-8bda-413a5088ca66,eventType=OrderCreated&quot;
}
&quot;{\&quot;id\&quot;:1,\&quot;customerId\&quot;:123,\&quot;orderDate\&quot;:\&quot;2019-01-31T12:13:01\&quot;,\&quot;lineItems\&quot;:[{\&quot;id\&quot;:1,\&quot;item\&quot;:\&quot;Debezium in Action\&quot;,\&quot;quantity\&quot;:2,\&quot;totalPrice\&quot;:39.98,\&quot;status\&quot;:\&quot;ENTERED\&quot;},{\&quot;id\&quot;:2,\&quot;item\&quot;:\&quot;Debezium for Dummies\&quot;,\&quot;quantity\&quot;:1,\&quot;totalPrice\&quot;:29.99,\&quot;status\&quot;:\&quot;ENTERED\&quot;}]}&quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;wrapping_up&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#wrapping_up&quot; /&gt;Wrapping up&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It is really simple and easy to setup and use the Debezium Outbox extension.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We have a complete &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/outbox&quot;&gt;example&lt;/a&gt; in our examples repository that uses the order service described here as well as a shipment service that consumes the events.
For more details on the extension, refer to the &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/integrations/outbox.html&quot;&gt;Outbox Quarkus Extension&lt;/a&gt; documentation.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;future_plans&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#future_plans&quot; /&gt;Future Plans&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The current implementation of the Debezium Outbox extension works quite well, but we acknowledge there is still room for improvement.
Some of the things we’ve already identified and have plans to include in future iterations of the extension are:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Avro serialization support for event payload&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Full outbox table column attribute control, e.g. definition, length, precision, scale, and converters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Complete outbox table customization using a user-supplied entity class.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Allow varied signatures of &lt;code&gt;ExportedEvent&lt;/code&gt; within a single application.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We are currently tracking all future changes to this extension in &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1711&quot;&gt;DBZ-1711&lt;/a&gt;.
As always we welcome any and all feedback, so feel free to let us know in that issue, on Gitter, or the mailing lists.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2020/01/16/debezium-1-1-alpha1-released/</id>
<title>Debezium 1.1.0.Alpha1 Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2020-01-16T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2020/01/16/debezium-1-1-alpha1-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<summary>



Did you know January 16th is National Nothing Day?
It&#8217;s the one day in the year without celebrating, observing or honoring anything.


Well, normally, that is.
Because we couldn&#8217;t stop ourselves from sharing the news of the Debezium 1.1.0.Alpha1 release with you!
It&#8217;s the first release after Debezium 1.0,
and there are some really useful features coming with it.
Let&#8217;s take a closer look.




Quarkus Outbox Pattern Extension


The outbox pattern is a great way for letting services update their own database
(e.g. to persist a new purchase order) as well as emitting corresponding events to other services in a reliable and consistent way.
The pattern avoids unsafe dual writes,...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Did you know January 16th is &lt;a href=&quot;https://en.wikipedia.org/wiki/National_Nothing_Day&quot;&gt;National Nothing Day&lt;/a&gt;?
It’s the one day in the year without celebrating, observing or honoring anything.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Well, normally, that is.
Because we couldn’t stop ourselves from sharing the news of the Debezium &lt;strong&gt;1.1.0.Alpha1&lt;/strong&gt; release with you!
It’s the first release after Debezium 1.0,
and there are some really useful features coming with it.
Let’s take a closer look.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;quarkus_outbox_pattern_extension&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#quarkus_outbox_pattern_extension&quot; /&gt;Quarkus Outbox Pattern Extension&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;a href=&quot;https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/&quot;&gt;outbox pattern&lt;/a&gt; is a great way for letting services update their own database
(e.g. to persist a new purchase order) as well as emitting corresponding events to other services in a reliable and consistent way.
The pattern avoids unsafe dual writes, but also doesn’t require distributed transactions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium already supports this pattern via its &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/configuration/outbox-event-router.html&quot;&gt;outbox event router&lt;/a&gt;,
which can be used to stream events from one outbox table into different topics in Apache Kafka.
As we saw a growing adoption of the pattern and usage of this router,
we wanted to take things to the next level and also provide a component to simplify the creation of outbox events within your application itself.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For that purpose there’s now a &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/integrations/outbox.html&quot;&gt;brand-new extension&lt;/a&gt; for &lt;a href=&quot;https://quarkus.io/&quot;&gt;Quarkus&lt;/a&gt;, a &quot;Kubernetes Native Java stack tailored for OpenJDK HotSpot and GraalVM, crafted from the best of breed Java libraries and standards&quot;.
Using this extension it becomes as simple as firing plain CDI events to produce outbox events:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class OrderService {

    @Inject
    Event&amp;lt;ExportedEvent&amp;lt;?, ?&amp;gt;&amp;gt; event;

    @Transactional
    public PurchaseOrder addOrder(PurchaseOrder order) {
        // process and persist the order...

        // create a corresponding outbox event to notify other services
        event.fire(OrderCreatedEvent.of(order));

        return order;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once the event has been persisted in the outbox table and the transaction has been committed,
Debezium will capture it and propagate it to downstream consumers via Kafka.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’re planning to publish a detailed post with a complete end-to-end example for implementing the outbox pattern with this Debezium and this Quarkus extension next week, so stay tuned.
To read up on the outbox pattern in the meantime, please refer to the blog post linked above or to any of &lt;a href=&quot;https://dzone.com/articles/implementing-the-outbox-pattern&quot;&gt;these&lt;/a&gt; &lt;a href=&quot;https://medium.com/engineering-varo/event-driven-architecture-and-the-outbox-pattern-569e6fba7216&quot;&gt;great&lt;/a&gt; &lt;a href=&quot;https://thoughts-on-java.org/outbox-pattern-with-cdc-and-debezium/&quot;&gt;posts&lt;/a&gt; from the community.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;cloudevents_support&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#cloudevents_support&quot; /&gt;CloudEvents Support&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://cloudevents.io/&quot;&gt;CloudEvents&lt;/a&gt; is a &quot;specification for describing event data in a common way&quot;, aiming at providing &quot;interoperability across services, platforms and systems&quot;.
By means of a new Kafka Connect message converter (&lt;code&gt;io.debezium.converters.CloudEventsConverter&lt;/code&gt;), Debezium now can &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/integrations/cloudevents.html&quot;&gt;emit change events&lt;/a&gt; that adhere to the CloudEvents specification:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;{
  &quot;id&quot; : &quot;name:test_server;lsn:29274832;txId:565&quot;,
  &quot;source&quot; : &quot;/debezium/postgresql/test_server&quot;,
  &quot;specversion&quot; : &quot;1.0&quot;,
  &quot;type&quot; : &quot;io.debezium.postgresql.datachangeevent&quot;,
  &quot;time&quot; : &quot;2020-01-13T13:55:39.738Z&quot;,
  &quot;datacontenttype&quot; : &quot;application/json&quot;,
  &quot;iodebeziumop&quot; : &quot;r&quot;,
  &quot;iodebeziumversion&quot; : &quot;1.1.0.Alpha1&quot;,
  &quot;iodebeziumconnector&quot; : &quot;postgresql&quot;,
  &quot;iodebeziumname&quot; : &quot;test_server&quot;,
  &quot;iodebeziumtsms&quot; : &quot;1578923739738&quot;,
  &quot;iodebeziumsnapshot&quot; : &quot;true&quot;,
  &quot;iodebeziumdb&quot; : &quot;postgres&quot;,
  &quot;iodebeziumschema&quot; : &quot;s1&quot;,
  &quot;iodebeziumtable&quot; : &quot;a&quot;,
  &quot;iodebeziumtxId&quot; : &quot;565&quot;,
  &quot;iodebeziumlsn&quot; : &quot;29274832&quot;,
  &quot;iodebeziumxmin&quot; : null,
  &quot;data&quot; : {
    &quot;before&quot; : null,
    &quot;after&quot; : {
      &quot;pk&quot; : 1,
      &quot;name&quot; : &quot;Bob&quot;
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With CloudEvents, each event contains a few defined attributes such as &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;source&lt;/code&gt; and &lt;code&gt;type&lt;/code&gt;.
The actual event payload can be found in the &lt;code&gt;data&lt;/code&gt; attribute, which in the case of Debezium is the structure of old and new state of the affected database record.
Most of the other Debezium change events attributes (&lt;code&gt;op&lt;/code&gt;, timestamp, source metadata) are mapped to custom attributes using the &lt;code&gt;iodebezium&lt;/code&gt; prefix.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;data&lt;/code&gt; attribute as well as the entire event can be encoded using JSON or Avro.
Initially, only the &quot;structured mode&quot; of CloudEvents is supported, i.e. all the attributes are part of the event structure, which is the Kafka record value in this case.
In a future release we’ll also add support for the CloudEvents &quot;binary mode&quot;,
where only the &lt;code&gt;data&lt;/code&gt; attribute is part of the event structure, while all other attributes will be mapped the (Kafka) header attributes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;CloudEvents support is under active development, so details around the format likely will change in future versions as this feature matures.
We’d love to get your feedback on this and learn from your insights and experiences with CloudEvents.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;further_changes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#further_changes&quot; /&gt;Further Changes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Besides these two larger features, a number of smaller improvements and fixes has been done for Debezium 1.1 Alpha1:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Column value masking for Postgres, allowing to replace sensitive column values with asterisks (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1685&quot;&gt;DBZ-1685&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Several fixes to the MySQL DDL parser related to trigger definitions (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1699&quot;&gt;DBZ-1699&lt;/a&gt;) and the &lt;code&gt;SIGNAL&lt;/code&gt; keyword (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1691&quot;&gt;DBZ-1691&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Two bugfixes around time and precision thereof (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1688&quot;&gt;DBZ-1688&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1707&quot;&gt;DBZ-1707&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Altogether, &lt;a href=&quot;https://debezium.io/releases/1.1/release-notes/#release-1.1.0-alpha1&quot;&gt;17 issues&lt;/a&gt; were fixed for this release.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Many thanks to &lt;a href=&quot;https://github.com/oscerd&quot;&gt;Andrea Cosentino&lt;/a&gt;, &lt;a href=&quot;https://github.com/vasilyulianko-visma&quot;&gt;Vasily Ulianko&lt;/a&gt;, &lt;a href=&quot;https://github.com/vedit&quot;&gt;Vedit Firat Arig&lt;/a&gt;, &lt;a href=&quot;https://github.com/liulangwa&quot;&gt;Yongjun Du&lt;/a&gt; and &lt;a href=&quot;https://github.com/Wang-Yu-Chao&quot;&gt;Yuchao Wang&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Going forward, we’ll continue with further Debezium 1.1 preview releases every two to three weeks.
Take a look at the &lt;a href=&quot;https://debezium.io/roadmap/&quot;&gt;roadmap&lt;/a&gt; to see what’s coming up, or get in touch to tell us about your specific feature requirements!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/12/18/debezium-1-0-0-final-released/</id>
<title>Streaming Now: Debezium 1.0 Final Is Out</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-12-18T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/12/18/debezium-1-0-0-final-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="mongodb"></category>
<category term="cassandra"></category>
<category term="oracle"></category>
<summary>



Today it&#8217;s my great pleasure to announce the availability of Debezium 1.0.0.Final!


Since the initial commit in November 2015,
the Debezium community has worked tirelessly to realize the vision of building a comprehensive open-source low-latency platform for change data capture (CDC) for a variety of databases.


Within those four years, Debezium&#8217;s feature set has grown tremendously: stable, highly configurable CDC connectors for MySQL, Postgres, MongoDB and SQL Server, incubating connectors for Apache Cassandra and Oracle, facilities for transforming and routing change data events, support for design patterns such as the outbox pattern and much more.
A very active and welcoming community of users, contributors...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Today it’s my great pleasure to announce the availability of Debezium &lt;strong&gt;1.0.0.Final&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Since the &lt;a href=&quot;https://github.com/debezium/debezium/commit/0a99ed67cd8f74d6f451b0a2d3809e23127e4698&quot;&gt;initial commit&lt;/a&gt; in November 2015,
the Debezium community has worked tirelessly to realize the vision of building a comprehensive open-source low-latency platform for change data capture (CDC) for a variety of databases.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Within those four years, Debezium’s feature set has grown tremendously: stable, highly configurable CDC connectors for MySQL, Postgres, MongoDB and SQL Server, incubating connectors for Apache Cassandra and Oracle, facilities for transforming and routing change data events, support for design patterns such as the outbox pattern and much more.
A very active and welcoming community of users, contributors and committers has formed around the project.
Debezium is deployed to production at lots of organizations from all kinds of industries,
some with huge installations, using hundreds of connectors to stream data changes out of thousands of databases.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The 1.0 release marks an important milestone for the project:
based on all the production feedback we got from the users of the 0.x versions, we figured it’s about time to express the maturity of the four stable connectors in the version number, too.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;why_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#why_debezium&quot; /&gt;Why Debezium?&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One of the things making it so enjoyable to work on Debezium as a tool for change data capture is the variety of potential use cases.
When presenting the project at conferences,
it’s just great to see how people quickly get excited when they realize all the possibilities enabled by Debezium and CDC.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In a nutshell, Debezium is one big enabler for letting you react to changes in your data with a low latency.
Or, as one conference attendee recently put it, it’s &quot;like the observer pattern, but for your database&quot;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here’s a few things we’ve seen Debezium being used for as a ingestion component in data streaming pipelines:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Replicating data from production databases to other databases and data warehouses&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Feeding data to search services like Elasticsearch or Apache Solr&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Updating or invalidating caches&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When using Debezium with Apache Kafka and its rich ecosystem of sink connectors,
setting up such integrations can be done without any coding,
just by means of deploying and configuring connectors in Kafka Connect:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/debezium_kafka_pipeline.png&quot; style=&quot;max-width:95%; margin-bottom:10px; margin-top:10px;&quot; class=&quot;responsive-image&quot; alt=&quot;Data Streaming Pipeline With Debezium&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But there are many other use cases of CDC which go beyond just moving data from A to B.
When adding stream processing into the picture, e.g. via Kafka Streams or Apache Flink,
CDC enables you to run &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/kstreams-live-update&quot;&gt;time-windowed streaming queries&lt;/a&gt;, continuously updated as your operational data changes
(&quot;what’s the aggregated order revenue per category within the last hour&quot;).
You can use CDC to &lt;a href=&quot;https://debezium.io/blog/2019/10/01/audit-logs-with-change-data-capture-and-stream-processing/&quot;&gt;build audit logs&lt;/a&gt; of your data,
telling who changed which data items at what time.
Or update denormalized views of your data, for the sake of efficient data retrieval, adhering to the CQRS pattern (Command Query Responsibility Segregation).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Finally, CDC can also play a vital role in microservices architectures;
exchanging data between services and keeping local views of data owned by other services achieves a higher independence, without having to rely on synchronous API calls.
One particularly interesting approach in this context is the &lt;a href=&quot;https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/&quot;&gt;outbox pattern&lt;/a&gt;,
which is &lt;a href=&quot;https://debezium.io/documentation/reference/1.0/configuration/outbox-event-router.html&quot;&gt;well supported&lt;/a&gt; by Debezium.
In case you don’t start on the green field (who ever does?),
CDC can be used to implement the &lt;a href=&quot;https://martinfowler.com/bliki/StranglerFigApplication.html&quot;&gt;strangler pattern&lt;/a&gt; for moving from a monolithic design to microservices.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can learn more about change data capture use cases with Debezium and Apache Kafka in &lt;a href=&quot;https://speakerdeck.com/gunnarmorling/practical-change-data-streaming-use-cases-with-apache-kafka-and-debezium-qcon-san-francisco-2019&quot;&gt;this presentation&lt;/a&gt; from QCon San Francisco.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But you don’t have to take our word for it:
you can find lots of blog posts, conference talks and examples by folks using Debezium in production in our compilation of &lt;a href=&quot;https://debezium.io/documentation/online-resources/&quot;&gt;resources&lt;/a&gt;.
If you’d like to get a glimpse of who else already is using Debezium,
see our rapidly growing &lt;a href=&quot;https://debezium.io/community/users/&quot;&gt;list of reference users&lt;/a&gt;
(or send us a &lt;a href=&quot;https://github.com/debezium/debezium.github.io/blob/develop/community/users.asciidoc&quot;&gt;pull request&lt;/a&gt; to get your name added if your organization already is running Debezium in production).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;debezium_1_0&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#debezium_1_0&quot; /&gt;Debezium 1.0&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now, let’s talk a little bit about the contents of the 1.0 release.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This version continues the effort we began in &lt;a href=&quot;https://debezium.io/blog/2019/10/02/debezium-0-10-0-final-released/&quot;&gt;0.10&lt;/a&gt; to make sure the emitted event structures and configuration options of the connectors are correct and consistent.
While we’ve always been very careful to ensure a smooth upgrading experience, you can expect even more stability in this regard going forward after the 1.0 release.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’ve expanded the test coverage of databases (Postgres 12, SQL Server 2019, MongoDB 4.2),
upgraded our container images to OpenJDK 11
and now build against the latest version of Apache Kafka
(2.4.0; earlier versions continue to be supported, too).
And last but not least, we’ve also fixed a large number of bugs.
Overall, &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20in%20(1.0.0.Beta1%2C%201.0.0.Beta2%2C%201.0.0.Beta3%2C%201.0.0.CR1%2C%201.0.0.Final)&quot;&gt;96 issues&lt;/a&gt; were addressed in Debezium 1.0 and its preview releases (&lt;a href=&quot;https://debezium.io/blog/2019/10/17/debezium-1-0-0-beta1-released/&quot;&gt;Beta1&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/10/24/debezium-1-0-0-beta2-released/&quot;&gt;Beta2&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/11/14/debezium-1-0-0-beta3-released/&quot;&gt;Beta3&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/12/12/debezium-1-0-0-cr1-released/&quot;&gt;CR1&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you’re on 0.10 right now, the upgrade is mostly a drop-in replacement.
When coming from earlier versions, please make sure to read the &lt;a href=&quot;https://debezium.io/releases/1.0/release-notes/&quot;&gt;migration notes&lt;/a&gt; to learn about deprecated options, upgrading procedures and more.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;the_most_important_part_the_debezium_community&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_most_important_part_the_debezium_community&quot; /&gt;The Most Important Part: The Debezium Community&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium couldn’t exist without its community of contributors and users.
I can’t begin to express how grateful I am for having the chance to be a member of this fantastic community,
interacting and working with folks from around the world towards our joint goal of building the leading open-source solution for change data capture.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;At this point, about 150 people have contributed to the different Debezium code repositories (please let me know if I’ve missed anybody):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;em&gt;Aaron Rosenberg, Addison Higham, Adrian Kreuziger, Akshath Patkar, Alexander Kovryga, Amit Sela, Andreas Bergmeier, Andras Istvan Nagy, Andrew Garrett, Andrew Tongen, Andrey Pustovetov, Anton Martynov, Arkoprabho Chakraborti, artiship, Ashhar Hasan, Attila Szucs, Barry LaFond, Bartosz Miedlar, Ben Williams, Bin Li, Bingqin Zhou, Braden Staudacher, Brandon Brown, Brandon Maguire, Cheng Pan, Ching Tsai, Chris Cranford, Chris Riccomini, Christian Posta, Chuck Ha, Cliff Wheadon, Collin Van Dyck, Cyril Scetbon, David Chen, David Feinblum, David Leibovic, David Szabo, Deepak Barr, Denis Mikhaylov, Dennis Campagna, Dennis Persson, Duncan Sands, Echo Xu, Eero Koplimets, Emrul Islam, Eric S. Kreiseir, Ewen Cheslack-Postava, Felix Eckhardt, Gagan Agrawal, Grant Cooksey, Guillaume Rosauro, Gunnar Morling, Gurnaaz Randhawa, Grzegorz Kołakowski, Hans-Peter Grahsl, Henryk Konsek, Horia Chiorean, Ian Axelrod, Ilia Bogdanov, Ivan Kovbas, Ivan Lorenz, Ivan Luzyanin, Ivan San Jose, Ivan Vucina, Jakub Cechacek, Jaromir Hamala, Javier Holguera, Jeremy Finzel, Jiri Pechanec, Johan Venant, John Martin, Jon Casstevens, Jordan Bragg, Jork Zijlstra, Josh Arenberg, Josh Stanfield, Joy Gao, Jure Kajzer, Keith Barber, Kevin Pullin, Kewen Chao, Krizhan Mariampillai, Leo Mei, Lev Zemlyanov, Listman Gamboa, Liu Hanlin, Luis Garcés-Erice, Maciej Bryński, MaoXiang Pan, Mario Mueller, Mariusz Strzelecki, Matteo Capitanio, Mathieu Rozieres, Matthias Wessendorf, Mike Graham, Mincong Huang, Moira Tagle, Muhammad Sufyian, Navdeep Agarwal, Nikhil Benesch, Olavi Mustanoja, Oliver Weiler, Olivier Lemasle, Omar Al-Safi, Ori Popowski, Orr Ganani, Peng Lyu, Peter Goransson, Peter Larsson, Philip Sanetra, Pradeep Mamillapalli, Prannoy Mittal, Preethi Sadagopan, pushpavanthar, Raf Liwoch, Ram Satish, Ramesh Reddy, Randall Hauch, Renato Mefi, Roman Kuchar, Sagar Rao, René Kerner, Rich O’Connell, Robert Coup, Sairam Polavarapu, Sanjay Kr Singh, Sanne Grinovero, Satyajit Vegesna, Saulius Valatka, Scofield Xu, Sherafudheen PM, Shivam Sharma, Shubham Rawat, Stanley Shyiko, Stathis Souris, Stephen Powis, Steven Siahetiong, Syed Muhammad Sufyian, Tautvydas Januskevicius, Taylor Rolison, Theofanis Despoudis, Thomas Deblock, Tom Bentley, Tomaz Lemos Fernandes, Tony Rizko, Wang-Yu-Chao, Wei Wu, WenZe Hu, William Pursell, Willie Cheong, Wout Scheepers, Yang Yang, Zheng Wang&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You’re amazing, and I would like to wholeheartedly thank each and everyone of you!
I’m sure our community will continue to grow in the future — I’d love it if we hit the mark of 200 contributors in 2020.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Equally important are our users; interacting with you in the chat, on the mailing list or at conferences and meet-ups is what helps to drive the direction of the project:
learning about your specific requirements and use cases (or bugs you’ve run into) is vital for deciding where to put the focus next.
A big thank you to you, too!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Some of you even have shared their experiences with Debezium in conference talks and blog posts.
Nothing beats hearing the war stories of others and being able to learn from their experiences,
so you speaking about your insights around Debezium and CDC is incredibly helpful and highly appreciated!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;whats_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#whats_next&quot; /&gt;What’s Next?&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s wrap up this post with a look to see what’s next in store for Debezium.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;After some long over-due holidays, we’re planning to begin the work on Debezium 1.1 in January.
Some of the potential features you can look forward to are:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Support for the CloudEvents specification as a portable event format&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A Quarkus extension for implementing the outbox pattern&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A stand-alone Debezium server which will let you stream data change events to messaging infrastructure such as Amazon Kinesis&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Means of exposing transactional boundaries on a separate topic, allowing to aggregate all the events originating from one source transaction and process them at once&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Further progression of the incubating community-led connectors for Oracle and Apache Cassandra&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Of course, this roadmap is strongly influenced by the community, i.e. you.
So if you would like to see any particular items here, please let us know.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We also have some exciting blog posts in the workings,
e.g. on how to combine Debezium with the brand-new Kafka Connect &lt;a href=&quot;https://camel.apache.org/blog/Camel-Kafka-connector-intro/&quot;&gt;connector for Apache Camel&lt;/a&gt; or how to use the recently added support for non-key joins in Kafka Streams (&lt;a href=&quot;https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=74684836&quot;&gt;KIP-213&lt;/a&gt;) with Debezium change events.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One more thing I’m super-thrilled about is Debezium becoming a supported component of the &lt;a href=&quot;https://www.redhat.com/en/blog/whats-new-red-hat-integration&quot;&gt;Red Hat Integration&lt;/a&gt; product.
Part of the current release is a &lt;a href=&quot;https://developers.redhat.com/blog/2019/11/22/red-hat-advances-debezium-cdc-connectors-for-apache-kafka-support-to-technical-preview/&quot;&gt;Tech Preview&lt;/a&gt; for the change data capture connectors for MySQL, Postgres, SQL Server and MongoDB.
This is great news for folks who wish to have commercial support by Red Hat for their CDC connectors.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For now, let’s celebrate the release of Debezium 1.0 and look forward to what’s coming in 2020.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Onwards and Upwards!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/12/13/externalized-secrets/</id>
<title>Secrets externalization with Debezium connectors</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-12-13T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/12/13/externalized-secrets/" rel="alternate" type="text/html" />
<author>
<name>Jiri Pechanec</name>
</author>
<category term="secrets"></category>
<category term="mysql"></category>
<category term="example"></category>
<summary>

When a Debezium connector is deployed to a Kafka Connect instance it is sometimes necessary to keep database credentials hidden from other users of the Connect API.


Let&#8217;s remind how a connector registration request looks like for the MySQL Debezium connector:



{
    "name": "inventory-connector",
    "config": {
        "connector.class": "io.debezium.connector.mysql.MySqlConnector",
        "tasks.max": "1",
        "database.hostname": "mysql",
        "database.port": "3306",
        "database.user": "debezium",
     ...
</summary>
<content type="html">
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When a Debezium connector is deployed to a Kafka Connect instance it is sometimes necessary to keep database credentials hidden from other users of the Connect API.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s remind how a connector registration request looks like for the MySQL Debezium connector:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
    &quot;name&quot;: &quot;inventory-connector&quot;,
    &quot;config&quot;: {
        &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
        &quot;tasks.max&quot;: &quot;1&quot;,
        &quot;database.hostname&quot;: &quot;mysql&quot;,
        &quot;database.port&quot;: &quot;3306&quot;,
        &quot;database.user&quot;: &quot;debezium&quot;,
        &quot;database.password&quot;: &quot;dbz&quot;,
        &quot;database.server.id&quot;: &quot;184054&quot;,
        &quot;database.server.name&quot;: &quot;dbserver1&quot;,
        &quot;database.whitelist&quot;: &quot;inventory&quot;,
        &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
        &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;username&lt;/code&gt; and &lt;code&gt;password&lt;/code&gt; are passed to the API as plain strings.
Worse yet, anybody who has access to the Kafka Connect cluster and its REST API can issue a &lt;code&gt;GET&lt;/code&gt; request to obtain a configuration of the connector including the database credentials:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -s http://localhost:8083/connectors/inventory-connector | jq .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
  &quot;name&quot;: &quot;inventory-connector&quot;,
  &quot;config&quot;: {
    &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
    &quot;database.user&quot;: &quot;debezium&quot;,
    &quot;database.server.id&quot;: &quot;184054&quot;,
    &quot;tasks.max&quot;: &quot;1&quot;,
    &quot;database.hostname&quot;: &quot;mysql&quot;,
    &quot;database.password&quot;: &quot;dbz&quot;,
    &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
    &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot;,
    &quot;name&quot;: &quot;inventory-connector&quot;,
    &quot;database.server.name&quot;: &quot;dbserver1&quot;,
    &quot;database.whitelist&quot;: &quot;inventory&quot;,
    &quot;database.port&quot;: &quot;3306&quot;
  },
  &quot;tasks&quot;: [
    {
      &quot;connector&quot;: &quot;inventory-connector&quot;,
      &quot;task&quot;: 0
    }
  ],
  &quot;type&quot;: &quot;source&quot;
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If one Kafka Connect cluster is shared by multiple connectors/teams, then this behaviour can be undesiable for security reasons.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To solve the problem &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-297%3A+Externalizing+Secrets+for+Connect+Configurations&quot;&gt;KIP-297&lt;/a&gt; (&quot;Externalizing Secrets for Connect Configurations&quot;) was implemented in Kafka 2.0.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The externalization expects there is at least one implementation class of the &lt;code&gt;org.apache.kafka.common.config.provider.ConfigProvider&lt;/code&gt; interface.
Kafka Connect provides the reference implementation &lt;code&gt;org.apache.kafka.common.config.provider.FileConfigProvider&lt;/code&gt; that reads secrets from a file.
Available config providers are configured at Kafka Connect worker level (e.g. in &lt;code&gt;connect-distributed.properties&lt;/code&gt;) and are referred to from the connector configuration.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;An example of worker configuration would be this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;config.providers=file
config.providers.file.class=org.apache.kafka.common.config.provider.FileConfigProvider&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;and the connector registration request will refer to it like so:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
    &quot;name&quot;: &quot;inventory-connector&quot;,
    &quot;config&quot;: {
        &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
        &quot;tasks.max&quot;: &quot;1&quot;,
        &quot;database.hostname&quot;: &quot;mysql&quot;,
        &quot;database.port&quot;: &quot;3306&quot;,
        &quot;database.user&quot;: &quot;${file:/secrets/mysql.properties:user}&quot;,
        &quot;database.password&quot;: &quot;${file:/secrets/mysql.properties:password}&quot;,
        &quot;database.server.id&quot;: &quot;184054&quot;,
        &quot;database.server.name&quot;: &quot;dbserver1&quot;,
        &quot;database.whitelist&quot;: &quot;inventory&quot;,
        &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
        &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here, the Placeholder &lt;code&gt;${file:/secrets/mysql.properties:user}&lt;/code&gt; says that the file config provider should be used, reading the property file &lt;code&gt;/secrets/mysql.properties&lt;/code&gt; and extracting the &lt;code&gt;user&lt;/code&gt; property from it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The file config provider is probably the simplest possible implementation, and it can be expected that other providers will appear that will integrate with secret repositories or identity management systems.
It should be noted though that the file config provider is satisfactory in Kubernetes/OpenShift deployments, as &lt;code&gt;secrets&lt;/code&gt; objects could be injected into cluster pods as files and thus consumed by it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’ve created a version of the Debezium &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/tutorial&quot;&gt;tutorial example&lt;/a&gt;, which demonstrates a deployment of externalized secrets. Please note the two environment variables in the Docker Compose &lt;code&gt;connect&lt;/code&gt; service:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-[source&quot; data-lang=&quot;[source&quot;&gt;...
- CONNECT_CONFIG_PROVIDERS=file
- CONNECT_CONFIG_PROVIDERS_FILE_CLASS=org.apache.kafka.common.config.provider.FileConfigProvider
...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;These environment variables are directly mapped into Kafka Connect worker properties as a functionality of the &lt;code&gt;debezium/connect&lt;/code&gt; image.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When you issue the REST call to get the connector configuration, you will see that the sensitive information is externalized and masked from unauthorized users:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -s http://localhost:8083/connectors/inventory-connector | jq .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
  &quot;name&quot;: &quot;inventory-connector&quot;,
  &quot;config&quot;: {
    &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
    &quot;database.user&quot;: &quot;${file:/secrets/mysql.properties:user}&quot;,
    &quot;database.server.id&quot;: &quot;184054&quot;,
    &quot;tasks.max&quot;: &quot;1&quot;,
    &quot;database.hostname&quot;: &quot;mysql&quot;,
    &quot;database.password&quot;: &quot;${file:/secrets/mysql.properties:password}&quot;,
    &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
    &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot;,
    &quot;name&quot;: &quot;inventory-connector&quot;,
    &quot;database.server.name&quot;: &quot;dbserver1&quot;,
    &quot;database.whitelist&quot;: &quot;inventory&quot;,
    &quot;database.port&quot;: &quot;3306&quot;
  },
  &quot;tasks&quot;: [
    {
      &quot;connector&quot;: &quot;inventory-connector&quot;,
      &quot;task&quot;: 0
    }
  ],
  &quot;type&quot;: &quot;source&quot;
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please refer to the &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/tutorial#using-externalized-secrets&quot;&gt;README&lt;/a&gt; of the tutorial example for complete instructions.&lt;/p&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/12/12/debezium-1-0-0-cr1-released/</id>
<title>Debezium 1.0.0.CR1 Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-12-12T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/12/12/debezium-1-0-0-cr1-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<summary>



Did you know December 12th is National Ding-a-Ling Day?
It&#8217;s the day to call old friends you haven&#8217;t heard from in a while.
So we thought we&#8217;d get in touch (not that is has been that long) with our friends, i.e. you, and share the news about the release of Debezium 1.0.0.CR1!


It&#8217;s the first, and ideally only, candidate release; so Debezium 1.0 should be out very soon.
Quite a few nice features found their way into CR1:




A SerDe (serializer/deserializer) for working with JSON-formatted change data events in Kafka Streams (DBZ-1533)


Advanced type support for Postgres: the long awaited enum types (DBZ-920), domain types (DBZ-1413)...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Did you know December 12th is National Ding-a-Ling Day?
It’s the day to call old friends you haven’t heard from in a while.
So we thought we’d get in touch (not that is has been &lt;strong&gt;that&lt;/strong&gt; long) with our friends, i.e. you, and share the news about the release of Debezium &lt;strong&gt;1.0.0.CR1&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It’s the first, and ideally only, candidate release; so Debezium 1.0 should be out very soon.
Quite a few nice features found their way into CR1:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A SerDe (serializer/deserializer) for &lt;a href=&quot;https://debezium.io/documentation/reference/1.0/configuration/serdes.html&quot;&gt;working with JSON-formatted change data events&lt;/a&gt; in Kafka Streams (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1533&quot;&gt;DBZ-1533&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Advanced type support for Postgres: the long awaited enum types (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-920&quot;&gt;DBZ-920&lt;/a&gt;), domain types (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1413&quot;&gt;DBZ-1413&lt;/a&gt;) and arrays of UUIDs (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1637&quot;&gt;DBZ-1637&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Graceful handling of MongoDB 4.0 transaction events (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1215&quot;&gt;DBZ-1215&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Full support for Java 11, including the upgrade to Java 11 in the Debezium container images for Apache Kafka and Connect (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-969&quot;&gt;DBZ-969&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1402&quot;&gt;DBZ-1402&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There was a number of bug fixes, too:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Graceful handling of an empty history topic for the MySQL connector (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1201&quot;&gt;DBZ-1201&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Correct column filtering for SQL Server connector (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1617&quot;&gt;DBZ-1617&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support for &lt;code&gt;ALTER TABLE …​ RENAME …​&lt;/code&gt; in MySQL (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1645&quot;&gt;DBZ-1645&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As were’re approaching the 1.0 Final release, we also took the time to check the configuration options of the different connectors for consistency.
Things were in pretty good shape already due to previous work towards unification in Debezium 0.10.
Only for the SQL Server and Oracle connectors, the snapshot mode &quot;initial_schema_only&quot; has been deprecated and will be removed in a future version. Please use &quot;schema_only&quot; instead, as known from the MySQL connector (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-585&quot;&gt;DBZ-585&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Overall, this release contains &lt;a href=&quot;https://debezium.io/releases/1.0/release-notes/#release-1.0.0-cr1&quot;&gt;24 changes&lt;/a&gt;.
As always, this release wouldn’t have been possible without the help from folks of the community:
&lt;a href=&quot;https://github.com/pan3793&quot;&gt;Cheng Pan&lt;/a&gt;,
&lt;a href=&quot;https://github.com/collinvandyck&quot;&gt;Collin Van Dyck&lt;/a&gt;,
&lt;a href=&quot;https://github.com/gnaazr95&quot;&gt;Gurnaaz Randhawa&lt;/a&gt;,
&lt;a href=&quot;https://github.com/grzegorz8&quot;&gt;Grzegorz Kołakowski&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ivansjg&quot;&gt;Ivan San Jose&lt;/a&gt;,
&lt;a href=&quot;https://github.com/theodesp&quot;&gt;Theofanis Despoudis&lt;/a&gt; and
&lt;a href=&quot;https://github.com/deblockt&quot;&gt;Thomas Deblock&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Thanks a lot to you! In total, not less than 144 people have contributed to the Debezium main code repository at this point.
Perhaps we can bump this number to 200 in 2020?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Barring any unforeseen issues, this candidate release should be the only one for Debezium 1.0,
and the final version should be in your hands before Christmas.
So please give the CR a try and let us know how it works for you!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/11/14/debezium-1-0-0-beta3-released/</id>
<title>Debezium 1.0.0.Beta3 Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-11-14T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/11/14/debezium-1-0-0-beta3-released/" rel="alternate" type="text/html" />
<author>
<name>Chris Cranford</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="cassandra"></category>
<summary>



While fall weather is in full swing, the Debezium community is not letting the unusually low, frigid temperatures get the best of us.  It is my pleasure to announce the release of Debezium 1.0.0.Beta3!


This new Debezium release includes several notable new features, enhancements, and fixes:




Built against Kafka Connect 2.3.1 (DBZ-1612)


Renamed drop_on_stop configuration parameter to drop.on.stop (DBZ-1595)


Standardized source information for Cassandra connector (DBZ-1408)


Propagate MongoDB replicator exceptions so they are visible from Kafka Connect&#8217;s status endpoint (DBZ-1583)


Envelope methods should accept Instant rather than long values for timestamps (DBZ-1607)


Erroneously reporting no tables captured (DBZ-1519)


Avoid Oracle connector attempting to analyze tables (DBZ-1569)


Toasted columns...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While fall weather is in full swing, the Debezium community is not letting the unusually low, frigid temperatures get the best of us.  It is my pleasure to announce the release of Debezium &lt;strong&gt;1.0.0.Beta3&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This new Debezium release includes several notable new features, enhancements, and fixes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Built against Kafka Connect 2.3.1 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1612&quot;&gt;DBZ-1612&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Renamed &lt;code&gt;drop_on_stop&lt;/code&gt; configuration parameter to &lt;code&gt;drop.on.stop&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1595&quot;&gt;DBZ-1595&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Standardized source information for Cassandra connector (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1408&quot;&gt;DBZ-1408&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Propagate MongoDB replicator exceptions so they are visible from Kafka Connect’s status endpoint (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1583&quot;&gt;DBZ-1583&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Envelope methods should accept &lt;code&gt;Instant&lt;/code&gt; rather than &lt;code&gt;long&lt;/code&gt; values for timestamps (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1607&quot;&gt;DBZ-1607&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Erroneously reporting no tables captured (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1519&quot;&gt;DBZ-1519&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Avoid Oracle connector attempting to analyze tables (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1569&quot;&gt;DBZ-1569&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Toasted columns should contain &lt;code&gt;null&lt;/code&gt; in &lt;em&gt;before&lt;/em&gt; rather than &lt;code&gt;__debezium_unavailable_value&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1570&quot;&gt;DBZ-1570&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support PostgreSQL 11+ &lt;code&gt;TRUNCATE&lt;/code&gt; operations using &lt;code&gt;pgoutput&lt;/code&gt; decoder (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1576&quot;&gt;DBZ-1576&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PostgreSQL connector times out in schema discovery for databases with many tables (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1579&quot;&gt;DBZ-1579&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Value of &lt;code&gt;ts_ms&lt;/code&gt; is not correct duing snapshot processing (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1588&quot;&gt;DBZ-1588&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Heartbeats are not generated for non-whitelisted tables (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1592&quot;&gt;DBZ-1592&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Additionally there were improvements to the Docker container images to reduce their overall size and some build infrastructure improvements to apply automatic code formatting rules.  Details about code formatting changes can be found in the &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/CONTRIBUTE.md#code-formatting&quot;&gt;CONTRIBUTE.md&lt;/a&gt; file.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In total, this release contains &lt;a href=&quot;https://debezium.io/releases/1.0/release-notes/#release-1.0.0-beta3&quot;&gt;27 changes&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Thanks to all the community members who helped make this happen:
&lt;a href=&quot;https://github.com/dvfeinblum&quot;&gt;David Feinblum&lt;/a&gt;,
&lt;a href=&quot;https://github.com/rk3rn3r&quot;&gt;René Kerner&lt;/a&gt;,
&lt;a href=&quot;https://github.com/lga-zurich&quot;&gt;Luis Garcés-Erice&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jfinzel&quot;&gt;Jeremy Finzel&lt;/a&gt;,
&lt;a href=&quot;https://github.com/datumgeek&quot;&gt;Mike Graham&lt;/a&gt;,
Yang Yang,
Addison Higham&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/10/24/debezium-1-0-0-beta2-released/</id>
<title>Debezium 1.0.0.Beta2 Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-10-24T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/10/24/debezium-1-0-0-beta2-released/" rel="alternate" type="text/html" />
<author>
<name>Chris Cranford</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="cassandra"></category>
<summary>



It is my pleasure to announce the release of Debezium 1.0.0.Beta2!


This new Debezium release includes several notable new features, enhancements, and fixes:




Support PostgreSQL LTREE columns with a logical data type (DBZ-1336)


Support for PostgreSQL 12 (DBZ-1542)


Validate configured PostgreSQL replication slot not contains no invalid characters (DBZ-1525)


Add MySQL DDL parser support for index creation VISIBLE and INVISIBLE keywords (DBZ-1534)


Add MySQL DDL parser support for granting SESSION_VARIABLES_ADMIN (DBZ-1535)


Fix MongoDB collection source struct field when collection name contains a dot (DBZ-1563)


Close idle transactions after performing a PostgreSQL snapshot (DBZ-1564)




Additionally the PostgreSQL connector was improved to warn users of a common situation where their configuration...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It is my pleasure to announce the release of Debezium &lt;strong&gt;1.0.0.Beta2&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This new Debezium release includes several notable new features, enhancements, and fixes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Support PostgreSQL &lt;code&gt;LTREE&lt;/code&gt; columns with a logical data type (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1336&quot;&gt;DBZ-1336&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support for PostgreSQL 12 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1542&quot;&gt;DBZ-1542&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Validate configured PostgreSQL replication slot not contains no invalid characters (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1525&quot;&gt;DBZ-1525&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add MySQL DDL parser support for index creation &lt;code&gt;VISIBLE&lt;/code&gt; and &lt;code&gt;INVISIBLE&lt;/code&gt; keywords (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1534&quot;&gt;DBZ-1534&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add MySQL DDL parser support for granting &lt;code&gt;SESSION_VARIABLES_ADMIN&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1535&quot;&gt;DBZ-1535&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fix MongoDB &lt;code&gt;collection&lt;/code&gt; source struct field when collection name contains a dot (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1563&quot;&gt;DBZ-1563&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Close idle transactions after performing a PostgreSQL snapshot (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1564&quot;&gt;DBZ-1564&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Additionally the PostgreSQL connector was improved to warn users of a common situation where their configuration does not enable heartbeats and the monitored table(s) change less frequent than tables that are not monitored.
This often lead to the write ahead logs (WAL) consuming additional disk space creating a WAL backlog as the connector only flushes LSN information to PostgreSQL if the log contains events for tables that are monitored.
Our hope is this will help automated tools identify this problem earlier while also giving hints on how to avoid it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In total, this release contains &lt;a href=&quot;https://debezium.io/releases/1.0/release-notes/#release-1.0.0-beta2&quot;&gt;13 fixes&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Thanks to all the community members who helped make this happen:
&lt;a href=&quot;https://github.com/grantcooksey&quot;&gt;Grant Cooksey&lt;/a&gt;,
&lt;a href=&quot;https://github.com/mincong-h&quot;&gt;Mingcong Huang&lt;/a&gt;,
&lt;a href=&quot;https://github.com/navdeep710&quot;&gt;navdeep710&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/10/22/audit-logs-with-kogito/</id>
<title>Admin Service for Audit Logs with Kogito</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-10-22T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/10/22/audit-logs-with-kogito/" rel="alternate" type="text/html" />
<author>
<name>Maciej Swiderski</name>
</author>
<category term="discussion"></category>
<category term="examples"></category>
<category term="apache-kafka"></category>
<category term="kafka-streams"></category>
<category term="kogito"></category>
<category term="featured"></category>
<summary>



As a follow up to the recent Building Audit Logs with Change Data Capture and Stream Processing blog post,
we’d like to extend the example with admin features to make it possible to capture and fix any missing transactional data.


In the above mentioned blog post, there is a log enricher service used to combine data inserted or updated in the Vegetable database table with transaction context data such as




Transaction id


User name who performed the work


Use case that was behind the actual change e.g. "CREATE VEGETABLE"




This all works well as long as all the changes are done via the vegetable service. But...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As a follow up to the recent &lt;a href=&quot;https://debezium.io/blog/2019/10/01/audit-logs-with-change-data-capture-and-stream-processing/&quot;&gt;Building Audit Logs with Change Data Capture and Stream Processing&lt;/a&gt; blog post,
we’d like to extend the example with admin features to make it possible to capture and fix any missing transactional data.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the above mentioned blog post, there is a log enricher service used to combine data inserted or updated in the Vegetable database table with transaction context data such as&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Transaction id&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;User name who performed the work&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use case that was behind the actual change e.g. &quot;CREATE VEGETABLE&quot;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This all works well as long as all the changes are done via the vegetable service. But is this always the case?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;What about maintenance activities or migration scripts executed directly on the database level?
There are still a lot of such activities going on, either on purpose or because that is our old habits we are trying to change…&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;maintenance_on_database_level&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#maintenance_on_database_level&quot; /&gt;Maintenance on database level&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So let’s assume there is a need to do some maintenance on the inventory database that will essentially make changes to the data stored in the vegetable table. To make it simple, let’s just add a new entry into the vegetable table:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;insert into inventory.vegetable (id, name, description) values (106, ‘cucumber, &#39;excellent&#39;);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once that is added you will see that the log enricher service is starting to print out quite a few log messages… and it does it constantly.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-plain&quot; data-lang=&quot;plain&quot;&gt;log-enricher_1        | 2019-10-11 10:30:46,099 INFO  [io.deb.dem.aud.enr.ChangeEventEnricher] (auditlog-enricher-c9e5d1bb-d953-42b4-8dc6-bbc328f5344f-StreamThread-1) Processing buffered change event for key {&quot;id&quot;:106}
log-enricher_1        | 2019-10-11 10:30:46,106 WARN  [io.deb.dem.aud.enr.ChangeEventEnricher] (auditlog-enricher-c9e5d1bb-d953-42b4-8dc6-bbc328f5344f-StreamThread-1) No metadata found for transaction {&quot;transaction_id&quot;:611}
log-enricher_1        | 2019-10-11 10:30:46,411 INFO  [io.deb.dem.aud.enr.ChangeEventEnricher] (auditlog-enricher-c9e5d1bb-d953-42b4-8dc6-bbc328f5344f-StreamThread-1) Processing buffered change event for key {&quot;id&quot;:106}
log-enricher_1        | 2019-10-11 10:30:46,415 WARN  [io.deb.dem.aud.enr.ChangeEventEnricher] (auditlog-enricher-c9e5d1bb-d953-42b4-8dc6-bbc328f5344f-StreamThread-1) No metadata found for transaction {&quot;transaction_id&quot;:611}
log-enricher_1        | 2019-10-11 10:30:46,921 INFO  [io.deb.dem.aud.enr.ChangeEventEnricher] (auditlog-enricher-c9e5d1bb-d953-42b4-8dc6-bbc328f5344f-StreamThread-1) Processing buffered change event for key {&quot;id&quot;:106}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Looking at the logs you can identify that it actually refers to the entry we just inserted (id 106).
In addition to that, it refers to missing transaction context data that it cannot find. That is the
consequence of doing it manually on database level instead of going through the  vegetable service.
There is no corresponding data in the &lt;code&gt;dbserver1.inventory.transaction_context_data&lt;/code&gt; Kafka topic and thus the log enricher cannot
correlate and by that merge/enrich them.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;kogito_to_the_rescue&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#kogito_to_the_rescue&quot; /&gt;Kogito to the rescue&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There would be a really good feature (or a neat feature as Gunnar said) if we could have some sort of admin service that
 could help in resolving this kind of problems. Mainly because if such entry is added it will block the entire
 enrichment activity as the first missing message will hold off all others.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And here comes &lt;a href=&quot;https://kogito.kie.org&quot;&gt;Kogito&lt;/a&gt; - a cloud native business automation toolkit to build intelligent
business applications based on battle tested capabilities. In other words, it brings business processes and rules
to solve particular business problems. In this case the business problem is blocked log enrichment which can lead to
some lost opportunities (of various types).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;What Kogito helps us with is to define our logic to understand what might get wrong, what needs to be done to resolve
it and what are the conditions that can lead to both problem and resolution.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In this particular case we use both processes and rules to make sure we get the context right and react to the events
behind the vegetable service. To be able to spot the erroneous situations we need to monitor two topics:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;dbserver1.inventory.vegetable&lt;/code&gt; - vegetable data change events&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;dbserver1.inventory.transaction_context_data&lt;/code&gt; - events from vegetable service with additional context data&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So for that we define two business processes where each will be started based on incoming messages - from individual
Kafka topics:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/auditing_kogito-process1.png&quot; class=&quot;responsive-image&quot; alt=&quot;Vegetable events process definition&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/auditing_kogito-process2.png&quot; class=&quot;responsive-image&quot; alt=&quot;Transaction context data process definition&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As illustrated above, both processes are initiated based on an incoming message. Then the logic afterwards is significantly different.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &quot;Transaction context data&quot; process is responsible for just retrieving the event and pushing it into processing phase - that
 essentially means to insert it into the so called &quot;working memory&quot; that is used for rule evaluation. And at that moment it’s done.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &quot;Vegetable event&quot; process starts in a similar way… it retrieves the message and then (first ignore snapshot messages
in the same way as the log enricher service) will wait for a predefined amount of time (2 seconds) before matching
vegetable and transaction context events. Once there is a match it will simple finish its execution. But if there is
no match found it will create a user task (that’s a task that requires human actors to provide data before process
can move forward).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is done via admin user interface (&lt;a href=&quot;http://localhost:8085/&quot;&gt;http://localhost:8085/&lt;/a&gt;) that allows to easily spot such instance and
work on them to fix missing data.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/auditing_kogito-ui.png&quot; class=&quot;responsive-image&quot; alt=&quot;Admin service UI for fixing missing transaction context data&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once the &lt;code&gt;Use case&lt;/code&gt; and &lt;code&gt;User name&lt;/code&gt; attributes are provided, the process will create a new transaction context event,
push it to the Kafka topic and complete itself.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;After the missing transaction context data event has been put on the topic the log enricher will resume its operation
and you will be able to see the following lines in the log:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-plain&quot; data-lang=&quot;plain&quot;&gt;log-enricher_1        | 2019-10-11 10:31:00,385 INFO  [io.deb.dem.aud.enr.ChangeEventEnricher] (auditlog-enricher-c9e5d1bb-d953-42b4-8dc6-bbc328f5344f-StreamThread-1) Processing buffered change event for key {&quot;id&quot;:106}
log-enricher_1        | 2019-10-11 10:31:00,389 INFO  [io.deb.dem.aud.enr.ChangeEventEnricher] (auditlog-enricher-c9e5d1bb-d953-42b4-8dc6-bbc328f5344f-StreamThread-1) Enriched change event for key {&quot;id&quot;:106}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With this you can easily administrate the audit logs to make sure any erroneous situations are resolved quickly
to not affect any other activities.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And if you would like to see everything in action, just watch this video:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;responsive-video&quot;&gt;
&lt;iframe width=&quot;1600&quot; height=&quot;900&quot; src=&quot;https://www.youtube.com/embed/BNcFaE0AVow&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; /&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Or try it yourself by running the &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/auditlog&quot;&gt;audit log example&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/10/17/debezium-newsletter-02-2019/</id>
<title>Debezium&#8217;s Newsletter 02/2019</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-10-17T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/10/17/debezium-newsletter-02-2019/" rel="alternate" type="text/html" />
<author>
<name>Chris Cranford</name>
</author>
<category term="community"></category>
<category term="news"></category>
<category term="newsletter"></category>
<summary>



Welcome to the Debezium community newsletter in which we share all things CDC related including blog posts, group discussions, as well as StackOverflow
questions that are relevant to our user community.




Upcoming Events




JokerConf - Practical change data streaming uses cases with Apache Kafka and Debezium


QCon San Fransisco - Practical change data streaming use cases with Apache Kafka and Debezium






Articles


There have been quite a number of blog posts about Debezium lately; here are some of the latest ones that you should not miss:


This recent blog by Gunnar Morling discusses how Debezium combined with Kafka stream post processing can deliver an enriched stream of...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Welcome to the Debezium community newsletter in which we share all things CDC related including blog posts, group discussions, as well as StackOverflow
questions that are relevant to our user community.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;upcoming_events&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#upcoming_events&quot; /&gt;Upcoming Events&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://jokerconf.com/en/2019/talks/6nkqqv8zj4gmujxmo64ov3/&quot;&gt;JokerConf - Practical change data streaming uses cases with Apache Kafka and Debezium&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://qconsf.com/sf2019/presentation/practical-change-data-streaming-use-cases-apache-kafka-debezium&quot;&gt;QCon San Fransisco - Practical change data streaming use cases with Apache Kafka and Debezium&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;articles&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#articles&quot; /&gt;Articles&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There have been quite a number of blog posts about Debezium lately; here are some of the latest ones that you should not miss:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This &lt;a href=&quot;https://debezium.io/blog/2019/10/01/audit-logs-with-change-data-capture-and-stream-processing/&quot;&gt;recent blog&lt;/a&gt; by Gunnar Morling discusses how Debezium combined with Kafka stream post processing can deliver an enriched stream of events suitable for even the most complicated of tasks like audit tracking.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Rod Shokrian from Varo &lt;a href=&quot;https://medium.com/engineering-varo/event-driven-architecture-and-the-outbox-pattern-569e6fba7216&quot;&gt;recently blogged&lt;/a&gt; about their CDC solution and experiences using Debezium in conjunction with the Outbox Pattern.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Community involvement is critical to each Open Source project and Debezium is no different.  Joy Gao &lt;a href=&quot;https://wecode.wepay.com/posts/streaming-cassandra-at-wepay-part-1&quot;&gt;blogs&lt;/a&gt; about her experience at WePay where CDC innovation brought Cassandra and Debezium together.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This &lt;a href=&quot;https://speakerdeck.com/jbfletch/using-kafka-to-discover-events-hidden-in-your-database&quot;&gt;slide deck&lt;/a&gt; by Anna McDonald showcases Debezium capturing database change events in complex architectures to emit enriched, derivative-based events across your enterprise.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There are quite a number of applicable use cases for Debezium.
Dave Cramer from Crunchy Data recently &lt;a href=&quot;https://info.crunchydata.com/blog/postgresql-change-data-capture-with-debezium&quot;&gt;blogged&lt;/a&gt; about his experiences using Debezium to replicate data between both a source and sink PostgreSQL environment using CDC and Apache Kafka.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Data analytics are vital across lots of industries.
This &lt;a href=&quot;https://medium.com/convoy-tech/logs-offsets-near-real-time-elt-with-apache-kafka-snowflake-473da1e4d776&quot;&gt;post&lt;/a&gt; by Adrian Kreuziger discusses how Convoy used Debezium and Apache Kafka to design a low-latency data warehouse solution for the trucking industry.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can also checkout our &lt;a href=&quot;https://debezium.io/documentation/online-resources&quot;&gt;online resources&lt;/a&gt; for more…​&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;examples&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#examples&quot; /&gt;Examples&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;An example is often much like a picture, its worth a thousand words.
Debezium’s &lt;a href=&quot;https://github.com/debezium/debezium-examples&quot;&gt;examples repository&lt;/a&gt; has recently undergone changes introducing new examples and update existing ones:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[NEW] &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/auditlog&quot;&gt;Build Audit Logs using Debezium and Kafka Streams&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[NEW] &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/postgres-toast&quot;&gt;Dealing with PostgreSQL TOAST columns&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[UPDATE] &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/end-to-end-demo&quot;&gt;End to End demo now supports propagating delete events&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[UPDATE] &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/unwrap-smt&quot;&gt;Unwrap SMT example now propagates delete events&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[UPDATE] &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/unwrap-mongodb-smt&quot;&gt;MongoDB Unwrap SMT example now propagates delete events&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;time_to_upgrade&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#time_to_upgrade&quot; /&gt;Time to upgrade&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium version &lt;a href=&quot;https://debezium.io/blog/2019/10/02/debezium-0-10-0-final-released/&quot;&gt;0.10.0.Final&lt;/a&gt; was released at the beginning of October.
If you are using the 0.9 branch, we urge you to check out the latest major release.
For details on the bug fixes, enhancements, and improvements that spanned 8 preview releases, check out the &lt;a href=&quot;https://debezium.io/releases/0.10/release-notes/&quot;&gt;release-notes&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium team has also begun active development on the next major version, 1.0.
If you want details on the bug fixes, enhancements, and improvements, you can view &lt;a href=&quot;https://debezium.io/releases/1.0/release-notes&quot;&gt;release-notes&lt;/a&gt; for more details.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;questions_and_answers&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#questions_and_answers&quot; /&gt;Questions and answers&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/58337376/debezium-postgres-error-parameter-include-unchanged-toast-was-deprecated&quot;&gt;Debezium postgres ERROR: Parameter &quot;include-unchanged-toast&quot; was deprecated&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/57095096/unable-to-register-debezium-kafka-connect-connector-in-ssl-enabled-kafka-clust&quot;&gt;Unable to register Debezium (Kafka-Connect) connector in SSL enabled Kafka cluster&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://groups.google.com/d/msg/debezium/1vKTWwcf71I/L98pV5nnBgAJ&quot;&gt;Debezium or database triggers&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;using_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#using_debezium&quot; /&gt;Using Debezium?&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We recently added a &lt;a href=&quot;https://www.debezium.io/community/users&quot;&gt;community users&lt;/a&gt; page to &lt;a href=&quot;https://www.debezium.io&quot; class=&quot;bare&quot;&gt;https://www.debezium.io&lt;/a&gt;.
If you are a user of Debezium and would like to be included, please send us a GitHub pull request or reach out to us directly.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;getting_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#getting_involved&quot; /&gt;Getting involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It can often be overwhelming when starting work on an existing code base.
We welcome community contributions and we want to make the process of getting started extremely easy.
Below is a list of open issues that are currently labeled with &lt;code&gt;easy-starter&lt;/code&gt; if you want to dive in quick.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Configure Avro serialization automatically when detecting link to schema registry (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-59&quot;&gt;DBZ-59&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add tests for using fallback values with default REPLICA IDENTITY (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1158&quot;&gt;DBZ-1158&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add ability to insert fields from op field in ExtractNewRecordState SMT (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1452&quot;&gt;DBZ-1452&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support CREATE TABLE …​ LIKE syntax for blacklisted source table (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1496&quot;&gt;DBZ-1496&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provide change event JSON Serde for Kafka Streams (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1533&quot;&gt;DBZ-1533&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Explore SMT for Externalizing large column values (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1541&quot;&gt;DBZ-1541&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Whitespaces not stripped from table.whitelist (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1546&quot;&gt;DBZ-1546&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;opportunities&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#opportunities&quot; /&gt;Opportunities&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We recently &lt;a href=&quot;https://twitter.com/debezium/status/1184514850627739649&quot;&gt;tweeted&lt;/a&gt; about an Internship opening on the Debezium project.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Are you in the Czech Republic area?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Are you passionate about Open Source?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do you think change data capture is interesting?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you answered yes to any or all of these, then you should definitely check out &lt;a href=&quot;https://global-redhat.icims.com/jobs/73814/software-developer-internship---debezium-project/job?hub=7&amp;amp;mobile=false&amp;amp;width=1470&amp;amp;height=500&amp;amp;bga=true&amp;amp;needsRedirect=false&amp;amp;jan1offset=60&amp;amp;jun1offset=120&quot;&gt;the details&lt;/a&gt; and apply!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;feedback&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#feedback&quot; /&gt;Feedback&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We intend to publish new additions of this newsletter periodically.
Should anyone have any suggestions on changes or what could be highlighted here, we welcome that feedback.
You can reach out to us via any of our community channels found &lt;a href=&quot;https://debezium.io/community&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/10/17/debezium-1-0-0-beta1-released/</id>
<title>Debezium 1.0.0.Beta1 Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-10-17T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/10/17/debezium-1-0-0-beta1-released/" rel="alternate" type="text/html" />
<author>
<name>Chris Cranford</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="cassandra"></category>
<summary>



History is in the making as Debezium begins to sprint to its 1.0 milestone.
It&#8217;s my pleasure to announce the release of Debezium 1.0.0.Beta1!


This new Debezium release includes several notable new features, enhancements, and fixes:




ExtractNewDocumentState and EventRouter SMTs propagate heartbeat &amp; schema change messages (DBZ-1513)


Provides alternative mapping for INTERVAL columns via interval.handling.mode (DBZ-1498)


Ensure message keys have the right column order (DBZ-1507)


Warn of table locking problems in connector logs (DBZ-1280)




Additionally, several PostgreSQL issues were fixed to improve snapshotting large databases environments (DBZ-685) and specific circumstances where write ahead logs (WAL) would continue to consume diskspace (DBZ-892).


In total, this release contains 18 fixes.


Thanks...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;History is in the making as Debezium begins to sprint to its 1.0 milestone.
It’s my pleasure to announce the release of Debezium &lt;strong&gt;1.0.0.Beta1&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This new Debezium release includes several notable new features, enhancements, and fixes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ExtractNewDocumentState and EventRouter SMTs propagate heartbeat &amp;amp; schema change messages (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1513&quot;&gt;DBZ-1513&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provides alternative mapping for &lt;code&gt;INTERVAL&lt;/code&gt; columns via &lt;code&gt;interval.handling.mode&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1498&quot;&gt;DBZ-1498&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ensure message keys have the right column order (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1507&quot;&gt;DBZ-1507&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Warn of table locking problems in connector logs (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1280&quot;&gt;DBZ-1280&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Additionally, several PostgreSQL issues were fixed to improve snapshotting large databases environments (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-685&quot;&gt;DBZ-685&lt;/a&gt;) and specific circumstances where write ahead logs (WAL) would continue to consume diskspace (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-892&quot;&gt;DBZ-892&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In total, this release contains &lt;a href=&quot;https://debezium.io/releases/1.0/release-notes/#release-1.0.0-beta1&quot;&gt;18 fixes&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Thanks to all the community members who helped make this happen:
&lt;a href=&quot;https://github.com/pushpavanthar&quot;&gt;Purushotham Pushpavanthar&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jfinsel&quot;&gt;Jeremy Finzel&lt;/a&gt;,
&lt;a href=&quot;https://github.com/grantcooksey&quot;&gt;Grant Cooksey&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/10/08/handling-unchanged-postgres-toast-values/</id>
<title>Strategies for Handling Unchanged Postgres TOAST Values</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-10-08T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/10/08/handling-unchanged-postgres-toast-values/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="discussion"></category>
<category term="examples"></category>
<category term="postgres"></category>
<category term="kafka-streams"></category>
<summary>



Let&#8217;s talk about TOAST.
Toast?
No, TOAST!


So what&#8217;s that?
TOAST (The Oversized-Attribute Storage Technique) is a mechanism in Postgres which stores large column values in multiple physical rows, circumventing the page size limit of 8 KB.


    


Typically, TOAST storage is transparent to the user, so you don&#8217;t really have to care about it.
There&#8217;s an exception, though:
if a table row has changed, any unchanged values that were stored using the TOAST mechanism are not included in the message that Debezium receives from the database,
unless they are part of the table’s replica identity.
Consequently, such unchanged TOAST column value will not be contained...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s talk about TOAST.
Toast?
No, TOAST!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So what’s that?
&lt;a href=&quot;https://www.postgresql.org/docs/current/storage-toast.html&quot;&gt;TOAST&lt;/a&gt; (The Oversized-Attribute Storage Technique) is a mechanism in Postgres which stores large column values in multiple physical rows, circumventing the page size limit of 8 KB.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/postgres_toast.jpg&quot; style=&quot;width:40%;&quot; class=&quot;responsive-image&quot; alt=&quot;TOAST!&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Typically, TOAST storage is transparent to the user, so you don’t really have to care about it.
There’s an exception, though:
if a table row has changed, any &lt;em&gt;unchanged&lt;/em&gt; values that were stored using the TOAST mechanism are not included in the message that Debezium receives from the database,
unless they are part of the table’s &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/connectors/postgresql.html#replica-identity&quot;&gt;replica identity&lt;/a&gt;.
Consequently, such unchanged TOAST column value will not be contained in Debezium data change events sent to Apache Kafka.
In this post we’re going to discuss different strategies for dealing with this situation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;!-- more --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When encountering an unchanged TOAST column value in the logical replication message received from the database,
the Debezium Postgres connector will represent that value with a configurable placeholder.
By default, that’s the literal &lt;code&gt;__debezium_unavailable_value&lt;/code&gt;,
but that value can be overridden using the &lt;code&gt;toasted.value.placeholder&lt;/code&gt; connector property.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s consider the following Postgres table definition as an example:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;CREATE TABLE customers (
  id SERIAL NOT NULL PRIMARY KEY,
  first_name VARCHAR(255) NOT NULL,
  last_name VARCHAR(255) NOT NULL,
  email VARCHAR(255) NOT NULL UNIQUE,
  biography TEXT
);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here, the &lt;code&gt;biography TEXT&lt;/code&gt; column is a TOAST-able column, as its value may exceed the page size limit.
So when issuing an update such as &lt;code&gt;update inventory.customers set first_name = &#39;Dana&#39; where id = 1004;&lt;/code&gt;,
you might receive a data change event in Apache Kafka which looks like this
(assuming the table has the default replica identity):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
  &quot;before&quot;: null,
  &quot;after&quot;: {
    &quot;id&quot;: 1004,
    &quot;first_name&quot;: &quot;Dana&quot;,
    &quot;last_name&quot;: &quot;Kretchmar&quot;,
    &quot;email&quot;: &quot;annek@noanswer.org&quot;,
    &quot;biography&quot;: &quot;__debezium_unavailable_value&quot;
  },
  &quot;source&quot;: {
    &quot;version&quot;: &quot;0.10.0.Final&quot;,
    &quot;connector&quot;: &quot;postgresql&quot;,
    &quot;name&quot;: &quot;dbserver1&quot;,
    &quot;ts_ms&quot;: 1570448151151,
    &quot;snapshot&quot;: &quot;false&quot;,
    &quot;db&quot;: &quot;sourcedb&quot;,
    &quot;schema&quot;: &quot;inventory&quot;,
    &quot;table&quot;: &quot;customers&quot;,
    &quot;txId&quot;: 627,
    &quot;lsn&quot;: 34650016,
    &quot;xmin&quot;: null
  },
  &quot;op&quot;: &quot;u&quot;,
  &quot;ts_ms&quot;: 1570448151611
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note how the &lt;code&gt;biography&lt;/code&gt; field (whose value hasn’t changed with the &lt;code&gt;UPDATE&lt;/code&gt;) has the special &lt;code&gt;__debezium_unavailable_value&lt;/code&gt; marker value.
Now, if change event consumers receive that placeholder value,
the question arises how they should react to this.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One way, and &lt;strong&gt;certainly the easiest&lt;/strong&gt; from a consumer’s perspective, is to avoid the situation in the first place.
This can be achieved by using a &quot;replica identity&quot; of &lt;code&gt;FULL&lt;/code&gt; for the Postgres table in question.
Alternatively, the replica identity can be based on an index which comprises the TOAST-able column.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;excluding_unchanged_values&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#excluding_unchanged_values&quot; /&gt;Excluding Unchanged Values&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If changing the source table’s replica identity is not an option,
one approach for consumers that update a sink datastore (e.g. a database, cache or search index) is to ignore any field of a change event which has the placeholder value.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This means that any column with the placeholder value must be omitted from the update statement executed on the sink datastore.
E.g. in terms of a SQL database, an specific &lt;code&gt;UPDATE&lt;/code&gt; statement must be built and executed which doesn’t contain the column(s) with the placeholder value.
Users of Hibernate ORM may feel reminded of the &quot;dynamic updates&quot; feature which works similar.
Some datastores and connectors might only support full updates, though, in which case this strategy isn’t viable.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;triggers&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#triggers&quot; /&gt;Triggers&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One interesting variation of the &quot;ignore&quot; approach is the usage of triggers in the sink database:
registered for the column that may receive the marker value, they can &quot;veto&quot; such change and just keep the previously stored value instead.
The following shows an example of such a trigger in Postgres:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;CREATE OR REPLACE FUNCTION ignore_unchanged_biography()
  RETURNS TRIGGER AS
$BODY$
BEGIN
  IF NEW.&quot;biography&quot; = &#39;__debezium_unavailable_value&#39;
  THEN
    NEW.&quot;biography&quot; = OLD.&quot;biography&quot;;
  END IF;

  RETURN NEW;
END;
$BODY$ LANGUAGE PLPGSQL;

CREATE TRIGGER customer_biography_trigger
BEFORE UPDATE OF &quot;biography&quot;
  ON customers
FOR EACH ROW
EXECUTE PROCEDURE ignore_unchanged_biography();&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This will keep the old value for the &lt;code&gt;biography&lt;/code&gt; column if it were to be set to the &lt;code&gt;__debezium_unavailable_value&lt;/code&gt; marker value.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;stateful_stream_processing&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#stateful_stream_processing&quot; /&gt;Stateful Stream Processing&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;An alternative approach to dealing with unchanged TOAST column values is a stateful stream processing application.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This application can persist the latest value of a TOAST column
(as obtained from a snapshot, an insert event or an update including the TOAST-able column) in a state store and
put the value back into change events with the marker value.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium makes sure that all change events for one particular record always go into the same partition,
so they they will be processed in the exact same order as they were created.
This ensures that the latest value is available in the statestore when receiving a change event with the marker value.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://kafka.apache.org/documentation/streams/&quot;&gt;Kafka Streams&lt;/a&gt; with its state store API comes in very handy for building such a service.
Based on &lt;a href=&quot;https://quarkus.io/&quot;&gt;Quarkus&lt;/a&gt; and its extension for building &lt;a href=&quot;https://quarkus.io/guides/kafka-streams-guide&quot;&gt;Kafka Streams applications&lt;/a&gt; running either on the JVM or natively via GraalVM,
a solution could look like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class TopologyProducer {

    private static final Logger LOG = LoggerFactory.getLogger(TopologyProducer.class);

    static final String BIOGRAPHY_STORE = &quot;biography-store&quot;;

    @ConfigProperty(name = &quot;pgtoast.customers.topic&quot;)
    String customersTopic;

    @ConfigProperty(name = &quot;pgtoast.customers.enriched.topic&quot;)
    String customersEnrichedTopic;

    @Produces
    public Topology buildTopology() {
        StreamsBuilder builder = new StreamsBuilder();

        StoreBuilder&amp;lt;KeyValueStore&amp;lt;JsonObject, String&amp;gt;&amp;gt; biographyStore = &lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;(1)&lt;/b&gt;
                Stores.keyValueStoreBuilder(
                    Stores.persistentKeyValueStore(BIOGRAPHY_STORE),
                    new JsonObjectSerde(),
                    new Serdes.StringSerde()
                );
        builder.addStateStore(biographyStore);

        builder.&amp;lt;JsonObject, JsonObject&amp;gt;stream(customersTopic) &lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;(2)&lt;/b&gt;
                .transformValues(ToastColumnValueProvider::new, BIOGRAPHY_STORE)
                .to(customersEnrichedTopic);

        return builder.build();
    }

    class ToastColumnValueProvider implements
            ValueTransformerWithKey&amp;lt;JsonObject, JsonObject, JsonObject&amp;gt; {

        private KeyValueStore&amp;lt;JsonObject, String&amp;gt; biographyStore;

        @Override
        @SuppressWarnings(&quot;unchecked&quot;)
        public void init(ProcessorContext context) {
            biographyStore = (KeyValueStore&amp;lt;JsonObject, String&amp;gt;) context.getStateStore(
                TopologyProducer.BIOGRAPHY_STORE);
        }

        @Override
        public JsonObject transform(JsonObject key, JsonObject value) {
            JsonObject payload = value.getJsonObject(&quot;payload&quot;);
            JsonObject newRowState = payload.getJsonObject(&quot;after&quot;);
            String biography = newRowState.getString(&quot;biography&quot;);

            if (isUnavailableValueMarker(biography)) { &lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;(3)&lt;/b&gt;
                String currentValue = biographyStore.get(key); &lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;b&gt;(4)&lt;/b&gt;

                if (currentValue == null) {
                    LOG.warn(&quot;No biography value found for key &#39;{}&#39;&quot;, key);
                }
                else {
                    value = Json.createObjectBuilder(value) &lt;i class=&quot;conum&quot; data-value=&quot;5&quot; /&gt;&lt;b&gt;(5)&lt;/b&gt;
                        .add(
                            &quot;payload&quot;,
                            Json.createObjectBuilder(payload)
                                .add(
                                    &quot;after&quot;,
                                    Json.createObjectBuilder(newRowState).add(
                                        &quot;biography&quot;,
                                        currentValue
                                    )
                                )
                        )
                        .build();
                }
            }
            else { &lt;i class=&quot;conum&quot; data-value=&quot;6&quot; /&gt;&lt;b&gt;(6)&lt;/b&gt;
                biographyStore.put(key, biography);
            }

            return value;
        }

        private boolean isUnavailableValueMarker(String value) {
            return &quot;__debezium_unavailable_value&quot;.contentEquals(value);
        }

        @Override
        public void close() {
        }
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;colist arabic&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Set up a state store for storing the latest &lt;code&gt;biography&lt;/code&gt; value per customer id&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The actual streaming pipeline: for each message on the customers topic, apply the logic for replacing the TOAST column marker value and write the transformed message to an output topic&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Check whether the &lt;code&gt;biography&lt;/code&gt; value from the incoming message is the marker&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;If so, get the current &lt;code&gt;biography&lt;/code&gt; value for the customer from the state store&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;5&quot; /&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Replace the marker value with the actual value obtained from the state store&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;6&quot; /&gt;&lt;b&gt;6&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;If the incoming message has an actual &lt;code&gt;biography&lt;/code&gt; value, put this to the state store&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now, if a consumer subscribes to the &quot;enriched&quot; topic,
it will see any customer change events with the actual value of any unchanged TOAST columns,
as materialized from the state store.
The fact that the Debezium connector originally emitted the special marker value,
is fully transparent at that point.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Primary Key Changes&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When a record’s primary key gets updated,
Debezium will create two change events: one &quot;delete&quot; event using the old key and one &quot;insert&quot; event with the new key.
When processing the second event, the stream processing application will not be able to look up the &lt;code&gt;biography&lt;/code&gt; value stored earlier on, as it has been under the old key.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One way to address this would be to expose the original key value e.g. as a message header of the insert event.
This requirement is tracked as &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1531&quot;&gt;DBZ-1531&lt;/a&gt;;
let us know if you’d like to contribute and implement this feature.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;when_to_use_what&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#when_to_use_what&quot; /&gt;When to Use What?&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’ve discussed different options for dealing with unchanged TOAST column values in Debezium’s data change events.
Which one should be used in which case then?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Changing the replica identity to &lt;code&gt;FULL&lt;/code&gt; is the easiest approach by far:
a single configuration to the source table avoids the problem to begin with.
It’s not the most efficient solution, though, and some DBAs might be reluctant to apply this setting.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When using the change events to update some kind of sink data store,
it may sound attractive at first to simply omit any field with the special marker value when issuing an update.
But this technique has some downsides: not all data stores and the corresponding connectors might support partial updates.
Instead there might only be the option to do full updates to a record in the sink data store based on the incoming data.
Even when that option exists, it might be sub-optimal.
E.g. for a SQL database, a statement just with the available values may be executed.
This is at odds with efficient usage of prepared statements and batching, though:
as the &quot;shape&quot; of the data may change between two updates to the same table,
the same prepared statement cannot be re-used and performance may suffer.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The trigger-based approach isn’t prone to these problems:
any updates to a table will have the same number of columns, so the consumer (e.g. a sink connector) may re-use the same prepared statement and batch multiple records into a single execution.
One thing to be aware of is the organizational cost associated with this approach:
triggers must be installed for each affected column and be kept in sync when table structures change.
This must be done individually in each sink datastore, and not all stores have may have support for triggers to begin with.
But where possible, triggers can be a great solution.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Finally, stream processing makes the usage of TOAST-able columns and the absence of their values in update events fully transparent to consumers.
The enrichment logic is implemented in a single place, from which all the consumers of the change event stream benefit,
without the need for individual solutions in each one of them.
Also, it’s the only viable solution if consumers themselves are stateless and don’t have any way to materialize the last value of such column, e.g. when streaming change events to a browser via web sockets or GraphQL subscriptions.
The price to pay is the overhead of maintaining and operating a separate service.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;On a side note, such stream processing application might also be provided as a configurable, ready-to-use component coming as a part of the Debezium platform.
This might be useful not only for Postgres, but also when thinking about other Debezium connectors.
For instance, in case of Cassandra, change events will only ever contain the updated fields;
a similar mode could be envisioned for MySQL by supporting its &quot;non full&quot; binlog mode.
In both cases, a stateful stream processing service could be used to hydrate full data change events based on earlier row state retrieved from a local state store and an incoming &quot;patch&quot; style change event.
If you think that’d be a useful addition to Debezium, please let us know.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As always, there are no silver bullets:
you should choose a solution based on your specific situation and requirements.
As a starting point you can find a basic implementation of the trigger and Kafka Streams approaches in the Debezium &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/postgres-toast&quot;&gt;examples repository&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Which approach would you prefer?
Or perhaps you have even further alternatives in mind?
Tell us about it in the comments below.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;em&gt;Many thanks to &lt;a href=&quot;https://twitter.com/dave_cramer/&quot;&gt;Dave Cramer&lt;/a&gt; and Jiri Pechanec for their feedback while working on this post and the accompanying example code!&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/10/02/debezium-0-10-0-final-released/</id>
<title>Debezium 0.10 Final Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-10-02T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/10/02/debezium-0-10-0-final-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="cassandra"></category>
<summary>



On behalf of the Debezium community it&#8217;s my great pleasure to announce the release of Debezium 0.10.0.Final!


As you&#8217;d expect it, there were not many changes since last week&#8217;s CR2,
one exception being a performance fix for the pgoutput plug-in of the Postgres connector,
which may have suffered from slow processing when dealing with many small transactions in a short period of time
(DBZ-1515).


This release finalizes the work of overall eight preview releases.
We have discussed the new features and changes in depth in earlier announcements,
but here are some highlights of Debezium 0.10:







Incubating CDC support for Apache Cassandra via our first community-led connector


Support for the...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;On behalf of the Debezium community it’s my great pleasure to announce the release of Debezium &lt;strong&gt;0.10.0.Final&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As you’d expect it, there were not many changes since last week’s CR2,
one exception being a performance fix for the &lt;code&gt;pgoutput&lt;/code&gt; plug-in of the Postgres connector,
which may have suffered from slow processing when dealing with many small transactions in a short period of time
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1515&quot;&gt;DBZ-1515&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This release finalizes the work of overall &lt;a href=&quot;https://debezium.io/releases/0.10/&quot;&gt;eight preview releases&lt;/a&gt;.
We have discussed the new features and changes in depth in earlier announcements,
but here are some highlights of Debezium 0.10:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;!-- more --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Incubating &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/connectors/cassandra.html&quot;&gt;CDC support for Apache Cassandra&lt;/a&gt; via our first community-led connector&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support for the &lt;code&gt;pgoutput&lt;/code&gt; logical decoding plug-in of Postgres 10 and later; support for &quot;exported&quot; snapshots with &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/connectors/postgresql.html&quot;&gt;Postgres&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Extended and more unified metrics across the different connectors&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;More consistent and improved &lt;code&gt;source&lt;/code&gt; structure in change events&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Significantly less memory usage in the &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/connectors/sqlserver.html&quot;&gt;SQL Server connector&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Many improvements and bugfixes to the DDL parser of the &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/connectors/mysql.html&quot;&gt;MySQL connector&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lots of improvements to the SMTs for &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/configuration/outbox-event-router.html&quot;&gt;routing outbox events&lt;/a&gt; and &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/configuration/event-flattening.html&quot;&gt;extracting the &quot;after&quot; state&lt;/a&gt; from change events&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Customizable message keys&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reworked &lt;a href=&quot;https://debezium.io/blog/2019/09/05/website-documentation-overhaul/&quot;&gt;website and restructured documentation&lt;/a&gt;, organized by releases&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In addition, we also removed some previously deprecated options and did some clean-up of the message structures produced by the Debezium connectors, e.g. in regards to certain type mappings.
When upgrading from earlier releases,
please make sure to carefully study the &lt;a href=&quot;https://debezium.io/releases/0.10/release-notes/&quot;&gt;release notes&lt;/a&gt;, which discuss in detail any changed or removed functionality, as well as options which for instance allow to keep the original &lt;code&gt;source&lt;/code&gt; structure for some time.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Pleas refer to the orignal announcements for more details (&lt;a href=&quot;https://debezium.io/blog/2019/05/29/debezium-0-10-0-alpha1-released/&quot;&gt;Alpha1&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/06/03/debezium-0-10-0-alpha2-released/&quot;&gt;Alpha2&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/06/12/debezium-0-10-0-beta1-released/&quot;&gt;Beta1&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/06/28/debezium-0-10-0-beta2-released/&quot;&gt;Beta2&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/07/25/debezium-0-10-0-beta3-released/&quot;&gt;Beta3&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/08/20/debezium-0-10-0-beta4-released/&quot;&gt;Beta4&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/09/10/debezium-0-10-0-cr1-released/&quot;&gt;CR1&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/09/26/debezium-0-10-0-cr2-released/&quot;&gt;CR2&lt;/a&gt;).
Altogether, a whopping &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20in%20(0.10.0.Alpha1%2C%200.10.0.Alpha2%2C%200.10.0.Beta1%2C%200.10.0.Beta2%2C%200.10.0.Beta3%2C%200.10.0.Beta4%2C%200.10.0.CR1%2C%200.10.0.CR2%2C%200.10.0.Final)&quot;&gt;171 issues&lt;/a&gt; were resolved in Debezium 0.10.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Such effort would not nearly be possible without all the fantastic people in the Debezium community.
Until today, almost &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/COPYRIGHT.txt&quot;&gt;130 people&lt;/a&gt; have contributed to the main Debezium code repository,
plus some more to the incubator and container image repositories.
But submitting pull requests with code changes is not the only way to help,
we’re equally thankful for each bug report, feature request, suggestions in the chat rooms etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another great thing to observe is the growing number of blog posts, conference presentations and other material covering Debezium in one way or another.
We maintain a list of &lt;a href=&quot;https://debezium.io/documentation/online-resources/&quot;&gt;Debezium-related resources&lt;/a&gt; on the website;
if you know of other contents which should be linked there, please file a PR for adding it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I can’t begin to express how lucky we feel about all these amazing contributions,
no matter whether small or large!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;coming_next_debezium_1_0&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#coming_next_debezium_1_0&quot; /&gt;Coming Next: Debezium 1.0!&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With Debezium 0.10.0.Final being done, the question is: what’s next?
If you thought 0.11, then we got to disappoint you — we’re finally setting course towards Debezium 1.0!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With all the community feedback we got (in parts from huge deployments with hundreds of Debezium connectors), and with the clean-up changes done for 0.10, we feel that it’s finally about time for the 1.0 release and the even increased expectations towards it in regards to compatibility and stability.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We don’t expect many new functionality for 1.0 in comparison over 0.10
(with exception of the incubating connectors),
the focus will primarily be on further bug fixing, stability and usability improvements.
In the good old tradition of open source, we don’t specify any timeline other than &quot;it’s done, when it’s done&quot;.
But it should be safe to say that it will be done quicker than 0.10:
going forward, we’d like to increase the release cadence and publish new minor releases more often, for sure doing less than eight preview releases as in 0.10.
Any contributions, input on the roadmap and other feedback will be very welcomed of course.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Upwards and onwards!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/10/01/audit-logs-with-change-data-capture-and-stream-processing/</id>
<title>Building Audit Logs with Change Data Capture and Stream Processing</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-10-01T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/10/01/audit-logs-with-change-data-capture-and-stream-processing/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="discussion"></category>
<category term="examples"></category>
<category term="apache-kafka"></category>
<category term="kafka-streams"></category>
<category term="featured"></category>
<summary>





It is a common requirement for business applications to maintain some form of audit log,
i.e. a persistent trail of all the changes to the application&#8217;s data.
If you squint a bit, a Kafka topic with Debezium data change events is quite similar to that:
sourced from database transaction logs, it describes all the changes to the records of an application.
What&#8217;s missing though is some metadata: why, when and by whom was the data changed?
In this post we&#8217;re going to explore how that metadata can be provided and exposed via change data capture (CDC), and how stream processing can be used to enrich...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;openblock teaser&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It is a common requirement for business applications to maintain some form of audit log,
i.e. a persistent trail of all the changes to the application’s data.
If you squint a bit, a Kafka topic with Debezium data change events is quite similar to that:
sourced from database transaction logs, it describes all the changes to the records of an application.
What’s missing though is some metadata: why, when and by whom was the data changed?
In this post we’re going to explore how that metadata can be provided and exposed via change data capture (CDC), and how stream processing can be used to enrich the actual data change events with such metadata.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Reasons for maintaining data audit trails are manyfold:
e.g. regulatory requirements may mandate businesses to keep complete historic information of their customer, purchase order, invoice or other data.
Also for an enterprise’s own purposes it can be very useful to have insight into why and how certain data has changed, e.g. allowing to improve business processes or analyze errors.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One common approach for creating audit trails are application-side libraries.
Hooked into the chosen persistence library,
they’d maintain specific column(s) in the data tables (&quot;createdBy&quot;, &quot;lastUpdated&quot; etc.),
and/or copy earlier record versions into some form of history tables.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There are some disadvantages to this, though:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;writing records in history tables as part of OLTP transactions increases the number of executed statements within the transaction (for each update or delete, also an insert must be written into the corresponding history table) and thus may cause longer response times of the application&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;oftentimes no audit events can be provided in case of bulk updates and deletes (e.g. &lt;code&gt;DELETE from purchaseorders where status = &#39;SHIPPED&#39;&lt;/code&gt;),
as the listeners used to hook the library into the persistence framework are not aware of all the affected records&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;changes done directly in the database cannot be tracked, e.g. when running a data load, doing batch processing in a stored procedure or when bypassing the application during an emergency data patch&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another technique are database triggers.
They won’t miss any operations, no matter whether issued from the application or the database itself.
They’ll also be able to process each record affected by a bulk statement.
On the downside, there’s still is the problem of increased latency when executing triggers as part of OLTP transactions.
Also, a process must be in place for installing and updating the triggers for each table.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;audit_logs_based_on_change_data_capture&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#audit_logs_based_on_change_data_capture&quot; /&gt;Audit Logs Based on Change Data Capture&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The aforementioned problems don’t exist when leveraging the transaction log as the source for an audit trail and using change data capture for retrieving the change information and sending it into a message broker or log such as Apache Kafka.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Running asynchronously, the CDC process can extract the change data without impacting OLTP transactions.
The transaction logs contain one entry whenever there’s a data change,
be it issued from the application or directly executed in the database.
There’ll be a log entry for each record updated or deleted in a bulk operation,
so a change event for each of them can be produced.
Also there is no impact on the data model, i.e. no special columns or history tables must be created.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But how can CDC access the metadata we’d discussed initially?
This could for instance be data such as the application user that performed a data change,
their IP address and device configuration, a tracing span id, or an identifier for the application use case.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As that metadata typically isn’t (nor shouldn’t) be stored in the actual business tables of an application, it must be provided separately.
One approach is to have a separate table where this metadata is stored.
For each executed transaction, the business application produces one record in that table, containing all the required metadata and using the transaction id as a primary key.
When running manual data changes, it is easy to also provide the metadata record with an additional insert.
As Debezium’s data change events contain the id of the transaction causing the specific change,
the data change events and the metadata records can be correlated.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the remainder of this post we’re going to take a closer look at how a business application can provide the transaction-scoped metadata and how data change events can be enriched with the corresponding metadata using the Kafka Streams API.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;solution_overview&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#solution_overview&quot; /&gt;Solution Overview&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The following image shows the overall solution design, based on the example of a microservice for managing vegetable data:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/auditing_overview.png&quot; class=&quot;responsive-image&quot; alt=&quot;Auditing With Change Data Capture and Stream Processing&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There are two services involved:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;vegetables-service&lt;/em&gt;: a simple REST service for inserting and updating vegetable data into a Postgres database;
as part of its processing, it will not only update its actual &quot;business table&quot; &lt;code&gt;vegetable&lt;/code&gt;,
but also insert some auditing metadata into a dedicated metadata table &lt;code&gt;transaction_context_data&lt;/code&gt;;
Debezium is used to stream change events from the two tables into corresponding topics in Apache Kafka&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;log-enricher&lt;/em&gt;: a stream processing application built with Kafka Streams and Quarkus,
which enriches the messages from the CDC topic containing the vegetable change events (&lt;code&gt;dbserver1.inventory.vegetable&lt;/code&gt;) with the corresponding metadata in the &lt;code&gt;dbserver1.inventory.transaction_context_data&lt;/code&gt; topic and writes the enriched vegetable change event back to Kafka into the &lt;code&gt;dbserver1.inventory.vegetable.enriched&lt;/code&gt; topic.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can find a &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/auditlog&quot;&gt;complete example&lt;/a&gt; with all the components and instructions for running them on GitHub.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;providing_auditing_metadata&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#providing_auditing_metadata&quot; /&gt;Providing Auditing Metadata&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s first discuss how an application such as the vegetable service can provide the required auditing metadata.
As an example, the following metadata should be made available for auditing purposes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The application user that did a data change, as represented by the &lt;code&gt;sub&lt;/code&gt; claim of a JWT token (&lt;a href=&quot;https://tools.ietf.org/html/rfc7519&quot;&gt;JSON Web Token&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The request timestamp, as represented by the &lt;code&gt;Date&lt;/code&gt; HTTP header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A use case identifier, as provided via a custom Java annotation on the invoked REST resource method&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here is a basic implementation of a REST resource for persisting a new vegetable using the &lt;a href=&quot;https://jcp.org/en/jsr/detail?id=370&quot;&gt;JAX-RS API&lt;/a&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Path(&quot;/vegetables&quot;)
@RequestScoped
@Produces(MediaType.APPLICATION_JSON)
@Consumes(MediaType.APPLICATION_JSON)
public class VegetableResource {

    @Inject
    VegetableService vegetableService;

    @POST
    @RolesAllowed({&quot;farmers&quot;})
    @Transactional
    @Audited(useCase=&quot;CREATE VEGETABLE&quot;)
    public Response createVegetable(Vegetable vegetable) {
        if (vegetable.getId() != null) {
            return Response.status(Status.BAD_REQUEST.getStatusCode()).build();
        }

        vegetable = vegetableService.createVegetable(vegetable);

        return Response.ok(vegetable).status(Status.CREATED).build();
    }

    // update, delete ...
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you’ve ever built REST services with JAX-RS before, the implementation will look familiar to you:
a resource method annotated with &lt;code&gt;@POST&lt;/code&gt; takes the incoming request payload and passes it to a service bean which is injected via CDI.
The &lt;code&gt;@Audited&lt;/code&gt; annotation is special, though.
It is a custom annotation type which serves two purposes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Specifying the use case that should be referenced in the audit log (&quot;CREATE VEGETABLE&quot;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Binding an &lt;a href=&quot;https://jcp.org/en/jsr/detail?id=318&quot;&gt;interceptor&lt;/a&gt; which will be triggered for each invocation of a method annotated with &lt;code&gt;@Audited&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;That interceptor kicks in whenever a method annotated with &lt;code&gt;@Audited&lt;/code&gt; is invoked and implements the logic for writing the transaction-scoped audit metadata.
It looks like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Interceptor &lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;(1)&lt;/b&gt;
@Audited(useCase = &quot;&quot;)
@Priority(value = Interceptor.Priority.APPLICATION + 100) &lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;(2)&lt;/b&gt;
public class TransactionInterceptor {

    @Inject
    JsonWebToken jwt; &lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;(3)&lt;/b&gt;

    @Inject
    EntityManager entityManager;

    @Inject
    HttpServletRequest request;

    @AroundInvoke
    public Object manageTransaction(InvocationContext ctx) throws Exception {
        BigInteger txtId = (BigInteger) entityManager &lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;b&gt;(4)&lt;/b&gt;
            .createNativeQuery(&quot;SELECT txid_current()&quot;)
            .getSingleResult();
        String useCase = ctx.getMethod().getAnnotation(Audited.class).useCase();

        TransactionContextData context = new TransactionContextData(); &lt;i class=&quot;conum&quot; data-value=&quot;5&quot; /&gt;&lt;b&gt;(5)&lt;/b&gt;

        context.transactionId = txtId.longValueExact();
        context.userName = jwt.&amp;lt;String&amp;gt;claim(&quot;sub&quot;).orElse(&quot;anonymous&quot;);
        context.clientDate = getRequestDate();
        context.useCase = useCase;

        entityManager.persist(context);

        return ctx.proceed(); &lt;i class=&quot;conum&quot; data-value=&quot;6&quot; /&gt;&lt;b&gt;(6)&lt;/b&gt;
    }

    private ZonedDateTime getRequestDate() {
        String requestDate = request.getHeader(HttpHeaders.DATE);
        return requestDate != null ?
            ZonedDateTime.parse(requestDate, DateTimeFormatter.RFC_1123_DATE_TIME) :
            null;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;colist arabic&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;@Interceptor&lt;/code&gt; and &lt;code&gt;@Audited&lt;/code&gt; mark this as an interceptor bound to our custom &lt;code&gt;@Audited&lt;/code&gt; annotion.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The &lt;code&gt;@Priority&lt;/code&gt; annotation controls at which point in the interceptor stack the auditing interceptor should be invoked.
Any application-provided interceptors should have a priority larger than &lt;code&gt;Priority.APPLICATION&lt;/code&gt; (2000);
in particular, this ensures that a transaction will have been started before by means of the &lt;code&gt;@Transactional&lt;/code&gt; annotation and its accompanying interceptor which run in the &lt;code&gt;Priority.PLATFORM_BEFORE&lt;/code&gt; range (&amp;lt; 1000).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The caller’s JWT token injected via the &lt;a href=&quot;https://microprofile.io/project/eclipse/microprofile-jwt-auth&quot;&gt;MicroProfile JWT RBAC&lt;/a&gt; API&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For each audited method the interceptor fires and will&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;obtain the current transaction id (the exact way for doing so is database-specific, in the example the &lt;code&gt;txid_current()&lt;/code&gt; function from Postgres is called) &lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;persist a &lt;code&gt;TransactionContextData&lt;/code&gt; entity via JPA; its primary key value is the transaction id selected before, and it has attributes for the user name (obtained from the JWT token),
the request date (obtained from the &lt;code&gt;DATE&lt;/code&gt; HTTP request header) and the use case identifier (obtained from the &lt;code&gt;@Audited&lt;/code&gt; annotation of the invoked method) &lt;i class=&quot;conum&quot; data-value=&quot;5&quot; /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;continue the call flow of the invoked method &lt;i class=&quot;conum&quot; data-value=&quot;6&quot; /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When invoking the REST service to create and update a few vegetables,
the following records should be created in the database
(refer to the README in the provided example for instructions on building the example code and &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/auditlog#inserting-some-data-and-observing-the-audit-log&quot;&gt;invoking the vegetable service&lt;/a&gt; with a suitable JWT token):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;vegetablesdb&amp;gt; select * from inventory.vegetable;
+------+---------------+---------+
| id   | description   | name    |
|------+---------------+---------|
| 1    | Spicy!        | Potato  |
| 11   | Delicious!    | Pumpkin |
| 10   | Tasty!        | Tomato  |
+------+---------------+---------+&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;vegetablesdb&amp;gt; select * from inventory.transaction_context_data;
+------------------+---------------------+------------------+----------------+
| transaction_id   | client_date         | usecase          | user_name      |
|------------------+---------------------+------------------+----------------|
| 608              | 2019-08-22 08:12:31 | CREATE VEGETABLE | farmerbob      |
| 609              | 2019-08-22 08:12:31 | CREATE VEGETABLE | farmerbob      |
| 610              | 2019-08-22 08:12:31 | UPDATE VEGETABLE | farmermargaret |
+------------------+---------------------+------------------+----------------+&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;enriching_change_events_with_auditing_metadata&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#enriching_change_events_with_auditing_metadata&quot; /&gt;Enriching Change Events with Auditing Metadata&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With the business data (vegetables) and the transaction-scoped metadata being stored in the database,
it’s time to set up the &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/connectors/postgresql.html&quot;&gt;Debezium Postgres connector&lt;/a&gt; and stream the data changes from the &lt;code&gt;vegetable&lt;/code&gt; and &lt;code&gt;transaction_context_data&lt;/code&gt; tables into corresponding Kafka topics.
Again refer to the example README file for the details of &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/auditlog#deploy-the-debezium-postgres-connector&quot;&gt;deploying the connector&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;dbserver1.inventory.vegetable&lt;/code&gt; topic should contain change events for created, updated and deleted vegetable records, whereas the &lt;code&gt;dbserver1.inventory.transaction_context_data&lt;/code&gt; topic should only contain create messages for each inserted metadata record.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Topic Retention&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In order to manage the growth of involved topics, the retention policy for each topic should be well-defined.
For instance for the actual audit log topic with the enriched change events, a time based retention policy might be suitable, keeping each log event for as long as needed as per your requirements.
The transaction metadata topic on the other hand can be fairly short-lived, as its entries are not needed any longer, once all corresponding data change events have been processed.
It may be a good idea to set up some monitoring of the end-to-end lag in order to make sure the log enricher stream application keeps up with the incoming messages and doesn’t fall behind that far so it is at risk of transaction messages being discarded before processing the corresponding change events.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now, if we look at messages from the two topics, we can see that they can be correlated based on the transaction id.
It is part of the &lt;code&gt;source&lt;/code&gt; structure of vegetable change events,
and it is the message key of transaction metadata events:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/auditing_input_messages.png&quot; class=&quot;responsive-image&quot; alt=&quot;Vegetable and Transaction Metadata Messages&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once we’ve found the corresponding transaction event for a given vegetable change event,
the &lt;code&gt;client_date&lt;/code&gt;, &lt;code&gt;usecase&lt;/code&gt; and &lt;code&gt;user_name&lt;/code&gt; attributes from the former can be added to the latter:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/auditing_output_message.png&quot; class=&quot;responsive-image&quot; alt=&quot;Enriched Vegetable Message&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This kind of message transformation is a perfect use case for &lt;a href=&quot;https://kafka.apache.org/documentation/streams/&quot;&gt;Kafka Streams&lt;/a&gt;,
a Java API for implementing stream processing applications on top of Kafka topics,
providing operators that let you filter, transform, aggregate and join Kafka messages.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As runtime environment for our stream processing application we’re going to use &lt;a href=&quot;https://quarkus.io/&quot;&gt;Quarkus&lt;/a&gt;,
which is &quot;a Kubernetes Native Java stack tailored for GraalVM &amp;amp; OpenJDK HotSpot, crafted from the best of breed Java libraries and standards&quot;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Building Kafka Streams Applications with Quarkus&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Amongst many others, Quarkus comes with an &lt;a href=&quot;https://quarkus.io/guides/kafka-streams-guide&quot;&gt;extension for Kafka Streams&lt;/a&gt;,
which allows to build stream processing applications running on the JVM and as native code compiled ahead-of-time.
It takes care of the lifecycle of the streaming topology,
so you don’t have to deal with details like registering JVM shutdown hooks,
awaiting the creation of all input topics and more.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The extension also comes with &quot;live development&quot; support,
which automatically reloads the stream processing application while you’re working on it,
allowing for very fast turnaround cycles during development.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;the_joining_logic&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_joining_logic&quot; /&gt;The Joining Logic&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When thinking about the actual implementation of the enrichment logic,
a &lt;a href=&quot;https://kafka.apache.org/23/documentation/streams/developer-guide/dsl-api.html#kstream-kstream-join&quot;&gt;stream-to-stream&lt;/a&gt; join might appear as a suitable solution.
By creating &lt;code&gt;KStream&lt;/code&gt;s for the two topics, we may try and implement the joining functionality.
One challenge though is how to define a suitable &lt;a href=&quot;https://kafka.apache.org/23/documentation/streams/developer-guide/dsl-api.html#windowing-sliding&quot;&gt;joining window&lt;/a&gt;,
as there is no timing guarantees between messages on the two topics,
and we must not miss any event.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another problem arises in regards to ordering guarantees of the change events.
By default, Debezium will use a table’s primary key as the message key for the corresponding Kafka messages.
This means that all messages for the same vegetable record will have the same key and thus will go into the same partition of the vegetables Kafka topic.
This in turn guarantees that a consumer of these events sees all the messages pertaining to the same vegetable record in the exact same order as they were created.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now, in order to join the two streams, the message key must be the same on both sides.
This means the vegetables topic must be re-keyed by transaction id
(we cannot re-key the transaction metadata topic, as there’s no information about concerned vegetables contained in the metadata events; and even if that were the case, one transaction might impact multiple vegetable records).
By doing so, we’d loose the original ordering guarantees, though.
One vegetable record might be modified in two subsequent transactions,
and its change events may end up in different partitions of the re-keyed topic,
which may cause a consumer to receive the second change event before the first one.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If a &lt;code&gt;KStream&lt;/code&gt;-&lt;code&gt;KStream&lt;/code&gt; join isn’t feasible, what else could be done?
&lt;a href=&quot;https://kafka.apache.org/23/documentation/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl-joins-kstream-globalktable&quot;&gt;A join&lt;/a&gt; between a &lt;code&gt;KStream&lt;/code&gt; and &lt;code&gt;GlobalKTable&lt;/code&gt; looks promising, too.
It doesn’t have the &lt;a href=&quot;https://kafka.apache.org/23/documentation/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl-joins-co-partitioning&quot;&gt;co-partitioning requirements&lt;/a&gt; of stream-to-stream joins,
as all partitions of the &lt;code&gt;GlobalKTable&lt;/code&gt; are present on all nodes of a distributed Kafka Streams application.
This seems like an acceptable trade-off, because the messages from the transaction metadata topic can be discarded rather quickly and the size of the corresponding table should be within reasonable bounds.
So we could have a &lt;code&gt;KStream&lt;/code&gt; sourced from the vegetables topic and a &lt;code&gt;GlobalKTable&lt;/code&gt; based on the transaction metadata topic.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But unfortunately, there is a timing issue:
as the messages are consumed from multiple topics, it may happen that at the point in time when an element from the vegetables stream is processed, the corresponding transaction metadata message isn’t available yet.
So depending on whether we’d be using an inner join or a left join,
we’d in this case either skip change events or propagate them without having enriched them with the transaction metadata.
Both outcomes are not desirable.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;customized_joins_with_buffering&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#customized_joins_with_buffering&quot; /&gt;Customized Joins With Buffering&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The combination of &lt;code&gt;KStream&lt;/code&gt; and &lt;code&gt;GlobalKTable&lt;/code&gt; still hints into the right direction.
Only that instead of relying on the built-in join operators we’ll have to implement a custom joining logic.
The basic idea is to buffer messages arriving on the vegetable &lt;code&gt;KStream&lt;/code&gt; until the corresponding transaction metadata message is available from the &lt;code&gt;GlobalKTable&lt;/code&gt;s state store.
This can be achieved by creating a custom &lt;a href=&quot;https://kafka.apache.org/23/javadoc/org/apache/kafka/streams/kstream/KStream.html#transform-org.apache.kafka.streams.kstream.TransformerSupplier-java.lang.String…​-&quot;&gt;transformer&lt;/a&gt; which implements the required buffering logic and is applied to the vegetable &lt;code&gt;KStream&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s begin with the streaming topology itself.
Thanks to the Quarkus Kafka Streams extension,
a CDI producer method returning the &lt;code&gt;Topology&lt;/code&gt; object is all that’s needed for that:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class TopologyProducer {

    static final String STREAM_BUFFER_NAME = &quot;stream-buffer-state-store&quot;;
    static final String STORE_NAME = &quot;transaction-meta-data&quot;;

    @ConfigProperty(name = &quot;audit.context.data.topic&quot;)
    String txContextDataTopic;

    @ConfigProperty(name = &quot;audit.vegetables.topic&quot;)
    String vegetablesTopic;

    @ConfigProperty(name = &quot;audit.vegetables.enriched.topic&quot;)
    String vegetablesEnrichedTopic;

    @Produces
    public Topology buildTopology() {
        StreamsBuilder builder = new StreamsBuilder();

        StoreBuilder&amp;lt;KeyValueStore&amp;lt;Long, JsonObject&amp;gt;&amp;gt; streamBufferStateStore =
                Stores
                    .keyValueStoreBuilder(
                        Stores.persistentKeyValueStore(STREAM_BUFFER_NAME),
                        new Serdes.LongSerde(),
                        new JsonObjectSerde()
                    )
                    .withCachingDisabled();
            builder.addStateStore(streamBufferStateStore); &lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;(1)&lt;/b&gt;

        builder.globalTable(txContextDataTopic, Materialized.as(STORE_NAME)); &lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;(2)&lt;/b&gt;

        builder.&amp;lt;JsonObject, JsonObject&amp;gt;stream(vegetablesTopic) &lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;(3)&lt;/b&gt;
                .filter((id, changeEvent) -&amp;gt; changeEvent != null)
                .filter((id, changeEvent) -&amp;gt; !changeEvent.getString(&quot;op&quot;).equals(&quot;r&quot;))
                .transform(() -&amp;gt; new ChangeEventEnricher(), STREAM_BUFFER_NAME)
                .to(vegetablesEnrichedTopic);

        return builder.build();
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;colist arabic&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;State store which will serve as the buffer for change events that cannot be processed yet&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;GlobalKTable&lt;/code&gt; based on the transaction metadata topic&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;KStream&lt;/code&gt; based on the vegetables topic; on this stream, any incoming tombstone markers are filtered, the reasoning being that the retention policy for an audit trail topic typically should be time-based than based on log compaction;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;similarly, snapshot events are filtered, assuming they are not relevant for an audit trail and there wouldn’t be any corresponding metadata provided by the application for the snapshot transaction initiated by the Debezium connector&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Any other messages are enriched with the corresponding transaction metadata via a custom &lt;code&gt;Transformer&lt;/code&gt; (see below) and finally are written to an output topic&lt;/p&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The topic names are injected using the &lt;a href=&quot;https://microprofile.io/project/eclipse/microprofile-config&quot;&gt;MicroProfile Config API&lt;/a&gt;, with the values being provided in Quarkus &lt;em&gt;application.properties&lt;/em&gt; configuration file.
Besides the topic names, this file also has the information about the Kafka bootstrap server, default serdes any more:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;audit.context.data.topic=dbserver1.inventory.transaction_context_data
audit.vegetables.topic=dbserver1.inventory.vegetable
audit.vegetables.enriched.topic=dbserver1.inventory.vegetable.enriched

# may be overridden with env vars
quarkus.kafka-streams.bootstrap-servers=localhost:9092
quarkus.kafka-streams.application-id=auditlog-enricher
quarkus.kafka-streams.topics=${audit.context.data.topic},${audit.vegetables.topic}

# pass-through
kafka-streams.cache.max.bytes.buffering=10240
kafka-streams.commit.interval.ms=1000
kafka-streams.metadata.max.age.ms=500
kafka-streams.auto.offset.reset=earliest
kafka-streams.metrics.recording.level=DEBUG
kafka-streams.default.key.serde=io.debezium.demos.auditing.enricher.JsonObjectSerde
kafka-streams.default.value.serde=io.debezium.demos.auditing.enricher.JsonObjectSerde
kafka-streams.processing.guarantee=exactly_once&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the next step let’s take a look at the &lt;code&gt;ChangeEventEnricher&lt;/code&gt; class, our custom transformer.
The implemention is based on the assumption that change events are serialized as JSON,
but of course it could be done equally well using other formats such as Avro or Protocol Buffers.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is a bit of code, but hopefully its decomposition into multiple smaller methods makes it comprehensible:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;class ChangeEventEnricher implements Transformer
        &amp;lt;JsonObject, JsonObject, KeyValue&amp;lt;JsonObject, JsonObject&amp;gt;&amp;gt; {

    private static final Long BUFFER_OFFSETS_KEY = -1L;

    private static final Logger LOG = LoggerFactory.getLogger(ChangeEventEnricher.class);

    private ProcessorContext context;
    private KeyValueStore&amp;lt;JsonObject, JsonObject&amp;gt; txMetaDataStore;
    private KeyValueStore&amp;lt;Long, JsonObject&amp;gt; streamBuffer; &lt;i class=&quot;conum&quot; data-value=&quot;5&quot; /&gt;&lt;b&gt;(5)&lt;/b&gt;

    @Override
    @SuppressWarnings(&quot;unchecked&quot;)
    public void init(ProcessorContext context) {
        this.context = context;
        streamBuffer = (KeyValueStore&amp;lt;Long, JsonObject&amp;gt;) context.getStateStore(
            TopologyProducer.STREAM_BUFFER_NAME
        );
        txMetaDataStore = (KeyValueStore&amp;lt;JsonObject, JsonObject&amp;gt;) context.getStateStore(
            TopologyProducer.STORE_NAME
        );

        context.schedule(
            Duration.ofSeconds(1),
            PunctuationType.WALL_CLOCK_TIME, ts -&amp;gt; enrichAndEmitBufferedEvents()
        ); &lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;b&gt;(4)&lt;/b&gt;
    }

    @Override
    public KeyValue&amp;lt;JsonObject, JsonObject&amp;gt; transform(JsonObject key, JsonObject value) {
        boolean enrichedAllBufferedEvents = enrichAndEmitBufferedEvents(); &lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;(3)&lt;/b&gt;

        if (!enrichedAllBufferedEvents) {
            bufferChangeEvent(key, value);
            return null;
        }

        KeyValue&amp;lt;JsonObject, JsonObject&amp;gt; enriched = enrichWithTxMetaData(key, value); &lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;(1)&lt;/b&gt;
        if (enriched == null) { &lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;(2)&lt;/b&gt;
            bufferChangeEvent(key, value);
        }

        return enriched;
    }

    /**
     * Enriches the buffered change event(s) with the metadata from the associated
     * transactions and forwards them.
     *
     * @return {@code true}, if all buffered events were enriched and forwarded,
     *         {@code false} otherwise.
     */
    private boolean enrichAndEmitBufferedEvents() { &lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;(3)&lt;/b&gt;
        Optional&amp;lt;BufferOffsets&amp;gt; seq = bufferOffsets();

        if (!seq.isPresent()) {
            return true;
        }

        BufferOffsets sequence = seq.get();

        boolean enrichedAllBuffered = true;

        for(long i = sequence.getFirstValue(); i &amp;lt; sequence.getNextValue(); i++) {
            JsonObject buffered = streamBuffer.get(i);

            LOG.info(&quot;Processing buffered change event for key {}&quot;,
                    buffered.getJsonObject(&quot;key&quot;));

            KeyValue&amp;lt;JsonObject, JsonObject&amp;gt; enriched = enrichWithTxMetaData(
                    buffered.getJsonObject(&quot;key&quot;), buffered.getJsonObject(&quot;changeEvent&quot;));
            if (enriched == null) {
                enrichedAllBuffered = false;
                break;
            }

            context.forward(enriched.key, enriched.value);
            streamBuffer.delete(i);
            sequence.incrementFirstValue();
        }

        if (sequence.isModified()) {
            streamBuffer.put(BUFFER_OFFSETS_KEY, sequence.toJson());
        }

        return enrichedAllBuffered;
    }

    /**
     * Adds the given change event to the stream-side buffer.
     */
    private void bufferChangeEvent(JsonObject key, JsonObject changeEvent) { &lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;(2)&lt;/b&gt;
        LOG.info(&quot;Buffering change event for key {}&quot;, key);

        BufferOffsets sequence = bufferOffsets().orElseGet(BufferOffsets::initial);

        JsonObject wrapper = Json.createObjectBuilder()
                .add(&quot;key&quot;, key)
                .add(&quot;changeEvent&quot;, changeEvent)
                .build();

        streamBuffer.putAll(Arrays.asList(
                KeyValue.pair(sequence.getNextValueAndIncrement(), wrapper),
                KeyValue.pair(BUFFER_OFFSETS_KEY, sequence.toJson())
        ));
    }

    /**
     * Enriches the given change event with the metadata from the associated
     * transaction.
     *
     * @return The enriched change event or {@code null} if no metadata for the
     *         associated transaction was found.
     */
    private KeyValue&amp;lt;JsonObject, JsonObject&amp;gt; enrichWithTxMetaData(JsonObject key,
            JsonObject changeEvent) { &lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;(1)&lt;/b&gt;
        JsonObject txId = Json.createObjectBuilder()
                .add(&quot;transaction_id&quot;, changeEvent.get(&quot;source&quot;).asJsonObject()
                        .getJsonNumber(&quot;txId&quot;).longValue())
                .build();

        JsonObject metaData = txMetaDataStore.get(txId);

        if (metaData != null) {
            LOG.info(&quot;Enriched change event for key {}&quot;, key);

            metaData = Json.createObjectBuilder(metaData.get(&quot;after&quot;).asJsonObject())
                    .remove(&quot;transaction_id&quot;)
                    .build();

            return KeyValue.pair(
                    key,
                    Json.createObjectBuilder(changeEvent)
                        .add(&quot;audit&quot;, metaData)
                        .build()
            );
        }

        LOG.warn(&quot;No metadata found for transaction {}&quot;, txId);
        return null;
    }

    private Optional&amp;lt;BufferOffsets&amp;gt; bufferOffsets() {
        JsonObject bufferOffsets = streamBuffer.get(BUFFER_OFFSETS_KEY);
        if (bufferOffsets == null) {
            return Optional.empty();
        }
        else {
            return Optional.of(BufferOffsets.fromJson(bufferOffsets));
        }
    }

    @Override
    public void close() {
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;colist arabic&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;When a vegetables change event arrives, look up the corresponding metadata in the state store of the
transaction topic’s &lt;code&gt;GlobalKTable&lt;/code&gt;, using the transaction id from the &lt;code&gt;source&lt;/code&gt; block of the change event as the key;
if the metadata could be found, add the metadata to change event (under the &lt;code&gt;audit&lt;/code&gt; field) and return that enriched event&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;If the metadata could not be found, add the incoming event into the buffer of change events and return&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Before actually getting to the incoming event, all buffered events are processed;
this is required to make sure that the original change events is retained;
only if all could be enriched, the incoming event will be processed, too&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;In order to emit buffered events also if no new change event is coming in,
a punctuation is scheduled that periodically processes the buffer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;5&quot; /&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;A buffer for vegetable events whose corresponding metadata hasn’t arrived yet&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The key piece is the buffer for unprocessable change events.
To maintain the order of events, the buffer must be processed in order of insertion,
beginning with the event inserted first
(think of a FIFO queue).
As there’s no guaranteed traversing order when getting all the entries from a &lt;code&gt;KeyValueStore&lt;/code&gt;,
this is implemented by using the values of a strictly increasing sequence as the keys.
A &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/auditlog/log-enricher/src/main/java/io/debezium/demos/auditing/enricher/BufferOffsets.java&quot;&gt;special entry&lt;/a&gt; in the key value store is used to store the information about the current &quot;oldest&quot; index in the buffer and the next sequence value.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One could also think of alternative implementations for such buffer, e.g. based on a Kafka topic or a custom &lt;code&gt;KeyValueStore&lt;/code&gt; implementation that ensures iteration order from oldest to newest entry.
Ultimately, it could also be useful if Kafka Streams came with built-in means of retrying a stream element that cannot be joined yet; this would avoid any custom buffering implementation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;title&quot;&gt;If Things Go Wrong&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For a reliable and consistent processing logic it’s vital to think about the behavior in case of failures,
e.g. if the stream application crashes after adding an element to the buffer but before updating the sequence value.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The key to this is the &lt;code&gt;exactly_once&lt;/code&gt; value of the &lt;code&gt;processing.guarantee&lt;/code&gt; property given in &lt;em&gt;application.properties&lt;/em&gt;.
This ensures a transactionally consistent processing; e.g. in the aforementioned scenario,
after a restart the original change event would be handled again, and the buffer state would look exactly like it did before the event was processed for the first time.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Consumers of the enriched vegetable events should apply an isolation level of &lt;code&gt;read_committed&lt;/code&gt;;
otherwise they may see uncommitted and thus duplicate messages in case of an application crash after a buffered event was forwarded but before it was removed from the buffer.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With the custom transformer logic in place, we can build the Quarkus project and run the stream processing application.
You should see messages like this in the &lt;code&gt;dbserver1.inventory.vegetable.enriched&lt;/code&gt; topic:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;{&quot;id&quot;:10}
{
    &quot;before&quot;: {
        &quot;id&quot;: 10,
        &quot;description&quot;: &quot;Yummy!&quot;,
        &quot;name&quot;: &quot;Tomato&quot;
    },
    &quot;after&quot;: {
        &quot;id&quot;: 10,
        &quot;description&quot;: &quot;Tasty!&quot;,
        &quot;name&quot;: &quot;Tomato&quot;
    },
    &quot;source&quot;: {
        &quot;version&quot;: &quot;0.10.0-SNAPSHOT&quot;,
        &quot;connector&quot;: &quot;postgresql&quot;,
        &quot;name&quot;: &quot;dbserver1&quot;,
        &quot;ts_ms&quot;: 1569700445392,
        &quot;snapshot&quot;: &quot;false&quot;,
        &quot;db&quot;: &quot;vegetablesdb&quot;,
        &quot;schema&quot;: &quot;inventory&quot;,
        &quot;table&quot;: &quot;vegetable&quot;,
        &quot;txId&quot;: 610,
        &quot;lsn&quot;: 34204240,
        &quot;xmin&quot;: null
    },
    &quot;op&quot;: &quot;u&quot;,
    &quot;ts_ms&quot;: 1569700445537,
    &quot;audit&quot;: {
        &quot;client_date&quot;: 1566461551000000,
        &quot;usecase&quot;: &quot;UPDATE VEGETABLE&quot;,
        &quot;user_name&quot;: &quot;farmermargaret&quot;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Of course, the buffer processing logic may be adjusted as per your specific requirements;
for instance instead of indefinitely waiting for corresponding transaction metadata,
we may also decide that it makes more sense to propagate change events unenriched after some waiting time or to raise an exception indicating the missing metadata.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In order to see whether the buffering works as expected, you could do a small experiment:
modify a vegetable record using SQL directly in the database.
Debezium will capture the event, but as there’s no corresponding transaction metadata provided,
the event will not be forwarded to the enriched vegetables topic.
If you add another vegetable using the REST API,
this one also will not be propagated:
although there is a metadata record for it, it’s blocked by the other change event.
Only once you have inserted a metadata record for the first change’s transaction into the &lt;code&gt;transaction_context_data&lt;/code&gt; table,
both change events will be processed and sent to the output topic.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot; /&gt;Summary&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In this blog post we’ve discussed how change data capture in combination with stream processing can be used to build audit logs in an efficient, low-overhead way.
In contrast to library and trigger-based approaches, the events that form the audit trail are retrieved via CDC from the database’s transaction logs,
and apart from the insertion of a single metadata record per transaction
(which in similar form would be required for any kind of audit log), no overhead to OLTP transactions is incurred.
Also audit log entries can be obtained when data records are subject to bulk updates or deletes,
something typically not possible with library-based auditing solutions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Additional metadata that typically should be part of an audit log,
can be provided by the application via a separate table,
which also is captured via Debezium.
With the help of Kafka Streams the actual data change events can be enriched with the data from that metadata table.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One aspect we haven’t discussed yet is querying the audit trail entries,
e.g. to examine specific earlier versions of the data.
To do so, the enriched change data events typically would be stored in a queryable database.
Unlike a basic data replication pipeline, not only the latest version of each record would be stored in the database in that case, but all the versions, i.e. the primary keys typically would be amended with the transaction id of each change.
This would allow to select single data records or even joins of multiple tables to get the data valid as per a given transaction id.
How this could be implemented in detail may be discussed in a future post.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Your feedback on this approach for building audit logs is very welcomed,
just post a comment below.
To get started with your own implementation,
you can check out &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/auditlog&quot;&gt;the code&lt;/a&gt; in the Debezium examples repository on GitHub.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;em&gt;Many thanks to &lt;a href=&quot;https://twitter.com/crancran77&quot;&gt;Chris Cranford&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/hpgrahsl&quot;&gt;Hans-Peter Grahsl&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/hashhar&quot;&gt;Ashhar Hasan&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/jbfletch_&quot;&gt;Anna McDonald&lt;/a&gt; and Jiri Pechanec for their feedback while working on this post and the accompanying example code!&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/09/26/debezium-0-10-0-cr2-released/</id>
<title>Debezium 0.10.0.CR2 Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-09-26T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/09/26/debezium-0-10-0-cr2-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="cassandra"></category>
<summary>



I&#8217;m very happy to announce the release of Debezium 0.10.0.CR2!


After the CR1 release we decided to do another candidate release, as there was not only a good number of bug fixes coming in, but also a few very useful feature implementations were provided by the community, which we didn&#8217;t want to delay.
So we adjusted the original plan a bit and now aim for Debezium 0.10 Final in the course of next week,
barring any unforeseen regressions.


As usual, let&#8217;s take a closer look at some of the new features and resolved bugs.




Customizable Message Keys


Being able to configure which columns of a table...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I’m very happy to announce the release of Debezium &lt;strong&gt;0.10.0.CR2&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;After the CR1 release we decided to do another candidate release, as there was not only a good number of bug fixes coming in, but also a few very useful feature implementations were provided by the community, which we didn’t want to delay.
So we adjusted the original plan a bit and now aim for Debezium 0.10 Final in the course of next week,
barring any unforeseen regressions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As usual, let’s take a closer look at some of the new features and resolved bugs.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;customizable_message_keys&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#customizable_message_keys&quot; /&gt;Customizable Message Keys&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Being able to configure which columns of a table should go into the key of corresponding Kafka messages has been a long-standing feature request (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1015&quot;&gt;DBZ-1015&lt;/a&gt;).
To recap, by default the message key of Debezium’s data change events will contain the primary key column(s) of the represented table.
That’s a sensible default, but sometimes more flexibility is desirable:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Some tables don’t have a primary key, esp. in legacy data models&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Choosing a different message key than the primary key may facilitate stream processing applications that operate on multiple change data topics&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The second case is especially of interest when building &lt;a href=&quot;https://kafka.apache.org/documentation/streams/&quot;&gt;Kafka Streams&lt;/a&gt; applications that join multiple CDC topics:
in general, topic joins can only be done if the message key is the same on both sides of the join.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For instance let’s assume we have two tables, &lt;code&gt;Customer&lt;/code&gt; and &lt;code&gt;CustomerDetails&lt;/code&gt; with different primary keys and a foreign key relationship from &lt;code&gt;CustomerDetails&lt;/code&gt; to &lt;code&gt;Customer&lt;/code&gt;.
By choosing that foreign key column as the message key for customer detail change events,
the two table streams could be joined without the need for re-keying the customer details topic.
To do so, the new &lt;code&gt;message.key.columns&lt;/code&gt; option can be used like so:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;message.key.columns=dbserver1.inventory.customerdetails:CustomerId&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Customizable message keys are supported for all the relational Debezium connectors.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;pluggable_serializers_for_cassandra&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#pluggable_serializers_for_cassandra&quot; /&gt;Pluggable Serializers for Cassandra&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;From previous announcements you might remember that Debezium’s Cassandra connector is a bit different from the other ones,
as it’s not based on the Kafka Connect framework.
As such, until now it didn’t support the notion of configurable message serializers;
Avro was the only supported message format.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As part of the ongoing efforts to align the Cassandra connector more closely with the other ones,
it now allows to configure different serializers, so you also could use JSON, ProtoBuf and other formats (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1405&quot;&gt;DBZ-1405&lt;/a&gt;).
The serializer framework from Kafka Connect is reused for that, so you can leverage all the existing serializers and configure them exactly the same way as done for any other connector.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;improved_handling_of_postgres_toast_columns&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#improved_handling_of_postgres_toast_columns&quot; /&gt;Improved handling of Postgres TOAST Columns&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://www.postgresql.org/docs/current/storage-toast.html&quot;&gt;TOAST columns&lt;/a&gt; are a mechanism in Postgres for dealing with column values that exceed the page size limit (typically 8 KB).
While the usage of TOAST is transparent when interacting with the database itself,
this is not the case when obtaining change events via logical decoding.
As TOASTed values are not stored within the physical data row itself,
logical decoding does not expose the value of &lt;strong&gt;unchanged&lt;/strong&gt; TOAST columns
(unless the column is part of the table’s replica identity).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This situation used to be handled in different ways by the the logical decoding plug-ins supported by Debezium (&lt;code&gt;pgoutput&lt;/code&gt;, &lt;code&gt;decoderbufs&lt;/code&gt; and &lt;code&gt;wal2json&lt;/code&gt;),
one approach being the retrieval of such column &quot;out of bands&quot;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Unfortunately, there’s no way for savely doing this when considering concurrent writes to such record.
So we reworked how TOAST columns are handled:
if a TOAST column’s value hasn’t changed and that column isn’t part of the table’s replica identity, its value will not be contained in &lt;code&gt;UPDATE&lt;/code&gt; or &lt;code&gt;DELETE&lt;/code&gt; events.
Instead, a configurable marker value will be exported in this case
(defaulting to &lt;code&gt;__debezium_unavailable_value&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This avoids the race conditions that were possible before, but of course raises the question how consumers should deal with this marker value.
There are multiple possible answers to that:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The value could simply be ignored; for instance a consumer that writes change events to a database, may omit that column from the &lt;code&gt;UPDATE&lt;/code&gt; statement it issues&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When not working with dynamic updates, a trigger may be installed in a sink database,
that ignores any updates that would set a column value to the marker, keeping the previous value&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When actually requiring complete change events including any TOAST column within the Kafka change data topic itself, a stateful Kafka Streams application could be built which hydrates incoming change events with the marker value based on the previous column value persisted in a state store&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Thinking about it, the last approach might be an interesting topic for a future blog post :-)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;bugfixes_and_other_changes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bugfixes_and_other_changes&quot; /&gt;Bugfixes and Other Changes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Besides these feature implementations, this release contains a number of bugfixes, too:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;When using the &lt;code&gt;pgoutput&lt;/code&gt; logical decoding plug-in for Postgres, custom publication names are supported (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1436&quot;&gt;DBZ-1436&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Postgres connector will retry for a configurable period of time to obtain a replication slot, which can be helpful when rebalancing existing connectors in a cluster (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1426&quot;&gt;DBZ-1426&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reserved characters in column names can be replaced when using Avro as message format (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1044&quot;&gt;DBZ-1044&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Default values without the time part for MySQL &lt;code&gt;DATETIME&lt;/code&gt; columns are supported now (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1501&quot;&gt;DBZ-1501&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MySQL &lt;code&gt;CREATE DATABASE&lt;/code&gt; and &lt;code&gt;CREATE TABLE&lt;/code&gt; statements with default character sets are supported (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1470&quot;&gt;DBZ-1470&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Testing for MongoDB has been expanded to also cover version 4.2 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1389&quot;&gt;DBZ-1389&lt;/a&gt;), and the Postgres driver has been updated to the latest and greatest version 42.2.7 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1462&quot;&gt;DBZ-1462&lt;/a&gt;).
We’re also happy to report that going forward, the Debezium container images are also available in the &lt;a href=&quot;https://quay.io/&quot;&gt;quay.io&lt;/a&gt; container registry (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1178&quot;&gt;DBZ-1178&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Overall, not less than &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.10.0.CR2&quot;&gt;30 issues&lt;/a&gt; were fixed in the 0.10 CR2 release.
Please refer to the &lt;a href=&quot;https://debezium.io/releases/0.10/release-notes#release-0-10-0-cr2&quot;&gt;release notes&lt;/a&gt; for the complete list of addressed issues and the upgrading procedure.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This release wouldn’t be possible without all the fantastic people from the Debezium community who contributed:
&lt;a href=&quot;https://github.com/bingqinzhou&quot;&gt;Bingqin Zhou&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ChingTsai&quot;&gt;Ching Tsai&lt;/a&gt;,
&lt;a href=&quot;https://github.com/willome&quot;&gt;Guillaume Rosauro&lt;/a&gt;,
&lt;a href=&quot;https://github.com/javierholguera&quot;&gt;Javier Holguera&lt;/a&gt;
&lt;a href=&quot;https://github.com/jerrinot&quot;&gt;Jaromir Hamala&lt;/a&gt;,
&lt;a href=&quot;https://github.com/josharenberg&quot;&gt;Josh Arenberg&lt;/a&gt; and
&lt;a href=&quot;https://github.com/taylor-rolison&quot;&gt;Taylor Rolison&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Many thanks to all of you!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/09/10/debezium-0-10-0-cr1-released/</id>
<title>Debezium 0.10.0.CR1 Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-09-10T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/09/10/debezium-0-10-0-cr1-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="cassandra"></category>
<summary>



The Debezium community is on the homestretch towards the 0.10 release and we&#8217;re happy to announce the availability of Debezium 0.10.0.CR1!


Besides a number of bugfixes to the different connectors, this release also brings a substantial improvement to the way initial snapshots can be done with Postgres.
Unless any major regressions show up, the final 0.10 release should follow very soon.




Exported Snapshots for Postgres


One capability of Postgres' logical decoding facility that has not been leveraged by Debezium so far is the notion of exported snapshots:
when a replication slot is created, a transaction with SNAPSHOT isolation mode can be started, which allows to...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium community is on the homestretch towards the 0.10 release and we’re happy to announce the availability of Debezium &lt;strong&gt;0.10.0.CR1&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Besides a number of bugfixes to the different connectors, this release also brings a substantial improvement to the way initial snapshots can be done with Postgres.
Unless any major regressions show up, the final 0.10 release should follow very soon.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;exported_snapshots_for_postgres&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#exported_snapshots_for_postgres&quot; /&gt;Exported Snapshots for Postgres&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One capability of Postgres&#39; logical decoding facility that has not been leveraged by Debezium so far is the notion of &lt;a href=&quot;https://www.postgresql.org/docs/10/logicaldecoding-explanation.html#id-1.8.14.8.5&quot;&gt;exported snapshots&lt;/a&gt;:
when a replication slot is created, a transaction with &lt;code&gt;SNAPSHOT&lt;/code&gt; isolation mode can be started, which allows to export tables from the database without taking any locks, exactly at the moment when the slot is created.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Based on earlier work which allows for much more flexibility in regards to how snapshot are handled by the Debezium Postgres connector,
the exported snapshot functionality can now be used via the new &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/connectors/postgresql.html#snapshots&quot;&gt;snapshot mode&lt;/a&gt; &lt;code&gt;exported&lt;/code&gt;
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1035&quot;&gt;DBZ-1035&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1440&quot;&gt;DBZ-1440&lt;/a&gt;).
We encourage you to give this a test ride and report back any issues.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Eventually, this mode should become the default snapshotting behavior, as it doesn’t require the connector to obtain any locks.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;bugfixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bugfixes&quot; /&gt;Bugfixes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Due to a value overflow multiple users had reported issues when capturing the values of temporal columns with values far out in the future, e.g. years after 3000
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-949&quot;&gt;DBZ-949&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1205&quot;&gt;DBZ-1205&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1255&quot;&gt;DBZ-1255&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While such values are perhaps not too common in typical enterprise applications
(after all, who wants to have their purchase order delivery scheduled in a thousand years from now),
there’s still several use cases working with such values, e.g. think of potentially large half-life times when modelling nuclear decay processes.
So we did a larger refactoring of the code dealing with temporal values, and we are happy to report that these issues have been fixed now.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Other bugfixes were done in regards to dealing with &quot;no-op&quot; events in MongoDB
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1464&quot;&gt;DBZ-1464&lt;/a&gt;) and the recently added propagation of &lt;code&gt;source&lt;/code&gt; fields to outgoing message when using the &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/configuration/event-flattening.html&quot;&gt;new record state extraction SMT&lt;/a&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1448&quot;&gt;DBZ-1448&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;cassandra_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#cassandra_connector&quot; /&gt;Cassandra Connector&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;After the &lt;a href=&quot;https://debezium.io/blog/2019/08/20/debezium-0-10-0-beta4-released/&quot;&gt;initial release&lt;/a&gt; of the Debezium connector for Cassandra,
work has begun to further align it with the other Debezium connectors
(unlike the relational connectors and the one for MongoDB, the Cassandra connector currently is not based on Apache Kafka Connect,
but runs as a stand-alone process).
The first outcome of this is that its configured via a properties file now
(similar to using Kafka Connect in standalone mode) and not via a YAML file.
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1406&quot;&gt;DBZ-1406&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The next step will be to make the aspect of message serialization configurable:
while currently only Avro is supported by the connector,
it will eventually support the notion of pluggable converters,
allowing you to use the JSON, Avro and any other converters you may already know from Kafka Connect.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;reworked_website_and_documentation&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#reworked_website_and_documentation&quot; /&gt;Reworked Website and Documentation&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When reading this blog post, it’s hard to miss:
the Debezium website has &lt;a href=&quot;https://debezium.io/blog/2019/09/05/website-documentation-overhaul/&quot;&gt;received a facelift&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Information for the current stable and development releases (0.9 and 0.10 at this time) is much easier to find now.
Also the documentation has been re-organized and is published in version-specific approach now,
i.e. you can now obtain the specific documentation applying for a particular release.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The blog section of the website has been reworked, too:
the main page shows an introductory snippet for the most recent posts,
whereas a number of &quot;featured&quot; blog posts is listed on the left.
These are typically earlier blog posts which explore advanced topics such as the &lt;a href=&quot;https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/&quot;&gt;outbox pattern&lt;/a&gt; in depth and which we wanted to make easier to discover and consume.
We hope you like the new website and documentation structure and it helps you to find all the information you’re looking for better than before.
If you run into any issues (formatting glitches, broken links etc.), please let us know.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Refer to the &lt;a href=&quot;https://debezium.io/releases/0.10/release-notes#release-0-10-0-cr1&quot;&gt;0.10.0.CR1 release notes&lt;/a&gt; for the complete list of addressed issues and the upgrading procedure.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As always, many thanks to all the awesome people from the Debezium community who contributed to this release:
&lt;a href=&quot;https://github.com/garrett528&quot;&gt;Andrew Garrett&lt;/a&gt;,
&lt;a href=&quot;https://github.com/bingqinzhou&quot;&gt;Bingqin Zhou&lt;/a&gt;
&lt;a href=&quot;https://github.com/cscetbon&quot;&gt;Cyril Scetbon&lt;/a&gt;,
&lt;a href=&quot;https://github.com/willome&quot;&gt;Guillaume Rosauro&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ivanobulo&quot;&gt;Ivan Luzyanin&lt;/a&gt;,
&lt;a href=&quot;https://github.com/levzem&quot;&gt;Lev Zemlyanov&lt;/a&gt; and
&lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/09/05/website-documentation-overhaul/</id>
<title>Site and Documentation Overhaul</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-09-05T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/09/05/website-documentation-overhaul/" rel="alternate" type="text/html" />
<author>
<name>Chris Cranford</name>
</author>
<category term="community"></category>
<category term="news"></category>
<category term="website"></category>
<summary>



This past summer has been a super exciting time for the team.
Not only have we been working hard on Debezium 0.10 but we have unveiled some recent changes to debezium.io.




New Releases Page


It is important that the Debezium community be able to find information easily about a given release series.
We have introduced a new Releases area on the site that describes details about each release series (e.g. 0.9, the current stable release and 0.10, the current development release) such as:




What database or Apache Kafka (Connect) platforms were tested


What Java version is supported


How to migrate to a specific release series


Where to download...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This past summer has been a super exciting time for the team.
Not only have we been working hard on Debezium 0.10 but we have unveiled some recent changes to &lt;a href=&quot;https://debezium.io/&quot;&gt;debezium.io&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;new_releases_page&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_releases_page&quot; /&gt;New Releases Page&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It is important that the Debezium community be able to find information easily about a given release series.
We have introduced a new &lt;a href=&quot;https://debezium.io/releases&quot;&gt;Releases&lt;/a&gt; area on the site that describes details about each release series (e.g. &lt;a href=&quot;https://debezium.io/releases/0.9&quot;&gt;0.9&lt;/a&gt;, the current &lt;em&gt;stable release&lt;/em&gt; and &lt;a href=&quot;https://debezium.io/releases/0.10&quot;&gt;0.10&lt;/a&gt;, the current &lt;em&gt;development&lt;/em&gt; release) such as:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;What database or Apache Kafka (Connect) platforms were tested&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What Java version is supported&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How to migrate to a specific release series&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Where to download the connectors or other series artifacts&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What changes were introduced in that series&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;And much more…​&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The goal is to make it simple and easy to find all information about a specific release series in a single place.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;new_documentation&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_documentation&quot; /&gt;New Documentation&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Probably one of the most limiting factors with how our documentation was published previously is that it focused solely on the idea of the latest stable version.
Also the documentation sources were separated from the actual code sources in the main code repository.
This presented several drawbacks:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Confusing to users of older releases&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prevented publishing documentation for development versions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Caused friction for contributors when implementing new features that need documentation updates&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;What we felt we needed to provide the community was documentation published by version.
This would allow documentation to be tailored specific to that version, allowing fluid changes for future versions without impact to prior versions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Such a solution also has the benefit that it enables the Debezium team to publish development version documentation easily, which is a critical step in helping users who test unstable releases.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With &lt;a href=&quot;https://antora.org/&quot;&gt;Antora&lt;/a&gt; we found a toolchain which addresses these needs.
It allows us to maintain different versions of the documentation right next to the actual code and aggregate them on the website.
Going forward, Debezium documentation can be found at &lt;a href=&quot;https://debezium.io/documentation&quot;&gt;Reference documentation&lt;/a&gt;.
This page allows visitors to quickly navigate to documentation for a specific version.
Once in the documentation, you can quickly navigate between various Debezium versions easily.
Built by the friendly folks behind AsciiDoctor, Antora comes with lots of well thought out details;
e.g. there’s an &quot;Edit this Page&quot; link on each page, which makes it very easy to create GitHub pull requests with documentation fixes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Finally, we also took the time to fill the long-standing place holder pages describing the Debezium &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/architecture.html&quot;&gt;Architecture&lt;/a&gt; and &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/features.html&quot;&gt;Features&lt;/a&gt; with some actual contents.
Woohoo!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;feedback&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#feedback&quot; /&gt;Feedback&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We certainly hope these recent changes make it much easier for the community.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If something is unclear, could be improved, or worse a link that isn’t working, we welcome feedback.
You can report such concerns to us by &lt;a href=&quot;https://issues.redhat.com/browse/DBZ&quot;&gt;opening an issue&lt;/a&gt; for our website.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/08/20/debezium-0-10-0-beta4-released/</id>
<title>Debezium 0.10.0.Beta4 Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-08-20T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/08/20/debezium-0-10-0-beta4-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



The temperatures are slowly cooling off after the biggest summer heat,
an the Debezium community is happy to announce the release of Debezium 0.10.0.Beta4.
In this release we&#8217;re happy to share some news we don&#8217;t get to share too often:
with Apache Cassandra,
another database gets added to the list of databases supported by Debezium!


In addition, we finished our efforts for rebasing the existing Postgres connector to Debezium framework structure established for the SQL Server and Oracle connectors.
This means more shared coded between these connectors, and in turn reduced maintenance efforts for the development team going forward;
but there&#8217;s one immediately tangible advantage for you...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The temperatures are slowly cooling off after the biggest summer heat,
an the Debezium community is happy to announce the release of Debezium &lt;strong&gt;0.10.0.Beta4&lt;/strong&gt;.
In this release we’re happy to share some news we don’t get to share too often:
with &lt;a href=&quot;http://cassandra.apache.org/&quot;&gt;Apache Cassandra&lt;/a&gt;,
another database gets added to the list of databases supported by Debezium!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In addition, we finished our efforts for rebasing the existing Postgres connector to Debezium framework structure established for the SQL Server and Oracle connectors.
This means more shared coded between these connectors, and in turn reduced maintenance efforts for the development team going forward;
but there’s one immediately tangible advantage for you coming with this, too:
the Postgres connector now exposes the same metrics you already know from the other connectors.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Finally, the new release contains a range of bugfixes and other useful improvements.
Let’s explore some details below.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;incubating_cassandra_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#incubating_cassandra_connector&quot; /&gt;Incubating Cassandra Connector&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you have been following this blog lately, you’ll have read about the latest addition to the Debezium family
in Joy Gao’s excellent posts about the new connector
(&lt;a href=&quot;https://debezium.io/blog/2019/07/12/streaming-cassandra-at-wepay-part-1/&quot;&gt;part 1&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/07/15/streaming-cassandra-at-wepay-part-2/&quot;&gt;part 2&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In case you haven’t read those yet, we’d highly recommend to do so in order to learn more about the challenges encountered when implementing a CDC connector for a distributed datastore such as Cassandra as well as the design decisions made in order to come up with a first &quot;minimal viable product&quot;.
Joy also did a &lt;a href=&quot;https://www.infoq.com/presentations/wepay-database-streaming/&quot;&gt;great talk at QCon&lt;/a&gt; last year, which touches on the topic of CDC for Cassandra.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Having been originally developed internally at &lt;a href=&quot;https://debezium.io/blog/2017/02/22/Debezium-at-WePay/&quot;&gt;long-term Debezium user&lt;/a&gt; WePay,
the &lt;a href=&quot;https://wecode.wepay.com/&quot;&gt;WePay team&lt;/a&gt; decided to open-source their work, put it under the Debezium umbrella and continue to evolve it there.
That’s really great news for the Debezium community!
We couldn’t be happier about this contribution and look forward to evolving this new connector together in the open.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;At this point the Cassandra connector is in &quot;incubating&quot; state,
i.e. its design and implementation are still pretty much in flux, the event structure which it creates may change in future releases etc.
Note that, unlike the other Debezium connectors, this one currently is not based on Kafka Connect.
Instead, it is implemented as a standalone process running on Cassandra node(s) themselves.
Refer to the blog posts linked above for the reasoning behind this design and possible future developments around this.
Needless to say, any ideas and contributions in this area will be highly welcomed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Together with the connector we’ve also provided an initial draft of the &lt;a href=&quot;https://debezium.io/docs/connectors/cassandra&quot;&gt;connector documentation&lt;/a&gt;;
this is still work-in-progress and will be amended in the next few days.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;further_new_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#further_new_features&quot; /&gt;Further New Features&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Postgres connector supports the metrics known from SQL Server and Oracle now (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-777&quot;&gt;DBZ-777&lt;/a&gt;).
When using the SQL Server connector, it is now ensured that tables are snapshotted in a deterministic order,
as defined by the given table whitelist configuration (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1254&quot;&gt;DBZ-1254&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There have also been two improvements to our SMTs (single message transformations):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The SMT for &lt;a href=&quot;https://debezium.io/docs/configuration/event-flattening/&quot;&gt;new record state extraction&lt;/a&gt; allows to add additional columns for propagating metadata fields from the &lt;code&gt;source&lt;/code&gt; block
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1395&quot;&gt;DBZ-1395&lt;/a&gt;, e.g. useful to propagate the transaction into sink tables).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The default structure produced by the &lt;a href=&quot;https://debezium.io/docs/configuration/outbox-event-router/&quot;&gt;outbox routing SMT&lt;/a&gt; has been further streamlined (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1385&quot;&gt;DBZ-1385&lt;/a&gt;);
the message value will now only contain the contents of the configured outbox table payload column.
In case you want to re-add the &lt;code&gt;eventType&lt;/code&gt; value, you can configure it as an &quot;additional field&quot;,
which either goes into the message as a header (recommended) or into the message value,
which as before will be a nested structure then.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;bugfixes_and_other_improvements&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bugfixes_and_other_improvements&quot; /&gt;Bugfixes and Other Improvements&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Finally, here’s an overview of asorted bugfixes in the 0.10 Beta4 release:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The MySQL connector handles &lt;code&gt;GRANT DELETE ON &amp;lt;table&amp;gt;&lt;/code&gt; statements correctly (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1411&quot;&gt;DBZ-1411&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Superfluous tables scans are avoided when using the &lt;code&gt;initial_schema_only&lt;/code&gt; snapshot strategy with SQL Server (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1417&quot;&gt;DBZ-1417&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The superfluous creation of connections is avoided when obtaining the xmin position of Postgres (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1381&quot;&gt;DBZ-1381&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The new record state extraction SMT handles heartbeat events correctly (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1430&quot;&gt;DBZ-1430&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please refer to the 0.10.0.Beta4 &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-10-0-beta4&quot;&gt;release notes&lt;/a&gt; for the complete list of addressed issues and the upgrading procedure.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A big thank you goes out to all the contributors from the Debezium community who worked on this release:
&lt;a href=&quot;https://github.com/jgao54&quot;&gt;Joy Gao&lt;/a&gt;,
&lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt; and
&lt;a href=&quot;https://github.com/willome&quot;&gt;Guillaume Rosauro&lt;/a&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/07/25/debezium-0-10-0-beta3-released/</id>
<title>Debezium 0.10.0.Beta3 Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-07-25T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/07/25/debezium-0-10-0-beta3-released/" rel="alternate" type="text/html" />
<author>
<name>Jiri Pechanec</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



The summer is at its peak but Debezium community is not relenting in its effort so the Debezium 0.10.0.Beta3 is released.


This version not only continues in incremental improvements of Debezium but also brings new shiny features.


All of you who are using PostgreSQL 10 and higher as a service offered by different cloud providers definitely felt the complications when you needed to deploy logical decoding plugin necessary to enable streaming.
This is no longer necessary. Debezium now supports (DBZ-766) pgoutput replication protocol that is available out-of-the-box since PostgreSQL 10.


There is a set of further minor improvements.
The tombstones for deletes are configurable for...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The summer is at its peak but Debezium community is not relenting in its effort so the Debezium &lt;strong&gt;0.10.0.Beta3&lt;/strong&gt; is released.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This version not only continues in incremental improvements of Debezium but also brings new shiny features.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;All of you who are using PostgreSQL 10 and higher as a service offered by different cloud providers definitely felt the complications when you needed to deploy logical decoding plugin necessary to enable streaming.
This is no longer necessary. Debezium now supports (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-766&quot;&gt;DBZ-766&lt;/a&gt;) &lt;a href=&quot;https://www.postgresql.org/docs/10/protocol-logical-replication.html&quot;&gt;pgoutput&lt;/a&gt; replication protocol that is available out-of-the-box since PostgreSQL 10.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There is a set of further minor improvements.
The tombstones for deletes are configurable for all connectors now (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1365&quot;&gt;DBZ-1365&lt;/a&gt;).
Also tables without primary keys are now supported for all connectors (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-916&quot;&gt;DBZ-916&lt;/a&gt;).
This further reduces the gap between old and new connectors capabilities.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There are improvements for heartbeat system.
Heartbeat messages now contain the timestamp (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1363&quot;&gt;DBZ-1363&lt;/a&gt;) of when they were created in their body.
The new messages are properly skipped by the Outbox router (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1388&quot;&gt;DBZ-1388&lt;/a&gt;).
MySQL connector additionally uses heartbeats for &lt;code&gt;BinlogReader&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1338&quot;&gt;DBZ-1338&lt;/a&gt;).
MongoDB connector now utilizes heartbeats too (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1198&quot;&gt;DBZ-1198&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As we now that metrics are very important for keeping Debezium happy in production we have extended the set of supported metrics.
A new metric count of events in error (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1222&quot;&gt;DBZ-1222&lt;/a&gt;) is added so it is easy to monitor any non-standards in processing.
Database history recovery can take a long time during startup so it is now possible to monitor the progress of it (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1356&quot;&gt;DBZ-1356&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The other changes include updating of Docker images to use Kafka 2.3.0 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1358&quot;&gt;DBZ-1358&lt;/a&gt;).
PostgreSQL supports lockless snapshotting (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1238&quot;&gt;DBZ-1238&lt;/a&gt;) and Outbox router now  process delete messages (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1320&quot;&gt;DBZ-1320&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We continue with stabilization of the 0.10 release line, with lots of bug fixes to the different connectors.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Multiple defects in MySQL parser have been fixed (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1398&quot;&gt;DBZ-1398&lt;/a&gt;, (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1397&quot;&gt;DBZ-1397&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1376&quot;&gt;DBZ-1376&lt;/a&gt;) and &lt;code&gt;SAVEPOINT&lt;/code&gt; statements are no longer recorded in database history (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-794&quot;&gt;DBZ-794&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Under certain circumstances, it was possible that PostgreSQL connector lost the first event while switching to streaming from the snapshot (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1400&quot;&gt;DBZ-1400&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please refer to the 0.10.0.Beta3 &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-10-0-beta3&quot;&gt;release notes&lt;/a&gt; to learn more about all resolved issues and the upgrading procedure.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Many thanks to everybody from the Debezium community who contributed to this release:
&lt;a href=&quot;https://github.com/addisonj&quot;&gt;Addison Higham&lt;/a&gt;,
&lt;a href=&quot;https://github.com/BinLi1988&quot;&gt;Bin Li&lt;/a&gt;,
&lt;a href=&quot;https://github.com/brbrown25&quot;&gt;Brandon Brown&lt;/a&gt; and
&lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/07/15/streaming-cassandra-at-wepay-part-2/</id>
<title>Streaming Cassandra at WePay - Part 2</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-07-15T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/07/15/streaming-cassandra-at-wepay-part-2/" rel="alternate" type="text/html" />
<author>
<name>Joy Gao</name>
</author>
<category term="cassandra"></category>
<summary>



This post originally appeared on the WePay Engineering blog.


In the first half of this blog post series, we explained our decision-making process of designing a streaming data pipeline for Cassandra at WePay. In this post, we will break down the pipeline into three sections and discuss each of them in more detail:




Cassandra to Kafka with CDC agent


Kafka with BigQuery with KCBQ


Transformation with BigQuery view






Cassandra to Kafka with CDC Agent


The Cassandra CDC agent is a JVM process that is intended to be deployed on each node in a Cassandra cluster. The agent is comprised of several interdependent processors, running concurrently and...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;This post originally appeared on the &lt;a href=&quot;https://wecode.wepay.com/posts/streaming-cassandra-at-wepay-part-2&quot;&gt;WePay Engineering blog&lt;/a&gt;.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the first half of this blog post series, we explained our decision-making process of designing a streaming data pipeline for Cassandra at WePay. In this post, we will break down the pipeline into three sections and discuss each of them in more detail:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;Cassandra to Kafka with CDC agent&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kafka with BigQuery with KCBQ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Transformation with BigQuery view&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;cassandra_to_kafka_with_cdc_agent&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#cassandra_to_kafka_with_cdc_agent&quot; /&gt;Cassandra to Kafka with CDC Agent&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Cassandra CDC agent is a JVM process that is intended to be deployed on each node in a Cassandra cluster. The agent is comprised of several interdependent processors, running concurrently and working together to publish change events to Kafka.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;snapshot_processor&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#snapshot_processor&quot; /&gt;Snapshot Processor&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This processor is responsible for bootstrapping new tables. It looks up the CDC configuration to determine the snapshot mode, and performs snapshot on CDC-enabled tables if needed. To snapshot a table, the agent performs a full table scan and converts each row in the result set into an individual create event, and then sequentially enqueues them to an in-memory &lt;a href=&quot;https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/BlockingQueue.html&quot;&gt;BlockingQueue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;commit_log_processor&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#commit_log_processor&quot; /&gt;Commit Log Processor&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This processor is responsible for watching the CDC directory for new commit logs, parsing the commit log files via Cassandra’s &lt;a href=&quot;https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogReader.java&quot;&gt;&lt;code&gt;CommitLogReader&lt;/code&gt;&lt;/a&gt;, transforming deserialized mutations into standardized change events, and finally enqueuing them to the same queue as the snapshot processor.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;At this point, some readers may have concerns in regard to running Snapshot Processor and Commit Log Processors concurrently rather than serially. The reason is that Cassandra uses a &lt;a href=&quot;https://datastax.github.io/cpp-driver/topics/basics/client_side_timestamps/&quot;&gt;client-side timestamp&lt;/a&gt; to determine event order, and resolves conflicts with last write wins. This client-side timestamp is deliberately stored in each change event. This is why snapshotting doesn’t have to proceed commit log processing – the ordering is determined later on when the data is queried in the data warehouse.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;queue_processor&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#queue_processor&quot; /&gt;Queue Processor&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This processor is responsible for dequeuing change events, transforming them into &lt;a href=&quot;https://avro.apache.org/docs/1.8.1/spec.html&quot;&gt;Avro&lt;/a&gt; records, and sending them to Kafka via a Kafka producer. It also tracks the position of the most recently sent event, so that on restart it is able to pick up from where it left off.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Implementing an in-memory queue in the CDC agent seems like overkill at first. Given there is only a single thread doing the enqueue and another thread doing the dequeue, the performance boost is negligible. The motivation here is to decouple the work of parsing commit logs, which should be done serially in the right order, from the work of serializing and publishing Kafka events, which can be parallelized by multiple threads for different tables. Although such parallelization is not implemented at the moment, we want the flexibility of adding this feature in the near future.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Some may also wonder why &lt;a href=&quot;https://docs.confluent.io/current/connect/index.html&quot;&gt;Kafka Connect&lt;/a&gt; is not used here as it seems like a natural fit for streaming. It is a great option if we wanted distributed parallel processing with fault tolerance. However, it is more complicated to deploy, monitor, and debug than a Kafka producer. For the purpose of building a minimum viable infrastructure, we chose Kafka producer at the time.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;schema_processor&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#schema_processor&quot; /&gt;Schema Processor&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In order to support automatic schema evolution, this processor periodically polls the database for the latest table schema, and updates the in-memory schema cache if a change is detected. Snapshot Processor and Commit Log Processor both look up table schema from this cache and attach it as part of the change event prior to enqueue. Then upon dequeue, the Queue Processor transforms the attached table schema into an Avro schema for record serialization.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;commit_log_post_processor&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#commit_log_post_processor&quot; /&gt;Commit Log Post Processor&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This processor is responsible for cleaning up commit logs after they have been processed. The default Commit Log Post Processor implementation will simply perform deletion. A custom Commit Log Post Processor can be configured for use case such as archiving commit log files to &lt;a href=&quot;https://aws.amazon.com/s3/&quot;&gt;S3&lt;/a&gt; or &lt;a href=&quot;https://cloud.google.com/storage/&quot;&gt;GCS&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;kafka_to_bigquery_with_kcbq&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#kafka_to_bigquery_with_kcbq&quot; /&gt;Kafka to BigQuery with KCBQ&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once the events arrive in Kafka, we use KCBQ to send the events data to BigQuery without performing special transformations, just like in our &lt;a href=&quot;https://wecode.wepay.com/posts/streaming-databases-in-realtime-with-mysql-debezium-kafka&quot;&gt;MySQL streaming data pipeline&lt;/a&gt;. We have written a previous &lt;a href=&quot;https://wecode.wepay.com/posts/kafka-bigquery-connector&quot;&gt;blog post&lt;/a&gt; explaining this connector in more details.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;transformation_with_bigquery_view&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#transformation_with_bigquery_view&quot; /&gt;Transformation with BigQuery View&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once the events are in BigQuery, this is where the heavy-lifting is being done. We create &lt;a href=&quot;https://cloud.google.com/bigquery/docs/views-intro&quot;&gt;virtual views&lt;/a&gt; on top of the raw tables to merge the data in a way that mirrors the source table in Cassandra. Note that each row in the raw tables contains limited data – only columns that have been modified have states. This means selecting the latest row for each primary key will not provide us with data that is consistent with source. Instead, the query must identify the latest cell in each column for each primary key. This can be achieved with self-joins on the primary key for each column in the table. Although joins are slow in MySQL, BigQuery’s parallel execution engine and columnar storage makes this possible. A view on top of a 1TB Cassandra table in BigQuery takes about 100 seconds to query.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;compaction&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#compaction&quot; /&gt;Compaction&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The fact that the BigQuery view is virtual implies each time the view is queried essentially triggers a full compaction of the raw data. This means the cost will go up with the number of queries, not to mention the duplicated events amplifies the amount of data that needs to be processed by a factor of N, where N is the replication factor. To save cost and improve performance, periodic compaction by materializing the view is necessary.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;future_development_work&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#future_development_work&quot; /&gt;Future Development Work&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;support_for_cassandra_4_0&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#support_for_cassandra_4_0&quot; /&gt;Support for Cassandra 4.0&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In Cassandra 4.0, the improved CDC feature allows the connector to be able to parse events in real-time as they are written rather than in micro-batches on each commit log flush. This reduces latency substantially.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;performance_optimization&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#performance_optimization&quot; /&gt;Performance Optimization&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As mentioned earlier, there is a single thread responsible for dequeuing, serializing, and publishing Kafka records. However, as the write throughput increases, if the performance of the agent does not keep up, it would result in a backlog of unprocessed commit logs which could potentially impact the health of our production database. The next step is to leverage parallel processing of events to optimize performance.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;streamline_with_debezium_and_kafka_connect&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#streamline_with_debezium_and_kafka_connect&quot; /&gt;Streamline with Debezium and Kafka Connect&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We initially built the Cassandra CDC agent as a standalone project. Now that it is open-sourced as a &lt;a href=&quot;https://debezium.io/&quot;&gt;Debezium&lt;/a&gt; connector, we can replace some of our custom classes with existing ones in Debezium. Another improvement is to support common features that all Debezium connectors have, such as support for multiple serialization formats. Finally, the CDC agent is not fault tolerant; robust alert and monitoring are required as part of deployment. One area to explore in the future is to build the CDC agent on top of Kafka Connect as a source connector, this further streamlines the Cassandra connector with other Debezium connectors, and provides scalability and fault tolerance for free.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;closing_remarks&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#closing_remarks&quot; /&gt;Closing Remarks&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Cassandra being a peer-to-peer distributed database poses some really interesting challenges for CDC that do not exist in relational databases like MySQL and Postgres, or even a single-master NoSQL database like MongoDB. Note that it is worth evaluating the limitations before rolling out your own real-time data pipeline for Cassandra.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Besides understanding Cassandra internals, we learned a few lessons on engineering productivity along the way:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;minimum_viable_product_philosophy&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#minimum_viable_product_philosophy&quot; /&gt;Minimum Viable Product Philosophy&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;By stripping away all features except for the essentials, we were able to build, test, and deploy a working solution in a reasonable time with limited resources. Had we aimed to design a pipeline that encompasses all features upfront, it would have taken a lot longer and required much more resources.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;community_involvement&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#community_involvement&quot; /&gt;Community Involvement&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Cassandra is an open-source project. Rather than tackling the problem solo, we were engaged with the Cassandra community from the very start (i.e. sharing experiences with committers and users via &lt;a href=&quot;https://www.meetup.com/Apache-Cassandra-Bay-Area/&quot;&gt;meetups&lt;/a&gt;, &lt;a href=&quot;https://user.cassandra.apache.narkive.com/njOxVaxP/using-cdc-feature-to-stream-c-to-kafka-design-proposal&quot;&gt;discussing proposals in mailing list&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/watch?v=0K0fYHsFBZg&quot;&gt;presenting proof-of-concept in conferences&lt;/a&gt;, etc.); all of which provided us with valuable feedback throughout the design and implementation stages.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/07/12/streaming-cassandra-at-wepay-part-1/</id>
<title>Streaming Cassandra at WePay - Part 1</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-07-12T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/07/12/streaming-cassandra-at-wepay-part-1/" rel="alternate" type="text/html" />
<author>
<name>Joy Gao</name>
</author>
<category term="cassandra"></category>
<category term="featured"></category>
<summary>



This post originally appeared on the WePay Engineering blog.


Historically, MySQL had been the de-facto database of choice for microservices at WePay. As WePay scales, the sheer volume of data written into some of our microservice databases demanded us to make a scaling decision between sharded MySQL (i.e. Vitess) and switching to a natively sharded NoSQL database. After a series of evaluations, we picked Cassandra, a NoSQL database, primarily because of its high availability, horizontal scalability, and ability to handle high write throughput.




Batch ETL Options


After introducing Cassandra to our infrastructure, our next challenge was to figure out a way to expose...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;This post originally appeared on the &lt;a href=&quot;https://wecode.wepay.com/posts/streaming-cassandra-at-wepay-part-1&quot;&gt;WePay Engineering blog&lt;/a&gt;.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Historically, MySQL had been the de-facto database of choice for microservices at WePay. As WePay scales, the sheer volume of data written into some of our microservice databases demanded us to make a scaling decision between sharded MySQL (i.e. &lt;a href=&quot;https://vitess.io&quot;&gt;Vitess&lt;/a&gt;) and switching to a natively sharded NoSQL database. After a series of evaluations, we picked Cassandra, a NoSQL database, primarily because of its high availability, horizontal scalability, and ability to handle high write throughput.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;batch_etl_options&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#batch_etl_options&quot; /&gt;Batch ETL Options&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;After introducing Cassandra to our infrastructure, our next challenge was to figure out a way to expose data in Cassandra to &lt;a href=&quot;https://cloud.google.com/bigquery/&quot;&gt;BigQuery&lt;/a&gt;, our data warehouse, for analytics and reporting. We quickly built an Airflow &lt;a href=&quot;https://github.com/apache/airflow/blob/master/airflow/contrib/hooks/cassandra_hook.py&quot;&gt;hook&lt;/a&gt; and &lt;a href=&quot;https://github.com/apache/airflow/blob/master/airflow/contrib/operators/cassandra_to_gcs.py&quot;&gt;operator&lt;/a&gt; to execute full loads. This obviously doesn’t scale, as it rewrites the entire database on each load. To scale the pipeline, we evaluated two incremental load approaches, but both have their shortcomings:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;Range query. This is a common ETL approach where data is extracted via a range query at regular intervals, such as hourly or daily. Anyone familiar with &lt;a href=&quot;https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key&quot;&gt;Cassandra data modelling&lt;/a&gt; would quickly realize how unrealistic this approach is. Cassandra tables need to be modeled to optimize query patterns used in production. Adding this query pattern for analytics in most cases means cloning the table with different clustering keys. RDBMS folks might suggest secondary index to support this query pattern, but &lt;a href=&quot;https://pantheon.io/blog/cassandra-scale-problem-secondary-indexes&quot;&gt;secondary index in Cassandra are local&lt;/a&gt;, therefore this approach would pose performance and scaling issues of its own.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Process unmerged SSTables. SSTables are Cassandra’s immutable storage files. Cassandra offers a &lt;a href=&quot;https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/tools/ToolsSSTabledump.html&quot;&gt;sstabledump&lt;/a&gt; CLI command that converts SSTable content into human-readable JSON. However, Cassandra is built on top of the concept of &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-structured_merge-tree&quot;&gt;Log-Structured Merge (LSM) Tree&lt;/a&gt;, meaning SSTables merge periodically into new compacted files. Depending on the compaction strategy, detecting unmerged SSTable files out-of-band may be challenging (we later learned about the &lt;a href=&quot;https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/operations/opsBackupIncremental.html&quot;&gt;incremental backup&lt;/a&gt; feature in Cassandra which only backs up uncompacted SSTables; so this approach would have worked as well.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Given these challenges, and having built and operated a &lt;a href=&quot;https://wecode.wepay.com/posts/streaming-databases-in-realtime-with-mysql-debezium-kafka&quot;&gt;streaming data pipeline for MySQL&lt;/a&gt;, we began to explore streaming options for Cassandra.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;streaming_options&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#streaming_options&quot; /&gt;Streaming Options&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;double_writing&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#double_writing&quot; /&gt;Double-Writing&lt;/h3&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/cassandra/double-write.png&quot; class=&quot;responsive-image&quot; alt=&quot;Image showing writer send two distinct writes&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The idea is to publish to Kafka every time a write is performed on Cassandra. This double-writing could be performed via the built-in trigger or a custom wrapper around the client. There are performance problems with this approach. First, due to the fact that we now need to write to two systems instead of one, write latency is increased. More importantly, when a write to one system fails due to a timeout, whether the write is successful or not is indeterministic. To guarantee data consistency on both systems, we would have to implement &lt;a href=&quot;https://en.wikipedia.org/wiki/Distributed_transaction&quot;&gt;distributed transactions&lt;/a&gt;, but multiple roundtrips for consensus will increase latency and reduce throughput further. This defeats the purpose of a high write-throughput database.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;kafka_as_event_source&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#kafka_as_event_source&quot; /&gt;Kafka as Event Source&lt;/h3&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/cassandra/event-source.png&quot; class=&quot;responsive-image&quot; alt=&quot;Image showing writes sent to Kafka and then downstream DB&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The idea is to write to Kafka rather than directly writing to Cassandra; and then apply the writes to Cassandra by consuming events from Kafka. Event sourcing is a pretty popular approach these days. However, if you already have existing services directly writing to Cassandra, it would require a change in application code and a nontrivial migration. This approach also violates &lt;a href=&quot;https://docs.oracle.com/cd/E17076_05/html/gsg_db_rep/C/rywc.html&quot;&gt;read-your-writes consistency&lt;/a&gt;: the requirement that if a process performs a write, then the same process performing a subsequent read must observe the write’s effects. Since writes are routed through Kafka, there will be a lag between when the write is issued and when it is applied; during this time, reads to Cassandra will result in stale data. This may cause unforeseeable production issues.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;parsing_commit_logs&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#parsing_commit_logs&quot; /&gt;Parsing Commit Logs&lt;/h3&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/cassandra/commit-log.png&quot; class=&quot;responsive-image&quot; alt=&quot;Image showing commit logs sent to Kafka&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Cassandra introduced a &lt;a href=&quot;http://cassandra.apache.org/doc/3.11.3/operating/cdc.html&quot;&gt;change data capture (CDC) feature&lt;/a&gt; in 3.0 to expose its commit logs. Commit logs are write-ahead logs in Cassandra designed to provide durability in case of machine crashes. They are typically discarded upon flush. With CDC enabled, they are instead transferred to a local CDC directory upon flush, which is then readable by other processes on the Cassandra node. This allows us to use the same CDC mechanism as in our MySQL streaming pipeline. It decouples production operations from analytics, and thus does not require additional work from application engineers.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Ultimately, after considering throughput, consistency, and separation of concerns, the final option – parsing commit logs – became the top contender.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;commit_log_deep_dive&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#commit_log_deep_dive&quot; /&gt;Commit Log Deep Dive&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Aside from exposing commit logs, Cassandra also provides &lt;code&gt;CommitLogReader&lt;/code&gt; and &lt;code&gt;CommitLogReadHandler&lt;/code&gt; classes to help with the deserialization of logs. It seems like the hard work has been done, and what’s left is applying transformations – converting deserialized representations into Avro records and publish them to Kafka. However, as we dug further into the implementation of the CDC feature and of Cassandra itself, we realized that there are many new challenges.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;delayed_processing&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#delayed_processing&quot; /&gt;Delayed Processing&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Commit logs only arrive in the CDC directory when it is full, in which case it would be flushed/discarded. This implies there is a delay between when the event is logged and when the event is captured. If little to no writes are executed, then the delay in event capturing could be arbitrarily long.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;space_management&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#space_management&quot; /&gt;Space Management&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In MySQL you can set binlog retention such that the logs will be automatically deleted after the configured retention period. However in Cassandra there is no such option. Once the commit logs are transferred to CDC directory, consumption must be in place to clean up commit logs after processing. If the available disk space for CDC directory exceeds a given threshold, further writes to the database will be rejected.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;duplicated_events&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#duplicated_events&quot; /&gt;Duplicated Events&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Commit logs on an individual Cassandra node do not reflect all writes to the cluster; they only reflect writes to the node. This makes it necessary to process commit logs on all nodes. But with a replication factor of N, N copies of each event are sent downstream.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;out_of_order_events&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#out_of_order_events&quot; /&gt;Out-of-Order Events&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Writes to an individual Cassandra node are logged serially as they arrive. However, these events may arrive out-of-order from when they are issued. Downstream consumers of these events must understand the event time and implement last write wins logic similar to &lt;a href=&quot;https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/dml/dmlAboutReads.html&quot;&gt;Cassandra’s read path&lt;/a&gt; to get the correct result.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;out_of_band_schema_change&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#out_of_band_schema_change&quot; /&gt;Out-of-Band Schema Change&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Schema changes of tables are communicated via a &lt;a href=&quot;https://en.wikipedia.org/wiki/Gossip_protocol&quot;&gt;gossip protocol&lt;/a&gt; and are not recorded in commit logs. Therefore changes in schema could only be detected on a best-effort basis.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;incomplete_row_data&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#incomplete_row_data&quot; /&gt;Incomplete Row Data&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Cassandra does not perform read before write, as a result change events do not capture the state of every column, they only capture the state of modified columns. This makes the change event less useful than if the full row is available.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once we acquired a deep understanding of Cassandra commit logs, we re-assessed our requirements against the given constraints in order to design a &lt;a href=&quot;https://riccomini.name/minimum-viable-infrastructure&quot;&gt;minimum viable infrastructure&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;minimum_viable_infrastructure&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#minimum_viable_infrastructure&quot; /&gt;Minimum Viable Infrastructure&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Borrowing from the &lt;a href=&quot;https://en.wikipedia.org/wiki/Minimum_viable_product&quot;&gt;minimum viable product&lt;/a&gt; philosophy, we want to design a data pipeline with a minimum set of features and requirements to satisfy our immediate customers. For Cassandra CDC, this means:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Production database’s health and performance should not be negatively impacted by introducing CDC; slowed operations and system downtimes are much costlier than a delay in the analytics pipeline&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Querying Cassandra tables in our data warehouse should match the results of querying the production database (barring delays); having duplicate and/or incomplete rows amplifies post-processing workload for every end user
With these criteria in front of us, we began to brainstorm for solutions, and ultimately came up with three approaches:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;stateless_stream_processing&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#stateless_stream_processing&quot; /&gt;Stateless Stream Processing&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This solution is inspired by Datastax’s &lt;a href=&quot;https://www.datastax.com/dev/blog/advanced-replication-in-dse-5-1&quot;&gt;advanced replication blog post&lt;/a&gt;. The idea is to deploy an agent on each Cassandra node to process local commit logs. Each agent is considered as “primary” for a subset of writes based on partition keys, such that every event has exactly one primary agent. Then during CDC, in order to avoid duplicate events, each agent only sends an event to Kafka if it is the primary agent for the event. To handle eventual consistency, each agent would sort events into per-table time-sliced windows as they arrive (but doesn’t publish them right away); when a window expires, events in that window are hashed, and the hash is compared against other nodes. If they don’t match, data is fetched from the inconsistent node so the correct value could be resolved by last write wins. Finally the corrected events in that window will be sent to Kafka. Any out-of-order event beyond the time-sliced windows would have to be logged into an out-of-sequence file and handled separately. Since deduplication and ordering are done in-memory, concerns with agent failover causing data loss, OOM issues impacting production database, and the overall complexity of this implementation stopped us from exploring it further.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;stateful_stream_processing&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#stateful_stream_processing&quot; /&gt;Stateful Stream Processing&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This solution is the most feature rich. The idea is that the agent on each Cassandra node will process commit logs and publish events to Kafka without deduplication and ordering. Then a stream processing engine will consume these raw events and do the heavy lifting (such as filtering out duplicate events with a cache, managing event orders with event-time windowing, and capturing state of unmodified columns by performing read before write on a state store), and then publish these derived events to a separate Kafka topic. Finally, &lt;a href=&quot;https://github.com/wepay/kafka-connect-bigquery&quot;&gt;KCBQ&lt;/a&gt; will be used to consume events from this topic and upload them to BigQuery. This approach is appealing because it solves the problem generically – anyone can subscribe to the latter Kafka topic without needing to handle deduplication and ordering on their own. However, this approach introduces a nontrivial amount of operational overhead; we would have to maintain a stream processing engine, a database, and a cache.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;processing_on_read&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#processing_on_read&quot; /&gt;Processing-On-Read&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Similar to the previous approach, the idea is to process commit logs on each Cassandra node and send events to Kafka without deduplication and ordering. Unlike the previous approach, the stream processing portion is completely eliminated. Instead the raw events will be directly uploaded to BigQuery via KCBQ. &lt;a href=&quot;https://cloud.google.com/bigquery/docs/views-intro&quot;&gt;Views&lt;/a&gt; are created on top of the raw tables to handle deduplication, ordering, and merging of columns to form complete rows. Because BigQuery views are virtual tables, the processing is done lazily each time the view is queried. To prevent the view query from getting too expensive, the views would be materialized periodically. This approach removes both operational complexity and code complexity by leveraging BigQuery’s &lt;a href=&quot;https://cloud.google.com/blog/products/gcp/bigquery-under-the-hood&quot;&gt;massively parallel query engine&lt;/a&gt;. However, the drawback is that non-KCBQ downstream consumers must do all the work on their own.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Given that our main purpose of streaming Cassandra is data warehousing, we ultimately decided to implement &lt;em&gt;processing-on-read&lt;/em&gt;. It provides the essential features for our existing use case, and offers the flexibility to expand into the other two more generic solutions mentioned above in the future.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;open_source&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#open_source&quot; /&gt;Open Source&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;During this process of building a real-time data pipeline for Cassandra, we have received a substantial amount of interest on this project. As a result, we have decided to open-source the Cassandra CDC agent under the &lt;a href=&quot;https://debezium.io&quot;&gt;Debezium&lt;/a&gt; umbrella as an &lt;a href=&quot;https://github.com/debezium/debezium-incubator&quot;&gt;incubating connector&lt;/a&gt;. If you would like to learn more or contribute, check out the work-in-progress pull request for &lt;a href=&quot;https://github.com/debezium/debezium-incubator/pull/98&quot;&gt;source code&lt;/a&gt; and &lt;a href=&quot;https://github.com/debezium/debezium.github.io/pull/325&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the second half of this blog post series, we will elaborate on the CDC implementation itself in more details. Stay tuned!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/07/08/tutorial-sentry-debezium-container-images/</id>
<title>Tutorial for Adding Sentry into Debezium Container Images</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-07-08T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/07/08/tutorial-sentry-debezium-container-images/" rel="alternate" type="text/html" />
<author>
<name>Renato Mefi</name>
</author>
<category term="sentry"></category>
<category term="docker"></category>
<summary>



Debezium has received a huge improvement to the structure of its container images recently,
making it extremely simple to extend its behaviour.


This is a small tutorial showing how you can for instance add Sentry,
"an open-source error tracking [software] that helps developers monitor and fix crashes in real time".
Here we&#8217;ll use it to collect and report any exceptions from Kafka Connect and its connectors.
Note that this is only applicable for Debezium 0.9+.


We need a few things to have Sentry working, and we&#8217;ll add all of them and later have a Dockerfile which gets it all glued correctly:




Configure Log4j


SSL certificate for sentry.io, since...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium has received a huge improvement to the structure of its container images &lt;a href=&quot;https://debezium.io/blog/2019/06/03/debezium-0-10-0-alpha2-released/&quot;&gt;recently&lt;/a&gt;,
making it extremely simple to extend its behaviour.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is a small tutorial showing how you can for instance add &lt;a href=&quot;https://sentry.io/welcome/&quot;&gt;Sentry&lt;/a&gt;,
&quot;an open-source error tracking [software] that helps developers monitor and fix crashes in real time&quot;.
Here we’ll use it to collect and report any exceptions from Kafka Connect and its connectors.
Note that this is only applicable for Debezium 0.9+.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We need a few things to have Sentry working, and we’ll add all of them and later have a Dockerfile which gets it all glued correctly:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Configure Log4j&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SSL certificate for &lt;a href=&quot;https://sentry.io&quot;&gt;sentry.io&lt;/a&gt;, since it’s not by default in the JVM trusted chain&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;sentry&lt;/code&gt; and &lt;code&gt;sentry-log4j&lt;/code&gt; libraries&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;log4j_configuration&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#log4j_configuration&quot; /&gt;Log4j Configuration&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s create a file &lt;em&gt;config/log4j.properties&lt;/em&gt; in our local project which is a copy of the one shipped with Debezium images and add Sentry to it.
Note we added &lt;code&gt;Sentry&lt;/code&gt; to &lt;code&gt;log4j.rootLogger&lt;/code&gt; and created the section &lt;code&gt;log4j.appender.Sentry&lt;/code&gt;, the rest remains as the original configuration:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-config&quot; data-lang=&quot;config&quot;&gt;kafka.logs.dir=logs

log4j.rootLogger=INFO, stdout, appender, Sentry

# Disable excessive reflection warnings - KAFKA-5229
log4j.logger.org.reflections=ERROR

log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.threshold=INFO
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d{ISO8601} %-5p  %X{dbz.connectorType}|%X{dbz.connectorName}|%X{dbz.connectorContext}  %m   [%c]%n

log4j.appender.appender=org.apache.log4j.DailyRollingFileAppender
log4j.appender.appender.DatePattern=&#39;.&#39;yyyy-MM-dd-HH
log4j.appender.appender.File=${kafka.logs.dir}/connect-service.log
log4j.appender.appender.layout=org.apache.log4j.PatternLayout
log4j.appender.appender.layout.ConversionPattern=%d{ISO8601} %-5p  %X{dbz.connectorType}|%X{dbz.connectorName}|%X{dbz.connectorContext}  %m   [%c]%n

log4j.appender.Sentry=io.sentry.log4j.SentryAppender
log4j.appender.Sentry.threshold=WARN&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;sentry_io_ssl_certificate&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#sentry_io_ssl_certificate&quot; /&gt;Sentry.io SSL certificate&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Download the &lt;em&gt;getsentry.pem&lt;/em&gt; file from &lt;a href=&quot;https://docs.sentry.io/ssl/&quot;&gt;sentry.io&lt;/a&gt; and put it in your project’s directory under &lt;em&gt;ssl/&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;the_dockerfile&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_dockerfile&quot; /&gt;The Dockerfile&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now we can glue everything together in our Debezium image:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Let’s first create a JKS file with our Sentry certificate; this uses a Docker multi-stage building process, where we are generating a &lt;code&gt;certificates.jks&lt;/code&gt; which we’ll later copy into our Kafka Connect with Debezium stage&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Copy &lt;code&gt;log4j.properties&lt;/code&gt; into &lt;code&gt;$KAFKA_HOME/config/log4j.properties&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Copy the JKS file from the multi-stage build&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set ENV with the Sentry version and m5sums&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Download Sentry dependencies, the script you see called &lt;code&gt;docker-maven-download&lt;/code&gt; is a helper which we ship by default in our images.
In this case we’re using it to download a JAR file from Maven Central and put it in the Kafka libs directory.
We do that by setting the ENV var &lt;code&gt;MAVEN_DEP_DESTINATION=$KAFKA_HOME/libs&lt;/code&gt;:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-dockerfile&quot; data-lang=&quot;dockerfile&quot;&gt;FROM fabric8/java-centos-openjdk8-jdk:1.6 as ssl-jks

ARG JKS_STOREPASS=&quot;any random password, you can also set it outside via the arguments from docker build&quot;

USER root:root

COPY /ssl /ssl

RUN chown -R jboss:jboss /ssl

USER jboss:jboss

WORKDIR /ssl

RUN keytool -import -noprompt -alias getsentry \
    -storepass &quot;${JKS_STOREPASS}&quot; \
    -keystore certificates.jks \
    -trustcacerts -file &quot;/ssl/getsentry.pem&quot;

FROM debezium/connect:0.10 AS kafka-connect

EXPOSE 8083

COPY config/log4j.properties &quot;$KAFKA_HOME/config/log4j.properties&quot;

COPY --from=ssl-jks --chown=kafka:kafka /ssl/certificates.jks /ssl/

ENV SENTRY_VERSION=1.7.23 \
    MAVEN_DEP_DESTINATION=$KAFKA_HOME/libs

RUN docker-maven-download \
        central io/sentry sentry &quot;$SENTRY_VERSION&quot; 4bf1d6538c9c0ebc22526e2094b9bbde &amp;amp;&amp;amp; \
    docker-maven-download \
        central io/sentry sentry-log4j &quot;$SENTRY_VERSION&quot; 74af872827bd7e1470fd966449637a77&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;build_and_run&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#build_and_run&quot; /&gt;Build and Run&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now we can simply build the image:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ docker build -t debezium/connect-sentry:1 --build-arg=JKS_STOREPASS=&quot;123456789&quot; .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When running the image we have now to configure our Kafka Connect application to load the JKS file by setting &lt;code&gt;KAFKA_OPTS: -Djavax.net.ssl.trustStore=/ssl/certificates.jks -Djavax.net.ssl.trustStorePassword=&amp;lt;YOUR TRUSTSTORE PASSWORD&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Sentry can be &lt;a href=&quot;https://docs.sentry.io/clients/java/config/#id2&quot;&gt;configured in many ways&lt;/a&gt;, I like to do it via environment variables, the minimum we can set is the Sentry DSN (which is necessary to point to your project) and the actual running environment name (i.e.: production, staging).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In this case we can configure the variables: &lt;code&gt;SENTRY_DSN=&amp;lt;GET THE DNS IN SENTRY’S DASHBOARD&amp;gt;&lt;/code&gt;, &lt;code&gt;SENTRY_ENVIRONMENT=dev&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In case you’d like to learn more about using the Debezium container images, please &lt;a href=&quot;https://debezium.io/docs/tutorial/#starting_docker&quot;&gt;check our tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And that’s it, a basic  a recipe for extending our Docker setup using Sentry as an example;
other modifications should also be as simple as this one.
As an example how a &lt;code&gt;RecordTooLarge&lt;/code&gt; exception from the Kafka producer would look like in this setup, see the picture below:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/sentry/example-record-too-large-exception.png&quot; class=&quot;responsive-image&quot; alt=&quot;Sentry Exception example&quot;&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#conclusion&quot; /&gt;Conclusion&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Thanks to the recent refactor of the Debezium container images, it got very easy to amend them with your custom extensions.
Downloading external dependencies and adding them to the images became a trivial task and we’d love to hear your feedback about it!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you are curious about the refactoring itself, you can find the details in pull request &lt;a href=&quot;https://github.com/debezium/docker-images/pull/131&quot;&gt;debezium/docker-images#131&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/06/28/debezium-0-10-0-beta2-released/</id>
<title>Debezium 0.10.0.Beta2 Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-06-28T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/06/28/debezium-0-10-0-beta2-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



It&#8217;s my pleasure to announce the release of Debezium 0.10.0.Beta2!


This further stabilizes the 0.10 release line, with lots of bug fixes to the different connectors.
23 issues were fixed for this release;
a couple of those relate to the DDL parser of the MySQL connector,
e.g. around RENAME INDEX (DBZ-1329),
SET NEW in triggers (DBZ-1331)
and function definitions with the COLLATE keyword (DBZ-1332).


For the Postgres connector we fixed a potential inconsistency when flushing processed LSNs to the database
(DBZ-1347).
Also the "include.unknown.datatypes" option works as expected now during snapshotting
(DBZ-1335)
and the connector won&#8217;t stumple upon materialized views during snapshotting any longer
(DBZ-1345).


The SQL Server connector will use much less...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.10.0.Beta2&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This further stabilizes the 0.10 release line, with lots of bug fixes to the different connectors.
&lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.10.0.Beta2&quot;&gt;23 issues&lt;/a&gt; were fixed for this release;
a couple of those relate to the DDL parser of the MySQL connector,
e.g. around &lt;code&gt;RENAME INDEX&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1329&quot;&gt;DBZ-1329&lt;/a&gt;),
&lt;code&gt;SET NEW&lt;/code&gt; in triggers (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1331&quot;&gt;DBZ-1331&lt;/a&gt;)
and function definitions with the &lt;code&gt;COLLATE&lt;/code&gt; keyword (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1332&quot;&gt;DBZ-1332&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For the Postgres connector we fixed a potential inconsistency when flushing processed LSNs to the database
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1347&quot;&gt;DBZ-1347&lt;/a&gt;).
Also the &quot;include.unknown.datatypes&quot; option works as expected now during snapshotting
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1335&quot;&gt;DBZ-1335&lt;/a&gt;)
and the connector won’t stumple upon materialized views during snapshotting any longer
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1345&quot;&gt;DBZ-1345&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The SQL Server connector will use much less memory in many situations
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1065&quot;&gt;DBZ-1065&lt;/a&gt;)
and it’s configurable now whether it should emit tombstone events for deletions or not
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-835&quot;&gt;DBZ-835&lt;/a&gt;).
This also was added for the Oracle connector, bringing consistency for this option across all the connectors.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note that this release can be used with Apache Kafka 2.x, but not with 1.x.
This was an unintentional change and compatibility with 1.x will be restored for the Beta3 release
(the issue to track is &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1361&quot;&gt;DBZ-1361&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please refer to the 0.10.0.Beta2 &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-10-0-beta2&quot;&gt;release notes&lt;/a&gt; to learn more about all resolved issues and the upgrading procedure.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Many thanks to everybody from the Debezium community who contributed to this release:
&lt;a href=&quot;https://github.com/pan3793&quot;&gt;Cheng Pan&lt;/a&gt;,
&lt;a href=&quot;https://github.com/willome&quot;&gt;Guillaume Rosauro&lt;/a&gt;,
&lt;a href=&quot;https://github.com/szczeles&quot;&gt;Mariusz Strzelecki&lt;/a&gt; and
&lt;a href=&quot;https://github.com/ssouris&quot;&gt;Stathis Souris&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/06/19/debezium-wears-fedora/</id>
<title>Debezium Wears Fedora</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-06-19T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/06/19/debezium-wears-fedora/" rel="alternate" type="text/html" />
<author>
<name>Jiri Pechanec</name>
</author>
<category term="postgres"></category>
<category term="fedora"></category>
<category term="vagrant"></category>
<summary>



The Debezium project strives to provide an easy deployment of connectors,
so users can try and run connectors of their choice mostly by getting the right connector archive and unpacking it into the plug-in path of Kafka Connect.


This is true for all connectors but for the Debezium PostgreSQL connector.
This connector is specific in the regard that it requires a logical decoding plug-in to be installed inside the PostgreSQL source database(s) themselves.
Currently, there are two supported logical plug-ins:




postgres-decoderbufs, which uses Protocol Buffers as a very compact transport format and which is maintained by the Debezium community


JSON-based, which is based on JSON and...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium project strives to provide an easy deployment of connectors,
so users can try and run connectors of their choice mostly by getting the right connector archive and unpacking it into the plug-in path of Kafka Connect.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is true for all connectors but for the &lt;a href=&quot;https://debezium.io/docs/connectors/postgresql/&quot;&gt;Debezium PostgreSQL connector&lt;/a&gt;.
This connector is specific in the regard that it requires a &lt;a href=&quot;https://www.postgresql.org/docs/current/logicaldecoding-explanation.html&quot;&gt;logical decoding plug-in&lt;/a&gt; to be installed inside the PostgreSQL source database(s) themselves.
Currently, there are two supported logical plug-ins:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/debezium/&quot;&gt;postgres-decoderbufs&lt;/a&gt;, which uses Protocol Buffers as a very compact transport format and which is maintained by the Debezium community&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/eulerto/wal2json&quot;&gt;JSON-based&lt;/a&gt;, which is based on JSON and which is maintained by its own upstream community&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;These plug-ins can be consumed and deployed in two ways;
the easiest one is to use one of our pre-made &lt;a href=&quot;https://hub.docker.com/r/debezium/postgres&quot;&gt;Postgres container images&lt;/a&gt;,
which contain both plug-ins and are already configured as required.
If you are using containers in your datacenter, and/or if you start a fresh database from scratch,
then this can be a great option.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The other approach is building from source.
Even if this is usually an easy task, it still brings a barrier to an easy start and requires a non-trivial knowledge of the Linux operating system.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To bridge the gap between those two extremes we’ve created and published an &lt;a href=&quot;https://src.fedoraproject.org/rpms/postgres-decoderbufs&quot;&gt;RPM package&lt;/a&gt;,
available for Fedora 30 and later.
By installing this package you will have the necessary binaries deployed, and the only task remaining is to configure PostgreSQL to enable the plug-in.
The RPM is based on the latest stable Debezium release,
&lt;a href=&quot;https://debezium.io/blog/2019/05/06/debezium-0-9-5-final-released/&quot;&gt;0.9.5.Final&lt;/a&gt; at this point.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;example&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#example&quot; /&gt;Example&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s show how the package works.
We will use the &lt;a href=&quot;https://www.vagrantup.com/&quot;&gt;Vagrant&lt;/a&gt; tool as an easy way for firing up a pre-provisioned virtual machine with Fedora.
Of course, that’s not a requirement and the same steps apply for any other way of running Fedora.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Create and start virtual machine with Fedora 30:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ vagrant init fedora/30-cloud-base

A `Vagrantfile` has been placed in this directory. You are now
ready to `vagrant up` your first virtual environment! Please read
the comments in the Vagrantfile as well as documentation on
`vagrantup.com` for more information on using Vagrant.

$ vagrant up

Bringing machine &#39;default&#39; up with &#39;virtualbox&#39; provider...
.
.
.
==&amp;gt; default: Machine booted and ready!&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Log into the virtual machine:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ vagrant ssh&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Install the PostgreSQL server and Protocol Buffers logical decoding plug-in:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo dnf -y install postgresql postgres-decoderbufs
.
.
.
Installed:
  postgres-decoderbufs-0.9.5-1.fc30.x86_64              postgresql-11.3-1.fc30.x86_64
  postgis-2.5.1-1.fc30.x86_64                           armadillo-9.400.4-1.fc30.x86_64
  blas-3.8.0-12.fc30.x86_64                             cairo-1.16.0-5.fc30.x86_64
  cups-libs-1:2.2.11-2.fc30.x86_64                      fontconfig-2.13.1-8.fc30.x86_64
  lapack-3.8.0-12.fc30.x86_64                           libgfortran-9.1.1-1.fc30.x86_64
  libpq-11.3-2.fc30.x86_64                              libquadmath-9.1.1-1.fc30.x86_64
  mariadb-connector-c-3.0.10-1.fc30.x86_64              mariadb-connector-c-config-3.0.10-1.fc30.noarch
  nss-3.44.0-2.fc30.x86_64                              nss-softokn-3.44.0-2.fc30.x86_64
  nss-softokn-freebl-3.44.0-2.fc30.x86_64               nss-sysinit-3.44.0-2.fc30.x86_64
  nss-util-3.44.0-2.fc30.x86_64                         poppler-0.73.0-9.fc30.x86_64
  postgresql-server-11.3-1.fc30.x86_64                  proj-5.2.0-2.fc30.x86_64
  proj-datumgrid-1.8-2.fc30.noarch                      uriparser-0.9.3-1.fc30.x86_64
  SuperLU-5.2.1-6.fc30.x86_64                           arpack-3.5.0-6.fc28.x86_64
  atk-2.32.0-1.fc30.x86_64                              avahi-libs-0.7-18.fc30.x86_64
  cfitsio-3.450-3.fc30.x86_64                           dejavu-fonts-common-2.37-1.fc30.noarch
  dejavu-sans-fonts-2.37-1.fc30.noarch                  fontpackages-filesystem-1.44-24.fc30.noarch
  freexl-1.0.5-3.fc30.x86_64                            fribidi-1.0.5-2.fc30.x86_64
  gdal-libs-2.3.2-7.fc30.x86_64                         gdk-pixbuf2-2.38.1-1.fc30.x86_64
  gdk-pixbuf2-modules-2.38.1-1.fc30.x86_64              geos-3.7.1-1.fc30.x86_64
  giflib-5.1.9-1.fc30.x86_64                            graphite2-1.3.10-7.fc30.x86_64
  gtk-update-icon-cache-3.24.8-1.fc30.x86_64            gtk2-2.24.32-4.fc30.x86_64
  harfbuzz-2.3.1-1.fc30.x86_64                          hdf5-1.8.20-6.fc30.x86_64
  hicolor-icon-theme-0.17-5.fc30.noarch                 jasper-libs-2.0.14-8.fc30.x86_64
  jbigkit-libs-2.1-16.fc30.x86_64                       lcms2-2.9-5.fc30.x86_64
  libXcomposite-0.4.4-16.fc30.x86_64                    libXcursor-1.1.15-5.fc30.x86_64
  libXdamage-1.1.4-16.fc30.x86_64                       libXfixes-5.0.3-9.fc30.x86_64
  libXft-2.3.2-12.fc30.x86_64                           libXi-1.7.9-9.fc30.x86_64
  libXinerama-1.1.4-3.fc30.x86_64                       libaec-1.0.4-1.fc30.x86_64
  libdap-3.20.3-1.fc30.x86_64                           libgeotiff-1.4.3-3.fc30.x86_64
  libgta-1.0.9-2.fc30.x86_64                            libjpeg-turbo-2.0.2-1.fc30.x86_64
  libkml-1.3.0-19.fc30.x86_64                           libspatialite-4.3.0a-11.fc30.x86_64
  libtiff-4.0.10-4.fc30.x86_64                          libwebp-1.0.2-2.fc30.x86_64
  netcdf-4.4.1.1-12.fc30.x86_64                         nspr-4.21.0-1.fc30.x86_64
  ogdi-3.2.1-4.fc30.x86_64                              openblas-0.3.5-5.fc30.x86_64
  openblas-openmp-0.3.5-5.fc30.x86_64                   openblas-serial-0.3.5-5.fc30.x86_64
  openblas-threads-0.3.5-5.fc30.x86_64                  openblas-threads64_-0.3.5-5.fc30.x86_64
  openjpeg2-2.3.1-1.fc30.x86_64                         pango-1.43.0-3.fc30.x86_64
  pixman-0.38.0-1.fc30.x86_64                           poppler-data-0.4.9-3.fc30.noarch
  protobuf-c-1.3.1-2.fc30.x86_64                        unixODBC-2.3.7-4.fc30.x86_64
  xerces-c-3.2.2-2.fc30.x86_64

Complete!&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Next, initialize the database:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo /usr/bin/postgresql-setup --initdb&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now enable the plug-in in the database server configuration file &lt;code&gt;/var/lib/pgsql/data/postgresql.conf&lt;/code&gt; by adding the following parameters:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# MODULES
shared_preload_libraries = &#39;decoderbufs&#39;

# REPLICATION
wal_level = logical             # minimal, archive, hot_standby, or logical (change requires restart)
max_wal_senders = 8             # max number of walsender processes (change requires restart)
wal_keep_segments = 4           # in logfile segments, 16MB each; 0 disables
#wal_sender_timeout = 60s       # in milliseconds; 0 disables
max_replication_slots = 4       # max number of replication slots (change requires restart)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Configure the security file &lt;code&gt;/var/lib/pgsql/data/pg_hba.conf&lt;/code&gt; for the database user that will be used by Debezium (e.g. &lt;code&gt;debezium&lt;/code&gt;) by adding these parameters:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;local   replication     debezium                          trust
host    replication     debezium  127.0.0.1/32            trust
host    replication     debezium  ::1/128                 trust&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Finally, restart PostgreSQL:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo systemctl restart postgresql&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And that’s it:
Now we have a PostgreSQL database, that is ready to stream changes to the Debezium PostgreSQL connector.
Of course, the plug-in can also be installed to an already existing database (Postgres versions 9 and later),
just by installing the RPM package and setting up the config and security files in the described way.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;outlook_pgoutput&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#outlook_pgoutput&quot; /&gt;Outlook: pgoutput&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While the decoderbufs plug-in is our recommended choice for a logical decoding plug-in,
there are cases where you may not be able to use it.
Most specifically, you typically don’t have the flexibility to install custom plug-ins in cloud-based environments such as Amazon RDS.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is why we’re exploring a &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/DBZ-766&quot;&gt;third alternative&lt;/a&gt; to decoderbufs and wal2sjon right now,
which is to leverage Postgres logical replication mechanism.
There’s a built-in plug-in, &lt;em&gt;pgoutput&lt;/em&gt; based on this, which exists in every Postgres database since version 10.
We’re still in the process of exploring the implications (and possible limitations) of using &lt;em&gt;pgoutput&lt;/em&gt;,
but so far things look promising and it may eventually be a valuable tool to have in the box.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Stay tuned for more details coming soon!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/06/12/debezium-0-10-0-beta1-released/</id>
<title>Debezium 0.10.0.Beta1 Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-06-12T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/06/12/debezium-0-10-0-beta1-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



Another week, another Debezium release&#8201;&#8212;&#8201;I&#8217;m happy to announce the release of Debezium 0.10.0.Beta1!


Besides the upgrade to Apache Kafka 2.2.1 (DBZ-1316),
this mostly fixes some bugs, including a regression to the MongoDB connector introduced in the Alpha2 release
(DBZ-1317).


A very welcomed usability improvement is that the connectors will log a warning now
if not at least one table is actually captured as per the whitelist/blacklist configuration
(DBZ-1242).
This helps to prevent the accidental exclusion all tables by means of an incorrect filter expression,
in which case the connectors "work as intended", but no events are propagated to the message broker.


Please see the release notes for the complete...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another week, another Debezium release — I’m happy to announce the release of Debezium &lt;strong&gt;0.10.0.Beta1&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Besides the upgrade to Apache Kafka 2.2.1 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1316&quot;&gt;DBZ-1316&lt;/a&gt;),
this mostly fixes some bugs, including a regression to the MongoDB connector introduced in the Alpha2 release
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1317&quot;&gt;DBZ-1317&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A very welcomed usability improvement is that the connectors will log a warning now
if not at least one table is actually captured as per the whitelist/blacklist configuration
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1242&quot;&gt;DBZ-1242&lt;/a&gt;).
This helps to prevent the accidental exclusion all tables by means of an incorrect filter expression,
in which case the connectors &quot;work as intended&quot;, but no events are propagated to the message broker.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please see the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-10-0-beta1&quot;&gt;release notes&lt;/a&gt; for the complete list of issues fixed in this release.
Also make sure to examine the upgrade guidelines for 0.10.0.Alpha1 and Alpha2 when upgrading from earlier versions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Many thanks to community members &lt;a href=&quot;https://github.com/pan3793&quot;&gt;Cheng Pan&lt;/a&gt; and &lt;a href=&quot;https://github.com/ChingTsai&quot;&gt;Ching Tsai&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/06/05/debezium-newsletter-01-2019/</id>
<title>Debezium&#8217;s Newsletter 01/2019</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-06-05T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/06/05/debezium-newsletter-01-2019/" rel="alternate" type="text/html" />
<author>
<name>Chris Cranford</name>
</author>
<category term="community"></category>
<category term="news"></category>
<category term="newsletter"></category>
<summary>



Welcome to the first edition of the Debezium community newsletter in which we share blog posts, group discussions, as well as StackOverflow
questions that are relevant to our user community.




Upcoming Events




Paris Data Engineers Meet-up


Oslo JavaZone 2019 - Change Data Streaming For Microservices With Apache Kafka and Debezium






Articles


Gunnar Morling recently attended Kafka Summit in London where he gave a talk on Change Data Streaming Patterns
for Microservices With Debezium.  You can watch the full presentation here.


Strimzi provides an easy way to run Apache Kafka on Kubernetes or Openshift.  This article
by Sincy Sebastian shows just how simple it is to replicate change...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Welcome to the first edition of the Debezium community newsletter in which we share blog posts, group discussions, as well as StackOverflow
questions that are relevant to our user community.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;upcoming_events&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#upcoming_events&quot; /&gt;Upcoming Events&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.meetup.com/fr-FR/Paris-Data-Engineers/events/260694777/&quot;&gt;Paris Data Engineers Meet-up&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://2019.javazone.no&quot;&gt;Oslo JavaZone 2019 - Change Data Streaming For Microservices With Apache Kafka and Debezium&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;articles&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#articles&quot; /&gt;Articles&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Gunnar Morling recently attended Kafka Summit in London where he gave a talk on Change Data Streaming Patterns
for Microservices With Debezium.  You can watch the full presentation &lt;a href=&quot;https://www.confluent.io/kafka-summit-lon19/change-data-streaming-patterns-microservices-debezium&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Strimzi provides an easy way to run Apache Kafka on Kubernetes or Openshift.  &lt;a href=&quot;https://medium.com/@sincysebastian/setup-kafka-with-debezium-using-strimzi-in-kubernetes-efd494642585&quot;&gt;This article&lt;/a&gt;
by Sincy Sebastian shows just how simple it is to replicate change events from MySQL to Elastic Search using Debezium.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium allows replicating data between heterogeneous data stores with ease.  &lt;a href=&quot;https://blog.couchbase.com/kafka-connect-mysql-couchbase-debezium/&quot;&gt;This article&lt;/a&gt; by Matthew Groves
explains how you can replicate data from MySQL to CouchBase.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As the size of data that systems maintain continues to grow, this begins to impact how we capture, compute, and report
real-time analytics. &lt;a href=&quot;https://medium.com/high-alpha/data-stream-processing-for-newbies-with-kafka-ksql-and-postgres-c30309cfaaf8&quot;&gt;This article&lt;/a&gt; by Maria Patterson
explains how you can use Debezium to stream data from Postgres, perform analytical calculations using KSQL, and then
stream those results back to Postgres for consumption.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In a &lt;a href=&quot;https://medium.com/@singaretti/streaming-de-dados-do-postgresql-utilizando-kafka-debezium-v2-d49f46d70b37&quot;&gt;recent article&lt;/a&gt; published in Portuguese,
Paulo Singaretti illustrates how they use Debezium and Kafka to stream changes from their relational database and then store
the change stream results in Google Cloud Services.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This &lt;a href=&quot;https://debezium.io/blog/2019/05/23/tutorial-using-debezium-connectors-with-apache-pulsar/&quot;&gt;recent blog&lt;/a&gt; by Jia Zhai provides
a complete tutorial showing how to use Debezium connectors with Apache Pulsar.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;time_to_upgrade&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#time_to_upgrade&quot; /&gt;Time to upgrade&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium version &lt;a href=&quot;https://debezium.io/blog/2019/05/06/debezium-0-9-5-final-released/&quot;&gt;0.9.5&lt;/a&gt; was just released.
If you are using the 0.9 branch you should definitely check out 0.9.5.  For details on the bug fixes as well as
the enhancements this version includes, check out the
&lt;a href=&quot;https://issues.redhat.com/secure/ReleaseNote.jspa?projectId=12317320&amp;amp;version=12341657&quot;&gt;release notes&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium team has also begun active development on the next major version, 0.10.  We recently published
a &lt;a href=&quot;https://debezium.io/blog/2019/05/29/debezium-0-10-0-alpha1-released/&quot;&gt;blog&lt;/a&gt; that provides an overview
behind what 0.10 is meant to deliver.  If you want details on the bug fixes and enhancements we’ve packed
into this release, you can view the &lt;a href=&quot;https://issues.redhat.com/issues/?jql=fixVersion%20IN%20(0.10.0.Alpha1%2C%200.10.0.Alpha2)%20ORDER%20BY%20KEY&quot;&gt;issue list&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;questions_and_answers&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#questions_and_answers&quot; /&gt;Questions and answers&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/55569090/how-to-let-debezium-start-reading-binlog-from-the-last-row&quot;&gt;How to have Debezium start reading binlog from last row&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/55633590/is-it-possible-to-apply-smt-single-message-transforms-to-messages-from-specifi&quot;&gt;Is it possible to apply Single Message Transforms to messages from specified topics&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/55648457/kafkaconnect-produces-cdc-event-with-null-value-when-reading-from-mongodb-with-d&quot;&gt;Kafka Connect produces CDC events with null values when reading from MongoDB with Debezium&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://groups.google.com/d/msgid/debezium/1bae4e45-c6c4-4190-9955-44f901b8ca04%40googlegroups.com?utm_medium=email&amp;amp;utm_source=footer&quot;&gt;Renaming Topics&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://groups.google.com/d/msgid/debezium/cfc333f1-b5f6-462b-a1c8-0f65bc91b725%40googlegroups.com?utm_medium=email&amp;amp;utm_source=footer&quot;&gt;Stream changes with differing column names between source and destination&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://groups.google.com/d/msgid/debezium/18c1239f-af69-4161-8adc-329a91aa4c7e%40googlegroups.com?utm_medium=email&amp;amp;utm_source=footer&quot;&gt;Can’t connect to debezium kafka from Docker host&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;feedback&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#feedback&quot; /&gt;Feedback&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We intend to publish new additions of this newsletter periodically.  Should anyone have any suggestions on changes or what could be highlighted here, we welcome that feedback.  You can reach out to us via any of our community channels found &lt;a href=&quot;https://debezium.io/community&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/06/03/debezium-0-10-0-alpha2-released/</id>
<title>Debezium 0.10.0.Alpha2 Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-06-03T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/06/03/debezium-0-10-0-alpha2-released/" rel="alternate" type="text/html" />
<author>
<name>Jiri Pechanec</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



Release early, release often&#8201;&#8212;&#8201;Less than a week since the Alpha1 we are announcing the release of Debezium 0.10.0.Alpha2!


This is an incremental release that completes some of the tasks started in the Alpha1 release and provides a few bugfixes and also quality improvements in our Docker images.


The change in the logic of the snapshot field has been delivered (DBZ-1295) as outlined in the last announcement.
All connectors now provide information which of the records is the last one in the snapshot phase so that downstream consumers can react to this.


Apache ZooKeeper was upgraded to version 3.4.14 to fix a security vulnerability (CVE-2019-0201).


Our...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Release early, release often — Less than a week since the Alpha1 we are announcing the release of Debezium &lt;strong&gt;0.10.0.Alpha2&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is an incremental release that completes some of the tasks started in the Alpha1 release and provides a few bugfixes and also quality improvements in our Docker images.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The change in the logic of the &lt;code&gt;snapshot&lt;/code&gt; field has been delivered (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1295&quot;&gt;DBZ-1295&lt;/a&gt;) as outlined in &lt;a href=&quot;2019/05/29/debezium-0-10-0-alpha1-released/#outlook&quot;&gt;the last announcement&lt;/a&gt;.
All connectors now provide information which of the records is the last one in the snapshot phase so that downstream consumers can react to this.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Apache ZooKeeper was upgraded to version 3.4.14 to fix a security vulnerability (&lt;a href=&quot;https://nvd.nist.gov/vuln/detail/CVE-2019-0201&quot;&gt;CVE-2019-0201&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Our regular contributor &lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato&lt;/a&gt; dived deeply into our image build scripts and enriched (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1279&quot;&gt;DBZ-1279&lt;/a&gt;) them with a Dockerfile linter.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Schema change events include the table name(s) in the metadata describing which tables are affected by the change (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-871&quot;&gt;DBZ-871&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/barrti&quot;&gt;Bartosz Miedlar&lt;/a&gt; has fixed a bug in MySQL ANTLR grammar causing issues with identifiers in backquotes (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1300&quot;&gt;DBZ-1300&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;whats_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#whats_next&quot; /&gt;What’s next?&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope we will be able to keep the recent release cadence and get lout the first beta version of 0.10 in two weeks.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Stay tuned for more!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/05/29/debezium-0-10-0-alpha1-released/</id>
<title>Debezium 0.10.0.Alpha1 &quot;Spring Clean-Up&quot; Edition Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-05-29T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/05/29/debezium-0-10-0-alpha1-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



I&#8217;m very happy to announce the release of Debezium 0.10.0.Alpha1!


The major theme for Debezium 0.10 will be to do some clean-up
(that&#8217;s what you do at this time of the year, right?);
we&#8217;ve planned to remove a few deprecated features and to streamline some details in the structure the CDC events produced by the different Debezium connectors.


This means that upgrading to Debezium 0.10 from earlier versions might take a bit more planning and consideration compared to earlier upgrades,
depending on your usage of features and options already marked as deprecated in 0.9 and before.
But no worries, we&#8217;re describing all changes in great detail...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I’m very happy to announce the release of Debezium &lt;strong&gt;0.10.0.Alpha1&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The major theme for Debezium 0.10 will be to do some clean-up
(that’s what you do at this time of the year, right?);
we’ve planned to remove a few deprecated features and to streamline some details in the structure the CDC events produced by the different Debezium connectors.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This means that upgrading to Debezium 0.10 from earlier versions might take a bit more planning and consideration compared to earlier upgrades,
depending on your usage of features and options already marked as deprecated in 0.9 and before.
But no worries, we’re describing all changes in great detail in this blog post and the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-10-0-alpha1&quot;&gt;release notes&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;why&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#why&quot; /&gt;Why?&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;First of all, let’s discuss a bit why we’re doing these changes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Over the last three years, Debezium has grown from supporting just a single database into an entire family of &lt;a href=&quot;https://debezium.io/docs/connectors/&quot;&gt;CDC connectors&lt;/a&gt; for a range of different relational databases and MongoDB,
as well as accompanying components such as message transformations for &lt;a href=&quot;https://debezium.io/docs/configuration/topic-routing/&quot;&gt;topic routing&lt;/a&gt; or &lt;a href=&quot;https://debezium.io/docs/configuration/outbox-event-router/&quot;&gt;implementing the outbox pattern&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As in any mature project, over time we figured that a few things should be done differently in the code base than we had thought at first.
For instance we moved from a hand-written parser for processing MySQL DDL statements to a much more robust implementation based on Antlr.
Also we realized the way certain temporal column types were exported was at risk of value overflow in certain conditions,
so we added a new mode not prone to these issues.
As a last example, we made options like the batch size used during snapshotting consistent across the different connectors.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Luckily, Debezium quickly gained traction and despite the 0.x version number, it is used heavily in production at a large number of organizations, and users rely on its stability.
So whenever we did such changes, we aimed at making the upgrade experience as smooth as possible;
usually that means that the previous behavior is still available but is marked as deprecated in the documentation,
while a new improved option, implementation etc. is added and made the default behavior.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;At the same time we realized that there are a couple of differences between the connectors which shouldn’t really be there.
Specifically, the &lt;code&gt;source&lt;/code&gt; block of change events has some differences which make a uniform handling by consumers more complex than it should be;
for instance the timestamp field is named &quot;ts_sec&quot; in MySQL events but &quot;ts_usec&quot; for Postgres.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With all this in mind, we decided that it is about time to clean up these issues.
This done for a couple of purposes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Keeping the code base maintainable and open for future development by removing legacy code such as deprecated options and their handling as well as the legacy MySQL DDL parser&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Making CDC events from different connectors easier to consume by unifying the &lt;code&gt;source&lt;/code&gt; block created by the different connectors as far as possible&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Preparing the project to go to version 1.0 with an even stronger promise of retaining backwards compatibility than already practiced today&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;what&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what&quot; /&gt;What?&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now as we have discussed why we feel it’s time for some &quot;clean-up&quot;, let’s take a closer look at the most relevant changes.
Please also refer to the &lt;a href=&quot;https://debezium.io/docs/releases/#breaking_changes&quot;&gt;&quot;breaking changes&quot;&lt;/a&gt; section of the migration notes for more details.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The legacy DDL parser for MySQL has been removed (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-736&quot;&gt;DBZ-736&lt;/a&gt;);
if you are not using the Antlr-based one yet (it was introduced in 0.8 and became the default in 0.9),
it’s highly recommended that you test it with your databases.
Should you run into any parsing errors, please report them so we can fix them for the 0.10 Final release.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The SMTs for retrieving the new record/document state from change events have been renamed from
&lt;code&gt;io.debezium.transforms.UnwrapFromEnvelope&lt;/code&gt; and &lt;code&gt;io.debezium.connector.mongodb.transforms.UnwrapFromMongoDbEnvelope&lt;/code&gt;
into &lt;code&gt;ExtractNewRecordState&lt;/code&gt; and &lt;code&gt;ExtractNewDocumentState&lt;/code&gt;, respectively
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-677&quot;&gt;DBZ-677&lt;/a&gt;).
The old names can still be used as of 0.10, but doing so will raise a warning.
They are planned for removal in Debezium 0.11.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Several connector options that were deprecated in earlier Debezium versions have been removed
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1234&quot;&gt;DBZ-1234&lt;/a&gt;):
the &lt;code&gt;drop.deletes&lt;/code&gt; option of new record/document state extraction SMTs (superseded by &lt;code&gt;delete.handling.mode&lt;/code&gt; option),
the &lt;code&gt;rows.fetch.size&lt;/code&gt; option (superseded by &lt;code&gt;snapshot.fetch.size&lt;/code&gt;),
the &lt;code&gt;adaptive&lt;/code&gt; value of &lt;code&gt;time.precision.mode&lt;/code&gt; option for MySQL (prone to value loss, use &lt;code&gt;adaptive_microseconds&lt;/code&gt; instead) and
the &lt;code&gt;snapshot.minimal.locks&lt;/code&gt; for the MySQL connector (superseded by &lt;code&gt;snapshot.locking.mode&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Several option names of the (incubating) SMT for the outbox pattern
have been renamed for the sake of consistency (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1289&quot;&gt;DBZ-1289&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Several fields within the &lt;code&gt;source&lt;/code&gt; block of CDC events have been renamed for the sake of consistency
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-596&quot;&gt;DBZ-596&lt;/a&gt;);
as this is technically a backwards-incompatible change when using Avro and the schema registry,
we’ve added a connector option &lt;code&gt;source.struct.version&lt;/code&gt; which, when set to the value &lt;code&gt;v1&lt;/code&gt;, will have connectors produce the previous &lt;code&gt;source&lt;/code&gt; structure.
&lt;code&gt;v2&lt;/code&gt; is the default and any consumers should be adjusted to work with the new &lt;code&gt;source&lt;/code&gt; structure as soon as possible.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;new_features_and_bugfixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_features_and_bugfixes&quot; /&gt;New Features and Bugfixes&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Besides these changes, the 0.10.0.Alpha1 release also contains some feature additions and bug fixes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The SQL Server connector supports custom SELECT statements for snapshotting (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1224&quot;&gt;DBZ-1224&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;database, schema and table/collection names have been added consistently to the &lt;code&gt;source&lt;/code&gt; block for CDC events from all connectors
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-875&quot;&gt;DBZ-875&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Client authentication works for the MySQL connector(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1228&quot;&gt;DBZ-1228&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The embedded engine doesn’t duplicate events after restarts any longer (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1276&quot;&gt;DBZ-1276&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A parser bug related to &lt;code&gt;CREATE INDEX&lt;/code&gt; statements was fixed (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1264&quot;&gt;DBZ-1264&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Overall, &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.10.0.Alpha1&quot;&gt;30 issues&lt;/a&gt; were addressed in this release.
Many thanks to &lt;a href=&quot;https://github.com/Arkoprabho&quot;&gt;Arkoprabho Chakraborti&lt;/a&gt;, &lt;a href=&quot;https://github.com/rsatishm&quot;&gt;Ram Satish&lt;/a&gt; and &lt;a href=&quot;https://github.com/Wang-Yu-Chao&quot;&gt;Yuchao Wang&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Speaking of contributors, we did some housekeeping to &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/COPYRIGHT.txt&quot;&gt;the list&lt;/a&gt; of everyone ever contributing to Debezium, too.
Not less than exactly &lt;strong&gt;111 individuals have contributed&lt;/strong&gt; code up to this point,
which is just phenomenal! Thank you so much everyone, you folks rock!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;outlook&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#outlook&quot; /&gt;Outlook&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Going forward, there are some more details we’d like to unify across the different connectors before going to Debezium 0.10 Final.
For instance the &lt;code&gt;source&lt;/code&gt; attribute &lt;code&gt;snapshot&lt;/code&gt; will be changed so it can take one of three states: &lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt; or &lt;code&gt;last&lt;/code&gt;
(indicating that this event is the last one created during initial snapshotting).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’ll also continue our efforts to to migrate the existing Postgres connector to the framework classes established for the SQL Server and Oracle connectors.
Another thing we’re actively exploring is how the Postgres could take advantage of the &quot;logical replication&quot; feature added in Postgres 10.
This may provide us with a way to ingest change events without requiring a custom server-side logical decoding plug-in,
which proves challenging in cloud environments where there’s typically just a limited set of logical decoding options available.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/05/23/tutorial-using-debezium-connectors-with-apache-pulsar/</id>
<title>Tutorial for Using Debezium Connectors With Apache Pulsar</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-05-23T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/05/23/tutorial-using-debezium-connectors-with-apache-pulsar/" rel="alternate" type="text/html" />
<author>
<name>Jia Zhai, StreamNative</name>
</author>
<category term="discussion"></category>
<category term="examples"></category>
<summary>



This is a guest post by Apache Pulsar PMC Member and Committer Jia Zhai.


Debezium is an open source project for change data capture (CDC). It is built on Apache Kafka Connect and supports multiple databases, such as MySQL, MongoDB, PostgreSQL, Oracle, and SQL Server.
Apache Pulsar includes a set of built-in connectors based on Pulsar IO framework, which is counter part to Apache Kafka Connect.


As of version 2.3.0, Pulsar IO comes with support for the Debezium source connectors out of the box, so you can leverage Debezium to stream changes from your databases into Apache Pulsar.
This tutorial walks you through setting...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;This is a guest post by Apache Pulsar PMC Member and Committer Jia Zhai.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://debezium.io&quot;&gt;Debezium&lt;/a&gt; is an open source project for change data capture (CDC). It is built on &lt;a href=&quot;https://kafka.apache.org/documentation/#connectapi&quot;&gt;Apache Kafka Connect&lt;/a&gt; and supports multiple databases, such as MySQL, MongoDB, PostgreSQL, Oracle, and SQL Server.
&lt;a href=&quot;http://pulsar.apache.org&quot;&gt;Apache Pulsar&lt;/a&gt; includes a set of &lt;a href=&quot;https://pulsar.apache.org/docs/en/io-connectors&quot;&gt;built-in connectors&lt;/a&gt; based on Pulsar IO framework, which is counter part to Apache Kafka Connect.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As of version 2.3.0, Pulsar IO comes with support for the &lt;a href=&quot;http://pulsar.apache.org/docs/en/2.3.0/io-cdc-debezium&quot;&gt;Debezium source connectors&lt;/a&gt; out of the box, so you can leverage Debezium to stream changes from your databases into Apache Pulsar.
This tutorial walks you through setting up the Debezium connector for MySQL with Pulsar IO.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;tutorial_steps&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#tutorial_steps&quot; /&gt;Tutorial Steps&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This tutorial is similar to the &lt;a href=&quot;https://debezium.io/docs/tutorial&quot;&gt;Debezium tutorial&lt;/a&gt;, except that storage of event streams is changed from Kafka to Pulsar.
It mainly includes six steps:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;Start a MySQL server;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start standalone Pulsar service;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start the Debezium connector in Pulsar IO. Pulsar IO reads database changes existing in MySQL server;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Subscribe Pulsar topics to monitor MySQL changes;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make changes in MySQL server, and verify that changes are recorded in Pulsar topics immediately;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Clean up.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;step_1_start_a_mysql_server&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#step_1_start_a_mysql_server&quot; /&gt;Step 1: Start a MySQL server&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Start a MySQL server that contains a database example, from which Debezium captures changes. Open a new terminal to start a new container that runs a MySQL database server pre-configured with a database named inventory:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker run --rm --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=debezium -e MYSQL_USER=mysqluser -e MYSQL_PASSWORD=mysqlpw debezium/example-mysql:0.9&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The following information is displayed:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;2019-03-25T14:12:41.178325Z 0 [Note] Event Scheduler: Loaded 0 events
2019-03-25T14:12:41.178670Z 0 [Note] mysqld: ready for connections.
Version: &#39;5.7.25-log&#39;  socket: &#39;/var/run/mysqld/mysqld.sock&#39;  port: 3306  MySQL Community Server (GPL)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;step_2_start_standalone_pulsar_service&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#step_2_start_standalone_pulsar_service&quot; /&gt;Step 2: Start standalone Pulsar service&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Start Pulsar service locally in standalone mode.
Support for running Debezium connectors in Pulsar IO is introduced in Pulsar 2.3.0.
Download &lt;a href=&quot;https://archive.apache.org/dist/pulsar/pulsar-2.3.0/apache-pulsar-2.3.0-bin.tar.gz&quot;&gt;Pulsar binary of 2.3.0 release&lt;/a&gt; and &lt;a href=&quot;https://archive.apache.org/dist/pulsar/pulsar-2.3.0/connectors/pulsar-io-kafka-connect-adaptor-2.3.0.nar&quot;&gt;pulsar-io-kafka-connect-adaptor-2.3.0.nar of 2.3.0 release&lt;/a&gt;.
In Pulsar, all Pulsar IO connectors are packaged as separate &lt;a href=&quot;https://medium.com/hashmapinc/nifi-nar-files-explained-14113f7796fd&quot;&gt;NAR&lt;/a&gt; files.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ wget https://archive.apache.org/dist/pulsar/pulsar-2.3.0/apache-pulsar-2.3.0-bin.tar.gz
$ wget https://archive.apache.org/dist/pulsar/pulsar-2.3.0/connectors/pulsar-io-kafka-connect-adaptor-2.3.0.nar
$ tar zxf apache-pulsar-2.3.0-bin.tar.gz
$ cd apache-pulsar-2.3.0
$ mkdir connectors
$ cp ../pulsar-io-kafka-connect-adaptor-2.3.0.nar connectors
$ bin/pulsar standalone&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/pulsar_tutorial/pulsar-mysql-1.png&quot; class=&quot;responsive-image&quot; alt=&quot;start pulsar standalone]&quot;&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;step_3_start_the_debezium_mysql_connector_in_pulsar_io&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#step_3_start_the_debezium_mysql_connector_in_pulsar_io&quot; /&gt;Step 3: Start the Debezium MySQL connector in Pulsar IO&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Start the Debezium MySQL connector in Pulsar IO, with local run mode, in another terminal tab.
The “debezium-mysql-source-config.yaml” file contains all the configuration, and main parameters are listed under the “configs” node. The .yaml file contains the &quot;task.class&quot; parameter. The configuration file also
includes MySQL related parameters (like server, port, user, password) and two names of Pulsar topics for &quot;history&quot; and &quot;offset&quot; storage.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ bin/pulsar-admin source localrun  --sourceConfigFile debezium-mysql-source-config.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The content in the “debezium-mysql-source-config.yaml” file is as follows.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;tenant: &quot;test&quot;
namespace: &quot;test-namespace&quot;
name: &quot;debezium-kafka-source&quot;
topicName: &quot;kafka-connect-topic&quot;
archive: &quot;connectors/pulsar-io-kafka-connect-adaptor-2.3.0.nar&quot;

parallelism: 1

configs:
  ## sourceTask
  task.class: &quot;io.debezium.connector.mysql.MySqlConnectorTask&quot;

  ## config for mysql, docker image: debezium/example-mysql:0.8
  database.hostname: &quot;localhost&quot;
  database.port: &quot;3306&quot;
  database.user: &quot;debezium&quot;
  database.password: &quot;dbz&quot;
  database.server.id: &quot;184054&quot;
  database.server.name: &quot;dbserver1&quot;
  database.whitelist: &quot;inventory&quot;

  database.history: &quot;org.apache.pulsar.io.debezium.PulsarDatabaseHistory&quot;
  database.history.pulsar.topic: &quot;history-topic&quot;
  database.history.pulsar.service.url: &quot;pulsar://127.0.0.1:6650&quot;
  ## KEY_CONVERTER_CLASS_CONFIG, VALUE_CONVERTER_CLASS_CONFIG
  key.converter: &quot;org.apache.kafka.connect.json.JsonConverter&quot;
  value.converter: &quot;org.apache.kafka.connect.json.JsonConverter&quot;
  ## PULSAR_SERVICE_URL_CONFIG
  pulsar.service.url: &quot;pulsar://127.0.0.1:6650&quot;
  ## OFFSET_STORAGE_TOPIC_CONFIG
  offset.storage.topic: &quot;offset-topic&quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Tables are created automatically in the aforementioned MySQL server. So Debezium connector reads history records from MySQL binlog file from the beginning. In the output you will find the connector has already been triggered and processed in 47 records.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/pulsar_tutorial/pulsar-mysql-2.png&quot; class=&quot;responsive-image&quot; alt=&quot;connector start process records&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For more information on how to manage connectors, see the &lt;a href=&quot;http://pulsar.apache.org/docs/en/io-managing/&quot;&gt;Pulsar IO documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Records that have been captured and read by Debezium are automatically published to Pulsar topics. When you start a new terminal, you will find the current topics in Pulsar with the following command:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ bin/pulsar-admin topics list public/default&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/pulsar_tutorial/pulsar-mysql-3.png&quot; class=&quot;responsive-image&quot; alt=&quot;list Pulsar topics&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For each table, which has been changed, the change data is stored in a separate Pulsar topic.  Except database table related topics, another two topics named “history-topic” and “offset-topic” are used to store history and offset related data.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;persistent://public/default/history-topic
persistent://public/default/offset-topic&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;step_4_subscribe_pulsar_topics_to_monitor_mysql_changes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#step_4_subscribe_pulsar_topics_to_monitor_mysql_changes&quot; /&gt;Step 4: Subscribe Pulsar topics to monitor MySQL changes&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Take the &lt;code&gt;persistent://public/default/dbserver1.inventory.products&lt;/code&gt; topic as an example.
Use the CLI command to consume this topic and monitor changes while the “products” table changes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt; $ bin/pulsar-client consume -s &quot;sub-products&quot; public/default/dbserver1.inventory.products -n 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The output is as follows:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;…
22:17:41.201 [pulsar-client-io-1-1] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [public/default/dbserver1.inventory.products][sub-products] Subscribing to topic on cnx [id: 0xfe0b4feb, L:/127.0.0.1:55585 - R:localhost/127.0.0.1:6650]
22:17:41.223 [pulsar-client-io-1-1] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [public/default/dbserver1.inventory.products][sub-products] Subscribed to topic on localhost/127.0.0.1:6650 -- consumer: 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can also consume the offset topic to monitor the offset changes while the table changes are stored in the &lt;code&gt;persistent://public/default/dbserver1.inventory.products&lt;/code&gt; Pulsar topic.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ bin/pulsar-client consume -s &quot;sub-offset&quot; offset-topic -n 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;step_5_make_changes_in_mysql_server_and_verify_that_changes_are_recorded_in_pulsar_topics_immediately&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#step_5_make_changes_in_mysql_server_and_verify_that_changes_are_recorded_in_pulsar_topics_immediately&quot; /&gt;Step 5: Make changes in MySQL server, and verify that changes are recorded in Pulsar topics immediately&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Start a MySQL CLI docker connector,  and you can make changes to the “products” table in MySQL server.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$docker run -it --rm --name mysqlterm --link mysql --rm mysql:5.7 sh -c &#39;exec mysql -h&quot;$MYSQL_PORT_3306_TCP_ADDR&quot; -P&quot;$MYSQL_PORT_3306_TCP_PORT&quot; -uroot -p&quot;$MYSQL_ENV_MYSQL_ROOT_PASSWORD&quot;&#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;After running the command, MySQL CLI is displayed, and you can change the names of the two items in the “products” table.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;mysql&amp;gt; use inventory;
mysql&amp;gt; show tables;
mysql&amp;gt; SELECT * FROM  products ;
mysql&amp;gt; UPDATE products SET name=&#39;1111111111&#39; WHERE id=101;
mysql&amp;gt; UPDATE products SET name=&#39;1111111111&#39; WHERE id=107;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/pulsar_tutorial/pulsar-mysql-4.png&quot; class=&quot;responsive-image&quot; alt=&quot;mysql updates&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the terminal where you consume products topic, you find that two changes have been added.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/pulsar_tutorial/pulsar-mysql-5.png&quot; class=&quot;responsive-image&quot; alt=&quot;table topic stores mysql updates&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the terminal where you consume the offset topic, you find that two offsets have been added.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/pulsar_tutorial/pulsar-mysql-6.png&quot; class=&quot;responsive-image&quot; alt=&quot;offset topic get updated&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the terminal where you local-run the connector, you find two more records have been processed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/pulsar_tutorial/pulsar-mysql-7.png&quot; class=&quot;responsive-image&quot; alt=&quot;table topic get more records&quot;&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;step_6_clean_up&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#step_6_clean_up&quot; /&gt;Step 6: Clean up.&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Use “Ctrl + C” to close terminals. Use “docker ps” and “docker kill” to stop MySQL related containers.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;mysql&amp;gt; quit

$ docker ps
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                               NAMES
84d66c2f591d        debezium/example-mysql:0.8   &quot;docker-entrypoint.s…&quot;   About an hour ago   Up About an hour    0.0.0.0:3306-&amp;gt;3306/tcp, 33060/tcp   mysql

$ docker kill 84d66c2f591d&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To delete Pulsar data, delete data directory in the Pulsar binary directory.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ pwd
/Users/jia/ws/releases/apache-pulsar-2.3.0

$ rm -rf data&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#conclusion&quot; /&gt;Conclusion&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Pulsar IO framework allows to run the Debezium connectors for change data capture, streaming data changes from different databases into Apache Pulsar. In this tutorial you’ve learned how to capture data changes in a MySQL database and propagate them to Pulsar. We are improving support for running the Debezium connectors with Apache Pulsar continuously, it will be much easier to use after Pulsar 2.4.0 release.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/05/06/debezium-0-9-5-final-released/</id>
<title>Debezium 0.9.5.Final Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-05-06T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/05/06/debezium-0-9-5-final-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



It&#8217;s my pleasure to announce the release of Debezium 0.9.5.Final!


This is a recommended update for all users of earlier versions; besides bug fixes also a few new features are provide.
The release contains 18 resolved issues overall.




Apache Kafka Update and New Features

This release has been built against and tested with Apache Kafka 2.2.0 (DBZ-1227).
Earlier versions are continued to be supported as well.


For all the connectors it is possible now to specify the batch size when taking snapshots (DBZ-1247).
The new connector option snapshot.fetch.size has been introduced for that.
This option replaces the earlier option rows.fetch.size which existed in some of the connectors and...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.9.5.Final&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is a recommended update for all users of earlier versions; besides bug fixes also a few new features are provide.
The release contains &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project+%3D+DBZ+AND+fixVersion+%3D+0.9.5.Final&quot;&gt;18 resolved issues&lt;/a&gt; overall.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;apache_kafka_update_and_new_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#apache_kafka_update_and_new_features&quot; /&gt;Apache Kafka Update and New Features&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This release has been built against and tested with Apache Kafka 2.2.0 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1227&quot;&gt;DBZ-1227&lt;/a&gt;).
Earlier versions are continued to be supported as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For all the connectors it is possible now to specify the batch size when taking snapshots (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1247&quot;&gt;DBZ-1247&lt;/a&gt;).
The new connector option &lt;code&gt;snapshot.fetch.size&lt;/code&gt; has been introduced for that.
This option replaces the earlier option &lt;code&gt;rows.fetch.size&lt;/code&gt; which existed in some of the connectors and which will be removed in Debezium 0.10.
Existing connector instances should therefore be re-configured to use the new option.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Continuing the work from Debezium 0.9.4, the Postgres connector supports some more column types:
&lt;code&gt;MACADDR&lt;/code&gt; and &lt;code&gt;MACADDR8&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1193&quot;&gt;DBZ-1193&lt;/a&gt;) as well as &lt;code&gt;INT4RANGE&lt;/code&gt;, &lt;code&gt;INT8RANGE&lt;/code&gt; and &lt;code&gt;NUMRANGE&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1076&quot;&gt;DBZ-1076&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;fixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#fixes&quot; /&gt;Fixes&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Amongst others, this release includes the following fixes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Failing to specify value for &lt;code&gt;database.server.name&lt;/code&gt; results in invalid Kafka topic name (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-212&quot;&gt;DBZ-212&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Postgres Connector times out in schema discovery for DBs with many tables (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1214&quot;&gt;DBZ-1214&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Oracle connector: JDBC transaction can only capture single DML record (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1223&quot;&gt;DBZ-1223&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lost precision for timestamp with timezone (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1236&quot;&gt;DBZ-1236&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;NullpointerException due to optional value for commitTime (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1241&quot;&gt;DBZ-1241&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Default value for datetime(0) is incorrectly handled (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1243&quot;&gt;DBZ-1243&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Microsecond precision is lost when reading timetz data from Postgres (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1260&quot;&gt;DBZ-1260&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please refer to the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-9-5-final&quot;&gt;release notes&lt;/a&gt; for the complete list of issues fixed in Debezium 0.9.5.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’re very thankful to the following community members who contributed to this release:
&lt;a href=&quot;https://github.com/addisonj&quot;&gt;Addison Higham&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jorkzijlstra&quot;&gt;Jork Zijlstra&lt;/a&gt;,
&lt;a href=&quot;https://github.com/krizhan&quot;&gt;Krizhan Mariampillai&lt;/a&gt;,
&lt;a href=&quot;https://github.com/mrozieres&quot;&gt;Mathieu Rozieres&lt;/a&gt; and
&lt;a href=&quot;https://github.com/ShubhamRwt&quot;&gt;Shubham Rawat&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;outlook&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#outlook&quot; /&gt;Outlook&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This release is planned to be the last in the 0.9 line.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’re now going to focus on Debezium 0.10, whose main topic will be to clean up a few things:
we’d like to remove a few deprecated options and features (e.g. the legacy DDL parser in the MySQL connector).
We’re also planning to do a thorough review of the event structure of the different connectors;
for instance in the &lt;code&gt;source&lt;/code&gt; block of CDC messages there are a some field names that should be unified.
We believe users will benefit from a more consistent experience across the connectors.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another focus area will be to migrate the existing Postgres connector to the framework classes established for the SQL Server and Oracle connectors.
This will allow to expose some new features for the Postgres connector, e.g. the monitoring capabilities already rolled out for the other two connectors.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/04/18/hello-debezium/</id>
<title>Debezium&#8217;s Team Grows</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-04-18T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/04/18/hello-debezium/" rel="alternate" type="text/html" />
<author>
<name>Chris Cranford</name>
</author>
<category term="community"></category>
<category term="news"></category>
<summary>

Hello everyone, my name is Chris Cranford and I recently joined the Debezium team.


My journey at Red Hat began just over three years ago; however I have been in this line of work for nearly
twenty years.  All throughout my career, I have advocated and supported open source software.  Many of my
initial software endeavors were based on open source software, several which are still heavily used today
such as Hibernate ORM.





When I first joined Red Hat, I had the pleasure to work on the Hibernate ORM team. I had been an end user
of the project since 2.0, so it was...
</summary>
<content type="html">
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Hello everyone, my name is Chris Cranford and I recently joined the Debezium team.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;My journey at Red Hat began just over three years ago; however I have been in this line of work for nearly
twenty years.  All throughout my career, I have advocated and supported open source software.  Many of my
initial software endeavors were based on open source software, several which are still heavily used today
such as &lt;a href=&quot;http://www.hibernate.org&quot;&gt;Hibernate ORM&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;!-- more --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When I first joined Red Hat, I had the pleasure to work on the Hibernate ORM team. I had been an end user
of the project since 2.0, so it was an excellent fit to be able to contribute full time to a project that
had served me well in the corporate world n-times over.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It wasn’t long ago when &lt;a href=&quot;http://twitter.com/gunnarmorling&quot;&gt;@gunnarmorling&lt;/a&gt; and I had a brief exchange about
Debezium.  I had not heard of the project and I was super stoked because I immediately saw parallel in
its goals and &lt;a href=&quot;http://www.hibernate.org/orm/envers&quot;&gt;Hibernate Envers&lt;/a&gt;, a change data capture solution that
is based on Hibernate’s event framework that I was currently maintaining.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I believe one of my first &quot;wow&quot; moments was when I realized how well Debezium fits into the micro-services
world.  The idea of being able to share data between micro-services in a very decoupled way is a massive
win for building reusable components and minimizes technical debt.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium just felt like the next logical step.  There are so many new and exciting things to come and
the team and myself cannot wait to share them.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So lets get started!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;--Chris&lt;/p&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/04/11/debezium-0-9-4-final-released/</id>
<title>Debezium 0.9.4.Final Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-04-11T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/04/11/debezium-0-9-4-final-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="docker"></category>
<summary>



It&#8217;s my pleasure to announce the release of Debezium 0.9.4.Final!


This is a drop-in replacement for earlier Debezium 0.9.x versions, containing mostly bug fixes and some improvements related to metrics.
Overall, 17 issues were resolved.




MySQL Connector Improvements


The Debezium connector for MySQL comes with two new metrics:




Whether GTID is enabled for offset tracking or not (DBZ-1221)


Number of filtered events (DBZ-1206)




It also supports database connections using TLS 1.2 (DBZ-1208) now.




New Postgres Datatypes


The Postgres connector now allows to capture changes to columns of the CIDR and INET types (DBZ-1189).




Bug Fixes


The fixed bugs include the following:




Closing connection after snapshotting (DBZ-1218)


Can parse ALTER statement affecting enum column...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.9.4.Final&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is a drop-in replacement for earlier Debezium 0.9.x versions, containing mostly bug fixes and some improvements related to metrics.
Overall, &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.9.4.Final&quot;&gt;17 issues&lt;/a&gt; were resolved.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;mysql_connector_improvements&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mysql_connector_improvements&quot; /&gt;MySQL Connector Improvements&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium connector for MySQL comes with two new metrics:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Whether GTID is enabled for offset tracking or not (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1221&quot;&gt;DBZ-1221&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Number of filtered events (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1206&quot;&gt;DBZ-1206&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It also supports database connections using TLS 1.2 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1208&quot;&gt;DBZ-1208&lt;/a&gt;) now.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;new_postgres_datatypes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_postgres_datatypes&quot; /&gt;New Postgres Datatypes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Postgres connector now allows to capture changes to columns of the &lt;code&gt;CIDR&lt;/code&gt; and &lt;code&gt;INET&lt;/code&gt; types (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1189&quot;&gt;DBZ-1189&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;bug_fixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bug_fixes&quot; /&gt;Bug Fixes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The fixed bugs include the following:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Closing connection after snapshotting (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1218&quot;&gt;DBZ-1218&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Can parse ALTER statement affecting enum column with character set options (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1203&quot;&gt;DBZ-1203&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Avoiding timeout after bootstrapping a new table (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1207&quot;&gt;DBZ-1207&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Check out the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-9-4-final&quot;&gt;release notes&lt;/a&gt; for the complete list of issues fixed in Debezium 0.9.4.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Many thanks to Debezium community members
&lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jordanbragg&quot;&gt;Jordan Bragg&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jgao54&quot;&gt;Joy Gao&lt;/a&gt;,
&lt;a href=&quot;https://github.com/preethi29&quot;&gt;Preethi Sadagopan&lt;/a&gt;,
&lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt;,
&lt;a href=&quot;https://github.com/sashakovryga&quot;&gt;Sasha Kovryga&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ShubhamRwt&quot;&gt;Shubham Rawat&lt;/a&gt; and
&lt;a href=&quot;https://github.com/Crim&quot;&gt;Stephen Powis&lt;/a&gt;
for their contributions to this release!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/03/26/debezium-0-9-3-final-released/</id>
<title>Debezium 0.9.3.Final Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-03-26T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/03/26/debezium-0-9-3-final-released/" rel="alternate" type="text/html" />
<author>
<name>Jiri Pechanec</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="mongodb"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="docker"></category>
<summary>



The Debezium team is happy to announce the release of Debezium 0.9.3.Final!


This is mostly a bug-fix release and a drop-in replacement for earlier Debezium 0.9.x versions, but there are few significant new features too.
Overall, 17 issues were resolved.








Container images will be released with a small delay due to some Docker Hub configuration issues.







New Features


The 0.9.3 release comes with two larger new features:




A feature request was made to execute a partial recovery of the replication process after losing the replication slot with the PostgreSQL database, e.g. after failing over to a secondary database host (DBZ-1082).
Instead of adding yet another snapshotting mode,...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium team is happy to announce the release of Debezium &lt;strong&gt;0.9.3.Final&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is mostly a bug-fix release and a drop-in replacement for earlier Debezium 0.9.x versions, but there are few significant new features too.
Overall, &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.9.3.Final&quot;&gt;17 issues&lt;/a&gt; were resolved.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
Container images will be released with a small delay due to some Docker Hub configuration issues.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;new_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_features&quot; /&gt;New Features&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The 0.9.3 release comes with two larger new features:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A feature request was made to execute a partial recovery of the replication process after losing the replication slot with the PostgreSQL database, e.g. after failing over to a secondary database host (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1082&quot;&gt;DBZ-1082&lt;/a&gt;).
Instead of adding yet another snapshotting mode, we took a step back and decided to make the Postgres snapshotting process more customizable by introducing a service provider interface (SPI). This lets you implement and register your own Java class for controlling the snaphotting process.
See the issue description of DBZ-1082 for one possible custom implementation of this SPI, which is based on Postgres&#39; &lt;code&gt;catalog_xmin&lt;/code&gt; property and selects all records altered after the last known xmin position.
To learn more about the SPI, see the the &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/debezium-connector-postgres/src/main/java/io/debezium/connector/postgresql/spi/Snapshotter.java&quot;&gt;Snapshotter&lt;/a&gt; contract.
Note that the feature is still in incubating phase and the SPI should be considered unstable for the time being.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Not long ago we published blogpost about implementing the &lt;a href=&quot;https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/&quot;&gt;outbox&lt;/a&gt; pattern with Debezium for propagating data changes between microservices.
Community member &lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt; expanded the idea and created a ready-made implementation of the single message transform (SMT) described in the post for routing events from the outbox table to specific topics.
This SMT is part of the Debezium core library now (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1169&quot;&gt;DBZ-1169&lt;/a&gt;).
Its usage will be described in the documentation soon; for the time being please refer to the &lt;a href=&quot;https://github.com/debezium/debezium/tree/master/debezium-core/src/main/java/io/debezium/transforms/outbox/EventRouter.java&quot;&gt;EventRouter&lt;/a&gt; type and the accompanying configuration class.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;bug_fixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bug_fixes&quot; /&gt;Bug fixes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We did a couple of fixes related to the &lt;a href=&quot;https://debezium.io/docs/connectors/postgres/&quot;&gt;Debezium Postgres connector&lt;/a&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A regression that introduced a deadlock in snapshotting process has been fixed (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1161&quot;&gt;DBZ-1161&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;hstore&lt;/code&gt; datatype works correctly in snapshot phase (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1162&quot;&gt;DBZ-1162&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;wal2json&lt;/code&gt; plug-in processes also empty events (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1181&quot;&gt;DBZ-1181&lt;/a&gt;) as e.g. originating from materialize view updates; this should help to resolve some of the issues where log files in Postgres couldn’t be discarded due to Debezium’s replication slot not advancing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The commit time is propely converted to microseconds (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1174&quot;&gt;DBZ-1174&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Also the &lt;a href=&quot;https://debezium.io/docs/connectors/mysql/&quot;&gt;Debezium MySQL connector&lt;/a&gt; saw a number of fixes especially in SQL parser:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;SERIAL&lt;/code&gt; datatype and default value is now supported (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1185&quot;&gt;DBZ-1185&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A specific detail in the MySQL grammar that allows to enumerate table options in &lt;code&gt;ALTER TABLE&lt;/code&gt; without comma works (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1186&quot;&gt;DBZ-1186&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A false alarm for empty MySQL password is no longer reported (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1188&quot;&gt;DBZ-1188&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is no longer necessary to create history topic manually for broker without default topic replication value (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1179&quot;&gt;DBZ-1179&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It is now possible to process multiple schemas with a single Oracle connector (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1166&quot;&gt;DBZ-1166&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Check out the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-9-3-final&quot;&gt;release notes&lt;/a&gt; for the complete list of issues fixed in Debezium 0.9.3.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Many thanks to Debezium community members &lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt;, &lt;a href=&quot;https://github.com/ShubhamRwt&quot;&gt;Shubham Rawat&lt;/a&gt;, &lt;a href=&quot;https://github.com/addisonj&quot;&gt;Addison Higham&lt;/a&gt;, &lt;a href=&quot;https://github.com/jcasstevens&quot;&gt;Jon Casstevens&lt;/a&gt;, &lt;a href=&quot;https://github.com/hashhar&quot;&gt;Ashar Hassan&lt;/a&gt; and &lt;a href=&quot;https://github.com/p5k6&quot;&gt;Josh Stanfield&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/03/14/debezium-meets-quarkus/</id>
<title>Debezium meets Quarkus</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-03-14T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/03/14/debezium-meets-quarkus/" rel="alternate" type="text/html" />
<author>
<name>Jiri Pechanec</name>
</author>
<category term="quarkus"></category>
<category term="examples"></category>
<category term="microservices"></category>
<category term="apache-kafka"></category>
<summary>





Last week&#8217;s announcement of Quarkus sparked a great amount of interest in the Java community:
crafted from the best of breed Java libraries and standards, it allows to build Kubernetes-native applications based on GraalVM &amp; OpenJDK HotSpot.
In this blog post we are going to demonstrate how a Quarkus-based microservice can consume Debezium&#8217;s data change events via Apache Kafka.
For that purpose, we&#8217;ll see what it takes to convert the shipment microservice from our recent post about the outbox pattern into Quarkus-based service.




Quarkus is a Java stack designed for the development of cloud-native applications based on the Java platform.
It combines and tightly integrates...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;openblock teaser&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Last week’s announcement of &lt;a href=&quot;https://quarkus.io/&quot;&gt;Quarkus&lt;/a&gt; sparked a great amount of interest in the Java community:
crafted from the best of breed Java libraries and standards, it allows to build Kubernetes-native applications based on GraalVM &amp;amp; OpenJDK HotSpot.
In this blog post we are going to demonstrate how a Quarkus-based microservice can consume Debezium’s data change events via Apache Kafka.
For that purpose, we’ll see what it takes to convert the shipment microservice from our recent post about the &lt;a href=&quot;2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern&quot;&gt;outbox pattern&lt;/a&gt; into Quarkus-based service.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Quarkus is a Java stack designed for the development of cloud-native applications based on the Java platform.
It combines and tightly integrates mature libraries such Hibernate ORM, Vert.x, Netty, RESTEasy and Apache Camel as well as the APIs from the &lt;a href=&quot;https://microprofile.io/&quot;&gt;Eclipse MicroProfile&lt;/a&gt; initiative,
such as &lt;a href=&quot;https://github.com/eclipse/microprofile-config&quot;&gt;Config&lt;/a&gt; or &lt;a href=&quot;https://github.com/eclipse/microprofile-reactive-messaging&quot;&gt;Reactive Messaging&lt;/a&gt;.
Using Quarkus, you can develop applications using both imperative and reactive styles, also combining both approaches as needed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It is designed for significantly reduced memory consumption and improved startup time.
Last but not least, Quarkus supports both OpenJDK HotSpot and GraalVM virtual machines.
With GraalVM it is possible to compile the application into a native binary and thus reduce the resource consumption and startup time even more.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To learn more about Quarkus itself, we recommend to take a look at its excellent &lt;a href=&quot;https://quarkus.io/get-started/&quot;&gt;Getting Started&lt;/a&gt; guide.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;consuming_kafka_messages_with_quarkus&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#consuming_kafka_messages_with_quarkus&quot; /&gt;Consuming Kafka Messages with Quarkus&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the original &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/outbox&quot;&gt;example application&lt;/a&gt; demonstrating the outbox pattern,
there was a microservice (&quot;shipment&quot;) based on Thorntail that consumed the events produced by the Debezium connector.
We’ve extended the example with a new service named &quot;shipment-service-quarkus&quot;.
It provides the same functionality as the &quot;shipment-service&quot; but is implemented as a microservice based on Quarkus instead of Thorntail.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This makes the overall architecture look like so:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/outbox_pattern_quarkus.png&quot; class=&quot;responsive-image&quot; alt=&quot;Outbox Pattern Overview&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To retrofit the original service into a Quarkus-based application, only a few changes were needed:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Quarkus right now supports only MariaDB but not MySQL; hence we have included an instance of MariaDB to which the service is writing&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;a href=&quot;https://javaee.github.io/jsonp/&quot;&gt;JSON-P API&lt;/a&gt; used do deserialize incoming JSON messages can currently not be used without RESTEasy (see &lt;a href=&quot;https://github.com/quarkusio/quarkus/issues/1480&quot;&gt;issue #1480&lt;/a&gt;, which should be fixed soon); so the code has been modified to use the Jackson API instead&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Instead of the Kafka consumer API, the &lt;a href=&quot;https://github.com/eclipse/microprofile-reactive-messaging&quot;&gt;Reactive Messaging API&lt;/a&gt; defined by MicroProfile is used to receive messages from Apache Kafka; as an implementation of that API, the one provided by the &lt;a href=&quot;https://github.com/smallrye/smallrye-reactive-messaging&quot;&gt;SmallRye project&lt;/a&gt; is used, which is bundled as a Quarkus extension&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While the first two steps are mere technicalities,
the Reactive Messaging API is a nice simplification over the polling loop in the original consumer.
All that’s needed to consume messages from a Kafka topic is to annotate a method with &lt;code&gt;@Incoming&lt;/code&gt;,
and it will automatically be invoked when a new message arrives:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class KafkaEventConsumer {

    @Incoming(&quot;orders&quot;)
    public CompletionStage&amp;lt;Void&amp;gt; onMessage(KafkaMessage&amp;lt;String, String&amp;gt; message)
            throws IOException {
        // handle message...

        return message.ack();
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &quot;orders&quot; message source is configured via the MicroProfile Config API,
which resolves it to the &quot;OrderEvents&quot; topic already known from the original outbox example.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;build_process&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#build_process&quot; /&gt;Build Process&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The build process is mostly the same as it was before.
Instead of using the Thorntail Maven plug-in, the Quarkus Maven plug-in is used now.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The following Quarkus extensions are used:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;io.quarkus:quarkus-hibernate-orm&lt;/em&gt;: support for Hibernate ORM and JPA&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;io.quarkus:quarkus-jdbc-mariadb&lt;/em&gt;: support for accessing MariaDB through JDBC&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;io.quarkus:quarkus-smallrye-reactive-messaging-kafka&lt;/em&gt;: support for accessing Kafka through the MicroProfile Reactive Messaging API&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;They pull in some other extensions too, e.g. &lt;em&gt;quarkus-arc&lt;/em&gt; (the Quarkus CDI runtime) and &lt;em&gt;quarkus-vertx&lt;/em&gt; (used by the reactive messaging support).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In addition, two more changes were needed:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A new build profile named &lt;code&gt;native&lt;/code&gt; has been added; this is used to compile the service into a native binary image using the Quarkus Maven plug-in&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the &lt;code&gt;native-image.docker-build&lt;/code&gt; system property is enabled when running the build; this means that the native image build is done inside of a Docker container, so that GraalVM doesn’t have to be installed on the developer’s machine&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;All the heavy-lifting is done by the Quarkus Maven plug-in which is configured in &lt;em&gt;pom.xml&lt;/em&gt; like so:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;  &amp;lt;build&amp;gt;
    &amp;lt;finalName&amp;gt;shipment&amp;lt;/finalName&amp;gt;
    &amp;lt;plugins&amp;gt;
      ...
      &amp;lt;plugin&amp;gt;
        &amp;lt;groupId&amp;gt;io.quarkus&amp;lt;/groupId&amp;gt;
        &amp;lt;artifactId&amp;gt;quarkus-maven-plugin&amp;lt;/artifactId&amp;gt;
        &amp;lt;version&amp;gt;${version.quarkus}&amp;lt;/version&amp;gt;
        &amp;lt;executions&amp;gt;
          &amp;lt;execution&amp;gt;
            &amp;lt;goals&amp;gt;
              &amp;lt;goal&amp;gt;build&amp;lt;/goal&amp;gt;
            &amp;lt;/goals&amp;gt;
          &amp;lt;/execution&amp;gt;
        &amp;lt;/executions&amp;gt;
      &amp;lt;/plugin&amp;gt;
    &amp;lt;/plugins&amp;gt;
  &amp;lt;/build&amp;gt;
  ...
    &amp;lt;profile&amp;gt;
      &amp;lt;id&amp;gt;native&amp;lt;/id&amp;gt;
      &amp;lt;build&amp;gt;
        &amp;lt;plugins&amp;gt;
          &amp;lt;plugin&amp;gt;
            &amp;lt;groupId&amp;gt;io.quarkus&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;quarkus-maven-plugin&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;${version.quarkus}&amp;lt;/version&amp;gt;
            &amp;lt;executions&amp;gt;
              &amp;lt;execution&amp;gt;
                &amp;lt;goals&amp;gt;
                  &amp;lt;goal&amp;gt;native-image&amp;lt;/goal&amp;gt;
                &amp;lt;/goals&amp;gt;
                &amp;lt;configuration&amp;gt;
                  &amp;lt;enableHttpUrlHandler&amp;gt;true&amp;lt;/enableHttpUrlHandler&amp;gt;
                  &amp;lt;autoServiceLoaderRegistration&amp;gt;false&amp;lt;/autoServiceLoaderRegistration&amp;gt;
                &amp;lt;/configuration&amp;gt;
              &amp;lt;/execution&amp;gt;
            &amp;lt;/executions&amp;gt;
          &amp;lt;/plugin&amp;gt;
        &amp;lt;/plugins&amp;gt;
      &amp;lt;/build&amp;gt;
    &amp;lt;/profile&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;configuration&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#configuration&quot; /&gt;Configuration&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As any Quarkus application, the shipment service is configured via the &lt;em&gt;application.properties&lt;/em&gt; file:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;quarkus.datasource.url: jdbc:mariadb://shipment-db-quarkus:3306/shipmentdb
quarkus.datasource.driver: org.mariadb.jdbc.Driver
quarkus.datasource.username: mariadbuser
quarkus.datasource.password: mariadbpw
quarkus.hibernate-orm.database.generation=drop-and-create
quarkus.hibernate-orm.log.sql=true

smallrye.messaging.source.orders.type=io.smallrye.reactive.messaging.kafka.Kafka
smallrye.messaging.source.orders.topic=OrderEvents
smallrye.messaging.source.orders.bootstrap.servers=kafka:9092
smallrye.messaging.source.orders.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
smallrye.messaging.source.orders.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
smallrye.messaging.source.orders.group.id=shipment-service-quarkus&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In our case it contains&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;the definition of a datasource (based on MariaDB) to which the shipment service writes its data,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the definition of a messaging source, which is backed by the &quot;OrderEvents&quot; Kafka topic, using the given bootstrap server, deserializers and Kafka consumer group id.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;execution&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#execution&quot; /&gt;Execution&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Docker Compose config file has been enriched with two services, MariaDB and the new Quarkus-based shipment service.
So when &lt;code&gt;docker-compose up&lt;/code&gt; is executed, two shipment services are started side-by-side: the original Thorntail-based one and the new one using Quarkus.
When the order services receives a new purchase order and exports a corresponding event to Apache Kafka via the outbox table,
that message is processed by both shipment services, as they are using distinct consumer group ids.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;performance_numbers&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#performance_numbers&quot; /&gt;Performance Numbers&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The numbers are definitely not scientific, but provide a good indication of the order-of-magnitude difference between the native Quarkus-based application and the Thorntail service running on the JVM:&lt;/p&gt;
&lt;/div&gt;
&lt;table class=&quot;tableblock frame-all grid-all stretch table table-bordered table-striped&quot;&gt;
&lt;colgroup&gt;
&lt;col style=&quot;width: 30%;&quot;&gt;
&lt;col style=&quot;width: 35%;&quot;&gt;
&lt;col style=&quot;width: 35%;&quot;&gt;
&lt;/col&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot; /&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;Quarkus service&lt;/th&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;Thorntail service&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;memory [MB]&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;33.8&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;1257&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;start time [ms]&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;260&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;5746&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;application package size [MB]&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;54&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;131&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/col&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The memory data were obtained via &lt;code&gt;htop&lt;/code&gt; utility.
The startup time was measured till the message about application readiness was printed.
As with all performance measurements, you should run your own comparisons based on your set-up and workload to gain insight into the actual differences for your specific use cases.&lt;/p&gt;
&lt;/div&gt;
&lt;/col&gt;
&lt;/colgroup&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot; /&gt;Summary&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In this post we have successfully demonstrated that it is possible to consume Debezium-generated events in a Java application written with the Quarkus Java stack.
We have also shown that it is possible to provide such application as a binary image and provided back-of-the-envelope performance numbers demonstrating significant savings in resources.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you’d like to see the awesomeness of deploying Java microservices as native images by yourself,
you can find the complete &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/outbox/shipment-service-quarkus&quot;&gt;source code&lt;/a&gt; of the implementation in the Debezium examples repo.
If you got any questions or feedback, please let us know in the comments below;
looking forward to hearing from you!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;em&gt;Many thanks to Guillaume Smet for reviewing an earlier version of this post!&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/table&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/02/25/debezium-0-9-2-final-released/</id>
<title>Debezium 0.9.2.Final Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-02-25T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/02/25/debezium-0-9-2-final-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="mongodb"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="docker"></category>
<summary>



The Debezium team is happy to announce the release of Debezium 0.9.2.Final!


This is mostly a bug-fix release and a drop-in replacement for earlier Debezium 0.9.x versions.
Overall, 18 issues were resolved.


A couple of fixes relate to the Debezium Postgres connector:




When not using REPLICA IDENTITY FULL, certain data types could trigger exceptions for update or delete events; those are fixed now
(DBZ-1141, DBZ-1149)


The connector won&#8217;t fail any longer when encountering a change to a row with an unaltered TOAST column value
(DBZ-1146)




Also the Debezium MySQL connector saw a number of fixes:




The connector works correctly now when using GTIDs and ANSI_QUOTES SQL mode (DBZ-1147)


The new...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium team is happy to announce the release of Debezium &lt;strong&gt;0.9.2.Final&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is mostly a bug-fix release and a drop-in replacement for earlier Debezium 0.9.x versions.
Overall, &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.9.2.Final&quot;&gt;18 issues&lt;/a&gt; were resolved.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A couple of fixes relate to the &lt;a href=&quot;https://debezium.io/docs/connectors/postgres/&quot;&gt;Debezium Postgres connector&lt;/a&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;When not using &lt;code&gt;REPLICA IDENTITY FULL&lt;/code&gt;, certain data types could trigger exceptions for update or delete events; those are fixed now
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1141&quot;&gt;DBZ-1141&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1149&quot;&gt;DBZ-1149&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The connector won’t fail any longer when encountering a change to a row with an unaltered TOAST column value
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1146&quot;&gt;DBZ-1146&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Also the &lt;a href=&quot;https://debezium.io/docs/connectors/mysql/&quot;&gt;Debezium MySQL connector&lt;/a&gt; saw a number of fixes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The connector works correctly now when using GTIDs and &lt;code&gt;ANSI_QUOTES&lt;/code&gt; SQL mode (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1147&quot;&gt;DBZ-1147&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The new Antlr-based DDL parsers can handle column names that are key words such as &lt;code&gt;MEDIUM&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1150&quot;&gt;DBZ-1150&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;TIME&lt;/code&gt; columns with a default value larger than 23:59:59 can be exported now (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1137&quot;&gt;DBZ-1137&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another important fix was done in the &lt;a href=&quot;https://debezium.io/docs/connectors/sqlserver/&quot;&gt;Debezium connector for SQL Server&lt;/a&gt;,
where the connector archive deployed to Maven Central accidentally contained all test-scoped and provided-scoped dependencies.
This has been resolved now, so the connector archive only contains the actually needed JARs and thus is much smaller (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1138&quot;&gt;DBZ-1138&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;new_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_features&quot; /&gt;New Features&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The 0.9.2 release also comes with two small new features:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;You can pass arbitrary parameters to the logical decoding plug-in used by the Postgres connector;
this can for instance be used with wal2json to limit the number of tables to capture on the server side
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1130&quot;&gt;DBZ-1130&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The MongoDB connector now has the long-awaited snapshotting mode &lt;code&gt;NEVER&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-867&quot;&gt;DBZ-867&lt;/a&gt;),
i.e. you can set up a new connector without taking an initial snapshot and instantly beginning streaming changes from the oplog&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;version_updates&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#version_updates&quot; /&gt;Version Updates&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As of this release, Debezium has been upgraded to Apache Kafka 2.1.1.
Amongst others, this release fixes an issue where the Kafka Connect REST API would expose connector credentials also when those were configured via secrets (&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5117&quot;&gt;KAFKA-5117&lt;/a&gt;).
We’ve also upgraded the binlog client used by the MySQL connector to version 0.19.0 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1140&quot;&gt;DBZ-1140&lt;/a&gt;),
which fixes a bug that had caused exceptions during rebalancing the connector before (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1132&quot;&gt;DBZ-1132&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Check out the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-9-2-final&quot;&gt;release notes&lt;/a&gt; for the complete list of issues fixed in Debezium 0.9.2.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Many thanks to Debezium community members &lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;, &lt;a href=&quot;https://github.com/kbarber2&quot;&gt;Keith Barber&lt;/a&gt;, &lt;a href=&quot;https://github.com/krizhan&quot;&gt;Krizhan Mariampillai&lt;/a&gt; and &lt;a href=&quot;https://github.com/taylor-rolison&quot;&gt;Taylor Rolison&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/</id>
<title>Reliable Microservices Data Exchange With the Outbox Pattern</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-02-19T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="discussion"></category>
<category term="examples"></category>
<category term="microservices"></category>
<category term="apache-kafka"></category>
<category term="featured"></category>
<summary>





As part of their business logic, microservices often do not only have to update their own local data store,
but they also need to notify other services about data changes that happened.
The outbox pattern describes an approach for letting services execute these two tasks in a safe and consistent manner;
it provides source services with instant "read your own writes" semantics,
while offering reliable, eventually consistent data exchange across service boundaries.




Update (13 Sept. 2019): To simplify usage of the outbox pattern, Debezium now provides a ready-to-use SMT for routing outbox events. The custom SMT discussed in this blog post is not needed any...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;openblock teaser&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As part of their business logic, microservices often do not only have to update their own local data store,
but they also need to notify other services about data changes that happened.
The outbox pattern describes an approach for letting services execute these two tasks in a safe and consistent manner;
it provides source services with instant &quot;read your own writes&quot; semantics,
while offering reliable, eventually consistent data exchange across service boundaries.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;em&gt;Update (13 Sept. 2019):&lt;/em&gt; To simplify usage of the outbox pattern, Debezium now provides a ready-to-use &lt;a href=&quot;https://debezium.io/documentation/reference/0.9/configuration/outbox-event-router.html&quot;&gt;SMT for routing outbox events&lt;/a&gt;. The custom SMT discussed in this blog post is not needed any longer.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you’ve built a couple of microservices,
you’ll probably agree that the &lt;a href=&quot;https://blog.christianposta.com/microservices/the-hardest-part-about-microservices-data/&quot;&gt;hardest part about them is data&lt;/a&gt;:
microservices don’t exist in isolation and very often they need to propagate data and data changes amongst each other.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For instance consider a microservice that manages purchase orders:
when a new order is placed, information about that order may have to be relayed to a shipment service
(so it can assemble shipments of one or more orders) and a customer service
(so it can update things like the customer’s total credit balance based on the new order).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There are different approaches for letting the order service know the other two about new purchase orders;
e.g. it could invoke some &lt;a href=&quot;https://en.wikipedia.org/wiki/Representational_state_transfer&quot;&gt;REST&lt;/a&gt;, &lt;a href=&quot;https://grpc.io/&quot;&gt;grpc&lt;/a&gt; or other (synchronous) API provided by these services.
This might create some undesired coupling, though: the sending service must know which other services to invoke and where to find them.
It also must be prepared for these services temporarily not being available.
Service meshes such as &lt;a href=&quot;https://istio.io/&quot;&gt;Istio&lt;/a&gt; can come in helpful here, by providing capabilities like request routing, retries, circuit breakers and much more.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The general issue of any synchronous approach is that one service cannot really function without the other services which it invokes.
While buffering and retrying might help in cases where other services only need to be &lt;em&gt;notified&lt;/em&gt; of certain events,
this is not the case if a service actually needs to &lt;em&gt;query&lt;/em&gt; other services for information.
For instance, when a purchase order is placed, the order service might need to obtain the information how many times the purchased item is on stock from an inventory service.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another downside of such a synchronous approach is that it lacks re-playability,
i.e. the possibility for new consumers to arrive after events have been sent and still be able to consume the entire event stream from the beginning.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Both problems can be addressed by using an asynchronous data exchange approach instead:
i.e having the order, inventory and other services propagate events through a durable message log such as &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Apache Kafka&lt;/a&gt;.
By subscribing to these event streams, each service will be notified about the data change of other services.
It can react to these events and, if needed, create a local representation of that data in its own data store,
using a representation tailored towards its own needs.
For instance, such view might be denormalized to efficiently support specific access patterns, or it may only contain a subset of the original data that’s relevant to the consuming service.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Durable logs also support re-playability,
i.e. new consumers can be added as needed, enabling use cases you might not have had in mind originally,
and without touching the source service.
E.g. consider a data warehouse which should keep information about all the orders ever placed, or some full-text search functionality on purchase orders based on &lt;a href=&quot;https://www.elastic.co/products/elasticsearch&quot;&gt;Elasticsearch&lt;/a&gt;.
Once the purchase order events are in a Kafka topic
(Kafka’s topic’s retention policy settings can be used to ensure that events remain in a topic as long as its needed for the given use cases and business requirements),
new consumers can subscribe, process the topic from the very beginning and materialize a view of all the data in a microservice’s database, search index, data warehouse etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Dealing with Topic Growth&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Depending on the amount of data (number and size of records, frequency of changes),
it may or may not be feasible to keep events in topics for a long or even indefinite time.
Very often, some or even all events pertaining to a given data item
(e.g. a specific purchase order) might be eligible for deletion from a business point of view after a given point in time.
See the box &quot;Deletion of Events from Kafka Topics&quot; further below for some more thoughts on the deletion of events from Kafka topics in order to keep their size within bounds.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;the_issue_of_dual_writes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_issue_of_dual_writes&quot; /&gt;The Issue of Dual Writes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In order to provide their functionality, microservices will typically have their own local data store.
For instance, the order service may use a relational database to persist the information about purchase orders.
When a new order is placed, this may result in an &lt;code&gt;INSERT&lt;/code&gt; operation in a table &lt;code&gt;PurchaseOrder&lt;/code&gt; in the service’s database.
At the same time, the service may wish to send an event about the new order to Apache Kafka,
so to propagate that information to other interested services.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Simply issuing these two requests may lead to potential inconsistencies, though.
The reason being that we cannot have one shared transaction that would span the service’s database as well as Apache Kafka,
as the latter doesn’t support to be enlisted in distributed (XA) transactions.
So in unfortunate circumstances it might happen that we end up with having the new purchase order persisted in the local database,
but not having sent the corresponding message to Kafka
(e.g. due to some networking issue).
Or, the other way around, we might have sent the message to Kafka but failed to persist the purchase order in the local database.
Both situations are undesirable;
this may cause no shipment to be created for a seemingly successfully placed order.
Or a shipment gets created, but then there’d be no trace about the corresponding purchase order in the order service itself.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So how can this situation be avoided?
The answer is to only modify &lt;em&gt;one&lt;/em&gt; of the two resources (the database &lt;em&gt;or&lt;/em&gt; Apache Kafka) and drive the update of the second one based on that, in an eventually consistent manner.
Let’s first consider the case of only writing to Apache Kafka.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When receiving a new purchase order, the order service would not do the &lt;code&gt;INSERT&lt;/code&gt; into its database synchronously;
instead, it would only send an event describing the new order to a Kafka topic.
So only one resource gets modified at a time, and if something goes wrong with that,
we’ll find out about it instantly and report back to the caller of the order service that the request failed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;At the same time, the service itself would subscribe to that Kafka topic.
That way, it will be notified when a new message arrives in the topic and it can persist the new purchase order in its database.
There’s one subtle challenge here, though, and that is the lack of &quot;read your own write&quot; semantics.
E.g. let’s assume the order service also has an API for searching for all the purchase orders of a given customer.
When invoking that API right after placing a new order, due to the asynchronous nature of processing messages from the Kafka topic,
it might happen that the purchase order has not yet been persisted in the service’s database and thus will not be returned by that query.
That can lead to a very confusing user experience, as users for instance may miss newly placed orders in their shopping history.
There are ways to deal with this situation, e.g. the service could keep newly placed purchase orders in memory and answer subsequent queries based on that.
This gets quickly non-trivial though when implementing more complex queries or considering that the order service might also comprise multiple nodes in a clustered set-up,
which would require propagation of that data within the cluster.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now how would things look like when only writing to the database synchronously and driving the export of a message to Apache Kafka based on that?
This is where the outbox pattern comes in.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;the_outbox_pattern&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_outbox_pattern&quot; /&gt;The Outbox Pattern&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The idea of this approach is to have an &quot;outbox&quot; table in the service’s database.
When receiving a request for placing a purchase order, not only an &lt;code&gt;INSERT&lt;/code&gt; into the &lt;code&gt;PurchaseOrder&lt;/code&gt; table is done,
but, as part of the same transaction,
also a record representing the event to be sent is inserted into that outbox table.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The record describes an event that happened in the service,
for instance it could be a JSON structure representing the fact that a new purchase order has been placed,
comprising data on the order itself, its order lines as well as contextual information such as a use case identifier.
By explicitly emitting events via records in the outbox table,
it can be ensured that events are structured in a way suitable for external consumers.
This also helps to make sure that event consumers won’t break
when for instance altering the internal domain model or the &lt;code&gt;PurchaseOrder&lt;/code&gt; table.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;An asynchronous process monitors that table for new entries.
If there are any, it propagates the events as messages to Apache Kafka.
This gives us a very nice balance of characteristics:
By synchronously writing to the &lt;code&gt;PurchaseOrder&lt;/code&gt; table, the source service benefits from &quot;read your own writes&quot; semantics.
A subsequent query for purchase orders will return the newly persisted order, as soon as that first transaction has been committed.
At the same time, we get reliable, asynchronous, eventually consistent data propagation to other services via Apache Kafka.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now, the outbox pattern isn’t actually a new idea.
It has been in use for quite some time.
In fact, even when using JMS-style message brokers, which actually could participate in distributed transactions,
it can be a preferable option to avoid any coupling and potential impact by downtimes of remote resources such as a message broker.
You can also find a description of the pattern on Chris Richardson’s excellent &lt;a href=&quot;https://microservices.io/patterns/data/application-events.html&quot;&gt;microservices.io&lt;/a&gt; site.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Nevertheless, the pattern gets much less attention than it deserves and it is especially useful in the context of microservices.
As we’ll see, the outbox pattern can be implemented in a very elegant and efficient way using change data capture and Debezium.
In the following, let’s explore how.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;an_implementation_based_on_change_data_capture&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#an_implementation_based_on_change_data_capture&quot; /&gt;An Implementation Based on Change Data Capture&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://debezium.io/blog/2018/07/19/advantages-of-log-based-change-data-capture/&quot;&gt;Log-based Change Data Capture&lt;/a&gt; (CDC) is a great fit for capturing new entries in the outbox table and stream them to Apache Kafka.
As opposed to any polling-based approach, event capture happens with a very low overhead in near-realtime.
Debezium comes with &lt;a href=&quot;https://debezium.io/docs/connectors/&quot;&gt;CDC connectors&lt;/a&gt; for several databases such as MySQL, Postgres and SQL Server.
The following example will use the &lt;a href=&quot;https://debezium.io/docs/connectors/postgresql&quot;&gt;Debezium connector for Postgres&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can find the complete &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/outbox&quot;&gt;source code of the example&lt;/a&gt; on GitHub.
Refer to the &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/outbox/README.md&quot;&gt;README.md&lt;/a&gt; for details on building and running the example code.
The example is centered around two microservices,
&lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/outbox/order-service&quot;&gt;order-service&lt;/a&gt; and &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/outbox/shipment-service&quot;&gt;shipment-service&lt;/a&gt;.
Both are implemented in Java, using &lt;a href=&quot;http://cdi-spec.org/&quot;&gt;CDI&lt;/a&gt; as the component model and JPA/Hibernate for accessing their respective databases.
The order service runs on &lt;a href=&quot;http://wildfly.org/&quot;&gt;WildFly&lt;/a&gt; and exposes a simple REST API for placing purchase orders and canceling specific order lines.
It uses a Postgres database as its local data store.
The shipment service is based on &lt;a href=&quot;http://thorntail.io/&quot;&gt;Thorntail&lt;/a&gt;; via Apache Kafka, it receives events exported by the order service and creates corresponding shipment entries in its own MySQL database.
Debezium tails the transaction log (&quot;write-ahead log&quot;, WAL) of the order service’s Postgres database in order to capture any new events in the outbox table and propagates them to Apache Kafka.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The overall architecture of the solution can be seen in the following picture:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/outbox_pattern.png&quot; class=&quot;responsive-image&quot; alt=&quot;Outbox Pattern Overview&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note that the pattern is in no way tied to these specific implementation choices.
It could equally well be realized using alternative technologies such as Spring Boot
(e.g. leveraging Spring Data’s &lt;a href=&quot;https://docs.spring.io/spring-data/commons/docs/current/api/index.html?org/springframework/data/domain/DomainEvents.html&quot;&gt;support for domain events&lt;/a&gt;),
plain JDBC or other programming languages than Java altogether.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now let’s take a closer look at some of the relevant components of the solution.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;the_outbox_table&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_outbox_table&quot; /&gt;The Outbox Table&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;outbox&lt;/code&gt; table resides in the database of the order service and has the following structure:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Column        |          Type          | Modifiers
--------------+------------------------+-----------
id            | uuid                   | not null
aggregatetype | character varying(255) | not null
aggregateid   | character varying(255) | not null
type          | character varying(255) | not null
payload       | jsonb                  | not null&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Its columns are these:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;id&lt;/code&gt;: unique id of each message; can be used by consumers to detect any duplicate events, e.g. when restarting to read messages after a failure.
Generated when creating a new event.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;aggregatetype&lt;/code&gt;: the type of the &lt;em&gt;aggregate root&lt;/em&gt; to which a given event is related;
the idea being, leaning on the same concept of domain-driven design,
that exported events should refer to an aggregate
(&lt;a href=&quot;https://martinfowler.com/bliki/DDD_Aggregate.html&quot;&gt;&quot;a cluster of domain objects that can be treated as a single unit&quot;&lt;/a&gt;),
where the aggregate root provides the sole entry point for accessing any of the entities within the aggregate.
This could for instance be &quot;purchase order&quot; or &quot;customer&quot;.&lt;/p&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This value will be used to route events to corresponding topics in Kafka,
so there’d be a topic for all events related to purchase orders,
one topic for all customer-related events etc.
Note that also events pertaining to a child entity contained within one such aggregate should use that same type.
So e.g. an event representing the cancelation of an individual order line
(which is part of the purchase order aggregate)
should also use the type of its aggregate root, &quot;order&quot;,
ensuring that also this event will go into the &quot;order&quot; Kafka topic.&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;aggregateid&lt;/code&gt;: the id of the aggregate root that is affected by a given event; this could for instance be the id of a purchase order or a customer id;
Similar to the aggregate type, events pertaining to a sub-entity contained within an aggregate should use the id of the containing aggregate root,
e.g. the purchase order id for an order line cancelation event.
This id will be used as the key for Kafka messages later on.
That way, all events pertaining to one aggregate root or any of its contained sub-entities will go into the same partition of that Kafka topic,
which ensures that consumers of that topic will consume all the events related to one and the same aggregate in the exact order as they were produced.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;type&lt;/code&gt;: the type of event, e.g. &quot;Order Created&quot; or &quot;Order Line Canceled&quot;. Allows consumers to trigger suitable event handlers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;payload&lt;/code&gt;: a JSON structure with the actual event contents, e.g. containing a purchase order, information about the purchaser, contained order lines, their price etc.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;sending_events_to_the_outbox&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#sending_events_to_the_outbox&quot; /&gt;Sending Events to the Outbox&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In order to &quot;send&quot; events to the outbox, code in the order service could in general just do an &lt;code&gt;INSERT&lt;/code&gt; into the outbox table.
However, it’s a good idea to go for a slightly more abstract API, allowing to adjust implementation details of the outbox later on more easily, if needed.
&lt;a href=&quot;https://docs.jboss.org/weld/reference/latest/en-US/html/events.html&quot;&gt;CDI events&lt;/a&gt; come in very handy for this.
They can be raised in the application code and will be processed &lt;em&gt;synchronously&lt;/em&gt; by the outbox event sender,
which will do the required &lt;code&gt;INSERT&lt;/code&gt; into the outbox table.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;All outbox event types should implement the following contract, resembling the structure of the outbox table shown before:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public interface ExportedEvent {

    String getAggregateId();
    String getAggregateType();
    JsonNode getPayload();
    String getType();
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To produce such event, application code uses an injected &lt;code&gt;Event&lt;/code&gt; instance, as e.g. here in the &lt;code&gt;OrderService&lt;/code&gt; class:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class OrderService {

    @PersistenceContext
    private EntityManager entityManager;

    @Inject
    private Event&amp;lt;ExportedEvent&amp;gt; event;

    @Transactional
    public PurchaseOrder addOrder(PurchaseOrder order) {
        order = entityManager.merge(order);

        event.fire(OrderCreatedEvent.of(order));
        event.fire(InvoiceCreatedEvent.of(order));

        return order;
    }

    @Transactional
    public PurchaseOrder updateOrderLine(long orderId, long orderLineId,
            OrderLineStatus newStatus) {
        // ...
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the &lt;code&gt;addOrder()&lt;/code&gt; method, the JPA entity manager is used to persist the incoming order in the database
and the injected &lt;code&gt;event&lt;/code&gt; is used to fire a corresponding &lt;code&gt;OrderCreatedEvent&lt;/code&gt; and an &lt;code&gt;InvoiceCreatedEvent&lt;/code&gt;.
Again, keep in mind that, despite the notion of &quot;event&quot;, these two things happen within one and the same transaction.
i.e. within this transaction, three records will be inserted into the database:
one in the table with purchase orders and two in the outbox table.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Actual event implementations are straight-forward;
as an example, here’s the &lt;code&gt;OrderCreatedEvent&lt;/code&gt; class:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public class OrderCreatedEvent implements ExportedEvent {

    private static ObjectMapper mapper = new ObjectMapper();

    private final long id;
    private final JsonNode order;

    private OrderCreatedEvent(long id, JsonNode order) {
        this.id = id;
        this.order = order;
    }

    public static OrderCreatedEvent of(PurchaseOrder order) {
        ObjectNode asJson = mapper.createObjectNode()
                .put(&quot;id&quot;, order.getId())
                .put(&quot;customerId&quot;, order.getCustomerId())
                .put(&quot;orderDate&quot;, order.getOrderDate().toString());

        ArrayNode items = asJson.putArray(&quot;lineItems&quot;);

        for (OrderLine orderLine : order.getLineItems()) {
        items.add(
                mapper.createObjectNode()
                .put(&quot;id&quot;, orderLine.getId())
                .put(&quot;item&quot;, orderLine.getItem())
                .put(&quot;quantity&quot;, orderLine.getQuantity())
                .put(&quot;totalPrice&quot;, orderLine.getTotalPrice())
                .put(&quot;status&quot;, orderLine.getStatus().name())
            );
        }

        return new OrderCreatedEvent(order.getId(), asJson);
    }

    @Override
    public String getAggregateId() {
        return String.valueOf(id);
    }

    @Override
    public String getAggregateType() {
        return &quot;Order&quot;;
    }

    @Override
    public String getType() {
        return &quot;OrderCreated&quot;;
    }

    @Override
    public JsonNode getPayload() {
        return order;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note how &lt;a href=&quot;https://github.com/FasterXML/jackson&quot;&gt;Jackson’s&lt;/a&gt; &lt;code&gt;ObjectMapper&lt;/code&gt; is used to create a JSON representation of the event’s payload.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now let’s take a look at the code that consumes any fired &lt;code&gt;ExportedEvent&lt;/code&gt; and does the corresponding write to the outbox table:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class EventSender {

    @PersistenceContext
    private EntityManager entityManager;

    public void onExportedEvent(@Observes ExportedEvent event) {
        OutboxEvent outboxEvent = new OutboxEvent(
                event.getAggregateType(),
                event.getAggregateId(),
                event.getType(),
                event.getPayload()
        );

        entityManager.persist(outboxEvent);
        entityManager.remove(outboxEvent);
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It’s rather simple: for each event the CDI runtime will invoke the &lt;code&gt;onExportedEvent()&lt;/code&gt; method.
An instance of the &lt;code&gt;OutboxEvent&lt;/code&gt; entity is persisted in the database — and removed right away!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This might be surprising at first.
But it makes sense when remembering how log-based CDC works:
it doesn’t examine the actual contents of the table in the database, but instead it tails the append-only transaction log.
The calls to &lt;code&gt;persist()&lt;/code&gt; and &lt;code&gt;remove()&lt;/code&gt; will create an &lt;code&gt;INSERT&lt;/code&gt; and a &lt;code&gt;DELETE&lt;/code&gt; entry in the log once the transaction commits.
After that, Debezium will process these events:
for any &lt;code&gt;INSERT&lt;/code&gt;, a message with the event’s payload will be sent to Apache Kafka.
&lt;code&gt;DELETE&lt;/code&gt; events on the other hand can be ignored,
as the removal from the outbox table is a mere technicality that doesn’t require any propagation to the message broker.
So we are able to capture the event added to the outbox table by means of CDC,
but when looking at the contents of the table itself, it will always be empty.
This means that no additional disk space is needed for the table
(apart from the log file elements which will automatically be discarded at some point)
and also no separate house-keeping process is required to stop it from growing indefinitely.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;registering_the_debezium_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#registering_the_debezium_connector&quot; /&gt;Registering the Debezium Connector&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With the outbox implementation in place, it’s time to register the Debezium Postgres connector,
so it can capture any new events in the outbox table and relay them to Apache Kafka.
That can be done by POST-ing the following JSON request to the REST API of Kafka Connect:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
    &quot;name&quot;: &quot;outbox-connector&quot;,
    &quot;config&quot;: {
        &quot;connector.class&quot; : &quot;io.debezium.connector.postgresql.PostgresConnector&quot;,
        &quot;tasks.max&quot; : &quot;1&quot;,
        &quot;database.hostname&quot; : &quot;order-db&quot;,
        &quot;database.port&quot; : &quot;5432&quot;,
        &quot;database.user&quot; : &quot;postgresuser&quot;,
        &quot;database.password&quot; : &quot;postgrespw&quot;,
        &quot;database.dbname&quot; : &quot;orderdb&quot;,
        &quot;database.server.name&quot; : &quot;dbserver1&quot;,
        &quot;schema.whitelist&quot; : &quot;inventory&quot;,
        &quot;table.whitelist&quot; : &quot;inventory.outboxevent&quot;,
        &quot;tombstones.on.delete&quot; : &quot;false&quot;,
        &quot;transforms&quot; : &quot;router&quot;,
        &quot;transforms.router.type&quot; : &quot;io.debezium.examples.outbox.routingsmt.EventRouter&quot;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This sets up an instance of &lt;code&gt;io.debezium.connector.postgresql.PostgresConnector&lt;/code&gt;,
capturing changes from the specified Postgres instance.
Note that by means of a table whitelist, only changes from the &lt;code&gt;outboxevent&lt;/code&gt; table are captured.
It also applies a single message transform (SMT) named &lt;code&gt;EventRouter&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Deletion of Events from Kafka Topics&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;By setting the &lt;code&gt;tombstones.on.delete&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt;, no deletion markers (&quot;tombstones&quot;) will be emitted by the connector when an event record gets deleted from the outbox table.
That makes sense, as the deletion from the outbox table shouldn’t affect the retention of events in the corresponding Kafka topics.
Instead, a specific retention time for the event topics may be configured in Kafka,
e.g. to retain all purchase order events for 30 days.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Alternatively, one could work with &lt;a href=&quot;https://kafka.apache.org/documentation/#compaction&quot;&gt;compacted topics&lt;/a&gt;.
This would require some changes to the design of events in the outbox table:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;they must describe the entire aggregate;
so for instance also an event representing the cancelation of a single order line should describe the complete current state of the containing purchase order;
that way consumers will be able to obtain the entire state of the purchase order also when only seeing the last event pertaining to a given order, after log compaction ran.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;they must have one more &lt;code&gt;boolean&lt;/code&gt; attribute indicating whether a particular event represents the deletion of the event’s aggregate root.
Such an event (e.g. of type &lt;code&gt;OrderDeleted&lt;/code&gt;) could then be used by the event routing SMT described in the next section to produce a deletion marker for that aggregate root.
Log compaction would then remove all events pertaining to the given purchase order when its &lt;code&gt;OrderDeleted&lt;/code&gt; event has been written to the topic.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Naturally, when deleting events, the event stream will not be re-playable from its very beginning any longer.
Depending on the specific business requirements, it might be sufficient to just keep the final state of a given purchase order, customer etc.
This could be achieved using compacted topics and a sufficiently value for the topic’s &lt;code&gt;delete.retention.ms&lt;/code&gt; setting.
Another option could be to move historic events to some sort of cold storage (e.g. an Amazon S3 bucket),
from where they can be retrieved if needed, followed by reading the latest events from the Kafka topics.
Which approach to follow depends on the specific requirements, expected amount of data and expertise in the team developing and operating the solution.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;topic_routing&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#topic_routing&quot; /&gt;Topic Routing&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;By default, the Debezium connectors will send all change events originating from one given table to the same topic,
i.e. we’d end up with a single Kafka topic named &lt;code&gt;dbserver1.inventory.outboxevent&lt;/code&gt; which would contain all events,
be it order events, customer events etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To simplify the implementation of consumers which are only interested in specific event types it makes more sense, though,
to have multiple topics, e.g. &lt;code&gt;OrderEvents&lt;/code&gt;, &lt;code&gt;CustomerEvents&lt;/code&gt; and so on.
For instance the shipment service might not be interested in any customer events.
By only subscribing to the &lt;code&gt;OrderEvents&lt;/code&gt; topic, it will be sure to never receive any customer events.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In order to route the change events captured from the outbox table to different topics, that custom SMT &lt;code&gt;EventRouter&lt;/code&gt; is used.
Here is the code of its &lt;code&gt;apply()&lt;/code&gt; method, which will be invoked by Kafka Connect for each record emitted by the Debezium connector:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Override
public R apply(R record) {
    // Ignoring tombstones just in case
    if (record.value() == null) {
        return record;
    }

    Struct struct = (Struct) record.value();
    String op = struct.getString(&quot;op&quot;);

    // ignoring deletions in the outbox table
    if (op.equals(&quot;d&quot;)) {
        return null;
    }
    else if (op.equals(&quot;c&quot;)) {
        Long timestamp = struct.getInt64(&quot;ts_ms&quot;);
        Struct after = struct.getStruct(&quot;after&quot;);

        String key = after.getString(&quot;aggregateid&quot;);
        String topic = after.getString(&quot;aggregatetype&quot;) + &quot;Events&quot;;

        String eventId = after.getString(&quot;id&quot;);
        String eventType = after.getString(&quot;type&quot;);
        String payload = after.getString(&quot;payload&quot;);

        Schema valueSchema = SchemaBuilder.struct()
            .field(&quot;eventType&quot;, after.schema().field(&quot;type&quot;).schema())
            .field(&quot;ts_ms&quot;, struct.schema().field(&quot;ts_ms&quot;).schema())
            .field(&quot;payload&quot;, after.schema().field(&quot;payload&quot;).schema())
            .build();

        Struct value = new Struct(valueSchema)
            .put(&quot;eventType&quot;, eventType)
            .put(&quot;ts_ms&quot;, timestamp)
            .put(&quot;payload&quot;, payload);

        Headers headers = record.headers();
        headers.addString(&quot;eventId&quot;, eventId);

        return record.newRecord(topic, null, Schema.STRING_SCHEMA, key, valueSchema, value,
                record.timestamp(), headers);
    }
    // not expecting update events, as the outbox table is &quot;append only&quot;,
    // i.e. event records will never be updated
    else {
        throw new IllegalArgumentException(&quot;Record of unexpected op type: &quot; + record);
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When receiving a delete event (&lt;code&gt;op&lt;/code&gt; = &lt;code&gt;d&lt;/code&gt;), it will discard that event,
as that deletion of event records from the outbox table is not relevant to downstream consumers.
Things get more interesting, when receiving a create event (&lt;code&gt;op&lt;/code&gt; = &lt;code&gt;c&lt;/code&gt;).
Such record will be propagated to Apache Kafka.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium’s change events have a complex structure, that contain the old (&lt;code&gt;before&lt;/code&gt;) and new (&lt;code&gt;after&lt;/code&gt;) state of the represented row.
The event structure to propagate is obtained from the &lt;code&gt;after&lt;/code&gt; state.
The &lt;code&gt;aggregatetype&lt;/code&gt; value from the captured event record is used to build the name of the topic to send the event to.
For instance, events with &lt;code&gt;aggregatetype&lt;/code&gt; set to &lt;code&gt;Order&lt;/code&gt; will be sent to the &lt;code&gt;OrderEvents&lt;/code&gt; topic.
&lt;code&gt;aggregateid&lt;/code&gt; is used as the message key, making sure all messages of that aggregate will go into the same partition of that topic.
The message value is a structure comprising the original event payload (encoded as JSON),
the timestamp indicating when the event was produced and the event type.
Finally, the event UUID is propagated as a Kafka header field.
This allows for efficient duplicate detection by consumers, without having to examine the actual message contents.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;events_in_apache_kafka&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#events_in_apache_kafka&quot; /&gt;Events in Apache Kafka&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now let’s take a look into the &lt;code&gt;OrderEvents&lt;/code&gt; and &lt;code&gt;CustomerEvents&lt;/code&gt; topics.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you have checked out the example sources and started all the components via Docker Compose
(see the &lt;em&gt;README.md&lt;/em&gt; file in the example project for more details),
you can place purchase orders via the order service’s REST API like so:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cat resources/data/create-order-request.json | http POST http://localhost:8080/order-service/rest/orders&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Similarly, specific order lines can be canceled:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cat resources/data/cancel-order-line-request.json | http PUT http://localhost:8080/order-service/rest/orders/1/lines/2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When using a tool such as the very practical &lt;a href=&quot;https://github.com/edenhill/kafkacat&quot;&gt;kafkacat&lt;/a&gt; utility,
you should now see messages like these in the &lt;code&gt;OrderEvents&lt;/code&gt; topic:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kafkacat -b kafka:9092 -C -o beginning -f &#39;Headers: %h\nKey: %k\nValue: %s\n&#39; -q -t OrderEvents&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Headers: eventId=d03dfb18-8af8-464d-890b-09eb8b2dbbdd
Key: &quot;4&quot;
Value: {&quot;eventType&quot;:&quot;OrderCreated&quot;,&quot;ts_ms&quot;:1550307598558,&quot;payload&quot;:&quot;{\&quot;id\&quot;: 4, \&quot;lineItems\&quot;: [{\&quot;id\&quot;: 7, \&quot;item\&quot;: \&quot;Debezium in Action\&quot;, \&quot;status\&quot;: \&quot;ENTERED\&quot;, \&quot;quantity\&quot;: 2, \&quot;totalPrice\&quot;: 39.98}, {\&quot;id\&quot;: 8, \&quot;item\&quot;: \&quot;Debezium for Dummies\&quot;, \&quot;status\&quot;: \&quot;ENTERED\&quot;, \&quot;quantity\&quot;: 1, \&quot;totalPrice\&quot;: 29.99}], \&quot;orderDate\&quot;: \&quot;2019-01-31T12:13:01\&quot;, \&quot;customerId\&quot;: 123}&quot;}
Headers: eventId=49f89ea0-b344-421f-b66f-c635d212f72c
Key: &quot;4&quot;
Value: {&quot;eventType&quot;:&quot;OrderLineUpdated&quot;,&quot;ts_ms&quot;:1550308226963,&quot;payload&quot;:&quot;{\&quot;orderId\&quot;: 4, \&quot;newStatus\&quot;: \&quot;CANCELLED\&quot;, \&quot;oldStatus\&quot;: \&quot;ENTERED\&quot;, \&quot;orderLineId\&quot;: 7}&quot;}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;payload&lt;/code&gt; field with the message values is the string-ified JSON representation of the original events.
The Debezium Postgres connector emits &lt;code&gt;JSONB&lt;/code&gt; columns as a string
(using the &lt;code&gt;io.debezium.data.Json&lt;/code&gt; logical type name),
which is why the quotes are escaped.
The &lt;a href=&quot;https://stedolan.github.io/jq/&quot;&gt;jq&lt;/a&gt; utility, and more specifically,
its &lt;code&gt;fromjson&lt;/code&gt; operator, come in handy for displaying the event payload in a more readable way:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kafkacat -b kafka:9092 -C -o beginning -t Order | jq &#39;.payload | fromjson&#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
  &quot;id&quot;: 4,
  &quot;lineItems&quot;: [
    {
      &quot;id&quot;: 7,
      &quot;item&quot;: &quot;Debezium in Action&quot;,
      &quot;status&quot;: &quot;ENTERED&quot;,
      &quot;quantity&quot;: 2,
      &quot;totalPrice&quot;: 39.98
    },
    {
      &quot;id&quot;: 8,
      &quot;item&quot;: &quot;Debezium for Dummies&quot;,
      &quot;status&quot;: &quot;ENTERED&quot;,
      &quot;quantity&quot;: 1,
      &quot;totalPrice&quot;: 29.99
    }
  ],
  &quot;orderDate&quot;: &quot;2019-01-31T12:13:01&quot;,
  &quot;customerId&quot;: 123
}
{
  &quot;orderId&quot;: 4,
  &quot;newStatus&quot;: &quot;CANCELLED&quot;,
  &quot;oldStatus&quot;: &quot;ENTERED&quot;,
  &quot;orderLineId&quot;: 7
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can also take a look at the &lt;code&gt;CustomerEvents&lt;/code&gt; topic to inspect the events representing the creation of an invoice when a purchase order is added.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;duplicate_detection_in_the_consuming_service&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#duplicate_detection_in_the_consuming_service&quot; /&gt;Duplicate Detection in the Consuming Service&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;At this point, our implementation of the outbox pattern is fully functional;
when the order service receives a request to place an order
(or cancel an order line),
it will persist the corresponding state in the &lt;code&gt;purchaseorder&lt;/code&gt; and &lt;code&gt;orderline&lt;/code&gt; tables of its database.
At the same time, within the same transaction, corresponding event entries will be added to the outbox table in the same database.
The Debezium Postgres connector captures any insertions into that table
and routes the events into the Kafka topic corresponding to the aggregate type represented by a given event.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To wrap things up, let’s explore how another microservice such as the shipment service can consume these messages.
The entry point into that service is a regular Kafka consumer implementation,
which is not too exciting and hence omitted here for the sake of brevity.
You can find its &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/outbox/shipment-service/src/main/java/io/debezium/examples/outbox/shipment/facade/KafkaEventConsumer.java&quot;&gt;source code&lt;/a&gt; in the example repository.
For each incoming message on the &lt;code&gt;Order&lt;/code&gt; topic, the consumer calls the &lt;code&gt;OrderEventHandler&lt;/code&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class OrderEventHandler {

    private static final Logger LOGGER = LoggerFactory.getLogger(OrderEventHandler.class);

    @Inject
    private MessageLog log;

    @Inject
    private ShipmentService shipmentService;

    @Transactional
    public void onOrderEvent(UUID eventId, String key, String event) {
        if (log.alreadyProcessed(eventId)) {
            LOGGER.info(&quot;Event with UUID {} was already retrieved, ignoring it&quot;, eventId);
            return;
        }

        JsonObject json = Json.createReader(new StringReader(event)).readObject();
        JsonObject payload = json.containsKey(&quot;schema&quot;) ? json.getJsonObject(&quot;payload&quot;) :json;

        String eventType = payload.getString(&quot;eventType&quot;);
        Long ts = payload.getJsonNumber(&quot;ts_ms&quot;).longValue();
        String eventPayload = payload.getString(&quot;payload&quot;);

        JsonReader payloadReader = Json.createReader(new StringReader(eventPayload));
        JsonObject payloadObject = payloadReader.readObject();

        if (eventType.equals(&quot;OrderCreated&quot;)) {
            shipmentService.orderCreated(payloadObject);
        }
        else if (eventType.equals(&quot;OrderLineUpdated&quot;)) {
            shipmentService.orderLineUpdated(payloadObject);
        }
        else {
            LOGGER.warn(&quot;Unkown event type&quot;);
        }

        log.processed(eventId);
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The first thing done by &lt;code&gt;onOrderEvent()&lt;/code&gt; is to check whether the event with the given UUID has been processed before.
If so, any further calls for that same event will be ignored.
This is to prevent any duplicate processing of events caused by the &quot;at least once&quot; semantics of this data pipeline.
For instance it could happen that the Debezium connector or the consuming service fail
before acknowledging the retrieval of a specific event with the source database or the messaging broker, respectively.
In that case, after a restart of Debezium or the consuming service,
a few events may be processed a second time.
Propagating the event UUID as a Kafka message header allows for an efficient detection and exclusion of duplicates in the consumer.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If a message is received for the first time, the message value is parsed and the business method of the &lt;code&gt;ShippingService&lt;/code&gt; method corresponding to the specific event type is invoked with the event payload.
Finally, the message is marked as processed with the message log.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This &lt;code&gt;MessageLog&lt;/code&gt; simply keeps track of all consumed events in a table within the service’s local database:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class MessageLog {

    @PersistenceContext
    private EntityManager entityManager;

    @Transactional(value=TxType.MANDATORY)
    public void processed(UUID eventId) {
        entityManager.persist(new ConsumedMessage(eventId, Instant.now()));
    }

    @Transactional(value=TxType.MANDATORY)
    public boolean alreadyProcessed(UUID eventId) {
        return entityManager.find(ConsumedMessage.class, eventId) != null;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;That way, should the transaction be rolled back for some reason, also the original message will not be marked as processed and an exception would bubble up to the Kafka event consumer loop.
This allows for re-trying to process the message later on.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note that a more complete implementation should take care of re-trying given messages only for a certain number of times,
before re-routing any unprocessable messages to a dead-letter queue or similar.
Also there should be some house-keeping on the message log table;
periodically, all events older than the consumer’s current offset committed with the broker may be deleted,
as it’s ensured that such messages won’t be propagated to the consumer another time.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot; /&gt;Summary&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The outbox pattern is a great way for propagating data amongst different microservices.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;By only modifying a single resource - the source service’s own database -
it avoids any potential inconsistencies of altering multiple resources at the same time which don’t share one common transactional context
(the database and Apache Kafka).
By writing to the database first, the source service has instant &quot;read your own writes&quot; semantics,
which is important for a consistent user experience, allowing query methods invoked following to a write to instantly reflect any data changes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;At the same time, the pattern enables asynchronous event propagation to other microservices.
Apache Kafka acts as a highly scalable and reliable backbone for the messaging amongst the services.
Given the right topic retention settings, new consumers may come up long after an event has been originally produced,
and build up their own local state based on the event history.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Putting Apache Kafka into the center of the overall architecture also ensures a decoupling of involved services.
If for instance single components of the solution fail or are not available for some time, e.g. during an update,
events will simply be processed later on: after a restart,
the Debezium connector will continue to tail the outbox table from the point where it left off before.
Similarly, any consumer will continue to process topics from its previous offset.
By keeping track of already successfully processed messages, duplicates can be detected and excluded from repeated handling.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Naturally, such event pipeline between different services is eventually consistent,
i.e. consumers such as the shipping service may lag a bit behind producers such as the order service.
Usually, that’s just fine, though, and can be handled in terms of the application’s business logic.
For instance there’ll typically be no need to create a shipment within the very same second as an order has been placed.
Also, end-to-end delays of the overall solution are typically low (seconds or even sub-second range),
thanks to log-based change data capture which allows for emission of events in near-realtime.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One last thing to keep in mind is that the structure of the events exposed via the outbox should be considered a part of the emitting service’s API.
I.e. when needed, their structure should be adjusted carefully and with compatibility considerations in mind.
This is to ensure to not accidentally break any consumers when upgrading the producing service.
At the same time, consumers should be lenient when handling messages and for instance not fail when encountering unknown attributes within received events.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;em&gt;Many thanks to Hans-Peter Grahsl, Jiri Pechanec, Justin Holmes and René Kerner for their feedback while writing this post!&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/02/13/debezium-webinar-at-devnation-live/</id>
<title>Debezium at DevNation Live</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-02-13T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/02/13/debezium-webinar-at-devnation-live/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="introduction"></category>
<category term="presentation"></category>
<summary>



Last week I had the pleasure to do a webinar on change data streaming patterns for microservices with the fabulous Burr Sutter at DevNation Live.


The recording of that 30 min session is available on YouTube now.
It also contains a demo that shows how to set-up a data streaming pipeline with Debezium and Apache Kafka,
running on OpenShift.
The demo begins at 12 min 40 into the recording.


Enjoy!



-->


&#160;
You can also find the slide deck (in a slightly extended version) on Speaker Deck:
&#160;
&#160;







About Debezium


Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Last week I had the pleasure to do a &lt;a href=&quot;https://developers.redhat.com/videos/youtube/QYbXDp4Vu-8/&quot;&gt;webinar on change data streaming patterns for microservices&lt;/a&gt; with the fabulous Burr Sutter at DevNation Live.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The recording of that 30 min session is available on YouTube now.
It also contains a demo that shows how to set-up a data streaming pipeline with Debezium and Apache Kafka,
running on OpenShift.
The demo begins at 12 min 40 into the recording.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Enjoy!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;responsive-video&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/QYbXDp4Vu-8&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot; /&gt;
&lt;!--&lt;iframe width=&quot;1600&quot; height=&quot;900&quot; src=&quot;https://www.youtube.com/embed/IOZ2Um6e430?rel=0&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;--&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt; &lt;br&gt;
You can also find the &lt;a href=&quot;https://speakerdeck.com/gunnarmorling/change-data-streaming-patterns-for-microservices-with-debezium-apache-kafka-meetup-hamburg&quot;&gt;slide deck&lt;/a&gt; (in a slightly extended version) on Speaker Deck:
 &lt;br&gt;
 &lt;br /&gt;
&lt;/br&gt;
&lt;div style=&quot;text-align-center&quot;&gt;
&lt;script async=&quot;&quot; class=&quot;speakerdeck-embed&quot; data-id=&quot;c390d77e50464c99916ede7368a279c2&quot; data-ratio=&quot;1.77777777777778&quot; src=&quot;//speakerdeck.com/assets/embed.js&quot; /&gt;
&lt;/div&gt;
&lt;/br&gt;
&lt;/p&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/02/13/debezium-0-9-1-final-released/</id>
<title>Debezium 0.9.1.Final Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-02-13T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/02/13/debezium-0-9-1-final-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="mongodb"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



Quickly following up to last week&#8217;s release of Debezium 0.9, it&#8217;s my pleasure today to announce the release of Debezium 0.9.1.Final!


This release fixes a couple of bugs which were reported after the 0.9 release.
Most importantly, there are two fixes to the new Debezium connector for SQL Server,
which deal with correct handling of LSNs after connector restarts (DBZ-1128, DBZ-1131).
The connector also uses more reasonable defaults for the selectMethod and fetchSize options of the SQL Server JDBC driver (DBZ-1065),
which can help to significantly increase through-put and reduce memory consumption of the connector.


The MySQL connector supports GENERATED columns now with the new Antlr-based...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Quickly following up to last week’s release of Debezium 0.9, it’s my pleasure today to announce the release of Debezium &lt;strong&gt;0.9.1.Final&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This release fixes a couple of bugs which were reported after the 0.9 release.
Most importantly, there are two fixes to the new &lt;a href=&quot;https://debezium.io/docs/connectors/sqlserver/&quot;&gt;Debezium connector for SQL Server&lt;/a&gt;,
which deal with correct handling of LSNs after connector restarts (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1128&quot;&gt;DBZ-1128&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1131&quot;&gt;DBZ-1131&lt;/a&gt;).
The connector also uses more reasonable defaults for the &lt;code&gt;selectMethod&lt;/code&gt; and &lt;code&gt;fetchSize&lt;/code&gt; options of the SQL Server JDBC driver (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1065&quot;&gt;DBZ-1065&lt;/a&gt;),
which can help to significantly increase through-put and reduce memory consumption of the connector.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The MySQL connector supports &lt;code&gt;GENERATED&lt;/code&gt; columns now with the new Antlr-based DDL parser (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1123&quot;&gt;DBZ-1123&lt;/a&gt;),
and for the Postgres connector the handling of primary key column definition changes was improved (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-997&quot;&gt;DBZ-997&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In terms of new features, there is a new container image provided on Docker Hub now:
the &lt;a href=&quot;https://hub.docker.com/r/debezium/tooling&quot;&gt;debezium/tooling&lt;/a&gt; image contains a couple of open-source CLI tools
(currently &lt;a href=&quot;https://github.com/edenhill/kafkacat&quot;&gt;kafkacat&lt;/a&gt;, &lt;a href=&quot;https://github.com/jakubroztocil/httpie&quot;&gt;httpie&lt;/a&gt;, &lt;a href=&quot;https://github.com/stedolan/jq&quot;&gt;jq&lt;/a&gt;, &lt;a href=&quot;https://github.com/dbcli/mycli&quot;&gt;mycli&lt;/a&gt; and &lt;a href=&quot;https://github.com/dbcli/pgcli&quot;&gt;pqcli&lt;/a&gt;)
which greatly help when working with Debezium connectors, Apache Kafka and Kafka Connect on the command line
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1125&quot;&gt;DBZ-1125&lt;/a&gt;).
A big thank you to the respective authors these fantastic tools!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/debezium_shell.gif&quot; class=&quot;responsive-image&quot; alt=&quot;CLI tools for working with Debezium&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Altogether, &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.9.1.Final&quot;&gt;12 issues&lt;/a&gt; were resolved in this release.
Please refer to the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-9-1-final&quot;&gt;release notes&lt;/a&gt; to learn more about all fixed bugs, update procedures etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Thanks a lot to community members &lt;a href=&quot;https://github.com/ivan-lorenz&quot;&gt;Ivan Lorenz&lt;/a&gt; and &lt;a href=&quot;https://github.com/tomazlemos&quot;&gt;Tomaz Lemos Fernandes&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/02/05/debezium-0-9-0-final-released/</id>
<title>Debezium 0.9.0.Final Released</title>
<updated>2020-04-17T07:16:16+00:00</updated>
<published>2019-02-05T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/02/05/debezium-0-9-0-final-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="mongodb"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



I&#8217;m delighted to announce the release of Debezium 0.9 Final!


This release only adds a small number of changes since last week&#8217;s CR1 release;
most prominently there&#8217;s some more metrics for the SQL Server connector
(lag behind master, number of transactions etc.)
and two bug fixes related to the handling of partitioned tables in MySQL (DBZ-1113) and Postgres (DBZ-1118).


Having been in the works for six months after the initial Alpha release,
Debezium 0.9 comes with a brand new connector for SQL Server,
lots of new features and improvements for the existing connectors,
updates to the latest versions of Apache Kafka and the supported databases
as well as a...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I’m delighted to announce the release of Debezium &lt;strong&gt;0.9 Final&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This release only adds a small number of changes since last week’s CR1 release;
most prominently there’s some more metrics for the SQL Server connector
(lag behind master, number of transactions etc.)
and two bug fixes related to the handling of partitioned tables in MySQL (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1113&quot;&gt;DBZ-1113&lt;/a&gt;) and Postgres (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1118&quot;&gt;DBZ-1118&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Having been in the works for six months after the initial Alpha release,
Debezium 0.9 comes with a brand new &lt;a href=&quot;https://debezium.io/docs/connectors/sqlserver/&quot;&gt;connector for SQL Server&lt;/a&gt;,
lots of new features and improvements for the existing connectors,
updates to the latest versions of Apache Kafka and the supported databases
as well as a wide range of bug fixes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Some key features of the release besides the aforementioned CDC connector for SQL Server are:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Initial snapshotting for the Oracle connector (which remains to be a &quot;tech preview&quot; at this point)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Brand-new metrics for the SQL Server and Oracle connectors and extended metrics for the MySQL connector&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Field filtering and renaming for MongoDB&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A new handler interface for the embedded engine&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lots of improvements around the &quot;event flattening&quot; SMT for MongoDB&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;More detailed source info in CDC events and optional metadata such as a column’s source type&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Option to delay snapshots for a given time&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support for HSTORE columns in Postgres&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Incubating support for picking up changes to the whitelist/blacklist configuration of the MySQL connector&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As a teaser on the connector metrics support, here’s a screenshot of &lt;a href=&quot;https://openjdk.java.net/projects/jmc/&quot;&gt;Java Mission Control&lt;/a&gt;
displaying the SQL Server connector metrics:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/java_mission_control.png&quot; class=&quot;responsive-image&quot; alt=&quot;Monitoring the Debezium SQL Server connector&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The list above is far from being exhaustive; please take a look at the preview release announcements
(&lt;a href=&quot;https://debezium.io/blog/2018/07/26/debezium-0-9-0-alpha1-released/&quot;&gt;Alpha1&lt;/a&gt;,
&lt;a href=&quot;https://debezium.io/blog/2018/10/04/debezium-0-9-0-alpha2-released/&quot;&gt;Alpha2&lt;/a&gt;,
&lt;a href=&quot;https://debezium.io/blog/2018/11/22/debezium-0-9-0-beta1-released/&quot;&gt;Beta1&lt;/a&gt;,
&lt;a href=&quot;https://debezium.io/blog/2018/12/19/debezium-0-9-0-beta2-released/&quot;&gt;Beta2&lt;/a&gt; and
&lt;a href=&quot;https://debezium.io/blog/2019/01/28/debezium-0-9-0-cr1-released/&quot;&gt;CR 1&lt;/a&gt;)
as well as the full list of a whopping &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20in%20(0.9.0.Alpha1%2C%200.9.0.Alpha2%2C%200.9.0.Beta1%2C%200.9.0.Beta2%2C%200.9.0.CR1%2C%200.9.0.Final)%20ORDER%20BY%20issuetype%20ASC&amp;amp;startIndex=120&quot;&gt;176 fixed issues&lt;/a&gt; in JIRA.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It’s hard to say which of the changes and new features I’m most excited about,
but one thing surely sticking out is the tremendous amount of community work on this release.
Not less than 34 different members of Debezium’s outstanding community have contributed to this release.
A huge and massive &quot;Thank You!&quot; to all of you:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/addisonj&quot;&gt;Addison Higham&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;,
&lt;a href=&quot;https://github.com/anton-martynov&quot;&gt;Anton Martynov&lt;/a&gt;,
&lt;a href=&quot;https://github.com/amitsela&quot;&gt;Amit Sela&lt;/a&gt;,
&lt;a href=&quot;https://github.com/artiship&quot;&gt;Artiship Artiship&lt;/a&gt;,
&lt;a href=&quot;https://github.com/CliffWheadon&quot;&gt;Cliff Wheadon&lt;/a&gt;,
&lt;a href=&quot;https://github.com/deepakbarr&quot;&gt;Deepak Barr&lt;/a&gt;,
&lt;a href=&quot;https://github.com/pimpelsang&quot;&gt;Eero Koplimets&lt;/a&gt;,
&lt;a href=&quot;https://github.com/gaganpaytm&quot;&gt;Gagan Agrawal&lt;/a&gt;,
&lt;a href=&quot;https://github.com/grzegorz8&quot;&gt;Grzegorz Kołakowski&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ian-axelrod&quot;&gt;Ian Axelrod&lt;/a&gt;,
&lt;a href=&quot;https://github.com/Ipshin&quot;&gt;Ilia Bogdanov&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ivankovbas&quot;&gt;Ivan Kovbas&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jgao54&quot;&gt;Joy Gao&lt;/a&gt;,
&lt;a href=&quot;https://github.com/kppullin&quot;&gt;Kevin Pullin&lt;/a&gt;,
&lt;a href=&quot;https://github.com/sweat123&quot;&gt;Lao Mei&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ooooorz&quot;&gt;Liu Hanlin&lt;/a&gt;,
&lt;a href=&quot;https://github.com/maver1ck&quot;&gt;Maciej Bryński&lt;/a&gt;,
&lt;a href=&quot;https://github.com/mtagle&quot;&gt;Moira Tagle&lt;/a&gt;,
&lt;a href=&quot;https://github.com/olavim&quot;&gt;Olavi Mustanoja&lt;/a&gt;,
&lt;a href=&quot;https://github.com/olivierlemasle&quot;&gt;Olivier Lemasle&lt;/a&gt;,
&lt;a href=&quot;https://github.com/oripwk&quot;&gt;Ori Popowski&lt;/a&gt;,
&lt;a href=&quot;https://github.com/PengLyu&quot;&gt;Peng Lyu&lt;/a&gt;,
&lt;a href=&quot;https://github.com/plarsson&quot;&gt;Peter Larsson&lt;/a&gt;,
&lt;a href=&quot;https://github.com/PSanetra&quot;&gt;Philip Sanetra&lt;/a&gt;,
&lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt;,
&lt;a href=&quot;https://github.com/sagarrao&quot;&gt;Sagar Rao&lt;/a&gt;,
&lt;a href=&quot;https://github.com/shivamsharma&quot;&gt;Shivam Sharma&lt;/a&gt;,
&lt;a href=&quot;https://github.com/SyedMuhammadSufyian&quot;&gt;Syed Muhammad Sufyian&lt;/a&gt;,
&lt;a href=&quot;https://github.com/tautautau&quot;&gt;Tautvydas Januskevicius&lt;/a&gt;,
&lt;a href=&quot;https://github.com/Tapppi&quot;&gt;Tapani Moilanen&lt;/a&gt;,
&lt;a href=&quot;https://github.com/trizko&quot;&gt;Tony Rizko&lt;/a&gt;
&lt;a href=&quot;https://github.com/wscheep&quot;&gt;Wout Scheepers&lt;/a&gt; and
&lt;a href=&quot;https://github.com/wangzheng422&quot;&gt;Zheng Wang&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When upgrading from earlier Debezium releases,
please make sure to read the information regarding update procedures and breaking changes in the &lt;a href=&quot;https://debezium.io/docs/releases/&quot;&gt;release notes&lt;/a&gt;.
One relevant change to the users of the Debezium connector for MySQL is that our new Antlr-based DDL parser is used by default now.
After lots of honing we felt it’s time for using the new parser by default now.
While the existing parser can still be used as a fallback as of Debezium 0.9,
it will be phased out in 0.10.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;next_steps&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#next_steps&quot; /&gt;Next Steps&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;After some drinks to celebrate this release, the plan is to do a 0.9.1 release rather quickly
(probably in two weeks from now),
providing improvements and potential bug fixes to the features and changes done in 0.9.
We’ll also begin the work on Debezium 0.10,
stay tuned for the details on that!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For further plans beyond that, take a look at our &lt;a href=&quot;https://debezium.io/docs/roadmap/&quot;&gt;road map&lt;/a&gt;.
Any suggestions and ideas are very welcomed on &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; or in the comments below.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you’re just about to begin using Debezium for streaming changes out of your database,
you might be interested in join us for the &lt;a href=&quot;https://www.redhat.com/en/events/webinar/change-data-streaming-patterns-microservices-kafka-and-debezium&quot;&gt;upcoming webinar&lt;/a&gt; on February 7th.
After a quick overview, you’ll see Debezium in action, as it streams changes to a browser-based dashboard and more.
You can also find lots of resources around Debezium and change data capture such as blog posts and presentations in our curated &lt;a href=&quot;https://debezium.io/docs/online-resources/&quot;&gt;list of online resources&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
</feed>
