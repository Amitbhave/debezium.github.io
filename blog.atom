<?xml version="1.0" encoding="utf-8" ?>
<feed xml:lang="en-US" xmlns="http://www.w3.org/2005/Atom">
  <id>http://debezium.io/</id>
  <title>Debezium Blog</title>
  <updated>2016-09-09T11:50:54+00:00</updated>
  <link href="/blog.atom" rel="self" type="application/atom+xml" />
  <link href="http://debezium.io/" rel="alternate" type="text/html" />
  <entry>
    <id>/blog/2016/08/30/Debezium-0-3-1-Released/</id>
    <title>Debezium 0.3.1 Released</title>
    <updated>2016-09-09T11:50:54+00:00</updated>
    <published>2016-08-30T00:00:00+00:00</published>
    <link href="/blog/2016/08/30/Debezium-0-3-1-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.3.1 is now available for use with Kafka Connect 0.10.0.1. This release contains an updated MySQL connector with a handful of bug fixes and two significant but backward-compatible changes. First, the MySQL connector now supports using secure connections to MySQL, adding to the existing ability to connect securely to Kafka. Second, the MySQL connector is able to capture MySQL string values using the proper character sets so that any values stored in the database can be captured correctly in events. See our release notes for details of these changes and for upgrading recommendations.
      
      
      We&#8217;ve also...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.3.1&lt;/strong&gt; is now available for use with Kafka Connect 0.10.0.1. This release contains an updated &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt; with a handful of bug fixes and two significant but backward-compatible changes. First, the MySQL connector now supports using secure connections to MySQL, adding to the existing ability to connect securely to Kafka. Second, the MySQL connector is able to capture MySQL string values using the proper character sets so that any values stored in the database can be captured correctly in events. See our &lt;a href=&quot;http://debezium.io/docs/releases#release-0-3-1&quot;&gt;release notes&lt;/a&gt; for details of these changes and for upgrading recommendations.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Chris, Akshath, barten, and and others for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/08/16/Debezium-0-3-0-Released/</id>
    <title>Debezium 0.3.0 Released</title>
    <updated>2016-09-09T11:50:54+00:00</updated>
    <published>2016-08-16T00:00:00+00:00</published>
    <link href="/blog/2016/08/16/Debezium-0-3-0-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      After a few weeks delay, Debezium 0.3.0 is now available for use with Kafka Connect 0.10.0.1. This release contains an updated MySQL connector with quite a few bug fixes, and a new MongoDB connector that captures the changes made to a MongoDB replica set or MongoDB sharded cluster. See the documentation for details about how to configure these connectors and how they work.
      
      
      We&#8217;ve also updated the Debezium Docker images (with labels 0.3 and latest) used in our tutorial.
      
      
      Thanks to Andrew, Bhupinder, Chris, David, Horia, Konstantin, Tony, and others for their help with the release, issues, discussions, contributions, and questions!
      
      
      
      
      About Debezium
      
      
      Debezium...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;After a few weeks delay, &lt;strong&gt;Debezium 0.3.0 is now available&lt;/strong&gt; for use with Kafka Connect 0.10.0.1. This release contains an updated &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt; with quite a few bug fixes, and a new &lt;strong&gt;&lt;em&gt;&lt;a href=&quot;http://debezium.io/docs/connectors/mongodb&quot;&gt;MongoDB connector&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt; that captures the changes made to a MongoDB replica set or MongoDB sharded cluster. See the &lt;a href=&quot;http://debezium.io/docs/connectors&quot;&gt;documentation&lt;/a&gt; for details about how to configure these connectors and how they work.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; (with labels &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;) used in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Andrew, Bhupinder, Chris, David, Horia, Konstantin, Tony, and others for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/08/16/Debezium-0-2-4-Released/</id>
    <title>Debezium 0.2.4 Released</title>
    <updated>2016-09-09T11:50:54+00:00</updated>
    <published>2016-08-16T00:00:00+00:00</published>
    <link href="/blog/2016/08/16/Debezium-0-2-4-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m happy to announce that Debezium 0.2.4 is now available for use with Kafka Connect 0.9.0.1. This release adds more verbose logging during MySQL snapshots, enables taking snapshots of very large MySQL databases, and correct a potential exception during graceful shutdown. See our release notes for details of these changes and for upgrading recommendations.
      
      
      We&#8217;ve also updated the Debezium Docker images (with label 0.2 and latest) used in our tutorial.
      
      
      Thanks to David and wangshao for their help with the release, issues, discussions, contributions, and questions!
      Stay tuned for our next release, which will be 0.3 and will have a new MongoDB connector...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m happy to announce that &lt;strong&gt;Debezium 0.2.4 is now available&lt;/strong&gt; for use with Kafka Connect 0.9.0.1. This release adds more verbose logging during MySQL snapshots, enables taking snapshots of very large MySQL databases, and correct a potential exception during graceful shutdown. See our &lt;a href=&quot;http://debezium.io/docs/releases#release-0-2-4&quot;&gt;release notes&lt;/a&gt; for details of these changes and for upgrading recommendations.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; (with label &lt;code&gt;0.2&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;) used in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to David and wangshao for their help with the release, issues, discussions, contributions, and questions!
      Stay tuned for our next release, which will be 0.3 and will have a new MongoDB connector and will support Kafka Connect 0.10.0.1.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/08/02/capturing-changes-from-mysql/</id>
    <title>Capturing changes from MySQL</title>
    <updated>2016-09-09T11:50:54+00:00</updated>
    <published>2016-08-02T00:00:00+00:00</published>
    <link href="/blog/2016/08/02/capturing-changes-from-mysql/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="mysql"></category>
    <summary>
      
      
      
      Change data capture is a hot topic. Debezium&#8217;s goal is to make change data capture easy for multiple DBMSes, but admittedly we&#8217;re still a young open source project and so far we&#8217;ve only released a connector for MySQL with a connector for MongoDB that&#8217;s just around the corner. So it&#8217;s great to see how others are using and implementing change data capture. In this post, we&#8217;ll review Yelp&#8217;s approach and see how it is strikingly similar to Debezium&#8217;s MySQL connector.
      
      
      
      
      Streaming data at Yelp
      
      
      The Yelp Engineering Blog recently began a series describing their real-time streaming data infrastructure. The first post provides...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Change data capture is a hot topic. Debezium’s goal is to make change data capture easy for multiple DBMSes, but admittedly we’re still a young open source project and so far we’ve only released a &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;connector for MySQL&lt;/a&gt; with a &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb&quot;&gt;connector for MongoDB&lt;/a&gt; that’s just around the corner. So it’s great to see how others are using and implementing change data capture. In this post, we’ll review Yelp’s approach and see how it is strikingly similar to Debezium’s MySQL connector.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_streaming_data_at_yelp&quot;&gt;Streaming data at Yelp&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;a href=&quot;http://engineeringblog.yelp.com/&quot;&gt;Yelp Engineering Blog&lt;/a&gt; recently began a series describing their real-time streaming data infrastructure. The &lt;a href=&quot;http://engineeringblog.yelp.com/2016/07/billions-of-messages-a-day-yelps-real-time-data-pipeline.html&quot;&gt;first post&lt;/a&gt; provides a good introduction and explains how moving from a monolith to a service-oriented architecture increased productivity, but also made it more challenging to work with data spread across the 100 services that own it. It’s totally worth your time to read it right now.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As Justin writes in the post, several reasons prompted them to create their own real time streaming data pipeline:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Ensuring data always remains consistent across services is always a difficult task, but especially so when things can and do go wrong. Transactions across services may be useful in some situations, but they’re not straightforward, are expensive, and can lead to request amplification where one service calls another, which coordinates with two others, etc.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Services that update data in multiple backend services suffer from the &lt;a href=&quot;http://www.confluent.io/blog/using-logs-to-build-a-solid-data-infrastructure-or-why-dual-writes-are-a-bad-idea/&quot;&gt;dual write problem&lt;/a&gt;, which is where a failure occurs after one backing service was updated but before the other could be updated and that always results in data inconsistencies that are difficult to track down and correct.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Combining and integrating data spread across multiple services can also be difficult and expensive, but it is even harder when that data is continously changing. One approach is to use bulk APIs, but these can beprohibitive to create, can result in inconsistencies, and pose real scalability problems when services need to continually receive the never-ending updates to data.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Yelp’s Real-Time Data Pipeline records changes to data on totally ordered distributed logs so that downstream consumers can receive and process the same changes in exactly the same order. Services can consume changes made by other services, and can therefore stay in sync without explicit interservice communication. This system uses among other things Kafka for event logs, a homegrown system named &lt;a href=&quot;http://engineeringblog.yelp.com/2016/08/streaming-mysql-tables-in-real-time-to-kafka.html&quot;&gt;MySQLStreamer&lt;/a&gt; to capture committed changes to MySQL tables, &lt;a href=&quot;http://avro.apache.org&quot;&gt;Avro&lt;/a&gt; for message format and schemas, and a custom &lt;a href=&quot;http://engineeringblog.yelp.com/2016/07/billions-of-messages-a-day-yelps-real-time-data-pipeline.html#yelps-real-time-data-pipeline&quot;&gt;Schematizer&lt;/a&gt; service that tracks consumers and enforces the Avro schemas used for messages on every Kafka topic.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_how_yelp_captures_mysql_changes&quot;&gt;How Yelp captures MySQL changes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Perhaps most interesting for Debezium is how Yelp captures the committed changes in their MySQL databases and write them to Kafka topics. Their &lt;a href=&quot;http://engineeringblog.yelp.com/2016/08/streaming-mysql-tables-in-real-time-to-kafka.html&quot;&gt;second post in the series&lt;/a&gt; goes into a lot more detail about their MySQLStreamer process that reads the MySQL binary log and continously processes the DDL statements and DML operations that appear in the log, generating the corresponding &lt;em&gt;insert&lt;/em&gt;, &lt;em&gt;update&lt;/em&gt;, &lt;em&gt;delete&lt;/em&gt;, and &lt;em&gt;refresh&lt;/em&gt; events, and writing these event messages to a separate Kafka topic for each MySQL table. We’ve &lt;a href=&quot;http://debezium.io/blog/2016-04-15-parsing-ddl&quot;&gt;mentioned before&lt;/a&gt; that MySQL’s row-level binlog events that result from the DML operation don’t include the full definition of the columns, so knowing what the columns mean in each event requires process the DDL statements that also appear in the binlog. Yelp uses a separate MySQL instance it calls the &lt;em&gt;schema tracker database&lt;/em&gt;, which behaves like a MySQL slave to which are applied only the DDL statements they read from the binlog. This technique lets Yelp’s MySQLStreamer system know the state of the database schema and the structure of its tables at the point in the binlog where they are processing events. This is pretty interesting, because it uses the MySQL engine to handle the DDL parsing.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Yelp’s MySQLStreamer process uses another MySQL database to track internal state describing its position in the binlog, what events have been successfully published to Kafka, and, because the binlog position varies on each replica, replica-independent information about each transaction. This latter information is similar to MySQL GTIDs, although Yelp is using earlier versions of MySQL that do not support GTIDs.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Of course, special consideration has to be taken for databases that have been around for a long time. The MySQL binlogs are capped and will not contain the &lt;em&gt;entire&lt;/em&gt; history of the databases, so Yelp’s MySQLStreamer process bootstraps the change data capture process of old databases by starting another clean MySQL replica, which will use the built-in MySQL replication mechanism with the &lt;a href=&quot;http://dev.mysql.com/doc/refman/5.7/en/blackhole-storage-engine.html&quot;&gt;MySQL blackhole database engine&lt;/a&gt; to obtain a consistent snapshot of the master and so that all activity is logged in the replica’s binlog while the replica actually stores no data.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Yelp’s MySQLStreamer mechanism is quite ingenious in its use of MySQL and multiple extra databases to capture changes from MySQL databases and write them to Kafka topics. The downside, of course, is that doing so does increase the operational complexity of the system.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_similar_purpose_similar_approach&quot;&gt;Similar purpose, similar approach&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source project that is building a change data capture for a variety of DBMSes. Like Yelp’s MySQLStreamer, Debezium’s &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL Connector&lt;/a&gt; can continously capture the committed changes to MySQL database rows and record these events in a separate Kafka topic for each table. When first started, Debezium’s MySQL Connector can perform an initial consistent snapshot and then begin reading the MySQL binlog. It uses both DDL and DML operations that appear in the binlog, directly &lt;a href=&quot;http://debezium.io/blog/2016-04-15-parsing-ddl&quot;&gt;parsing and using the DDL statements&lt;/a&gt; to learn the changes to each table’s structure and the mapping of each insert, update, and delete binlog event. And each resulting change event written to Kafka includes information about the originating MySQL server and its binlog position, as well as the before and/or after states of the affected row.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;However, unlike Yelp’s MySQLStreamer, the Debezium MySQL connector doesn’t need or use extra MySQL databases to parse DDL or to store the connector’s state. Instead, Debezium is built on top of Kafka Connect, which is a new Kafka library that provides much of the generic functionality of reliably pulling data from external systems, pushing it into Kafka topics, and tracking what data has already been processed. Kafka Connect stores this state inside Kafka itself, simplifying the operational footprint. Debezium’s MySQL connector can then focus on the details of performing a consistent snapshot when required, reading the binlog, and converting the binlog events into useful change events.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Yelp’s real time data pipeline makes use of a custom Avro schema registry, and uses those Avro schemas to encode each event into a compact binary representation while keeping the metadata about the structure of the event. It’s possible to do this with Debezium, too: simply run &lt;a href=&quot;http://docs.confluent.io/3.0.0/schema-registry/docs/index.html&quot;&gt;Confluent’s Schema Registry&lt;/a&gt; as a service and then configure the Kafka Connect worker to use the &lt;a href=&quot;http://debezium.io/docs/faq#avro-converter&quot;&gt;Avro Converter&lt;/a&gt;. As the converter serializes each event, it looks at the structure defined by the connector and, when that structure changes, generates an updated Avro Schema and registers it with the Schema Registry. That new Avro schema is then used to encode the event (and others with an identical structure) into a compact binary form written to Kafka. And of course, consumers then also use the same Avro converter so that as events are deserialized, the converter coordinates with the Schema Registry whenever it needs an Avro schema it doesn’t know about. As a result, the events are stored in a compact manner while the events' content and metadata remain available, while Schema Registry captures and maintains the history of the Avro schema for each table as it evolves over time.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_capturing_changes_from_mysql_with_debezium&quot;&gt;Capturing changes from MySQL with Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’re interested in change data capture with MySQL (or any other DBMSes), give Debezium a try by going through &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;our tutorial&lt;/a&gt; that walks you through starting Kafka, Kafka Connect, and Debezium’s MySQL Connector to see exactly what change data events look like and how they can be used. Best of all, it’s open source with a growing community of developers that has had the benefit of building on top of recently-created Kafka Connect framework. Our MySQL connector is ready now, but we’re working on &lt;a href=&quot;http://debezium.io/docs/connectors/&quot;&gt;connectors for other DBMSes&lt;/a&gt;. Specifically, our upcoming 0.3 release will include our &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb&quot;&gt;MongoDB Connector&lt;/a&gt;, with 0.4 including connectors for PostgreSQL and/or Oracle.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;em&gt;Correction: A previous version of this post incorrectly stated that Yelp was using a MySQL version that did support GTIDs, when in fact they are using a version that does &lt;strong&gt;not&lt;/strong&gt; support MySQL GTIDs. The post has been corrected, and the author regrets the mistake.&lt;/em&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/07/26/Debezium-0-2-3-Released/</id>
    <title>Debezium 0.2.3 Released</title>
    <updated>2016-09-09T11:50:54+00:00</updated>
    <published>2016-07-26T00:00:00+00:00</published>
    <link href="/blog/2016/07/26/Debezium-0-2-3-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m happy to announce that Debezium 0.2.3 is now available for use with Kafka Connect 0.9.0.1. This release corrects the MySQL connector behavior when working with TINYINT and SMALLINT columns or with TIME, DATE, and TIMESTAMP columns. See our release notes for details of these changes and for upgrading recommendations.
      
      
      We&#8217;ve also updated the Debezium Docker images (with label 0.2 and latest) used in our tutorial.
      
      
      Thanks to Chris, Christian, Laogang, and Tony for their help with the release, issues, discussions, contributions, and questions!
      Stay tuned for our next release, which will be 0.3 and will have a new MongoDB connector and will...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m happy to announce that &lt;strong&gt;Debezium 0.2.3 is now available&lt;/strong&gt; for use with Kafka Connect 0.9.0.1. This release corrects the MySQL connector behavior when working with &lt;code&gt;TINYINT&lt;/code&gt; and &lt;code&gt;SMALLINT&lt;/code&gt; columns or with &lt;code&gt;TIME&lt;/code&gt;, &lt;code&gt;DATE&lt;/code&gt;, and &lt;code&gt;TIMESTAMP&lt;/code&gt; columns. See our &lt;a href=&quot;http://debezium.io/docs/releases#release-0-2-3&quot;&gt;release notes&lt;/a&gt; for details of these changes and for upgrading recommendations.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; (with label &lt;code&gt;0.2&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;) used in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Chris, Christian, Laogang, and Tony for their help with the release, issues, discussions, contributions, and questions!
      Stay tuned for our next release, which will be 0.3 and will have a new MongoDB connector and will support Kafka Connect 0.10.0.0.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/06/22/Debezium-0-2-2-Released/</id>
    <title>Debezium 0.2.2 Released</title>
    <updated>2016-09-09T11:50:54+00:00</updated>
    <published>2016-06-22T00:00:00+00:00</published>
    <link href="/blog/2016/06/22/Debezium-0-2-2-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m happy to announce that Debezium 0.2.2 is now available. This release fixes several bugs in the MySQL connector that can produce change events with incorrect source metadata, and that eliminates the possibility a poorly-timed connector crash causing the connector to only process some of the rows in a multi-row MySQL event. See our release notes for details of these changes and for upgrading recommendations.
      
      
      Also, thanks to a community member for reporting that Debezium 0.2.x can only be used with Kafka Connect 0.9.0.1. Debezium 0.2.x cannot be used with Kafka Connect 0.10.0.0 because of its backward incompatible changes to the...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m happy to announce that &lt;strong&gt;Debezium 0.2.2 is now available&lt;/strong&gt;. This release fixes several bugs in the MySQL connector that can produce change events with incorrect &lt;code&gt;source&lt;/code&gt; metadata, and that eliminates the possibility a poorly-timed connector crash causing the connector to only process some of the rows in a multi-row MySQL event. See our &lt;a href=&quot;http://debezium.io/docs/releases#release-0-2-2&quot;&gt;release notes&lt;/a&gt; for details of these changes and for upgrading recommendations.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also, thanks to a community member for &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-80&quot;&gt;reporting&lt;/a&gt; that Debezium 0.2.x can only be used with Kafka Connect 0.9.0.1. Debezium 0.2.x cannot be used with Kafka Connect 0.10.0.0 because of its &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3006&quot;&gt;backward incompatible changes to the consumer API&lt;/a&gt;. Our next release of Debezium will support Kafka 0.10.x.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; (with label &lt;code&gt;0.2&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;) used in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;. And stay tuned, because we’re hoping to add a MongoDB connector in our next release.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Chris, Christian, Konstantin, James, and Bhupinder for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/06/10/Debezium-0/</id>
    <title>Debezium 0.2.1 Released</title>
    <updated>2016-09-09T11:50:54+00:00</updated>
    <published>2016-06-10T00:00:00+00:00</published>
    <link href="/blog/2016/06/10/Debezium-0/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m happy to announce that Debezium 0.2.1 is now available. The MySQL connector has been significantly improved and is now able to monitor and produce change events for HA MySQL clusters using GTIDs, perform a consistent snapshot when starting up the first time, and has a completely redesigned event message structure that provides a ton more information with every event. Our change log has all the details about bugs, enhancements, new features, and backward compatibility notices. We&#8217;ve also updated our tutorial.
      
      
      
      
      
      Note
      
      
      
      What happened to 0.2.0? Well, we released it to Maven Central before we&#8217;d noticed a few problems that we thought...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m happy to announce that &lt;strong&gt;Debezium 0.2.1 is now available&lt;/strong&gt;. The &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt; has been significantly improved and is now able to monitor and produce change events for &lt;a href=&quot;http://debezium.io/docs/connectors/mysql#ha-mysql-clusters#enabling-gtids&quot;&gt;HA MySQL clusters&lt;/a&gt; using &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;GTIDs&lt;/a&gt;, perform a &lt;a href=&quot;http://debezium.io/docs/connectors/mysql#snapshots&quot;&gt;consistent snapshot&lt;/a&gt; when starting up the first time, and has a completely redesigned &lt;a href=&quot;http://debezium.io/docs/connectors/mysql#events&quot;&gt;event message structure&lt;/a&gt; that provides a ton more information with every event. Our &lt;a href=&quot;http://debezium.io/docs/releases&quot;&gt;change log&lt;/a&gt; has all the details about bugs, enhancements, new features, and backward compatibility notices. We’ve also updated our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;admonitionblock note&quot;&gt;
      &lt;table&gt;&lt;tr&gt;
      &lt;td class=&quot;icon&quot;&gt;
      &lt;div class=&quot;title&quot;&gt;Note&lt;/div&gt;
      &lt;/td&gt;
      &lt;td class=&quot;content&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;What happened to 0.2.0? Well, we released it to Maven Central before we’d noticed a &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-71&quot;&gt;few&lt;/a&gt; &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-72&quot;&gt;problems&lt;/a&gt; that we thought it best to fix right away. Thus 0.2.1 was born.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/td&gt;
      &lt;/tr&gt;&lt;/table&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_installing_the_mysql_connector&quot;&gt;Installing the MySQL connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’ve already installed &lt;a href=&quot;https://zookeeper.apache.org&quot;&gt;Zookeeper&lt;/a&gt;, &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt;, and &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt;, then using Debezium’s MySQL connector is easy. Simply download the &lt;a href=&quot;https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/0.2.1/debezium-connector-mysql-0.2.1-plugin.tar.gz&quot;&gt;connector’s plugin archive&lt;/a&gt;, extract the JARs into your Kafka Connect environment, and add the directory with the JARs to &lt;a href=&quot;http://docs.confluent.io/3.0.0/connect/userguide.html#installing-connector-plugins&quot;&gt;Kafka Connect’s classpath&lt;/a&gt;. Restart your Kafka Connect process to pick up the new JARs.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If immutable containers are your thing, then check out &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium’s Docker images&lt;/a&gt; for Zookeeper, Kafka, and Kafka Connect with the MySQL connector already pre-installed and ready to go. Our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt; even walks you through using these images, and this is a great way to learn what Debezium is all about. You can even &lt;a href=&quot;http://debezium.io/blog/2016/05/31/Debezium-on-Kubernetes&quot;&gt;run Debezium on Kubernetes and OpenShift&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_using_the_mysql_connector&quot;&gt;Using the MySQL connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To use the connector to produce change events for a particular MySQL server or cluster, simply create a &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/#configuration&quot;&gt;configuration file for the MySQL Connector&lt;/a&gt; and use the &lt;a href=&quot;http://docs.confluent.io/3.0.0/connect/userguide.html#rest-interface&quot;&gt;Kafka Connect REST API&lt;/a&gt; to add that connector to your Kafka Connect cluster. When the connector starts, it will grab a consistent snapshot of the databases in your MySQL server and start reading the MySQL binlog, producing events for every inserted, updated, and deleted row. The connector can optionally produce events with the DDL statements that were applied, and you can even choose to produce events for a subset of the databases and tables. Optionally ignore, mask, or truncate columns that are sensitive, too large, or not needed. See the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL connector’s documentation&lt;/a&gt; for all the details.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_using_the_libraries&quot;&gt;Using the libraries&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Although Debezium is really intended to be used as turnkey services, all of Debezium’s JARs and other artifacts are available in &lt;a href=&quot;http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22io.debezium%22&quot;&gt;Maven Central&lt;/a&gt;. You might want to use our &lt;a href=&quot;http://debezium.io/blog/2016/04/15/parsing-ddl/&quot;&gt;MySQL DDL parser&lt;/a&gt; from our MySQL connector library to parse those DDL statments in your consumers.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We do provide a small library so applications can &lt;a href=&quot;http://debezium.io/docs/embedded&quot;&gt;embed any Kafka Connect connector&lt;/a&gt; and consume data change events read directly from the source system. This provides a much lighter weight system (since Zookeeper, Kafka, and Kafka Connect services are not needed), but as a consequence is not as fault tolerant or reliable since the application must manage and maintain all state normally kept inside Kafka’s distributed and replicated logs. It’s perfect for use in tests, and with careful consideration it may be useful in some applications.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;. And stay tuned, because we’re hoping to add a MongoDB connector in our next release.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Emmanuel, Chris, Christian, Konstantin, David, Akshath, James, Ewen, Cheng, and Paul for their help with the release, discussions, design assistance, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/05/31/Debezium-on-Kubernetes/</id>
    <title>Debezium on Kubernetes</title>
    <updated>2016-09-09T11:50:54+00:00</updated>
    <published>2016-05-31T00:00:00+00:00</published>
    <link href="/blog/2016/05/31/Debezium-on-Kubernetes/" rel="alternate" type="text/html" />
    <author>
      <name>Christian Posta</name>
    </author>
    <category term="mysql"></category>
    <category term="sql"></category>
    <category term="kubernetes"></category>
    <category term="docker"></category>
    <category term="kafka"></category>
    <summary>
      
      
      
      Our Debezium Tutorial walks you step by step through using Debezium by installing, starting, and linking together all of the Docker containers running on a single host machine. Of course, you can use things like Docker Compose or your own scripts to make this easier, although that would just automating running all the containers on a single machine. What you really want is to run the containers on a cluster of machines. In this blog, we&#8217;ll run Debezium using a container cluster manager from Red Hat and Google called Kubernetes.
      
      
      Kubernetes is a container (Docker/Rocket/Hyper.sh) cluster management tool. Like many other...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;Debezium Tutorial&lt;/a&gt; walks you step by step through using Debezium by installing, starting, and linking together all of the Docker containers running on a single host machine. Of course, you can use things like Docker Compose or your own scripts to make this easier, although that would just automating running all the containers on a single machine. What you really want is to run the containers on a cluster of machines. In this blog, we’ll run Debezium using a container cluster manager from Red Hat and Google called &lt;a href=&quot;http://kubernetes.io&quot;&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://kubernetes.io&quot;&gt;Kubernetes&lt;/a&gt; is a container (Docker/Rocket/Hyper.sh) cluster management tool. Like many other popular cluster management and compute resource scheduling platforms, Kubernetes' roots are in Google, who is no stranger to running containers at scale. They start, stop, and cluster &lt;a href=&quot;https://cloudplatform.googleblog.com/2015/01/in-coming-weeks-we-will-be-publishing.html&quot;&gt;2 billion containers per week&lt;/a&gt; and they contributed a lot of the Linux kernel underpinnings that make containers possible. &lt;a href=&quot;http://research.google.com/pubs/pub43438.html&quot;&gt;One of their famous papers&lt;/a&gt; talks about an internal cluster manager named Borg. With Kubernetes, Google got tired of everyone implementing their papers in Java so they decided to implement this one themselves :)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kubernetes is written in Go-lang and is quickly becoming the de-facto API for scheduling, managing, and clustering containers at scale. This blog isn’t intended to be a primer on Kubernetes, so we recommend heading over to the &lt;a href=&quot;http://kubernetes.io/docs/getting-started-guides/&quot;&gt;Getting Started&lt;/a&gt; docs to learn more about Kubernetes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_getting_started&quot;&gt;Getting started&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To get started, we need to have access to a Kubernetes cluster. Getting one started is pretty easy: just follow the &lt;a href=&quot;http://kubernetes.io/docs/getting-started-guides/&quot;&gt;getting started&lt;/a&gt; guides. A favorite of ours is &lt;a href=&quot;https://blog.openshift.com/one-vagrant-image-openshift-origin-v3/&quot;&gt;OpenShift’s all in one VM&lt;/a&gt; or the &lt;a href=&quot;http://developers.redhat.com/products/cdk/overview/&quot;&gt;Red Hat Container Development Kit&lt;/a&gt; which provide a hardened, production-ready distribution of Kubernetes. Once you’ve installed it and logged in, you should be able to run &lt;code&gt;kubectl get pod&lt;/code&gt; to get a list of Kubernetes pods you may have running. You don’t need anything running else inside Kubernetes to get started.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To get and build the Kubernetes manifest files (yaml descriptors), go clone the &lt;a href=&quot;https://github.com/debezium/debezium-kubernetes&quot;&gt;Debezium Kubernetes&lt;/a&gt; repo and run the following command:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ mvn clean
      $ mvn install&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This project uses the awesome &lt;a href=&quot;http://fabric8.io/guide/mavenPlugin.html&quot;&gt;Fabric8 Maven plugin&lt;/a&gt; to automatically generate the Kubernetes manifest files. Here’s an example of what gets generated in &lt;code&gt;$PROJECT_ROOT/zk-standalone/target/classes/kubernetes.yml&lt;/code&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;nowrap&quot;&gt;apiVersion: &quot;v1&quot;
      items:
      - apiVersion: &quot;v1&quot;
        kind: &quot;Service&quot;
        metadata:
          annotations: {}
          labels:
            project: &quot;zookeeper&quot;
            provider: &quot;debezium&quot;
            version: &quot;0.1-SNAPSHOT&quot;
            group: &quot;io.debezium&quot;
          name: &quot;zookeeper&quot;
        spec:
          deprecatedPublicIPs: []
          externalIPs: []
          ports:
          - port: 2181
            protocol: &quot;TCP&quot;
            targetPort: 2181
          selector:
            project: &quot;zookeeper&quot;
            provider: &quot;debezium&quot;
            group: &quot;io.debezium&quot;
      - apiVersion: &quot;v1&quot;
        kind: &quot;ReplicationController&quot;
        metadata:
          annotations:
            fabric8.io/git-branch: &quot;master&quot;
            fabric8.io/git-commit: &quot;004e222462749fbaf12c3ee33edca9b077ee9003&quot;
          labels:
            project: &quot;zookeeper&quot;
            provider: &quot;debezium&quot;
            version: &quot;0.1-SNAPSHOT&quot;
            group: &quot;io.debezium&quot;
          name: &quot;zk-standalone&quot;
        spec:
          replicas: 1
          selector:
            project: &quot;zookeeper&quot;
            provider: &quot;debezium&quot;
            version: &quot;0.1-SNAPSHOT&quot;
            group: &quot;io.debezium&quot;
          template:
            metadata:
              annotations: {}
              labels:
                project: &quot;zookeeper&quot;
                provider: &quot;debezium&quot;
                version: &quot;0.1-SNAPSHOT&quot;
                group: &quot;io.debezium&quot;
            spec:
              containers:
              - args: []
                command: []
                env:
                - name: &quot;KUBERNETES_NAMESPACE&quot;
                  valueFrom:
                    fieldRef:
                      fieldPath: &quot;metadata.namespace&quot;
                image: &quot;docker.io/debezium/zookeeper:0.1&quot;
                imagePullPolicy: &quot;IfNotPresent&quot;
                name: &quot;zk-standalone&quot;
                ports:
                - containerPort: 3888
                  name: &quot;election&quot;
                - containerPort: 2888
                  name: &quot;peer&quot;
                - containerPort: 2181
                  name: &quot;client&quot;
                securityContext: {}
                volumeMounts: []
              imagePullSecrets: []
              nodeSelector: {}
              volumes: []
      kind: &quot;List&quot;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_starting_zookeeper_and_kafka_on_kubernetes&quot;&gt;Starting Zookeeper and Kafka on Kubernetes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To start &lt;a href=&quot;http://zookeeper.apache.org&quot;&gt;Apache Zookeeper&lt;/a&gt; or &lt;a href=&quot;http://kafka.apache.org&quot;&gt;Apache Kafka&lt;/a&gt; inside Kubernetes you have two options. If you have the &lt;code&gt;kubectl&lt;/code&gt; command line (or the &lt;code&gt;oc&lt;/code&gt; tool from the OpenShift client distros) on your machine you can apply any of the newly generated Kubernetes manifest files like this:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl create -f &amp;lt;path_to_file&amp;gt;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Or you can use the Fabric8 Maven plugin and its &lt;code&gt;fabric8:apply&lt;/code&gt; goal to apply the manifest files. Note for either of these two options to work, you must be currently logged into your Kubernetes cluster. (Also, OpenShift’s &lt;code&gt;oc login &amp;lt;url&amp;gt;&lt;/code&gt; makes this super easy, or see &lt;a href=&quot;http://blog.christianposta.com/kubernetes/logging-into-a-kubernetes-cluster-with-kubectl/&quot;&gt;Logging into a Kubernetes Cluster with kubectl&lt;/a&gt; for more information.)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;First, let’s deploy Zookeeper to our Kubernetes cluster. We need to be in &lt;code&gt;$PROJECT_ROOT/zk-standalone&lt;/code&gt; directory, and then we’ll apply our Kubernetes configuration.  First, let’s see how to do this with the &lt;code&gt;kubectl&lt;/code&gt; command:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd zk-standalone
      $ kubectl create -f target/classes/kubernetes.yml
      
      service &quot;zookeeper&quot; created
      replicationcontroller &quot;zk-standalone&quot; created&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You can do the same thing with Maven and the fabric8 maven plugin:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd zk-standalone
      $ mvn fabric8:apply
      
      Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=1512m; support was removed in 8.0
      [INFO] Scanning for projects...
      [INFO]
      [INFO] ------------------------------------------------------------------------
      [INFO] Building zk-standalone 0.1-SNAPSHOT
      [INFO] ------------------------------------------------------------------------
      [INFO]
      [INFO] --- fabric8-maven-plugin:2.2.115:apply (default-cli) @ zk-standalone ---
      [INFO] Using kubernetes at: https://172.28.128.4:8443/ in namespace ticket
      [INFO] Kubernetes JSON: /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/zk-standalone/target/classes/kubernetes.json
      [INFO] OpenShift platform detected
      [INFO] Using namespace: ticket
      [INFO] Looking at repo with directory /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/.git
      [INFO] Creating a Service from kubernetes.json namespace ticket name zookeeper
      [INFO] Created Service: zk-standalone/target/fabric8/applyJson/ticket/service-zookeeper.json
      [INFO] Creating a ReplicationController from kubernetes.json namespace ticket name zk-standalone
      [INFO] Created ReplicationController: zk-standalone/target/fabric8/applyJson/ticket/replicationcontroller-zk-standalone.json
      [INFO] ------------------------------------------------------------------------
      [INFO] BUILD SUCCESS
      [INFO] ------------------------------------------------------------------------
      [INFO] Total time: 2.661 s
      [INFO] Finished at: 2016-05-19T15:59:26-07:00
      [INFO] Final Memory: 26M/260M
      [INFO] ------------------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Zookeeper is deployed, so let’s continue with deploying Kafka. Navigate to &lt;code&gt;$PROJECT_ROOT/kafka&lt;/code&gt;, and then apply the Kafka deployment configuration:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd ../kafka
      $ kubectl create -f target/classes/kubernetes.yml
      
      service &quot;kafka&quot; created
      replicationcontroller &quot;kafka&quot; created&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Or with fabric8 maven plugin:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd ../kafka
      $ mvn fabric8:apply
      
      Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=1512m; support was removed in 8.0
      [INFO] Scanning for projects...
      [INFO]
      [INFO] ------------------------------------------------------------------------
      [INFO] Building kafka 0.1-SNAPSHOT
      [INFO] ------------------------------------------------------------------------
      [INFO]
      [INFO] --- fabric8-maven-plugin:2.2.115:apply (default-cli) @ kafka ---
      [INFO] Using kubernetes at: https://172.28.128.4:8443/ in namespace ticket
      [INFO] Kubernetes JSON: /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/kafka/target/classes/kubernetes.json
      [INFO] OpenShift platform detected
      [INFO] Using namespace: ticket
      [INFO] Looking at repo with directory /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/.git
      [INFO] Creating a Service from kubernetes.json namespace ticket name kafka
      [INFO] Created Service: kafka/target/fabric8/applyJson/ticket/service-kafka.json
      [INFO] Creating a ReplicationController from kubernetes.json namespace ticket name kafka
      [INFO] Created ReplicationController: kafka/target/fabric8/applyJson/ticket/replicationcontroller-kafka.json
      [INFO] ------------------------------------------------------------------------
      [INFO] BUILD SUCCESS
      [INFO] ------------------------------------------------------------------------
      [INFO] Total time: 2.563 s
      [INFO] Finished at: 2016-05-19T16:03:25-07:00
      [INFO] Final Memory: 26M/259M
      [INFO] ------------------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Use the &lt;code&gt;kubectl get pod&lt;/code&gt; command to see what is running:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl get pod
      
      NAME                  READY     STATUS    RESTARTS   AGE
      kafka-mqmxt           1/1       Running   0          46s
      zk-standalone-4mo02   1/1       Running   0          4m&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Did you notice that we didn’t manually &quot;link&quot; the containers as we started them? Kubernetes has a cluster service discovery feature called &lt;a href=&quot;http://kubernetes.io/docs/user-guide/services/&quot;&gt;Kubernetes Services&lt;/a&gt; that load-balances against and lets us use internal DNS (or cluster IPs) to discover pods. For example, in the &lt;code&gt;kubernetes.yml&lt;/code&gt; deployment configuration for Kafka, you’ll see the following:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;nowrap&quot;&gt;    ...
          containers:
          - args: []
            command: []
            env:
            - name: &quot;KAFKA_ADVERTISED_PORT&quot;
              value: &quot;9092&quot;
            - name: &quot;KAFKA_ADVERTISED_HOST_NAME&quot;
              value: &quot;kafka&quot;
            - name: &quot;KAFKA_ZOOKEEPER_CONNECT&quot;
              value: &quot;zookeeper:2181&quot;
            - name: &quot;KAFKA_PORT&quot;
              value: &quot;9092&quot;
            - name: &quot;KUBERNETES_NAMESPACE&quot;
              valueFrom:
                fieldRef:
                  fieldPath: &quot;metadata.namespace&quot;
            image: &quot;docker.io/debezium/kafka:0.1&quot;
            imagePullPolicy: &quot;IfNotPresent&quot;
            name: &quot;kafka&quot;
          ...&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re specifying values for the &lt;code&gt;KAFKA_ZOOKEEPER_CONNECT&lt;/code&gt; environment variable used by the Docker image, and thus enabling Kafka to discover Zookeeper pods wherever they are running. Although we could have used any hostname, to keep things simple we use just &lt;code&gt;zookeeper&lt;/code&gt; for the DNS name. So, if you were to log in to one of the pods and try to reach the host named &lt;code&gt;zookeeper&lt;/code&gt;, Kubernetes would transparently resolve that request to one of the Zookeeper pods (if there are multiple). Slick! This discovery mechanism is used for the rest of the components, too. (Note, this cluster IP that the DNS resolves to &lt;strong&gt;never&lt;/strong&gt; changes for the life of the Kubernetes Service regardless of how many Pods exist for a given service. This means you can rely on this service discovery without all of the DNS caching issues you may otherwise run into.)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The next step is to create a &lt;code&gt;schema-changes&lt;/code&gt; topic that Debezium’s MySQL connector will use. Let’s use the Kafka tools to create this:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ KAFKA_POD_NAME=$(kubectl get pod | grep -i running | grep kafka | awk '{ print $1 }')
      
      $ kubectl exec $KAFKA_POD_NAME --  /kafka/bin/kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic schema-changes.inventory&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_start_up_a_mysql_database_on_kubernetes&quot;&gt;Start up a MySQL Database on Kubernetes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Starting the MySQL database follows the same instructions as installing Zookeeper or Kafka. We will navigate to the &lt;code&gt;$PROJECT_ROOT/mysql56&lt;/code&gt; directory, and we’ll use the &lt;a href=&quot;https://github.com/openshift/mysql&quot;&gt;MySQL 5.6 OpenShift Docker image&lt;/a&gt; so that it runs on both vanilla Kubernetes and OpenShift v3.x. Here’s the &lt;code&gt;kubectl&lt;/code&gt; command to start up our MySQL instance:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd ../mysql56
      $ kubectl create -f target/classes/kubernetes.yml
      
      service &quot;mysql&quot; created
      replicationcontroller &quot;mysql56&quot; created&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Or the equivalent Maven command:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd mysql56
      $ mvn fabric8:apply&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now, when we run &lt;code&gt;kubectl get pod&lt;/code&gt; we should see our MySQL database running, too:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;nowrap&quot;&gt;NAME                  READY     STATUS    RESTARTS   AGE
      kafka-mqmxt           1/1       Running   0          17m
      mysql56-b4f36         1/1       Running   0          9m
      zk-standalone-4mo02   1/1       Running   0          21m&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s run a command to get client access to the database. First, set a few environment variables to the pod’s name and IP address:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ MYSQL_POD_NAME=$(kubectl get pod | grep Running | grep ^mysql | awk '{ print $1 }')
      $ MYSQL_POD_IP=$(kubectl describe pod $MYSQL_POD_NAME | grep IP | awk '{ print $2 }')&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Then, log in to the Kubernetes pod that’s running the MySQL database, and start the MySQL command client:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl exec -it $MYSQL_POD_NAME   -- /opt/rh/rh-mysql56/root/usr/bin/mysql -h$MYSQL_POD_IP -P3306 -uroot -padmin
      Warning: Using a password on the command line interface can be insecure.
      Welcome to the MySQL monitor.  Commands end with ; or \g.
      Your MySQL connection id is 1
      Server version: 5.6.26-log MySQL Community Server (GPL)
      
      Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.
      
      Oracle is a registered trademark of Oracle Corporation and/or its
      affiliates. Other names may be trademarks of their respective
      owners.
      
      Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
      
      mysql&amp;gt;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This shows that the &lt;code&gt;kubectl&lt;/code&gt; command line lets us easily get access to a pod or Docker container regardless of where it’s running in the cluster.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Next, exit out of the mysql shell (type &lt;code&gt;exit&lt;/code&gt;) and run the following command to download a &lt;a href=&quot;https://gist.github.com/christian-posta/e20ddb5c945845b4b9f6eba94a98af09/raw&quot;&gt;SQL script&lt;/a&gt; that populates an &lt;code&gt;inventory&lt;/code&gt; sample database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl exec  -it $MYSQL_POD_NAME -- bash -c &quot;curl -s -L https://gist.github.com/christian-posta/e20ddb5c945845b4b9f6eba94a98af09/raw | /opt/rh/rh-mysql56/root/usr/bin/mysql -h$MYSQL_POD_IP -P3306 -uroot -padmin&quot;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now, if we log back into the MySQL pod we can show the databases and tables:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl exec -it $MYSQL_POD_NAME   -- /opt/rh/rh-mysql56/root/usr/bin/mysql -h$MYSQL_POD_IP -P3306 -uroot -padmin -e 'use inventory; show tables;'
      
      +---------------------+
      | Tables_in_inventory |
      +---------------------+
      | customers           |
      | orders              |
      | products            |
      | products_on_hand    |
      +---------------------+
      4 rows in set (0.00 sec)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_start_kafka_connect_and_debezium&quot;&gt;Start Kafka Connect and Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Navigate into the directory &lt;code&gt;$PROJECT_ROOT/connect-mysql&lt;/code&gt; directory. Here, we’ll start a Kubernetes pod that runs Kafka Connect with the Debezium MySQL connector already installed. The Debezium MySQL connector connects to a MySQL database, reads the binlog, and writes those row events to Kafka. Start up Kafka Connect with Debezium on Kubernetes similarly to the previous components:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd ../connect-mysql
      $ kubectl create -f target/classes/kubernetes.yml
      
      service &quot;connect-mysql&quot; created
      replicationcontroller &quot;connect-mysql&quot; created&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Or with the fabric8 maven plugin:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd ../connect-mysql
      $ mvn fabric8:apply
      Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=1512m; support was removed in 8.0
      [INFO] Scanning for projects...
      [INFO]
      [INFO] ------------------------------------------------------------------------
      [INFO] Building connect-mysql 0.1-SNAPSHOT
      [INFO] ------------------------------------------------------------------------
      [INFO]
      [INFO] --- fabric8-maven-plugin:2.2.115:apply (default-cli) @ connect-mysql ---
      [INFO] Using kubernetes at: https://172.28.128.4:8443/ in namespace ticket
      [INFO] Kubernetes JSON: /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/connect-mysql/target/classes/kubernetes.json
      [INFO] OpenShift platform detected
      [INFO] Using namespace: ticket
      [INFO] Looking at repo with directory /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/.git
      [INFO] Creating a Service from kubernetes.json namespace ticket name connect-mysql
      [INFO] Created Service: connect-mysql/target/fabric8/applyJson/ticket/service-connect-mysql.json
      [INFO] Creating a ReplicationController from kubernetes.json namespace ticket name connect-mysql
      [INFO] Created ReplicationController: connect-mysql/target/fabric8/applyJson/ticket/replicationcontroller-connect-mysql.json
      [INFO] ------------------------------------------------------------------------
      [INFO] BUILD SUCCESS
      [INFO] ------------------------------------------------------------------------
      [INFO] Total time: 2.255 s
      [INFO] Finished at: 2016-05-25T09:21:04-07:00
      [INFO] Final Memory: 27M/313M
      [INFO] ------------------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Just like in the Docker tutorial for Debezium, we now want to send a JSON object to the Kafka Connect API to start up our Debezium connector. First, we need to expose the API for the Kafka Connect cluster. You can do this however you want: on Kubernetes (&lt;a href=&quot;http://kubernetes.io/docs/user-guide/ingress/&quot;&gt;Ingress definitions&lt;/a&gt;, &lt;a href=&quot;http://kubernetes.io/docs/user-guide/services/&quot;&gt;NodePort services&lt;/a&gt;, etc) or on OpenShift you can use &lt;a href=&quot;https://docs.openshift.com/enterprise/3.2/architecture/core_concepts/routes.html&quot;&gt;OpenShift Routes&lt;/a&gt;. For this simple example, we’ll use simple Pod port-forwarding to forward the &lt;code&gt;connect-mysql&lt;/code&gt; pod’s &lt;code&gt;8083&lt;/code&gt; port to our local machine (again, regardless of where the Pod is actually running the cluster. (This is such an incredible feature of Kubernetes that makes it so easy to develop distributed services!)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s determine the pod name and then use port forwarding to our local machine:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ CONNECT_POD_NAME=$(kubectl get pod | grep -i running | grep ^connect | awk '{ print $1 }')
      $ kubectl port-forward $CONNECT_POD_NAME 8083:8083
      
      I0525 09:30:08.390491    6651 portforward.go:213] Forwarding from 127.0.0.1:8083 -&amp;gt; 8083
      I0525 09:30:08.390631    6651 portforward.go:213] Forwarding from [::1]:8083 -&amp;gt; 8083&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We are forwarding the pod’s port &lt;code&gt;8083&lt;/code&gt; to our local machine’s &lt;code&gt;8083&lt;/code&gt;. Now if we hit &lt;code&gt;&lt;a href=&quot;http://localhost:8083&quot; class=&quot;bare&quot;&gt;http://localhost:8083&lt;/a&gt;&lt;/code&gt; it will be directed to the pod which runs our Kafka Connect and Debezium services.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Since it may be useful to see the output from the pod to see whether or not there are any exceptions, start another terminal and type the following to follow the Kafka Connect output:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ CONNECT_POD_NAME=$(kubectl get pod | grep -i running | grep ^connect | awk '{ print $1 }')
      $ kubectl logs -f $CONNECT_POD_NAME&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now, let’s use an HTTP client to post the Debezium Connector/Task to the endpoint we’ve just exposed locally:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; -H &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ -d '{ &quot;name&quot;: &quot;inventory-connector&quot;, &quot;config&quot;: { &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;, &quot;tasks.max&quot;: &quot;1&quot;, &quot;database.hostname&quot;: &quot;mysql&quot;, &quot;database.port&quot;: &quot;3306&quot;, &quot;database.user&quot;: &quot;replicator&quot;, &quot;database.password&quot;: &quot;replpass&quot;, &quot;database.server.id&quot;: &quot;184054&quot;, &quot;database.server.name&quot;: &quot;mysql-server-1&quot;, &quot;database.binlog&quot;: &quot;mysql-bin.000001&quot;, &quot;database.whitelist&quot;: &quot;inventory&quot;, &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;, &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot; } }'&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If we’re watching the log output for the &lt;code&gt;connect-mysql&lt;/code&gt; pod, we’ll see it eventually end up looking something like this:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;nowrap&quot;&gt;2016-05-27 18:50:14,580 - WARN  [kafka-producer-network-thread | producer-1:NetworkClient$DefaultMetadataUpdater@582] - Error while fetching metadata with correlation id 2 : {mysql-server-1.inventory.products=LEADER_NOT_AVAILABLE}
      2016-05-27 18:50:14,690 - WARN  [kafka-producer-network-thread | producer-1:NetworkClient$DefaultMetadataUpdater@582] - Error while fetching metadata with correlation id 3 : {mysql-server-1.inventory.products=LEADER_NOT_AVAILABLE}
      2016-05-27 18:50:14,911 - WARN  [kafka-producer-network-thread | producer-1:NetworkClient$DefaultMetadataUpdater@582] - Error while fetching metadata with correlation id 7 : {mysql-server-1.inventory.products_on_hand=LEADER_NOT_AVAILABLE}
      2016-05-27 18:50:15,136 - WARN  [kafka-producer-network-thread | producer-1:NetworkClient$DefaultMetadataUpdater@582] - Error while fetching metadata with correlation id 10 : {mysql-server-1.inventory.customers=LEADER_NOT_AVAILABLE}
      2016-05-27 18:50:15,362 - WARN  [kafka-producer-network-thread | producer-1:NetworkClient$DefaultMetadataUpdater@582] - Error while fetching metadata with correlation id 13 : {mysql-server-1.inventory.orders=LEADER_NOT_AVAILABLE}&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;These error are just Kafka’s way of telling us the topics didn’t exist but were created.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If we now do a listing of our topics inside Kafka, we should see a Kafka topic for each table in the mysql &lt;code&gt;inventory&lt;/code&gt; database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl exec  $KAFKA_POD_NAME --  /kafka/bin/kafka-topics.sh --list --zookeeper zookeeper:2181
      __consumer_offsets
      my-connect-configs
      my-connect-offsets
      mysql-server-1.inventory.customers
      mysql-server-1.inventory.orders
      mysql-server-1.inventory.products
      mysql-server-1.inventory.products_on_hand
      schema-changes.inventory&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s take a look at what’s in one of these topics:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl exec  $KAFKA_POD_NAME --  /kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --new-consumer --topic mysql-server-1.inventory.customers --from-beginning --property print.key=true
      {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers/pk&quot;},&quot;payload&quot;:{&quot;id&quot;:1001}}   {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;first_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;last_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;email&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers&quot;},&quot;payload&quot;:{&quot;id&quot;:1001,&quot;first_name&quot;:&quot;Sally&quot;,&quot;last_name&quot;:&quot;Thomas&quot;,&quot;email&quot;:&quot;sally.thomas@acme.com&quot;}}
      {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers/pk&quot;},&quot;payload&quot;:{&quot;id&quot;:1002}}   {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;first_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;last_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;email&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers&quot;},&quot;payload&quot;:{&quot;id&quot;:1002,&quot;first_name&quot;:&quot;George&quot;,&quot;last_name&quot;:&quot;Bailey&quot;,&quot;email&quot;:&quot;gbailey@foobar.com&quot;}}
      {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers/pk&quot;},&quot;payload&quot;:{&quot;id&quot;:1003}}   {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;first_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;last_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;email&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers&quot;},&quot;payload&quot;:{&quot;id&quot;:1003,&quot;first_name&quot;:&quot;Edward&quot;,&quot;last_name&quot;:&quot;Walker&quot;,&quot;email&quot;:&quot;ed@walker.com&quot;}}
      {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers/pk&quot;},&quot;payload&quot;:{&quot;id&quot;:1004}}   {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;first_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;last_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;email&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers&quot;},&quot;payload&quot;:{&quot;id&quot;:1004,&quot;first_name&quot;:&quot;Anne&quot;,&quot;last_name&quot;:&quot;Kretchmar&quot;,&quot;email&quot;:&quot;annek@noanswer.org&quot;}}&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;What happened? When we started Debezium’s MySQL connector, it started reading the binary replication log from the MySQL server, and it replayed all of the history and generated an event for each INSERT, UPDATE, and DELETE operation (though in our sample &lt;code&gt;inventory&lt;/code&gt; database we only had INSERTs). If we or some client apps were to commit other changes to the database, Debezium would see those immediately and write those to the correct topic. In other words, Debezium records all of the changes to our MySQL database as events in Kafka topics! And from there, any tool, connector, or service can independnetly consume those event streams from Kafka and process them or put them into a different database, into Hadoop, elasticsearch, data grid, etc.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_cleanup&quot;&gt;Cleanup&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you want to delete the connector, simply issue a REST request to remove it:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X DELETE -H &quot;Accept:application/json&quot; http://localhost:8083/connectors/inventory-connector&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/04/15/parsing-ddl/</id>
    <title>Parsing DDL</title>
    <updated>2016-09-09T11:50:54+00:00</updated>
    <published>2016-04-15T00:00:00+00:00</published>
    <link href="/blog/2016/04/15/parsing-ddl/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="mysql"></category>
    <category term="sql"></category>
    <summary>
      
      
      
      When our MySQL connector is reading the binlog of a MySQL server or cluster, it parses the DDL statements in the log and builds an in-memory model of each table&#8217;s schema as it evolves over time. This process is important because the connector generates events for each table using the definition of the table at the time of each event. We can&#8217;t use the database&#8217;s current schema, since it may have changed since the point in time (or position in the log) where the connector is reading.
      
      
      Parsing DDL of MySQL or any other major relational database can seem to be...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When our &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt; is reading the binlog of a MySQL server or cluster, it parses the DDL statements in the log and builds an in-memory model of each table’s schema as it evolves over time. This process is important because the connector generates events for each table using the definition of the table at the time of each event. We can’t use the database’s &lt;em&gt;current&lt;/em&gt; schema, since it may have changed since the point in time (or position in the log) where the connector is reading.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Parsing DDL of MySQL or any other major relational database can seem to be a daunting task. Usually each DBMS has a highly-customized SQL grammar, and although the &lt;em&gt;data manipulation language&lt;/em&gt; (DML) statements are often fairly close the standards, the &lt;em&gt;data definition language&lt;/em&gt; (DDL) statements are usually less so and involve more DBMS-specific features.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So given this, why did we write our own DDL parser for MySQL? Let’s first look at what Debezium needs a DDL parser to do.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_parsing_ddl_in_the_debezium_mysql_connector&quot;&gt;Parsing DDL in the Debezium MySQL connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The MySQL binlog contains various kinds of events. For example, when a row is inserted into a table, the binlog event contains an indirect reference to the table and the values for each column in the table, but there is no information about the columns that make up the table. The only thing in the binlog referencing table structures are SQL DDL statements that were generated by MySQL when it processed user-supplied DDL statements.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The connector also produces messages using Kafka Connect Schemas, which are simple data structures that define the various names and types of each field, and the way the fields are organized. So, when we generate an event message for the table insert, we first have to have a Kafka Connect &lt;code&gt;Schema&lt;/code&gt; object with all the appropriate fields, and then we have to convert the ordered array of column values into a Kafka Connect &lt;code&gt;Struct&lt;/code&gt; object using the fields and the individual column values in the table insert event.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Luckily, when we come across a DDL statement we can update our in-memory model and then use this to generate a &lt;code&gt;Schema&lt;/code&gt; object. At the same time, we can create a component that will use this &lt;code&gt;Schema&lt;/code&gt; object to create a &lt;code&gt;Struct&lt;/code&gt; object from the ordered array of column values that appear in the events. All of this can be done once and used for all row events on that table, until we come across another DDL statement that changes the table’s schema at which point we updated our model again.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So all of this requires parsing all of the DDL statements, though for our purposes we only have to &lt;em&gt;understand&lt;/em&gt; a small subset of the DDL grammer. We then have to use that subset of statements to update our in-memory model of our tables. And since our in-memory table model is not specific to MySQL, the rest of the functionality to generate &lt;code&gt;Schema&lt;/code&gt; objects and components that convert an array of values into &lt;code&gt;Struct&lt;/code&gt; objects used in messages is all generic.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_existing_ddl_libraries&quot;&gt;Existing DDL libraries&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Unfortunately, there aren’t really that many 3rd party open source libraries for parsing DDL statements for MySQL, PostgreSQL, or other popular RDBMSes. &lt;a href=&quot;http://jsqlparser.sourceforge.net&quot;&gt;JSqlParser&lt;/a&gt; is often cited, but it has a &lt;em&gt;single grammar&lt;/em&gt; that is a combination of multiple DBMS grammars and therefore is not a strict parser for any specific DBMS. Adding support for other DBMSes by updating the composite grammar would likely be difficult.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Other libraries, such as &lt;a href=&quot;https://github.com/prestodb/presto/tree/master/presto-parser&quot;&gt;PrestoDB&lt;/a&gt;, define their own SQL grammar and are unable to handle the intracacies and nuances of the MySQL DDL grammar. The Antlr parser generator project has a &lt;a href=&quot;https://github.com/antlr/grammars-v4/tree/master/mysql&quot;&gt;grammar for MySQL 5.6&lt;/a&gt;, but this is limited to a small subset of DML and has no support for DDL or newer 5.7 features. There are &lt;a href=&quot;http://www.antlr3.org/grammar/list.html&quot;&gt;older SQL-related grammars for Antlr 3&lt;/a&gt;, but these are often massive, suffer from bugs, and limited to specific DBMSes. The &lt;a href=&quot;http://teiid.jboss.org&quot;&gt;Teiid project&lt;/a&gt; is a data virtualization engine that sits atop a wide variety of DBMSes and data sources, and it’s tooling has a series of &lt;a href=&quot;https://github.com/Teiid-Designer/teiid-modeshape&quot;&gt;DDL parsers&lt;/a&gt; that construct ASTs in a special repository (the author actually helped develop these). There are also Ruby libraries, like &lt;a href=&quot;https://github.com/square/mysql-parser&quot;&gt;Square’s MySQL Parser library&lt;/a&gt;. There is also a &lt;a href=&quot;http://www.sqlparser.com/sql-parser-java.php&quot;&gt;proprietary commercial product&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_our_ddl_parser_framework&quot;&gt;Our DDL parser framework&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Since we couldn’t find a useful 3rd party open source library, we chose to create our own DDL parser framework limited to our needs:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Parse DDL statements and update our in-memory model.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Focus on consuming those essential statements (e.g., create, alter, and drop tables and views), while completely ignoring other statements without having to parse them.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Structure the parser code similarly to the &lt;a href=&quot;http://dev.mysql.com/doc/refman/5.7/en/sql-syntax-data-definition.html&quot;&gt;MySQL DDL grammar documentation&lt;/a&gt; and use method names that mirror the rules in the grammar. This will make it easier to maintain over time.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Allow creation of parsers for PostgreSQL, Oracle, SQLServer, and other DBMSes as needed.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Support customization through subclassing: be able to easily override narrow portions of the logic without having to copy lots of code.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Make it easy to develop, debug, and test parsers.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The resulting framework includes a tokenizer that converts one or more DDL statements in a string into a rewindable sequence of tokens, where each token represents punctuation, quoted strings, case-insentivie words and symbols, numbers, keywords, comments, and terminating characters  (such as &lt;code&gt;;&lt;/code&gt; for MySQL). The DDL parser, then, walks the token stream looking for patterns using a simple and easy to read fluent API, calling methods on itself to process the various sets of tokens. The parser also uses an internal &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/relational/ddl/DataTypeParser.java&quot;&gt;data type parser&lt;/a&gt; for processing SQL data type expressions, such as &lt;code&gt;INT&lt;/code&gt;, &lt;code&gt;VARCHAR(64)&lt;/code&gt;, &lt;code&gt;NUMERIC(32,3)&lt;/code&gt;, &lt;code&gt;TIMESTAMP(8) WITH TIME ZONE&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/debezium-connector-mysql/src/main/java/io/debezium/connector/mysql/MySqlDdlParser.java&quot;&gt;MySqlDdlParser&lt;/a&gt; class extends a &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/relational/ddl/DdlParser.java&quot;&gt;base class&lt;/a&gt; and provides all of the MySQL-specific parsing logic. For example, the DDL statements:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;# Create and populate our products using a single insert with many rows
      CREATE TABLE products (
        id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
        name VARCHAR(255) NOT NULL,
        description VARCHAR(512),
        weight FLOAT
      );
      ALTER TABLE products AUTO_INCREMENT = 101;
      
      # Create and populate the products on hand using multiple inserts
      CREATE TABLE products_on_hand (
        product_id INTEGER NOT NULL PRIMARY KEY,
        quantity INTEGER NOT NULL,
        FOREIGN KEY (product_id) REFERENCES products(id)
      );&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;can be easily parsed with:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;String ddlStatements = ...
      DdlParser parser = new MySqlDdlParser();
      Tables tables = new Tables();
      parser.parse(ddl, tables);&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Here, the &lt;code&gt;Tables&lt;/code&gt; object is our in-memory representation of our named table definitions. The parser processes the DDL statements, applying each to the appropriate table definition inside the &lt;code&gt;Tables&lt;/code&gt; object.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_how_it_works&quot;&gt;How it works&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Each &lt;code&gt;DdlParser&lt;/code&gt; implementation has the following public method that will parse the statements in the supplied String:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;    public final void parse(String ddlContent, Tables databaseTables) {
              Tokenizer tokenizer = new DdlTokenizer(!skipComments(), this::determineTokenType);
              TokenStream stream = new TokenStream(ddlContent, tokenizer, false);
              stream.start();
              parse(stream, databaseTables);
          }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Here, the method creates a new &lt;code&gt;TokenStream&lt;/code&gt; from the content using a &lt;code&gt;DdlTokenizer&lt;/code&gt; that knows how to separate the characters in the string into the various typed token objects. It then calls another &lt;code&gt;parse&lt;/code&gt; method that does the bulk of the work:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;    public final void parse(TokenStream ddlContent, Tables databaseTables)
                                 throws ParsingException, IllegalStateException {
              this.tokens = ddlContent;
              this.databaseTables = databaseTables;
              Marker marker = ddlContent.mark();
              try {
                  while (ddlContent.hasNext()) {
                      parseNextStatement(ddlContent.mark());
                      // Consume the statement terminator if it is still there ...
                      tokens.canConsume(DdlTokenizer.STATEMENT_TERMINATOR);
                  }
              } catch (ParsingException e) {
                  ddlContent.rewind(marker);
                  throw e;
              } catch (Throwable t) {
                  parsingFailed(ddlContent.nextPosition(),
                                &quot;Unexpected exception (&quot; + t.getMessage() + &quot;) parsing&quot;, t);
              }
          }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This sets up some local state, marks the current starting point, and tries to parse DDL statements until no more can be found. If the parsing logic fails to find a match, it generates a &lt;code&gt;ParsingException&lt;/code&gt; with the offending line and column plus a message signaling what was found and what was expected. In such cases, this method rewinds the token stream (in case the caller wishes to try an alternative different parser).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Each time the &lt;code&gt;parseNextStatement&lt;/code&gt; method is called, the starting position of that statement is passed into the method, giving it the starting position of the statement. Our &lt;code&gt;MySqlDdlParser&lt;/code&gt; subclass overrides the &lt;code&gt;parseNextStatement&lt;/code&gt; method to use the first token in the statement to determine the kinds of statement allowed in the MySQL DDL grammar:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;    @Override
          protected void parseNextStatement(Marker marker) {
              if (tokens.matches(DdlTokenizer.COMMENT)) {
                  parseComment(marker);
              } else if (tokens.matches(&quot;CREATE&quot;)) {
                  parseCreate(marker);
              } else if (tokens.matches(&quot;ALTER&quot;)) {
                  parseAlter(marker);
              } else if (tokens.matches(&quot;DROP&quot;)) {
                  parseDrop(marker);
              } else if (tokens.matches(&quot;RENAME&quot;)) {
                  parseRename(marker);
              } else {
                  parseUnknownStatement(marker);
              }
          }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When a matching token is found, the method calls the appropriate method. For example, if the statement begins with &lt;code&gt;CREATE TABLE …​&lt;/code&gt;, then the &lt;code&gt;parseCreate&lt;/code&gt; method is called with the same marker that identifies the starting position of the statement:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;    @Override
          protected void parseCreate(Marker marker) {
              tokens.consume(&quot;CREATE&quot;);
              if (tokens.matches(&quot;TABLE&quot;) || tokens.matches(&quot;TEMPORARY&quot;, &quot;TABLE&quot;)) {
                  parseCreateTable(marker);
              } else if (tokens.matches(&quot;VIEW&quot;)) {
                  parseCreateView(marker);
              } else if (tokens.matchesAnyOf(&quot;DATABASE&quot;, &quot;SCHEMA&quot;)) {
                  parseCreateUnknown(marker);
              } else if (tokens.matchesAnyOf(&quot;EVENT&quot;)) {
                  parseCreateUnknown(marker);
              } else if (tokens.matchesAnyOf(&quot;FUNCTION&quot;, &quot;PROCEDURE&quot;)) {
                  parseCreateUnknown(marker);
              } else if (tokens.matchesAnyOf(&quot;UNIQUE&quot;, &quot;FULLTEXT&quot;, &quot;SPATIAL&quot;, &quot;INDEX&quot;)) {
                  parseCreateIndex(marker);
              } else if (tokens.matchesAnyOf(&quot;SERVER&quot;)) {
                  parseCreateUnknown(marker);
              } else if (tokens.matchesAnyOf(&quot;TABLESPACE&quot;)) {
                  parseCreateUnknown(marker);
              } else if (tokens.matchesAnyOf(&quot;TRIGGER&quot;)) {
                  parseCreateUnknown(marker);
              } else {
                  // It could be several possible things (including more
                  // elaborate forms of those matches tried above),
                  sequentially(this::parseCreateView,
                               this::parseCreateUnknown);
              }
          }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Here, the method first consumes the token with the &lt;code&gt;CREATE&lt;/code&gt; literal, and then tries to match the tokens with various patterns of token literals. If a match is found, this method delegates to other more specific parsing methods. Note how the fluent API of the framework makes it quite easy to understand the match patterns.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s go one step further. Assuming our DDL statement starts with &lt;code&gt;CREATE TABLE products (&lt;/code&gt;, then the parser will then invoke the &lt;code&gt;parseCreateTable&lt;/code&gt; method, again with the same marker denoting the start of the statement:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;    protected void parseCreateTable(Marker start) {
              tokens.canConsume(&quot;TEMPORARY&quot;);
              tokens.consume(&quot;TABLE&quot;);
              boolean onlyIfNotExists = tokens.canConsume(&quot;IF&quot;, &quot;NOT&quot;, &quot;EXISTS&quot;);
              TableId tableId = parseQualifiedTableName(start);
              if ( tokens.canConsume(&quot;LIKE&quot;)) {
                  TableId originalId = parseQualifiedTableName(start);
                  Table original = databaseTables.forTable(originalId);
                  if ( original != null ) {
                      databaseTables.overwriteTable(tableId, original.columns(),
                                                    original.primaryKeyColumnNames());
                  }
                  consumeRemainingStatement(start);
                  debugParsed(start);
                  return;
              }
              if (onlyIfNotExists &amp;amp;&amp;amp; databaseTables.forTable(tableId) != null) {
                  // The table does exist, so we should do nothing ...
                  consumeRemainingStatement(start);
                  debugParsed(start);
                  return;
              }
              TableEditor table = databaseTables.editOrCreateTable(tableId);
      
              // create_definition ...
              if (tokens.matches('(')) parseCreateDefinitionList(start, table);
              // table_options ...
              parseTableOptions(start, table);
              // partition_options ...
              if (tokens.matches(&quot;PARTITION&quot;)) {
                  parsePartitionOptions(start, table);
              }
              // select_statement
              if (tokens.canConsume(&quot;AS&quot;) || tokens.canConsume(&quot;IGNORE&quot;, &quot;AS&quot;)
                  || tokens.canConsume(&quot;REPLACE&quot;, &quot;AS&quot;)) {
                  parseAsSelectStatement(start, table);
              }
      
              // Update the table definition ...
              databaseTables.overwriteTable(table.create());
              debugParsed(start);
          }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This method tries to mirror the &lt;a href=&quot;http://dev.mysql.com/doc/refman/5.7/en/create-table.html&quot;&gt;MySQL &lt;code&gt;CREATE TABLE&lt;/code&gt; grammar rules&lt;/a&gt;, which start with:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name
          (create_definition,...)
          [table_options]
          [partition_options]
      
      CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name
          [(create_definition,...)]
          [table_options]
          [partition_options]
          select_statement
      
      CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name
          { LIKE old_tbl_name | (LIKE old_tbl_name) }
      
      create_definition:
          ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;code&gt;CREATE&lt;/code&gt; literal was already consumed before our &lt;code&gt;parseCreateTable&lt;/code&gt; begins, so it first tries to consume the &lt;code&gt;TEMPORARY&lt;/code&gt; literal if available, the &lt;code&gt;TABLE&lt;/code&gt; literal, the &lt;code&gt;IF NOT EXISTS&lt;/code&gt; fragment if avaialble, and then consumes and parses the qualified name of the table. If the statement includes &lt;code&gt;LIKE otherTable&lt;/code&gt;, it uses the &lt;code&gt;databaseTables&lt;/code&gt; (which is the reference to our &lt;code&gt;Tables&lt;/code&gt; object) to overwrite the definition of the named table with that of the referenced table. Otherwise, it obtains an editor for the new table, and then (like the grammar rules) parses a list of &lt;em&gt;create_definition&lt;/em&gt; fragments, followed by &lt;em&gt;table_options&lt;/em&gt;, &lt;em&gt;partition_options&lt;/em&gt;, and possibly a &lt;em&gt;select_statement&lt;/em&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Take a look at the full &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/debezium-connector-mysql/src/main/java/io/debezium/connector/mysql/MySqlDdlParser.java&quot;&gt;MySqlDdlParser&lt;/a&gt; class to see far more details.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_wrap_up&quot;&gt;Wrap up&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This post goes into some detail about why the MySQL connector uses the DDL statements in the binlog, though we only scratched the surface about &lt;em&gt;how&lt;/em&gt; the connector does the DDL parsing with its framework, and how that can be reused in future parsers for other DBMS dialects.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Try our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt; to see the MySQL connector in action, and stay tuned for more connectors, releases, and news.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/04/14/Debezium-website/</id>
    <title>Debezium Website</title>
    <updated>2016-09-09T11:50:54+00:00</updated>
    <published>2016-04-14T00:00:00+00:00</published>
    <link href="/blog/2016/04/14/Debezium-website/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="website"></category>
    <summary>
      
      As you may have noticed, we have a new website with documentation, a blog, and information about the Debezium community and how you can contribute. Let us know what you think, and contribute improvements.
      ...
    </summary>
    <content type="html">
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As you may have noticed, we have a &lt;a href=&quot;http://debezium.io&quot;&gt;new website&lt;/a&gt; with &lt;a href=&quot;http://debezium.io/docs&quot;&gt;documentation&lt;/a&gt;, a &lt;a href=&quot;http://debezium.io/blog&quot;&gt;blog&lt;/a&gt;, and information about the &lt;a href=&quot;http://debezium.io/community&quot;&gt;Debezium community&lt;/a&gt; and how you can &lt;a href=&quot;http://debezium.io/docs/contribute&quot;&gt;contribute&lt;/a&gt;. Let us know what you think, and &lt;a href=&quot;http://debezium.io/docs/contribute&quot;&gt;contribute improvements&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/03/18/Debezium-0-1-Released/</id>
    <title>Debezium 0.1 Released</title>
    <updated>2016-09-09T11:50:54+00:00</updated>
    <published>2016-03-18T00:00:00+00:00</published>
    <link href="/blog/2016/03/18/Debezium-0-1-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      Debezium is a distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of Kafka and provides Kafka Connect compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is open source under the Apache...
    </summary>
    <content type="html">
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is a distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now the good news — &lt;strong&gt;&lt;em&gt;Debezium 0.1 is now available&lt;/em&gt;&lt;/strong&gt; and includes several significant features:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;A &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;connector for MySQL&lt;/a&gt; to monitor MySQL databases. It’s a Kafka Connect source connector, so simply install it into a Kafka Connect service (see below) and use the service’s REST API to configure and manage connectors to each DBMS server. The connector reads the MySQL binlog and generates data change events for every committed row-level modification in the monitored databases. The MySQL connector generates events based upon the tables' structure at the time the row is changed, and it automatically handles changes to the table structures.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;A small library so applications can &lt;a href=&quot;http://debezium.io/docs/embedded&quot;&gt;embed any Kafka Connect connector&lt;/a&gt; and consume data change events read directly from the source system. This provides a much lighter weight system (since Zookeeper, Kafka, and Kafka Connect services are not needed), but as a consequence is not as fault tolerant or reliable since the application must maintain state normally kept inside Kafka’s distributed and replicated logs. Thus the application becomes completely responsible for managing all state.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Although Debezium is really intended to be used as turnkey services, all of Debezium’s JARs and other artifacts are available in &lt;a href=&quot;http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22io.debezium%22&quot;&gt;Maven Central&lt;/a&gt;. Detailed information about the features, tasks, and bugs are outlined in our release notes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To make it easier to use a Debezium’s connector inside your own Kafka Connect service, we created a plugin archive (in both zip and tar.gz formats) that includes all JARs used by the connector not already included in Kafka Connect 0.9.0.1. Simply download, extract to your Kafka Connect 0.9.0.1 installation, and add all of the JARs to the service’s classpath. Once the service is restarted, you can then use the REST API to configure and manage connector instances that monitor the databases of your choice. &lt;a href=&quot;http://search.maven.org/#artifactdetails%7Cio.debezium%7Cdebezium-connector-mysql%7C0.1.0%7Cjar&quot;&gt;MySQL connector plugin archive&lt;/a&gt; is located in Maven Central, so it’s even possible to use Maven to build a customized Kafka Connect service. We’ll generate these plugins for future connectors, too.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium platform has a lot of moving parts in Zookeeper, Kafka, and Kafka Connect. To make it much easier for you to try it out and play with it, we created &lt;a href=&quot;https://hub.docker.com/u/debezium/&quot;&gt;Docker images&lt;/a&gt; and a &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt; that walks you through using Debezium. First, it has you use Docker to start a container for each of these services and a MySQL server with an example &quot;inventory&quot; database. It shows you how to use the RESTful API to register a connector to monitor the inventory database, how to watch the streams of data changes for various tables, and how changing the database produces new change events with very low latency. It also walks you through shutting down the Kafka Connect service, changing data while the service is not monitoring the database, and then restarting the Kafka Connect service to see how all of the data changes that occurred while the service was not running are still captured correctly in the streams. This tutorial really is a great way to interactively learn the basics of Debezium and change data capture.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;. We plan to release 0.2 very soon with at least one additional connector.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Emmanuel, Chris, Akshath, James, and Paul for their help with the release, questions, and discussions!&lt;/p&gt;
      &lt;/div&gt;
    </content>
  </entry>
</feed>
