<?xml version="1.0" encoding="utf-8" ?>
<feed xml:lang="en-US" xmlns="http://www.w3.org/2005/Atom">
  <id>http://debezium.io/</id>
  <title>Debezium Blog</title>
  <updated>2019-08-01T03:22:58+00:00</updated>
  <link href="http://debezium.io/blog.atom" rel="self" type="application/atom+xml" />
  <link href="http://debezium.io/" rel="alternate" type="text/html" />
  <entry>
    <id>http://debezium.io/blog/2019/07/25/debezium-0-10-0-beta3-released/</id>
    <title>Debezium 0.10.0.Beta3 Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-07-25T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/07/25/debezium-0-10-0-beta3-released/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="mongodb"></category>
    <category term="sqlserver"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      The summer is at its peak but Debezium community is not relenting in its effort so the Debezium 0.10.0.Beta3 is released.
      
      
      This version not only continues in incremental improvements of Debezium but also brings new shiny features.
      
      
      All of you who are using PostgreSQL 10 and higher as a service offered by different cloud providers definitely felt the complications when you needed to deploy logical decoding plugin necessary to enable streaming.
      This is no longer necessary. Debezium now supports (DBZ-766) pgoutput replication protocol that is available out-of-the-box since PostgreSQL 10.
      
      
      There is a set of further minor improvements.
      The tombstones for deletes are configurable for...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The summer is at its peak but Debezium community is not relenting in its effort so the Debezium &lt;strong&gt;0.10.0.Beta3&lt;/strong&gt; is released.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This version not only continues in incremental improvements of Debezium but also brings new shiny features.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;All of you who are using PostgreSQL 10 and higher as a service offered by different cloud providers definitely felt the complications when you needed to deploy logical decoding plugin necessary to enable streaming.
      This is no longer necessary. Debezium now supports (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-766&quot;&gt;DBZ-766&lt;/a&gt;) &lt;a href=&quot;https://www.postgresql.org/docs/10/protocol-logical-replication.html&quot;&gt;pgoutput&lt;/a&gt; replication protocol that is available out-of-the-box since PostgreSQL 10.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;There is a set of further minor improvements.
      The tombstones for deletes are configurable for all connectors now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1365&quot;&gt;DBZ-1365&lt;/a&gt;).
      Also tables without primary keys are now supported for all connectors (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-916&quot;&gt;DBZ-916&lt;/a&gt;).
      This further reduces the gap between old and new connectors capabilities.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;There are improvements for heartbeat system.
      Heartbeat messages now contain the timestamp (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1363&quot;&gt;DBZ-1363&lt;/a&gt;) of when they were created in their body.
      The new messages are properly skipped by the Outbox router (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1388&quot;&gt;DBZ-1388&lt;/a&gt;).
      MySQL connector additionally uses heartbeats for &lt;code&gt;BinlogReader&lt;/code&gt; (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1338&quot;&gt;DBZ-1338&lt;/a&gt;).
      MongoDB connector now utilizes heartbeats too (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1198&quot;&gt;DBZ-1198&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As we now that metrics are very important for keeping Debezium happy in production we have extended the set of supported metrics.
      A new metric count of events in error (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1222&quot;&gt;DBZ-1222&lt;/a&gt;) is added so it is easy to monitor any non-standards in processing.
      Database history recovery can take a long time during startup so it is now possible to monitor the progress of it (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1356&quot;&gt;DBZ-1356&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The other changes include updating of Docker images to use Kafka 2.3.0 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1358&quot;&gt;DBZ-1358&lt;/a&gt;).
      PostgreSQL supports lockless snapshotting (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1238&quot;&gt;DBZ-1238&lt;/a&gt;) and Outbox router now  process delete messages (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1320&quot;&gt;DBZ-1320&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We continue with stabilization of the 0.10 release line, with lots of bug fixes to the different connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Multiple defects in MySQL parser have been fixed (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1398&quot;&gt;DBZ-1398&lt;/a&gt;, (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1397&quot;&gt;DBZ-1397&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1376&quot;&gt;DBZ-1376&lt;/a&gt;) and &lt;code&gt;SAVEPOINT&lt;/code&gt; statements are no longer recorded in database history (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-794&quot;&gt;DBZ-794&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Under certain circumstances, it was possible that PostgreSQL connector lost the first event while switching to streaming from the snapshot (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1400&quot;&gt;DBZ-1400&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please refer to the 0.10.0.Beta3 &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-10-0-beta3&quot;&gt;release notes&lt;/a&gt; to learn more about all resolved issues and the upgrading procedure.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Many thanks to everybody from the Debezium community who contributed to this release:
      &lt;a href=&quot;https://github.com/addisonj&quot;&gt;Addison Higham&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/BinLi1988&quot;&gt;Bin Li&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/brbrown25&quot;&gt;Brandon Brown&lt;/a&gt; and
      &lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/07/15/streaming-cassandra-at-wepay-part-2/</id>
    <title>Streaming Cassandra at WePay - Part 2</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-07-15T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/07/15/streaming-cassandra-at-wepay-part-2/" rel="alternate" type="text/html" />
    <author>
      <name>Joy Gao</name>
    </author>
    <category term="cassandra"></category>
    <summary>
      
      
      
      This post originally appeared on the WePay Engineering blog.
      
      
      In the first half of this blog post series, we explained our decision-making process of designing a streaming data pipeline for Cassandra at WePay. In this post, we will break down the pipeline into three sections and discuss each of them in more detail:
      
      
      
      
      Cassandra to Kafka with CDC agent
      
      
      Kafka with BigQuery with KCBQ
      
      
      Transformation with BigQuery view
      
      
      
      
      
      
      Cassandra to Kafka with CDC Agent
      
      
      The Cassandra CDC agent is a JVM process that is intended to be deployed on each node in a Cassandra cluster. The agent is comprised of several interdependent processors, running concurrently and...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;strong&gt;&lt;em&gt;This post originally appeared on the &lt;a href=&quot;https://wecode.wepay.com/posts/streaming-cassandra-at-wepay-part-2&quot;&gt;WePay Engineering blog&lt;/a&gt;.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In the first half of this blog post series, we explained our decision-making process of designing a streaming data pipeline for Cassandra at WePay. In this post, we will break down the pipeline into three sections and discuss each of them in more detail:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;Cassandra to Kafka with CDC agent&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Kafka with BigQuery with KCBQ&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Transformation with BigQuery view&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;cassandra_to_kafka_with_cdc_agent&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#cassandra_to_kafka_with_cdc_agent&quot;&gt;&lt;/a&gt;Cassandra to Kafka with CDC Agent&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Cassandra CDC agent is a JVM process that is intended to be deployed on each node in a Cassandra cluster. The agent is comprised of several interdependent processors, running concurrently and working together to publish change events to Kafka.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;snapshot_processor&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#snapshot_processor&quot;&gt;&lt;/a&gt;Snapshot Processor&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This processor is responsible for bootstrapping new tables. It looks up the CDC configuration to determine the snapshot mode, and performs snapshot on CDC-enabled tables if needed. To snapshot a table, the agent performs a full table scan and converts each row in the result set into an individual create event, and then sequentially enqueues them to an in-memory &lt;a href=&quot;https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/BlockingQueue.html&quot;&gt;BlockingQueue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;commit_log_processor&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#commit_log_processor&quot;&gt;&lt;/a&gt;Commit Log Processor&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This processor is responsible for watching the CDC directory for new commit logs, parsing the commit log files via Cassandra’s &lt;a href=&quot;https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogReader.java&quot;&gt;&lt;code&gt;CommitLogReader&lt;/code&gt;&lt;/a&gt;, transforming deserialized mutations into standardized change events, and finally enqueuing them to the same queue as the snapshot processor.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;At this point, some readers may have concerns in regard to running Snapshot Processor and Commit Log Processors concurrently rather than serially. The reason is that Cassandra uses a &lt;a href=&quot;https://datastax.github.io/cpp-driver/topics/basics/client_side_timestamps/&quot;&gt;client-side timestamp&lt;/a&gt; to determine event order, and resolves conflicts with last write wins. This client-side timestamp is deliberately stored in each change event. This is why snapshotting doesn’t have to proceed commit log processing – the ordering is determined later on when the data is queried in the data warehouse.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;queue_processor&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#queue_processor&quot;&gt;&lt;/a&gt;Queue Processor&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This processor is responsible for dequeuing change events, transforming them into &lt;a href=&quot;https://avro.apache.org/docs/1.8.1/spec.html&quot;&gt;Avro&lt;/a&gt; records, and sending them to Kafka via a Kafka producer. It also tracks the position of the most recently sent event, so that on restart it is able to pick up from where it left off.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Implementing an in-memory queue in the CDC agent seems like overkill at first. Given there is only a single thread doing the enqueue and another thread doing the dequeue, the performance boost is negligible. The motivation here is to decouple the work of parsing commit logs, which should be done serially in the right order, from the work of serializing and publishing Kafka events, which can be parallelized by multiple threads for different tables. Although such parallelization is not implemented at the moment, we want the flexibility of adding this feature in the near future.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Some may also wonder why &lt;a href=&quot;https://docs.confluent.io/current/connect/index.html&quot;&gt;Kafka Connect&lt;/a&gt; is not used here as it seems like a natural fit for streaming. It is a great option if we wanted distributed parallel processing with fault tolerance. However, it is more complicated to deploy, monitor, and debug than a Kafka producer. For the purpose of building a minimum viable infrastructure, we chose Kafka producer at the time.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;schema_processor&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#schema_processor&quot;&gt;&lt;/a&gt;Schema Processor&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In order to support automatic schema evolution, this processor periodically polls the database for the latest table schema, and updates the in-memory schema cache if a change is detected. Snapshot Processor and Commit Log Processor both look up table schema from this cache and attach it as part of the change event prior to enqueue. Then upon dequeue, the Queue Processor transforms the attached table schema into an Avro schema for record serialization.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;commit_log_post_processor&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#commit_log_post_processor&quot;&gt;&lt;/a&gt;Commit Log Post Processor&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This processor is responsible for cleaning up commit logs after they have been processed. The default Commit Log Post Processor implementation will simply perform deletion. A custom Commit Log Post Processor can be configured for use case such as archiving commit log files to &lt;a href=&quot;https://aws.amazon.com/s3/&quot;&gt;S3&lt;/a&gt; or &lt;a href=&quot;https://cloud.google.com/storage/&quot;&gt;GCS&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;kafka_to_bigquery_with_kcbq&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#kafka_to_bigquery_with_kcbq&quot;&gt;&lt;/a&gt;Kafka to BigQuery with KCBQ&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once the events arrive in Kafka, we use KCBQ to send the events data to BigQuery without performing special transformations, just like in our &lt;a href=&quot;https://wecode.wepay.com/posts/streaming-databases-in-realtime-with-mysql-debezium-kafka&quot;&gt;MySQL streaming data pipeline&lt;/a&gt;. We have written a previous &lt;a href=&quot;https://wecode.wepay.com/posts/kafka-bigquery-connector&quot;&gt;blog post&lt;/a&gt; explaining this connector in more details.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;transformation_with_bigquery_view&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#transformation_with_bigquery_view&quot;&gt;&lt;/a&gt;Transformation with BigQuery View&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once the events are in BigQuery, this is where the heavy-lifting is being done. We create &lt;a href=&quot;https://cloud.google.com/bigquery/docs/views-intro&quot;&gt;virtual views&lt;/a&gt; on top of the raw tables to merge the data in a way that mirrors the source table in Cassandra. Note that each row in the raw tables contains limited data – only columns that have been modified have states. This means selecting the latest row for each primary key will not provide us with data that is consistent with source. Instead, the query must identify the latest cell in each column for each primary key. This can be achieved with self-joins on the primary key for each column in the table. Although joins are slow in MySQL, BigQuery’s parallel execution engine and columnar storage makes this possible. A view on top of a 1TB Cassandra table in BigQuery takes about 100 seconds to query.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;compaction&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#compaction&quot;&gt;&lt;/a&gt;Compaction&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The fact that the BigQuery view is virtual implies each time the view is queried essentially triggers a full compaction of the raw data. This means the cost will go up with the number of queries, not to mention the duplicated events amplifies the amount of data that needs to be processed by a factor of N, where N is the replication factor. To save cost and improve performance, periodic compaction by materializing the view is necessary.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;future_development_work&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#future_development_work&quot;&gt;&lt;/a&gt;Future Development Work&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;support_for_cassandra_4_0&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#support_for_cassandra_4_0&quot;&gt;&lt;/a&gt;Support for Cassandra 4.0&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In Cassandra 4.0, the improved CDC feature allows the connector to be able to parse events in real-time as they are written rather than in micro-batches on each commit log flush. This reduces latency substantially.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;performance_optimization&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#performance_optimization&quot;&gt;&lt;/a&gt;Performance Optimization&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As mentioned earlier, there is a single thread responsible for dequeuing, serializing, and publishing Kafka records. However, as the write throughput increases, if the performance of the agent does not keep up, it would result in a backlog of unprocessed commit logs which could potentially impact the health of our production database. The next step is to leverage parallel processing of events to optimize performance.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;streamline_with_debezium_and_kafka_connect&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#streamline_with_debezium_and_kafka_connect&quot;&gt;&lt;/a&gt;Streamline with Debezium and Kafka Connect&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We initially built the Cassandra CDC agent as a standalone project. Now that it is open-sourced as a &lt;a href=&quot;https://debezium.io/&quot;&gt;Debezium&lt;/a&gt; connector, we can replace some of our custom classes with existing ones in Debezium. Another improvement is to support common features that all Debezium connectors have, such as support for multiple serialization formats. Finally, the CDC agent is not fault tolerant; robust alert and monitoring are required as part of deployment. One area to explore in the future is to build the CDC agent on top of Kafka Connect as a source connector, this further streamlines the Cassandra connector with other Debezium connectors, and provides scalability and fault tolerance for free.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;closing_remarks&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#closing_remarks&quot;&gt;&lt;/a&gt;Closing Remarks&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Cassandra being a peer-to-peer distributed database poses some really interesting challenges for CDC that do not exist in relational databases like MySQL and Postgres, or even a single-master NoSQL database like MongoDB. Note that it is worth evaluating the limitations before rolling out your own real-time data pipeline for Cassandra.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Besides understanding Cassandra internals, we learned a few lessons on engineering productivity along the way:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;minimum_viable_product_philosophy&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#minimum_viable_product_philosophy&quot;&gt;&lt;/a&gt;Minimum Viable Product Philosophy&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;By stripping away all features except for the essentials, we were able to build, test, and deploy a working solution in a reasonable time with limited resources. Had we aimed to design a pipeline that encompasses all features upfront, it would have taken a lot longer and required much more resources.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;community_involvement&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#community_involvement&quot;&gt;&lt;/a&gt;Community Involvement&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Cassandra is an open-source project. Rather than tackling the problem solo, we were engaged with the Cassandra community from the very start (i.e. sharing experiences with committers and users via &lt;a href=&quot;https://www.meetup.com/Apache-Cassandra-Bay-Area/&quot;&gt;meetups&lt;/a&gt;, &lt;a href=&quot;https://user.cassandra.apache.narkive.com/njOxVaxP/using-cdc-feature-to-stream-c-to-kafka-design-proposal&quot;&gt;discussing proposals in mailing list&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/watch?v=0K0fYHsFBZg&quot;&gt;presenting proof-of-concept in conferences&lt;/a&gt;, etc.); all of which provided us with valuable feedback throughout the design and implementation stages.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/07/12/streaming-cassandra-at-wepay-part-1/</id>
    <title>Streaming Cassandra at WePay - Part 1</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-07-12T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/07/12/streaming-cassandra-at-wepay-part-1/" rel="alternate" type="text/html" />
    <author>
      <name>Joy Gao</name>
    </author>
    <category term="cassandra"></category>
    <summary>
      
      
      
      This post originally appeared on the WePay Engineering blog.
      
      
      Historically, MySQL had been the de-facto database of choice for microservices at WePay. As WePay scales, the sheer volume of data written into some of our microservice databases demanded us to make a scaling decision between sharded MySQL (i.e. Vitess) and switching to a natively sharded NoSQL database. After a series of evaluations, we picked Cassandra, a NoSQL database, primarily because of its high availability, horizontal scalability, and ability to handle high write throughput.
      
      
      
      
      Batch ETL Options
      
      
      After introducing Cassandra to our infrastructure, our next challenge was to figure out a way to expose...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;strong&gt;&lt;em&gt;This post originally appeared on the &lt;a href=&quot;https://wecode.wepay.com/posts/streaming-cassandra-at-wepay-part-1&quot;&gt;WePay Engineering blog&lt;/a&gt;.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Historically, MySQL had been the de-facto database of choice for microservices at WePay. As WePay scales, the sheer volume of data written into some of our microservice databases demanded us to make a scaling decision between sharded MySQL (i.e. &lt;a href=&quot;https://vitess.io&quot;&gt;Vitess&lt;/a&gt;) and switching to a natively sharded NoSQL database. After a series of evaluations, we picked Cassandra, a NoSQL database, primarily because of its high availability, horizontal scalability, and ability to handle high write throughput.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;batch_etl_options&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#batch_etl_options&quot;&gt;&lt;/a&gt;Batch ETL Options&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;After introducing Cassandra to our infrastructure, our next challenge was to figure out a way to expose data in Cassandra to &lt;a href=&quot;https://cloud.google.com/bigquery/&quot;&gt;BigQuery&lt;/a&gt;, our data warehouse, for analytics and reporting. We quickly built an Airflow &lt;a href=&quot;https://github.com/apache/airflow/blob/master/airflow/contrib/hooks/cassandra_hook.py&quot;&gt;hook&lt;/a&gt; and &lt;a href=&quot;https://github.com/apache/airflow/blob/master/airflow/contrib/operators/cassandra_to_gcs.py&quot;&gt;operator&lt;/a&gt; to execute full loads. This obviously doesn’t scale, as it rewrites the entire database on each load. To scale the pipeline, we evaluated two incremental load approaches, but both have their shortcomings:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;Range query. This is a common ETL approach where data is extracted via a range query at regular intervals, such as hourly or daily. Anyone familiar with &lt;a href=&quot;https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key&quot;&gt;Cassandra data modelling&lt;/a&gt; would quickly realize how unrealistic this approach is. Cassandra tables need to be modeled to optimize query patterns used in production. Adding this query pattern for analytics in most cases means cloning the table with different clustering keys. RDBMS folks might suggest secondary index to support this query pattern, but &lt;a href=&quot;https://pantheon.io/blog/cassandra-scale-problem-secondary-indexes&quot;&gt;secondary index in Cassandra are local&lt;/a&gt;, therefore this approach would pose performance and scaling issues of its own.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Process unmerged SSTables. SSTables are Cassandra’s immutable storage files. Cassandra offers a &lt;a href=&quot;https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/tools/ToolsSSTabledump.html&quot;&gt;sstabledump&lt;/a&gt; CLI command that converts SSTable content into human-readable JSON. However, Cassandra is built on top of the concept of &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-structured_merge-tree&quot;&gt;Log-Structured Merge (LSM) Tree&lt;/a&gt;, meaning SSTables merge periodically into new compacted files. Depending on the compaction strategy, detecting unmerged SSTable files out-of-band may be challenging (we later learned about the &lt;a href=&quot;https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/operations/opsBackupIncremental.html&quot;&gt;incremental backup&lt;/a&gt; feature in Cassandra which only backs up uncompacted SSTables; so this approach would have worked as well.)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Given these challenges, and having built and operated a &lt;a href=&quot;https://wecode.wepay.com/posts/streaming-databases-in-realtime-with-mysql-debezium-kafka&quot;&gt;streaming data pipeline for MySQL&lt;/a&gt;, we began to explore streaming options for Cassandra.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;streaming_options&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#streaming_options&quot;&gt;&lt;/a&gt;Streaming Options&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;double_writing&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#double_writing&quot;&gt;&lt;/a&gt;Double-Writing&lt;/h3&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/cassandra/double-write.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Image showing writer send two distinct writes&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The idea is to publish to Kafka every time a write is performed on Cassandra. This double-writing could be performed via the built-in trigger or a custom wrapper around the client. There are performance problems with this approach. First, due to the fact that we now need to write to two systems instead of one, write latency is increased. More importantly, when a write to one system fails due to a timeout, whether the write is successful or not is indeterministic. To guarantee data consistency on both systems, we would have to implement &lt;a href=&quot;https://en.wikipedia.org/wiki/Distributed_transaction&quot;&gt;distributed transactions&lt;/a&gt;, but multiple roundtrips for consensus will increase latency and reduce throughput further. This defeats the purpose of a high write-throughput database.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;kafka_as_event_source&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#kafka_as_event_source&quot;&gt;&lt;/a&gt;Kafka as Event Source&lt;/h3&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/cassandra/event-source.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Image showing writes sent to Kafka and then downstream DB&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The idea is to write to Kafka rather than directly writing to Cassandra; and then apply the writes to Cassandra by consuming events from Kafka. Event sourcing is a pretty popular approach these days. However, if you already have existing services directly writing to Cassandra, it would require a change in application code and a nontrivial migration. This approach also violates &lt;a href=&quot;https://docs.oracle.com/cd/E17076_05/html/gsg_db_rep/C/rywc.html&quot;&gt;read-your-writes consistency&lt;/a&gt;: the requirement that if a process performs a write, then the same process performing a subsequent read must observe the write’s effects. Since writes are routed through Kafka, there will be a lag between when the write is issued and when it is applied; during this time, reads to Cassandra will result in stale data. This may cause unforeseeable production issues.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;parsing_commit_logs&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#parsing_commit_logs&quot;&gt;&lt;/a&gt;Parsing Commit Logs&lt;/h3&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/cassandra/commit-log.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Image showing commit logs sent to Kafka&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Cassandra introduced a &lt;a href=&quot;http://cassandra.apache.org/doc/3.11.3/operating/cdc.html&quot;&gt;change data capture (CDC) feature&lt;/a&gt; in 3.0 to expose its commit logs. Commit logs are write-ahead logs in Cassandra designed to provide durability in case of machine crashes. They are typically discarded upon flush. With CDC enabled, they are instead transferred to a local CDC directory upon flush, which is then readable by other processes on the Cassandra node. This allows us to use the same CDC mechanism as in our MySQL streaming pipeline. It decouples production operations from analytics, and thus does not require additional work from application engineers.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Ultimately, after considering throughput, consistency, and separation of concerns, the final option – parsing commit logs – became the top contender.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;commit_log_deep_dive&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#commit_log_deep_dive&quot;&gt;&lt;/a&gt;Commit Log Deep Dive&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Aside from exposing commit logs, Cassandra also provides &lt;code&gt;CommitLogReader&lt;/code&gt; and &lt;code&gt;CommitLogReadHandler&lt;/code&gt; classes to help with the deserialization of logs. It seems like the hard work has been done, and what’s left is applying transformations – converting deserialized representations into Avro records and publish them to Kafka. However, as we dug further into the implementation of the CDC feature and of Cassandra itself, we realized that there are many new challenges.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;delayed_processing&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#delayed_processing&quot;&gt;&lt;/a&gt;Delayed Processing&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Commit logs only arrive in the CDC directory when it is full, in which case it would be flushed/discarded. This implies there is a delay between when the event is logged and when the event is captured. If little to no writes are executed, then the delay in event capturing could be arbitrarily long.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;space_management&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#space_management&quot;&gt;&lt;/a&gt;Space Management&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In MySQL you can set binlog retention such that the logs will be automatically deleted after the configured retention period. However in Cassandra there is no such option. Once the commit logs are transferred to CDC directory, consumption must be in place to clean up commit logs after processing. If the available disk space for CDC directory exceeds a given threshold, further writes to the database will be rejected.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;duplicated_events&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#duplicated_events&quot;&gt;&lt;/a&gt;Duplicated Events&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Commit logs on an individual Cassandra node do not reflect all writes to the cluster; they only reflect writes to the node. This makes it necessary to process commit logs on all nodes. But with a replication factor of N, N copies of each event are sent downstream.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;out_of_order_events&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#out_of_order_events&quot;&gt;&lt;/a&gt;Out-of-Order Events&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Writes to an individual Cassandra node are logged serially as they arrive. However, these events may arrive out-of-order from when they are issued. Downstream consumers of these events must understand the event time and implement last write wins logic similar to &lt;a href=&quot;https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/dml/dmlAboutReads.html&quot;&gt;Cassandra’s read path&lt;/a&gt; to get the correct result.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;out_of_band_schema_change&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#out_of_band_schema_change&quot;&gt;&lt;/a&gt;Out-of-Band Schema Change&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Schema changes of tables are communicated via a &lt;a href=&quot;https://en.wikipedia.org/wiki/Gossip_protocol&quot;&gt;gossip protocol&lt;/a&gt; and are not recorded in commit logs. Therefore changes in schema could only be detected on a best-effort basis.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;incomplete_row_data&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#incomplete_row_data&quot;&gt;&lt;/a&gt;Incomplete Row Data&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Cassandra does not perform read before write, as a result change events do not capture the state of every column, they only capture the state of modified columns. This makes the change event less useful than if the full row is available.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once we acquired a deep understanding of Cassandra commit logs, we re-assessed our requirements against the given constraints in order to design a &lt;a href=&quot;https://riccomini.name/minimum-viable-infrastructure&quot;&gt;minimum viable infrastructure&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;minimum_viable_infrastructure&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#minimum_viable_infrastructure&quot;&gt;&lt;/a&gt;Minimum Viable Infrastructure&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Borrowing from the &lt;a href=&quot;https://en.wikipedia.org/wiki/Minimum_viable_product&quot;&gt;minimum viable product&lt;/a&gt; philosophy, we want to design a data pipeline with a minimum set of features and requirements to satisfy our immediate customers. For Cassandra CDC, this means:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Production database’s health and performance should not be negatively impacted by introducing CDC; slowed operations and system downtimes are much costlier than a delay in the analytics pipeline&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Querying Cassandra tables in our data warehouse should match the results of querying the production database (barring delays); having duplicate and/or incomplete rows amplifies post-processing workload for every end user
      With these criteria in front of us, we began to brainstorm for solutions, and ultimately came up with three approaches:&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;stateless_stream_processing&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#stateless_stream_processing&quot;&gt;&lt;/a&gt;Stateless Stream Processing&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This solution is inspired by Datastax’s &lt;a href=&quot;https://www.datastax.com/dev/blog/advanced-replication-in-dse-5-1&quot;&gt;advanced replication blog post&lt;/a&gt;. The idea is to deploy an agent on each Cassandra node to process local commit logs. Each agent is considered as “primary” for a subset of writes based on partition keys, such that every event has exactly one primary agent. Then during CDC, in order to avoid duplicate events, each agent only sends an event to Kafka if it is the primary agent for the event. To handle eventual consistency, each agent would sort events into per-table time-sliced windows as they arrive (but doesn’t publish them right away); when a window expires, events in that window are hashed, and the hash is compared against other nodes. If they don’t match, data is fetched from the inconsistent node so the correct value could be resolved by last write wins. Finally the corrected events in that window will be sent to Kafka. Any out-of-order event beyond the time-sliced windows would have to be logged into an out-of-sequence file and handled separately. Since deduplication and ordering are done in-memory, concerns with agent failover causing data loss, OOM issues impacting production database, and the overall complexity of this implementation stopped us from exploring it further.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;stateful_stream_processing&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#stateful_stream_processing&quot;&gt;&lt;/a&gt;Stateful Stream Processing&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This solution is the most feature rich. The idea is that the agent on each Cassandra node will process commit logs and publish events to Kafka without deduplication and ordering. Then a stream processing engine will consume these raw events and do the heavy lifting (such as filtering out duplicate events with a cache, managing event orders with event-time windowing, and capturing state of unmodified columns by performing read before write on a state store), and then publish these derived events to a separate Kafka topic. Finally, &lt;a href=&quot;https://github.com/wepay/kafka-connect-bigquery&quot;&gt;KCBQ&lt;/a&gt; will be used to consume events from this topic and upload them to BigQuery. This approach is appealing because it solves the problem generically – anyone can subscribe to the latter Kafka topic without needing to handle deduplication and ordering on their own. However, this approach introduces a nontrivial amount of operational overhead; we would have to maintain a stream processing engine, a database, and a cache.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;processing_on_read&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#processing_on_read&quot;&gt;&lt;/a&gt;Processing-On-Read&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Similar to the previous approach, the idea is to process commit logs on each Cassandra node and send events to Kafka without deduplication and ordering. Unlike the previous approach, the stream processing portion is completely eliminated. Instead the raw events will be directly uploaded to BigQuery via KCBQ. &lt;a href=&quot;https://cloud.google.com/bigquery/docs/views-intro&quot;&gt;Views&lt;/a&gt; are created on top of the raw tables to handle deduplication, ordering, and merging of columns to form complete rows. Because BigQuery views are virtual tables, the processing is done lazily each time the view is queried. To prevent the view query from getting too expensive, the views would be materialized periodically. This approach removes both operational complexity and code complexity by leveraging BigQuery’s &lt;a href=&quot;https://cloud.google.com/blog/products/gcp/bigquery-under-the-hood&quot;&gt;massively parallel query engine&lt;/a&gt;. However, the drawback is that non-KCBQ downstream consumers must do all the work on their own.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Given that our main purpose of streaming Cassandra is data warehousing, we ultimately decided to implement &lt;em&gt;processing-on-read&lt;/em&gt;. It provides the essential features for our existing use case, and offers the flexibility to expand into the other two more generic solutions mentioned above in the future.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;open_source&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#open_source&quot;&gt;&lt;/a&gt;Open Source&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;During this process of building a real-time data pipeline for Cassandra, we have received a substantial amount of interest on this project. As a result, we have decided to open-source the Cassandra CDC agent under the &lt;a href=&quot;https://debezium.io&quot;&gt;Debezium&lt;/a&gt; umbrella as an &lt;a href=&quot;https://github.com/debezium/debezium-incubator&quot;&gt;incubating connector&lt;/a&gt;. If you would like to learn more or contribute, check out the work-in-progress pull request for &lt;a href=&quot;https://github.com/debezium/debezium-incubator/pull/98&quot;&gt;source code&lt;/a&gt; and &lt;a href=&quot;https://github.com/debezium/debezium.github.io/pull/325&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In the second half of this blog post series, we will elaborate on the CDC implementation itself in more details. Stay tuned!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/07/08/tutorial-sentry-debezium-container-images/</id>
    <title>Tutorial for Adding Sentry into Debezium Container Images</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-07-08T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/07/08/tutorial-sentry-debezium-container-images/" rel="alternate" type="text/html" />
    <author>
      <name>Renato Mefi</name>
    </author>
    <category term="sentry"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      Debezium has received a huge improvement to the structure of its container images recently,
      making it extremely simple to extend its behaviour.
      
      
      This is a small tutorial showing how you can for instance add Sentry,
      "an open-source error tracking [software] that helps developers monitor and fix crashes in real time".
      Here we&#8217;ll use it to collect and report any exceptions from Kafka Connect and its connectors.
      Note that this is only applicable for Debezium 0.9+.
      
      
      We need a few things to have Sentry working, and we&#8217;ll add all of them and later have a Dockerfile which gets it all glued correctly:
      
      
      
      
      Configure Log4j
      
      
      SSL certificate for sentry.io, since...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium has received a huge improvement to the structure of its container images &lt;a href=&quot;http://debezium.io/blog/2019/06/03/debezium-0-10-0-alpha2-released/&quot;&gt;recently&lt;/a&gt;,
      making it extremely simple to extend its behaviour.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This is a small tutorial showing how you can for instance add &lt;a href=&quot;https://sentry.io/welcome/&quot;&gt;Sentry&lt;/a&gt;,
      &quot;an open-source error tracking [software] that helps developers monitor and fix crashes in real time&quot;.
      Here we’ll use it to collect and report any exceptions from Kafka Connect and its connectors.
      Note that this is only applicable for Debezium 0.9+.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We need a few things to have Sentry working, and we’ll add all of them and later have a Dockerfile which gets it all glued correctly:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Configure Log4j&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;SSL certificate for &lt;a href=&quot;https://sentry.io&quot;&gt;sentry.io&lt;/a&gt;, since it’s not by default in the JVM trusted chain&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The &lt;code&gt;sentry&lt;/code&gt; and &lt;code&gt;sentry-log4j&lt;/code&gt; libraries&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;log4j_configuration&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#log4j_configuration&quot;&gt;&lt;/a&gt;Log4j Configuration&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s create a file &lt;em&gt;config/log4j.properties&lt;/em&gt; in our local project which is a copy of the one shipped with Debezium images and add Sentry to it.
      Note we added &lt;code&gt;Sentry&lt;/code&gt; to &lt;code&gt;log4j.rootLogger&lt;/code&gt; and created the section &lt;code&gt;log4j.appender.Sentry&lt;/code&gt;, the rest remains as the original configuration:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-config&quot; data-lang=&quot;config&quot;&gt;kafka.logs.dir=logs
      
      log4j.rootLogger=INFO, stdout, appender, Sentry
      
      # Disable excessive reflection warnings - KAFKA-5229
      log4j.logger.org.reflections=ERROR
      
      log4j.appender.stdout=org.apache.log4j.ConsoleAppender
      log4j.appender.stdout.threshold=INFO
      log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
      log4j.appender.stdout.layout.ConversionPattern=%d{ISO8601} %-5p  %X{dbz.connectorType}|%X{dbz.connectorName}|%X{dbz.connectorContext}  %m   [%c]%n
      
      log4j.appender.appender=org.apache.log4j.DailyRollingFileAppender
      log4j.appender.appender.DatePattern='.'yyyy-MM-dd-HH
      log4j.appender.appender.File=${kafka.logs.dir}/connect-service.log
      log4j.appender.appender.layout=org.apache.log4j.PatternLayout
      log4j.appender.appender.layout.ConversionPattern=%d{ISO8601} %-5p  %X{dbz.connectorType}|%X{dbz.connectorName}|%X{dbz.connectorContext}  %m   [%c]%n
      
      log4j.appender.Sentry=io.sentry.log4j.SentryAppender
      log4j.appender.Sentry.threshold=WARN&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;sentry_io_ssl_certificate&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#sentry_io_ssl_certificate&quot;&gt;&lt;/a&gt;Sentry.io SSL certificate&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Download the &lt;em&gt;getsentry.pem&lt;/em&gt; file from &lt;a href=&quot;https://docs.sentry.io/ssl/&quot;&gt;sentry.io&lt;/a&gt; and put it in your project’s directory under &lt;em&gt;ssl/&lt;/em&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;the_dockerfile&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_dockerfile&quot;&gt;&lt;/a&gt;The Dockerfile&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now we can glue everything together in our Debezium image:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Let’s first create a JKS file with our Sentry certificate; this uses a Docker multi-stage building process, where we are generating a &lt;code&gt;certificates.jks&lt;/code&gt; which we’ll later copy into our Kafka Connect with Debezium stage&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Copy &lt;code&gt;log4j.properties&lt;/code&gt; into &lt;code&gt;$KAFKA_HOME/config/log4j.properties&lt;/code&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Copy the JKS file from the multi-stage build&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Set ENV with the Sentry version and m5sums&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Download Sentry dependencies, the script you see called &lt;code&gt;docker-maven-download&lt;/code&gt; is a helper which we ship by default in our images.
      In this case we’re using it to download a JAR file from Maven Central and put it in the Kafka libs directory.
      We do that by setting the ENV var &lt;code&gt;MAVEN_DEP_DESTINATION=$KAFKA_HOME/libs&lt;/code&gt;:&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-dockerfile&quot; data-lang=&quot;dockerfile&quot;&gt;FROM fabric8/java-centos-openjdk8-jdk:1.6 as ssl-jks
      
      ARG JKS_STOREPASS=&quot;any random password, you can also set it outside via the arguments from docker build&quot;
      
      USER root:root
      
      COPY /ssl /ssl
      
      RUN chown -R jboss:jboss /ssl
      
      USER jboss:jboss
      
      WORKDIR /ssl
      
      RUN keytool -import -noprompt -alias getsentry \
          -storepass &quot;${JKS_STOREPASS}&quot; \
          -keystore certificates.jks \
          -trustcacerts -file &quot;/ssl/getsentry.pem&quot;
      
      FROM debezium/connect:0.10 AS kafka-connect
      
      EXPOSE 8083
      
      COPY config/log4j.properties &quot;$KAFKA_HOME/config/log4j.properties&quot;
      
      COPY --from=ssl-jks --chown=kafka:kafka /ssl/certificates.jks /ssl/
      
      ENV SENTRY_VERSION=1.7.23 \
          MAVEN_DEP_DESTINATION=$KAFKA_HOME/libs
      
      RUN docker-maven-download \
              central io/sentry sentry &quot;$SENTRY_VERSION&quot; 4bf1d6538c9c0ebc22526e2094b9bbde &amp;amp;&amp;amp; \
          docker-maven-download \
              central io/sentry sentry-log4j &quot;$SENTRY_VERSION&quot; 74af872827bd7e1470fd966449637a77&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;build_and_run&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#build_and_run&quot;&gt;&lt;/a&gt;Build and Run&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now we can simply build the image:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ docker build -t debezium/connect-sentry:1 --build-arg=JKS_STOREPASS=&quot;123456789&quot; .&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When running the image we have now to configure our Kafka Connect application to load the JKS file by setting &lt;code&gt;KAFKA_OPTS: -Djavax.net.ssl.trustStore=/ssl/certificates.jks -Djavax.net.ssl.trustStorePassword=&amp;lt;YOUR TRUSTSTORE PASSWORD&amp;gt;&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Sentry can be &lt;a href=&quot;https://docs.sentry.io/clients/java/config/#id2&quot;&gt;configured in many ways&lt;/a&gt;, I like to do it via environment variables, the minimum we can set is the Sentry DSN (which is necessary to point to your project) and the actual running environment name (i.e.: production, staging).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In this case we can configure the variables: &lt;code&gt;SENTRY_DSN=&amp;lt;GET THE DNS IN SENTRY’S DASHBOARD&amp;gt;&lt;/code&gt;, &lt;code&gt;SENTRY_ENVIRONMENT=dev&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In case you’d like to learn more about using the Debezium container images, please &lt;a href=&quot;http://debezium.io/docs/tutorial/#starting_docker&quot;&gt;check our tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;And that’s it, a basic  a recipe for extending our Docker setup using Sentry as an example;
      other modifications should also be as simple as this one.
      As an example how a &lt;code&gt;RecordTooLarge&lt;/code&gt; exception from the Kafka producer would look like in this setup, see the picture below:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/sentry/example-record-too-large-exception.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Sentry Exception example&quot; /&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;conclusion&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#conclusion&quot;&gt;&lt;/a&gt;Conclusion&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to the recent refactor of the Debezium container images, it got very easy to amend them with your custom extensions.
      Downloading external dependencies and adding them to the images became a trivial task and we’d love to hear your feedback about it!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you are curious about the refactoring itself, you can find the details in pull request &lt;a href=&quot;https://github.com/debezium/docker-images/pull/131&quot;&gt;debezium/docker-images#131&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/06/28/debezium-0-10-0-beta2-released/</id>
    <title>Debezium 0.10.0.Beta2 Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-06-28T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/06/28/debezium-0-10-0-beta2-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="mongodb"></category>
    <category term="sqlserver"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s my pleasure to announce the release of Debezium 0.10.0.Beta2!
      
      
      This further stabilizes the 0.10 release line, with lots of bug fixes to the different connectors.
      23 issues were fixed for this release;
      a couple of those relate to the DDL parser of the MySQL connector,
      e.g. around RENAME INDEX (DBZ-1329),
      SET NEW in triggers (DBZ-1331)
      and function definitions with the COLLATE keyword (DBZ-1332).
      
      
      For the Postgres connector we fixed a potential inconsistency when flushing processed LSNs to the database
      (DBZ-1347).
      Also the "include.unknown.datatypes" option works as expected now during snapshotting
      (DBZ-1335)
      and the connector won&#8217;t stumple upon materialized views during snapshotting any longer
      (DBZ-1345).
      
      
      The SQL Server connector will use much less...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.10.0.Beta2&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This further stabilizes the 0.10 release line, with lots of bug fixes to the different connectors.
      &lt;a href=&quot;https://issues.jboss.org/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.10.0.Beta2&quot;&gt;23 issues&lt;/a&gt; were fixed for this release;
      a couple of those relate to the DDL parser of the MySQL connector,
      e.g. around &lt;code&gt;RENAME INDEX&lt;/code&gt; (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1329&quot;&gt;DBZ-1329&lt;/a&gt;),
      &lt;code&gt;SET NEW&lt;/code&gt; in triggers (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1331&quot;&gt;DBZ-1331&lt;/a&gt;)
      and function definitions with the &lt;code&gt;COLLATE&lt;/code&gt; keyword (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1332&quot;&gt;DBZ-1332&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For the Postgres connector we fixed a potential inconsistency when flushing processed LSNs to the database
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1347&quot;&gt;DBZ-1347&lt;/a&gt;).
      Also the &quot;include.unknown.datatypes&quot; option works as expected now during snapshotting
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1335&quot;&gt;DBZ-1335&lt;/a&gt;)
      and the connector won’t stumple upon materialized views during snapshotting any longer
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1345&quot;&gt;DBZ-1345&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The SQL Server connector will use much less memory in many situations
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1065&quot;&gt;DBZ-1065&lt;/a&gt;)
      and it’s configurable now whether it should emit tombstone events for deletions or not
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-835&quot;&gt;DBZ-835&lt;/a&gt;).
      This also was added for the Oracle connector, bringing consistency for this option across all the connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note that this release can be used with Apache Kafka 2.x, but not with 1.x.
      This was an unintentional change and compatibility with 1.x will be restored for the Beta3 release
      (the issue to track is &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1361&quot;&gt;DBZ-1361&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please refer to the 0.10.0.Beta2 &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-10-0-beta2&quot;&gt;release notes&lt;/a&gt; to learn more about all resolved issues and the upgrading procedure.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Many thanks to everybody from the Debezium community who contributed to this release:
      &lt;a href=&quot;https://github.com/pan3793&quot;&gt;Cheng Pan&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/willome&quot;&gt;Guillaume Rosauro&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/szczeles&quot;&gt;Mariusz Strzelecki&lt;/a&gt; and
      &lt;a href=&quot;https://github.com/ssouris&quot;&gt;Stathis Souris&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/06/19/debezium-wears-fedora/</id>
    <title>Debezium Wears Fedora</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-06-19T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/06/19/debezium-wears-fedora/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="postgres"></category>
    <category term="fedora"></category>
    <category term="vagrant"></category>
    <summary>
      
      
      
      The Debezium project strives to provide an easy deployment of connectors,
      so users can try and run connectors of their choice mostly by getting the right connector archive and unpacking it into the plug-in path of Kafka Connect.
      
      
      This is true for all connectors but for the Debezium PostgreSQL connector.
      This connector is specific in the regard that it requires a logical decoding plug-in to be installed inside the PostgreSQL source database(s) themselves.
      Currently, there are two supported logical plug-ins:
      
      
      
      
      postgres-decoderbufs, which uses Protocol Buffers as a very compact transport format and which is maintained by the Debezium community
      
      
      JSON-based, which is based on JSON and...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium project strives to provide an easy deployment of connectors,
      so users can try and run connectors of their choice mostly by getting the right connector archive and unpacking it into the plug-in path of Kafka Connect.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This is true for all connectors but for the &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql/&quot;&gt;Debezium PostgreSQL connector&lt;/a&gt;.
      This connector is specific in the regard that it requires a &lt;a href=&quot;https://www.postgresql.org/docs/current/logicaldecoding-explanation.html&quot;&gt;logical decoding plug-in&lt;/a&gt; to be installed inside the PostgreSQL source database(s) themselves.
      Currently, there are two supported logical plug-ins:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/debezium/&quot;&gt;postgres-decoderbufs&lt;/a&gt;, which uses Protocol Buffers as a very compact transport format and which is maintained by the Debezium community&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/eulerto/wal2json&quot;&gt;JSON-based&lt;/a&gt;, which is based on JSON and which is maintained by its own upstream community&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;These plug-ins can be consumed and deployed in two ways;
      the easiest one is to use one of our pre-made &lt;a href=&quot;https://hub.docker.com/r/debezium/postgres&quot;&gt;Postgres container images&lt;/a&gt;,
      which contain both plug-ins and are already configured as required.
      If you are using containers in your datacenter, and/or if you start a fresh database from scratch,
      then this can be a great option.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The other approach is building from source.
      Even if this is usually an easy task, it still brings a barrier to an easy start and requires a non-trivial knowledge of the Linux operating system.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To bridge the gap between those two extremes we’ve created and published an &lt;a href=&quot;https://src.fedoraproject.org/rpms/postgres-decoderbufs&quot;&gt;RPM package&lt;/a&gt;,
      available for Fedora 30 and later.
      By installing this package you will have the necessary binaries deployed, and the only task remaining is to configure PostgreSQL to enable the plug-in.
      The RPM is based on the latest stable Debezium release,
      &lt;a href=&quot;http://debezium.io/blog/2019/05/06/debezium-0-9-5-final-released/&quot;&gt;0.9.5.Final&lt;/a&gt; at this point.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;example&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#example&quot;&gt;&lt;/a&gt;Example&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s show how the package works.
      We will use the &lt;a href=&quot;https://www.vagrantup.com/&quot;&gt;Vagrant&lt;/a&gt; tool as an easy way for firing up a pre-provisioned virtual machine with Fedora.
      Of course, that’s not a requirement and the same steps apply for any other way of running Fedora.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Create and start virtual machine with Fedora 30:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ vagrant init fedora/30-cloud-base
      
      A `Vagrantfile` has been placed in this directory. You are now
      ready to `vagrant up` your first virtual environment! Please read
      the comments in the Vagrantfile as well as documentation on
      `vagrantup.com` for more information on using Vagrant.
      
      $ vagrant up
      
      Bringing machine 'default' up with 'virtualbox' provider...
      .
      .
      .
      ==&amp;gt; default: Machine booted and ready!&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Log into the virtual machine:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ vagrant ssh&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Install the PostgreSQL server and Protocol Buffers logical decoding plug-in:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo dnf -y install postgresql postgres-decoderbufs
      .
      .
      .
      Installed:
        postgres-decoderbufs-0.9.5-1.fc30.x86_64              postgresql-11.3-1.fc30.x86_64
        postgis-2.5.1-1.fc30.x86_64                           armadillo-9.400.4-1.fc30.x86_64
        blas-3.8.0-12.fc30.x86_64                             cairo-1.16.0-5.fc30.x86_64
        cups-libs-1:2.2.11-2.fc30.x86_64                      fontconfig-2.13.1-8.fc30.x86_64
        lapack-3.8.0-12.fc30.x86_64                           libgfortran-9.1.1-1.fc30.x86_64
        libpq-11.3-2.fc30.x86_64                              libquadmath-9.1.1-1.fc30.x86_64
        mariadb-connector-c-3.0.10-1.fc30.x86_64              mariadb-connector-c-config-3.0.10-1.fc30.noarch
        nss-3.44.0-2.fc30.x86_64                              nss-softokn-3.44.0-2.fc30.x86_64
        nss-softokn-freebl-3.44.0-2.fc30.x86_64               nss-sysinit-3.44.0-2.fc30.x86_64
        nss-util-3.44.0-2.fc30.x86_64                         poppler-0.73.0-9.fc30.x86_64
        postgresql-server-11.3-1.fc30.x86_64                  proj-5.2.0-2.fc30.x86_64
        proj-datumgrid-1.8-2.fc30.noarch                      uriparser-0.9.3-1.fc30.x86_64
        SuperLU-5.2.1-6.fc30.x86_64                           arpack-3.5.0-6.fc28.x86_64
        atk-2.32.0-1.fc30.x86_64                              avahi-libs-0.7-18.fc30.x86_64
        cfitsio-3.450-3.fc30.x86_64                           dejavu-fonts-common-2.37-1.fc30.noarch
        dejavu-sans-fonts-2.37-1.fc30.noarch                  fontpackages-filesystem-1.44-24.fc30.noarch
        freexl-1.0.5-3.fc30.x86_64                            fribidi-1.0.5-2.fc30.x86_64
        gdal-libs-2.3.2-7.fc30.x86_64                         gdk-pixbuf2-2.38.1-1.fc30.x86_64
        gdk-pixbuf2-modules-2.38.1-1.fc30.x86_64              geos-3.7.1-1.fc30.x86_64
        giflib-5.1.9-1.fc30.x86_64                            graphite2-1.3.10-7.fc30.x86_64
        gtk-update-icon-cache-3.24.8-1.fc30.x86_64            gtk2-2.24.32-4.fc30.x86_64
        harfbuzz-2.3.1-1.fc30.x86_64                          hdf5-1.8.20-6.fc30.x86_64
        hicolor-icon-theme-0.17-5.fc30.noarch                 jasper-libs-2.0.14-8.fc30.x86_64
        jbigkit-libs-2.1-16.fc30.x86_64                       lcms2-2.9-5.fc30.x86_64
        libXcomposite-0.4.4-16.fc30.x86_64                    libXcursor-1.1.15-5.fc30.x86_64
        libXdamage-1.1.4-16.fc30.x86_64                       libXfixes-5.0.3-9.fc30.x86_64
        libXft-2.3.2-12.fc30.x86_64                           libXi-1.7.9-9.fc30.x86_64
        libXinerama-1.1.4-3.fc30.x86_64                       libaec-1.0.4-1.fc30.x86_64
        libdap-3.20.3-1.fc30.x86_64                           libgeotiff-1.4.3-3.fc30.x86_64
        libgta-1.0.9-2.fc30.x86_64                            libjpeg-turbo-2.0.2-1.fc30.x86_64
        libkml-1.3.0-19.fc30.x86_64                           libspatialite-4.3.0a-11.fc30.x86_64
        libtiff-4.0.10-4.fc30.x86_64                          libwebp-1.0.2-2.fc30.x86_64
        netcdf-4.4.1.1-12.fc30.x86_64                         nspr-4.21.0-1.fc30.x86_64
        ogdi-3.2.1-4.fc30.x86_64                              openblas-0.3.5-5.fc30.x86_64
        openblas-openmp-0.3.5-5.fc30.x86_64                   openblas-serial-0.3.5-5.fc30.x86_64
        openblas-threads-0.3.5-5.fc30.x86_64                  openblas-threads64_-0.3.5-5.fc30.x86_64
        openjpeg2-2.3.1-1.fc30.x86_64                         pango-1.43.0-3.fc30.x86_64
        pixman-0.38.0-1.fc30.x86_64                           poppler-data-0.4.9-3.fc30.noarch
        protobuf-c-1.3.1-2.fc30.x86_64                        unixODBC-2.3.7-4.fc30.x86_64
        xerces-c-3.2.2-2.fc30.x86_64
      
      Complete!&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Next, initialize the database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo /usr/bin/postgresql-setup --initdb&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now enable the plug-in in the database server configuration file &lt;code&gt;/var/lib/pgsql/data/postgresql.conf&lt;/code&gt; by adding the following parameters:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# MODULES
      shared_preload_libraries = 'decoderbufs'
      
      # REPLICATION
      wal_level = logical             # minimal, archive, hot_standby, or logical (change requires restart)
      max_wal_senders = 8             # max number of walsender processes (change requires restart)
      wal_keep_segments = 4           # in logfile segments, 16MB each; 0 disables
      #wal_sender_timeout = 60s       # in milliseconds; 0 disables
      max_replication_slots = 4       # max number of replication slots (change requires restart)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Configure the security file &lt;code&gt;/var/lib/pgsql/data/pg_hba.conf&lt;/code&gt; for the database user that will be used by Debezium (e.g. &lt;code&gt;debezium&lt;/code&gt;) by adding these parameters:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;local   replication     debezium                          trust
      host    replication     debezium  127.0.0.1/32            trust
      host    replication     debezium  ::1/128                 trust&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, restart PostgreSQL:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo systemctl restart postgresql&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;And that’s it:
      Now we have a PostgreSQL database, that is ready to stream changes to the Debezium PostgreSQL connector.
      Of course, the plug-in can also be installed to an already existing database (Postgres versions 9 and later),
      just by installing the RPM package and setting up the config and security files in the described way.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;outlook_pgoutput&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#outlook_pgoutput&quot;&gt;&lt;/a&gt;Outlook: pgoutput&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While the decoderbufs plug-in is our recommended choice for a logical decoding plug-in,
      there are cases where you may not be able to use it.
      Most specifically, you typically don’t have the flexibility to install custom plug-ins in cloud-based environments such as Amazon RDS.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This is why we’re exploring a &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-766&quot;&gt;third alternative&lt;/a&gt; to decoderbufs and wal2sjon right now,
      which is to leverage Postgres logical replication mechanism.
      There’s a built-in plug-in, &lt;em&gt;pgoutput&lt;/em&gt; based on this, which exists in every Postgres database since version 10.
      We’re still in the process of exploring the implications (and possible limitations) of using &lt;em&gt;pgoutput&lt;/em&gt;,
      but so far things look promising and it may eventually be a valuable tool to have in the box.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Stay tuned for more details coming soon!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/06/12/debezium-0-10-0-beta1-released/</id>
    <title>Debezium 0.10.0.Beta1 Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-06-12T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/06/12/debezium-0-10-0-beta1-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="mongodb"></category>
    <category term="sqlserver"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      Another week, another Debezium release&#8201;&#8212;&#8201;I&#8217;m happy to announce the release of Debezium 0.10.0.Beta1!
      
      
      Besides the upgrade to Apache Kafka 2.2.1 (DBZ-1316),
      this mostly fixes some bugs, including a regression to the MongoDB connector introduced in the Alpha2 release
      (DBZ-1317).
      
      
      A very welcomed usability improvement is that the connectors will log a warning now
      if not at least one table is actually captured as per the whitelist/blacklist configuration
      (DBZ-1242).
      This helps to prevent the accidental exclusion all tables by means of an incorrect filter expression,
      in which case the connectors "work as intended", but no events are propagated to the message broker.
      
      
      Please see the release notes for the complete...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Another week, another Debezium release — I’m happy to announce the release of Debezium &lt;strong&gt;0.10.0.Beta1&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Besides the upgrade to Apache Kafka 2.2.1 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1316&quot;&gt;DBZ-1316&lt;/a&gt;),
      this mostly fixes some bugs, including a regression to the MongoDB connector introduced in the Alpha2 release
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1317&quot;&gt;DBZ-1317&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A very welcomed usability improvement is that the connectors will log a warning now
      if not at least one table is actually captured as per the whitelist/blacklist configuration
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1242&quot;&gt;DBZ-1242&lt;/a&gt;).
      This helps to prevent the accidental exclusion all tables by means of an incorrect filter expression,
      in which case the connectors &quot;work as intended&quot;, but no events are propagated to the message broker.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-10-0-beta1&quot;&gt;release notes&lt;/a&gt; for the complete list of issues fixed in this release.
      Also make sure to examine the upgrade guidelines for 0.10.0.Alpha1 and Alpha2 when upgrading from earlier versions.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Many thanks to community members &lt;a href=&quot;https://github.com/pan3793&quot;&gt;Cheng Pan&lt;/a&gt; and &lt;a href=&quot;https://github.com/ChingTsai&quot;&gt;Ching Tsai&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/06/05/debezium-newsletter-01-2019/</id>
    <title>Debezium&#8217;s Newsletter 01/2019</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-06-05T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/06/05/debezium-newsletter-01-2019/" rel="alternate" type="text/html" />
    <author>
      <name>Chris Cranford</name>
    </author>
    <category term="community"></category>
    <category term="news"></category>
    <category term="newsletter"></category>
    <summary>
      
      
      
      Welcome to the first edition of the Debezium community newsletter in which we share blog posts, group discussions, as well as StackOverflow
      questions that are relevant to our user community.
      
      
      
      
      Upcoming Events
      
      
      
      
      Paris Data Engineers Meet-up
      
      
      Oslo JavaZone 2019 - Change Data Streaming For Microservices With Apache Kafka and Debezium
      
      
      
      
      
      
      Articles
      
      
      Gunnar Morling recently attended Kafka Summit in London where he gave a talk on Change Data Streaming Patterns
      for Microservices With Debezium.  You can watch the full presentation here.
      
      
      Strimzi provides an easy way to run Apache Kafka on Kubernetes or Openshift.  This article
      by Sincy Sebastian shows just how simple it is to replicate change...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Welcome to the first edition of the Debezium community newsletter in which we share blog posts, group discussions, as well as StackOverflow
      questions that are relevant to our user community.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;upcoming_events&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#upcoming_events&quot;&gt;&lt;/a&gt;Upcoming Events&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.meetup.com/fr-FR/Paris-Data-Engineers/events/260694777/&quot;&gt;Paris Data Engineers Meet-up&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://2019.javazone.no&quot;&gt;Oslo JavaZone 2019 - Change Data Streaming For Microservices With Apache Kafka and Debezium&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;articles&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#articles&quot;&gt;&lt;/a&gt;Articles&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Gunnar Morling recently attended Kafka Summit in London where he gave a talk on Change Data Streaming Patterns
      for Microservices With Debezium.  You can watch the full presentation &lt;a href=&quot;https://www.confluent.io/kafka-summit-lon19/change-data-streaming-patterns-microservices-debezium&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Strimzi provides an easy way to run Apache Kafka on Kubernetes or Openshift.  &lt;a href=&quot;https://medium.com/@sincysebastian/setup-kafka-with-debezium-using-strimzi-in-kubernetes-efd494642585&quot;&gt;This article&lt;/a&gt;
      by Sincy Sebastian shows just how simple it is to replicate change events from MySQL to Elastic Search using Debezium.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium allows replicating data between heterogeneous data stores with ease.  &lt;a href=&quot;https://blog.couchbase.com/kafka-connect-mysql-couchbase-debezium/&quot;&gt;This article&lt;/a&gt; by Matthew Groves
      explains how you can replicate data from MySQL to CouchBase.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As the size of data that systems maintain continues to grow, this begins to impact how we capture, compute, and report
      real-time analytics. &lt;a href=&quot;https://medium.com/high-alpha/data-stream-processing-for-newbies-with-kafka-ksql-and-postgres-c30309cfaaf8&quot;&gt;This article&lt;/a&gt; by Maria Patterson
      explains how you can use Debezium to stream data from Postgres, perform analytical calculations using KSQL, and then
      stream those results back to Postgres for consumption.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In a &lt;a href=&quot;https://medium.com/@singaretti/streaming-de-dados-do-postgresql-utilizando-kafka-debezium-v2-d49f46d70b37&quot;&gt;recent article&lt;/a&gt; published in Portuguese,
      Paulo Singaretti illustrates how they use Debezium and Kafka to stream changes from their relational database and then store
      the change stream results in Google Cloud Services.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This &lt;a href=&quot;https://debezium.io/blog/2019/05/23/tutorial-using-debezium-connectors-with-apache-pulsar/&quot;&gt;recent blog&lt;/a&gt; by Jia Zhai provides
      a complete tutorial showing how to use Debezium connectors with Apache Pulsar.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;time_to_upgrade&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#time_to_upgrade&quot;&gt;&lt;/a&gt;Time to upgrade&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium version &lt;a href=&quot;https://debezium.io/blog/2019/05/06/debezium-0-9-5-final-released/&quot;&gt;0.9.5&lt;/a&gt; was just released.
      If you are using the 0.9 branch you should definitely check out 0.9.5.  For details on the bug fixes as well as
      the enhancements this version includes, check out the
      &lt;a href=&quot;https://issues.jboss.org/secure/ReleaseNote.jspa?projectId=12317320&amp;amp;version=12341657&quot;&gt;release notes&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium team has also begun active development on the next major version, 0.10.  We recently published
      a &lt;a href=&quot;https://debezium.io/blog/2019/05/29/debezium-0-10-0-alpha1-released/&quot;&gt;blog&lt;/a&gt; that provides an overview
      behind what 0.10 is meant to deliver.  If you want details on the bug fixes and enhancements we’ve packed
      into this release, you can view the &lt;a href=&quot;https://issues.jboss.org/issues/?jql=fixVersion%20IN%20(0.10.0.Alpha1%2C%200.10.0.Alpha2)%20ORDER%20BY%20KEY&quot;&gt;issue list&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;questions_and_answers&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#questions_and_answers&quot;&gt;&lt;/a&gt;Questions and answers&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/55569090/how-to-let-debezium-start-reading-binlog-from-the-last-row&quot;&gt;How to have Debezium start reading binlog from last row&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/55633590/is-it-possible-to-apply-smt-single-message-transforms-to-messages-from-specifi&quot;&gt;Is it possible to apply Single Message Transforms to messages from specified topics&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/55648457/kafkaconnect-produces-cdc-event-with-null-value-when-reading-from-mongodb-with-d&quot;&gt;Kafka Connect produces CDC events with null values when reading from MongoDB with Debezium&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://groups.google.com/d/msgid/debezium/1bae4e45-c6c4-4190-9955-44f901b8ca04%40googlegroups.com?utm_medium=email&amp;amp;utm_source=footer&quot;&gt;Renaming Topics&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://groups.google.com/d/msgid/debezium/cfc333f1-b5f6-462b-a1c8-0f65bc91b725%40googlegroups.com?utm_medium=email&amp;amp;utm_source=footer&quot;&gt;Stream changes with differing column names between source and destination&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://groups.google.com/d/msgid/debezium/18c1239f-af69-4161-8adc-329a91aa4c7e%40googlegroups.com?utm_medium=email&amp;amp;utm_source=footer&quot;&gt;Can’t connect to debezium kafka from Docker host&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;feedback&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#feedback&quot;&gt;&lt;/a&gt;Feedback&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We intend to publish new additions of this newsletter periodically.  Should anyone have any suggestions on changes or what could be highlighted here, we welcome that feedback.  You can reach out to us via any of our community channels found &lt;a href=&quot;http://www.debezium.io/community&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/06/03/debezium-0-10-0-alpha2-released/</id>
    <title>Debezium 0.10.0.Alpha2 Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-06-03T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/06/03/debezium-0-10-0-alpha2-released/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="mongodb"></category>
    <category term="sqlserver"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      Release early, release often&#8201;&#8212;&#8201;Less than a week since the Alpha1 we are announcing the release of Debezium 0.10.0.Alpha2!
      
      
      This is an incremental release that completes some of the tasks started in the Alpha1 release and provides a few bugfixes and also quality improvements in our Docker images.
      
      
      The change in the logic of the snapshot field has been delivered (DBZ-1295) as outlined in the last announcement.
      All connectors now provide information which of the records is the last one in the snapshot phase so that downstream consumers can react to this.
      
      
      Apache ZooKeeper was upgraded to version 3.4.14 to fix a security vulnerability (CVE-2019-0201).
      
      
      Our...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Release early, release often — Less than a week since the Alpha1 we are announcing the release of Debezium &lt;strong&gt;0.10.0.Alpha2&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This is an incremental release that completes some of the tasks started in the Alpha1 release and provides a few bugfixes and also quality improvements in our Docker images.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The change in the logic of the &lt;code&gt;snapshot&lt;/code&gt; field has been delivered (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1295&quot;&gt;DBZ-1295&lt;/a&gt;) as outlined in &lt;a href=&quot;2019/05/29/debezium-0-10-0-alpha1-released/#outlook&quot;&gt;the last announcement&lt;/a&gt;.
      All connectors now provide information which of the records is the last one in the snapshot phase so that downstream consumers can react to this.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Apache ZooKeeper was upgraded to version 3.4.14 to fix a security vulnerability (&lt;a href=&quot;https://nvd.nist.gov/vuln/detail/CVE-2019-0201&quot;&gt;CVE-2019-0201&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Our regular contributor &lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato&lt;/a&gt; dived deeply into our image build scripts and enriched (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1279&quot;&gt;DBZ-1279&lt;/a&gt;) them with a Dockerfile linter.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Schema change events include the table name(s) in the metadata describing which tables are affected by the change (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-871&quot;&gt;DBZ-871&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/barrti&quot;&gt;Bartosz Miedlar&lt;/a&gt; has fixed a bug in MySQL ANTLR grammar causing issues with identifiers in backquotes (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1300&quot;&gt;DBZ-1300&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope we will be able to keep the recent release cadence and get lout the first beta version of 0.10 in two weeks.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Stay tuned for more!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/05/29/debezium-0-10-0-alpha1-released/</id>
    <title>Debezium 0.10.0.Alpha1 &quot;Spring Clean-Up&quot; Edition Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-05-29T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/05/29/debezium-0-10-0-alpha1-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="mongodb"></category>
    <category term="sqlserver"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m very happy to announce the release of Debezium 0.10.0.Alpha1!
      
      
      The major theme for Debezium 0.10 will be to do some clean-up
      (that&#8217;s what you do at this time of the year, right?);
      we&#8217;ve planned to remove a few deprecated features and to streamline some details in the structure the CDC events produced by the different Debezium connectors.
      
      
      This means that upgrading to Debezium 0.10 from earlier versions might take a bit more planning and consideration compared to earlier upgrades,
      depending on your usage of features and options already marked as deprecated in 0.9 and before.
      But no worries, we&#8217;re describing all changes in great detail...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m very happy to announce the release of Debezium &lt;strong&gt;0.10.0.Alpha1&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The major theme for Debezium 0.10 will be to do some clean-up
      (that’s what you do at this time of the year, right?);
      we’ve planned to remove a few deprecated features and to streamline some details in the structure the CDC events produced by the different Debezium connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This means that upgrading to Debezium 0.10 from earlier versions might take a bit more planning and consideration compared to earlier upgrades,
      depending on your usage of features and options already marked as deprecated in 0.9 and before.
      But no worries, we’re describing all changes in great detail in this blog post and the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-10-0-alpha1&quot;&gt;release notes&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;why&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#why&quot;&gt;&lt;/a&gt;Why?&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;First of all, let’s discuss a bit why we’re doing these changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Over the last three years, Debezium has grown from supporting just a single database into an entire family of &lt;a href=&quot;http://debezium.io/docs/connectors/&quot;&gt;CDC connectors&lt;/a&gt; for a range of different relational databases and MongoDB,
      as well as accompanying components such as message transformations for &lt;a href=&quot;http://debezium.io/docs/configuration/topic-routing/&quot;&gt;topic routing&lt;/a&gt; or &lt;a href=&quot;http://debezium.io/docs/configuration/outbox-event-router/&quot;&gt;implementing the outbox pattern&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As in any mature project, over time we figured that a few things should be done differently in the code base than we had thought at first.
      For instance we moved from a hand-written parser for processing MySQL DDL statements to a much more robust implementation based on Antlr.
      Also we realized the way certain temporal column types were exported was at risk of value overflow in certain conditions,
      so we added a new mode not prone to these issues.
      As a last example, we made options like the batch size used during snapshotting consistent across the different connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Luckily, Debezium quickly gained traction and despite the 0.x version number, it is used heavily in production at a large number of organizations, and users rely on its stability.
      So whenever we did such changes, we aimed at making the upgrade experience as smooth as possible;
      usually that means that the previous behavior is still available but is marked as deprecated in the documentation,
      while a new improved option, implementation etc. is added and made the default behavior.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;At the same time we realized that there are a couple of differences between the connectors which shouldn’t really be there.
      Specifically, the &lt;code&gt;source&lt;/code&gt; block of change events has some differences which make a uniform handling by consumers more complex than it should be;
      for instance the timestamp field is named &quot;ts_sec&quot; in MySQL events but &quot;ts_usec&quot; for Postgres.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With all this in mind, we decided that it is about time to clean up these issues.
      This done for a couple of purposes:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Keeping the code base maintainable and open for future development by removing legacy code such as deprecated options and their handling as well as the legacy MySQL DDL parser&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Making CDC events from different connectors easier to consume by unifying the &lt;code&gt;source&lt;/code&gt; block created by the different connectors as far as possible&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Preparing the project to go to version 1.0 with an even stronger promise of retaining backwards compatibility than already practiced today&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;what&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what&quot;&gt;&lt;/a&gt;What?&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now as we have discussed why we feel it’s time for some &quot;clean-up&quot;, let’s take a closer look at the most relevant changes.
      Please also refer to the &lt;a href=&quot;https://debezium.io/docs/releases/#breaking_changes&quot;&gt;&quot;breaking changes&quot;&lt;/a&gt; section of the migration notes for more details.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;The legacy DDL parser for MySQL has been removed (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-736&quot;&gt;DBZ-736&lt;/a&gt;);
      if you are not using the Antlr-based one yet (it was introduced in 0.8 and became the default in 0.9),
      it’s highly recommended that you test it with your databases.
      Should you run into any parsing errors, please report them so we can fix them for the 0.10 Final release.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The SMTs for retrieving the new record/document state from change events have been renamed from
      &lt;code&gt;io.debezium.transforms.UnwrapFromEnvelope&lt;/code&gt; and &lt;code&gt;io.debezium.connector.mongodb.transforms.UnwrapFromMongoDbEnvelope&lt;/code&gt;
      into &lt;code&gt;ExtractNewRecordState&lt;/code&gt; and &lt;code&gt;ExtractNewDocumentState&lt;/code&gt;, respectively
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-677&quot;&gt;DBZ-677&lt;/a&gt;).
      The old names can still be used as of 0.10, but doing so will raise a warning.
      They are planned for removal in Debezium 0.11.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Several connector options that were deprecated in earlier Debezium versions have been removed
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1234&quot;&gt;DBZ-1234&lt;/a&gt;):
      the &lt;code&gt;drop.deletes&lt;/code&gt; option of new record/document state extraction SMTs (superseded by &lt;code&gt;delete.handling.mode&lt;/code&gt; option),
      the &lt;code&gt;rows.fetch.size&lt;/code&gt; option (superseded by &lt;code&gt;snapshot.fetch.size&lt;/code&gt;),
      the &lt;code&gt;adaptive&lt;/code&gt; value of &lt;code&gt;time.precision.mode&lt;/code&gt; option for MySQL (prone to value loss, use &lt;code&gt;adaptive_microseconds&lt;/code&gt; instead) and
      the &lt;code&gt;snapshot.minimal.locks&lt;/code&gt; for the MySQL connector (superseded by &lt;code&gt;snapshot.locking.mode&lt;/code&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Several option names of the (incubating) SMT for the outbox pattern
      have been renamed for the sake of consistency (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1289&quot;&gt;DBZ-1289&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Several fields within the &lt;code&gt;source&lt;/code&gt; block of CDC events have been renamed for the sake of consistency
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-596&quot;&gt;DBZ-596&lt;/a&gt;);
      as this is technically a backwards-incompatible change when using Avro and the schema registry,
      we’ve added a connector option &lt;code&gt;source.struct.version&lt;/code&gt; which, when set to the value &lt;code&gt;v1&lt;/code&gt;, will have connectors produce the previous &lt;code&gt;source&lt;/code&gt; structure.
      &lt;code&gt;v2&lt;/code&gt; is the default and any consumers should be adjusted to work with the new &lt;code&gt;source&lt;/code&gt; structure as soon as possible.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;new_features_and_bugfixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_features_and_bugfixes&quot;&gt;&lt;/a&gt;New Features and Bugfixes&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Besides these changes, the 0.10.0.Alpha1 release also contains some feature additions and bug fixes:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;The SQL Server connector supports custom SELECT statements for snapshotting (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1224&quot;&gt;DBZ-1224&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;database, schema and table/collection names have been added consistently to the &lt;code&gt;source&lt;/code&gt; block for CDC events from all connectors
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-875&quot;&gt;DBZ-875&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Client authentication works for the MySQL connector(&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1228&quot;&gt;DBZ-1228&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The embedded engine doesn’t duplicate events after restarts any longer (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1276&quot;&gt;DBZ-1276&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;A parser bug related to &lt;code&gt;CREATE INDEX&lt;/code&gt; statements was fixed (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1264&quot;&gt;DBZ-1264&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Overall, &lt;a href=&quot;https://issues.jboss.org/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.10.0.Alpha1&quot;&gt;30 issues&lt;/a&gt; were addressed in this release.
      Many thanks to &lt;a href=&quot;https://github.com/Arkoprabho&quot;&gt;Arkoprabho Chakraborti&lt;/a&gt;, &lt;a href=&quot;https://github.com/rsatishm&quot;&gt;Ram Satish&lt;/a&gt; and &lt;a href=&quot;https://github.com/Wang-Yu-Chao&quot;&gt;Yuchao Wang&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Speaking of contributors, we did some housekeeping to &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/COPYRIGHT.txt&quot;&gt;the list&lt;/a&gt; of everyone ever contributing to Debezium, too.
      Not less than exactly &lt;strong&gt;111 individuals have contributed&lt;/strong&gt; code up to this point,
      which is just phenomenal! Thank you so much everyone, you folks rock!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;outlook&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#outlook&quot;&gt;&lt;/a&gt;Outlook&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Going forward, there are some more details we’d like to unify across the different connectors before going to Debezium 0.10 Final.
      For instance the &lt;code&gt;source&lt;/code&gt; attribute &lt;code&gt;snapshot&lt;/code&gt; will be changed so it can take one of three states: &lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt; or &lt;code&gt;last&lt;/code&gt;
      (indicating that this event is the last one created during initial snapshotting).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll also continue our efforts to to migrate the existing Postgres connector to the framework classes established for the SQL Server and Oracle connectors.
      Another thing we’re actively exploring is how the Postgres could take advantage of the &quot;logical replication&quot; feature added in Postgres 10.
      This may provide us with a way to ingest change events without requiring a custom server-side logical decoding plug-in,
      which proves challenging in cloud environments where there’s typically just a limited set of logical decoding options available.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/05/23/tutorial-using-debezium-connectors-with-apache-pulsar/</id>
    <title>Tutorial for Using Debezium Connectors With Apache Pulsar</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-05-23T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/05/23/tutorial-using-debezium-connectors-with-apache-pulsar/" rel="alternate" type="text/html" />
    <author>
      <name>Jia Zhai, StreamNative</name>
    </author>
    <category term="discussion"></category>
    <category term="examples"></category>
    <summary>
      
      
      
      This is a guest post by Apache Pulsar PMC Member and Committer Jia Zhai.
      
      
      Debezium is an open source project for change data capture (CDC). It is built on Apache Kafka Connect and supports multiple databases, such as MySQL, MongoDB, PostgreSQL, Oracle, and SQL Server.
      Apache Pulsar includes a set of built-in connectors based on Pulsar IO framework, which is counter part to Apache Kafka Connect.
      
      
      As of version 2.3.0, Pulsar IO comes with support for the Debezium source connectors out of the box, so you can leverage Debezium to stream changes from your databases into Apache Pulsar.
      This tutorial walks you through setting...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;strong&gt;&lt;em&gt;This is a guest post by Apache Pulsar PMC Member and Committer Jia Zhai.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://debezium.io&quot;&gt;Debezium&lt;/a&gt; is an open source project for change data capture (CDC). It is built on &lt;a href=&quot;https://kafka.apache.org/documentation/#connectapi&quot;&gt;Apache Kafka Connect&lt;/a&gt; and supports multiple databases, such as MySQL, MongoDB, PostgreSQL, Oracle, and SQL Server.
      &lt;a href=&quot;http://pulsar.apache.org&quot;&gt;Apache Pulsar&lt;/a&gt; includes a set of &lt;a href=&quot;https://pulsar.apache.org/docs/en/io-connectors&quot;&gt;built-in connectors&lt;/a&gt; based on Pulsar IO framework, which is counter part to Apache Kafka Connect.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As of version 2.3.0, Pulsar IO comes with support for the &lt;a href=&quot;http://pulsar.apache.org/docs/en/2.3.0/io-cdc-debezium&quot;&gt;Debezium source connectors&lt;/a&gt; out of the box, so you can leverage Debezium to stream changes from your databases into Apache Pulsar.
      This tutorial walks you through setting up the Debezium connector for MySQL with Pulsar IO.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;tutorial_steps&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#tutorial_steps&quot;&gt;&lt;/a&gt;Tutorial Steps&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This tutorial is similar to the &lt;a href=&quot;https://debezium.io/docs/tutorial&quot;&gt;Debezium tutorial&lt;/a&gt;, except that storage of event streams is changed from Kafka to Pulsar.
      It mainly includes six steps:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;Start a MySQL server;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Start standalone Pulsar service;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Start the Debezium connector in Pulsar IO. Pulsar IO reads database changes existing in MySQL server;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Subscribe Pulsar topics to monitor MySQL changes;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Make changes in MySQL server, and verify that changes are recorded in Pulsar topics immediately;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Clean up.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;step_1_start_a_mysql_server&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#step_1_start_a_mysql_server&quot;&gt;&lt;/a&gt;Step 1: Start a MySQL server&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Start a MySQL server that contains a database example, from which Debezium captures changes. Open a new terminal to start a new container that runs a MySQL database server pre-configured with a database named inventory:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker run --rm --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=debezium -e MYSQL_USER=mysqluser -e MYSQL_PASSWORD=mysqlpw debezium/example-mysql:0.9&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The following information is displayed:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;2019-03-25T14:12:41.178325Z 0 [Note] Event Scheduler: Loaded 0 events
      2019-03-25T14:12:41.178670Z 0 [Note] mysqld: ready for connections.
      Version: '5.7.25-log'  socket: '/var/run/mysqld/mysqld.sock'  port: 3306  MySQL Community Server (GPL)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;step_2_start_standalone_pulsar_service&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#step_2_start_standalone_pulsar_service&quot;&gt;&lt;/a&gt;Step 2: Start standalone Pulsar service&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Start Pulsar service locally in standalone mode.
      Support for running Debezium connectors in Pulsar IO is introduced in Pulsar 2.3.0.
      Download &lt;a href=&quot;https://archive.apache.org/dist/pulsar/pulsar-2.3.0/apache-pulsar-2.3.0-bin.tar.gz&quot;&gt;Pulsar binary of 2.3.0 release&lt;/a&gt; and &lt;a href=&quot;https://archive.apache.org/dist/pulsar/pulsar-2.3.0/connectors/pulsar-io-kafka-connect-adaptor-2.3.0.nar&quot;&gt;pulsar-io-kafka-connect-adaptor-2.3.0.nar of 2.3.0 release&lt;/a&gt;.
      In Pulsar, all Pulsar IO connectors are packaged as separate &lt;a href=&quot;https://medium.com/hashmapinc/nifi-nar-files-explained-14113f7796fd&quot;&gt;NAR&lt;/a&gt; files.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ wget https://archive.apache.org/dist/pulsar/pulsar-2.3.0/apache-pulsar-2.3.0-bin.tar.gz
      $ wget https://archive.apache.org/dist/pulsar/pulsar-2.3.0/connectors/pulsar-io-kafka-connect-adaptor-2.3.0.nar
      $ tar zxf apache-pulsar-2.3.0-bin.tar.gz
      $ cd apache-pulsar-2.3.0
      $ mkdir connectors
      $ cp ../pulsar-io-kafka-connect-adaptor-2.3.0.nar connectors
      $ bin/pulsar standalone&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/pulsar_tutorial/pulsar-mysql-1.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;start pulsar standalone]&quot; /&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;step_3_start_the_debezium_mysql_connector_in_pulsar_io&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#step_3_start_the_debezium_mysql_connector_in_pulsar_io&quot;&gt;&lt;/a&gt;Step 3: Start the Debezium MySQL connector in Pulsar IO&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Start the Debezium MySQL connector in Pulsar IO, with local run mode, in another terminal tab.
      The “debezium-mysql-source-config.yaml” file contains all the configuration, and main parameters are listed under the “configs” node. The .yaml file contains the &quot;task.class&quot; parameter. The configuration file also
      includes MySQL related parameters (like server, port, user, password) and two names of Pulsar topics for &quot;history&quot; and &quot;offset&quot; storage.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ bin/pulsar-admin source localrun  --sourceConfigFile debezium-mysql-source-config.yaml&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The content in the “debezium-mysql-source-config.yaml” file is as follows.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;tenant: &quot;test&quot;
      namespace: &quot;test-namespace&quot;
      name: &quot;debezium-kafka-source&quot;
      topicName: &quot;kafka-connect-topic&quot;
      archive: &quot;connectors/pulsar-io-kafka-connect-adaptor-2.3.0.nar&quot;
      
      parallelism: 1
      
      configs:
        ## sourceTask
        task.class: &quot;io.debezium.connector.mysql.MySqlConnectorTask&quot;
      
        ## config for mysql, docker image: debezium/example-mysql:0.8
        database.hostname: &quot;localhost&quot;
        database.port: &quot;3306&quot;
        database.user: &quot;debezium&quot;
        database.password: &quot;dbz&quot;
        database.server.id: &quot;184054&quot;
        database.server.name: &quot;dbserver1&quot;
        database.whitelist: &quot;inventory&quot;
      
        database.history: &quot;org.apache.pulsar.io.debezium.PulsarDatabaseHistory&quot;
        database.history.pulsar.topic: &quot;history-topic&quot;
        database.history.pulsar.service.url: &quot;pulsar://127.0.0.1:6650&quot;
        ## KEY_CONVERTER_CLASS_CONFIG, VALUE_CONVERTER_CLASS_CONFIG
        key.converter: &quot;org.apache.kafka.connect.json.JsonConverter&quot;
        value.converter: &quot;org.apache.kafka.connect.json.JsonConverter&quot;
        ## PULSAR_SERVICE_URL_CONFIG
        pulsar.service.url: &quot;pulsar://127.0.0.1:6650&quot;
        ## OFFSET_STORAGE_TOPIC_CONFIG
        offset.storage.topic: &quot;offset-topic&quot;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Tables are created automatically in the aforementioned MySQL server. So Debezium connector reads history records from MySQL binlog file from the beginning. In the output you will find the connector has already been triggered and processed in 47 records.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/pulsar_tutorial/pulsar-mysql-2.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;connector start process records&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For more information on how to manage connectors, see the &lt;a href=&quot;http://pulsar.apache.org/docs/en/io-managing/&quot;&gt;Pulsar IO documentation&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Records that have been captured and read by Debezium are automatically published to Pulsar topics. When you start a new terminal, you will find the current topics in Pulsar with the following command:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ bin/pulsar-admin topics list public/default&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/pulsar_tutorial/pulsar-mysql-3.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;list Pulsar topics&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For each table, which has been changed, the change data is stored in a separate Pulsar topic.  Except database table related topics, another two topics named “history-topic” and “offset-topic” are used to store history and offset related data.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;persistent://public/default/history-topic
      persistent://public/default/offset-topic&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;step_4_subscribe_pulsar_topics_to_monitor_mysql_changes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#step_4_subscribe_pulsar_topics_to_monitor_mysql_changes&quot;&gt;&lt;/a&gt;Step 4: Subscribe Pulsar topics to monitor MySQL changes&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Take the &lt;code&gt;persistent://public/default/dbserver1.inventory.products&lt;/code&gt; topic as an example.
      Use the CLI command to consume this topic and monitor changes while the “products” table changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt; $ bin/pulsar-client consume -s &quot;sub-products&quot; public/default/dbserver1.inventory.products -n 0&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The output is as follows:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;…
      22:17:41.201 [pulsar-client-io-1-1] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [public/default/dbserver1.inventory.products][sub-products] Subscribing to topic on cnx [id: 0xfe0b4feb, L:/127.0.0.1:55585 - R:localhost/127.0.0.1:6650]
      22:17:41.223 [pulsar-client-io-1-1] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [public/default/dbserver1.inventory.products][sub-products] Subscribed to topic on localhost/127.0.0.1:6650 -- consumer: 0&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You can also consume the offset topic to monitor the offset changes while the table changes are stored in the &lt;code&gt;persistent://public/default/dbserver1.inventory.products&lt;/code&gt; Pulsar topic.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ bin/pulsar-client consume -s &quot;sub-offset&quot; offset-topic -n 0&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;step_5_make_changes_in_mysql_server_and_verify_that_changes_are_recorded_in_pulsar_topics_immediately&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#step_5_make_changes_in_mysql_server_and_verify_that_changes_are_recorded_in_pulsar_topics_immediately&quot;&gt;&lt;/a&gt;Step 5: Make changes in MySQL server, and verify that changes are recorded in Pulsar topics immediately&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Start a MySQL CLI docker connector,  and you can make changes to the “products” table in MySQL server.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$docker run -it --rm --name mysqlterm --link mysql --rm mysql:5.7 sh -c 'exec mysql -h&quot;$MYSQL_PORT_3306_TCP_ADDR&quot; -P&quot;$MYSQL_PORT_3306_TCP_PORT&quot; -uroot -p&quot;$MYSQL_ENV_MYSQL_ROOT_PASSWORD&quot;'&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;After running the command, MySQL CLI is displayed, and you can change the names of the two items in the “products” table.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;mysql&amp;gt; use inventory;
      mysql&amp;gt; show tables;
      mysql&amp;gt; SELECT * FROM  products ;
      mysql&amp;gt; UPDATE products SET name='1111111111' WHERE id=101;
      mysql&amp;gt; UPDATE products SET name='1111111111' WHERE id=107;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/pulsar_tutorial/pulsar-mysql-4.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;mysql updates&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In the terminal where you consume products topic, you find that two changes have been added.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/pulsar_tutorial/pulsar-mysql-5.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;table topic stores mysql updates&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In the terminal where you consume the offset topic, you find that two offsets have been added.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/pulsar_tutorial/pulsar-mysql-6.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;offset topic get updated&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In the terminal where you local-run the connector, you find two more records have been processed.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/pulsar_tutorial/pulsar-mysql-7.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;table topic get more records&quot; /&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;step_6_clean_up&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#step_6_clean_up&quot;&gt;&lt;/a&gt;Step 6: Clean up.&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Use “Ctrl + C” to close terminals. Use “docker ps” and “docker kill” to stop MySQL related containers.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;mysql&amp;gt; quit
      
      $ docker ps
      CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                               NAMES
      84d66c2f591d        debezium/example-mysql:0.8   &quot;docker-entrypoint.s…&quot;   About an hour ago   Up About an hour    0.0.0.0:3306-&amp;gt;3306/tcp, 33060/tcp   mysql
      
      $ docker kill 84d66c2f591d&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To delete Pulsar data, delete data directory in the Pulsar binary directory.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ pwd
      /Users/jia/ws/releases/apache-pulsar-2.3.0
      
      $ rm -rf data&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;conclusion&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#conclusion&quot;&gt;&lt;/a&gt;Conclusion&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Pulsar IO framework allows to run the Debezium connectors for change data capture, streaming data changes from different databases into Apache Pulsar. In this tutorial you’ve learned how to capture data changes in a MySQL database and propagate them to Pulsar. We are improving support for running the Debezium connectors with Apache Pulsar continuously, it will be much easier to use after Pulsar 2.4.0 release.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/05/06/debezium-0-9-5-final-released/</id>
    <title>Debezium 0.9.5.Final Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-05-06T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/05/06/debezium-0-9-5-final-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="mongodb"></category>
    <category term="sqlserver"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s my pleasure to announce the release of Debezium 0.9.5.Final!
      
      
      This is a recommended update for all users of earlier versions; besides bug fixes also a few new features are provide.
      The release contains 18 resolved issues overall.
      
      
      
      
      Apache Kafka Update and New Features
      
      This release has been built against and tested with Apache Kafka 2.2.0 (DBZ-1227).
      Earlier versions are continued to be supported as well.
      
      
      For all the connectors it is possible now to specify the batch size when taking snapshots (DBZ-1247).
      The new connector option snapshot.fetch.size has been introduced for that.
      This option replaces the earlier option rows.fetch.size which existed in some of the connectors and...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.9.5.Final&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This is a recommended update for all users of earlier versions; besides bug fixes also a few new features are provide.
      The release contains &lt;a href=&quot;https://issues.jboss.org/issues/?jql=project+%3D+DBZ+AND+fixVersion+%3D+0.9.5.Final&quot;&gt;18 resolved issues&lt;/a&gt; overall.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;apache_kafka_update_and_new_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#apache_kafka_update_and_new_features&quot;&gt;&lt;/a&gt;Apache Kafka Update and New Features&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release has been built against and tested with Apache Kafka 2.2.0 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1227&quot;&gt;DBZ-1227&lt;/a&gt;).
      Earlier versions are continued to be supported as well.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For all the connectors it is possible now to specify the batch size when taking snapshots (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1247&quot;&gt;DBZ-1247&lt;/a&gt;).
      The new connector option &lt;code&gt;snapshot.fetch.size&lt;/code&gt; has been introduced for that.
      This option replaces the earlier option &lt;code&gt;rows.fetch.size&lt;/code&gt; which existed in some of the connectors and which will be removed in Debezium 0.10.
      Existing connector instances should therefore be re-configured to use the new option.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Continuing the work from Debezium 0.9.4, the Postgres connector supports some more column types:
      &lt;code&gt;MACADDR&lt;/code&gt; and &lt;code&gt;MACADDR8&lt;/code&gt; (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1193&quot;&gt;DBZ-1193&lt;/a&gt;) as well as &lt;code&gt;INT4RANGE&lt;/code&gt;, &lt;code&gt;INT8RANGE&lt;/code&gt; and &lt;code&gt;NUMRANGE&lt;/code&gt; (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1076&quot;&gt;DBZ-1076&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;fixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#fixes&quot;&gt;&lt;/a&gt;Fixes&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Amongst others, this release includes the following fixes:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Failing to specify value for &lt;code&gt;database.server.name&lt;/code&gt; results in invalid Kafka topic name (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-212&quot;&gt;DBZ-212&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Postgres Connector times out in schema discovery for DBs with many tables (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1214&quot;&gt;DBZ-1214&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Oracle connector: JDBC transaction can only capture single DML record (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1223&quot;&gt;DBZ-1223&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Lost precision for timestamp with timezone (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1236&quot;&gt;DBZ-1236&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;NullpointerException due to optional value for commitTime (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1241&quot;&gt;DBZ-1241&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Default value for datetime(0) is incorrectly handled (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1243&quot;&gt;DBZ-1243&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Microsecond precision is lost when reading timetz data from Postgres (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1260&quot;&gt;DBZ-1260&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please refer to the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-9-5-final&quot;&gt;release notes&lt;/a&gt; for the complete list of issues fixed in Debezium 0.9.5.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re very thankful to the following community members who contributed to this release:
      &lt;a href=&quot;https://github.com/addisonj&quot;&gt;Addison Higham&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/jorkzijlstra&quot;&gt;Jork Zijlstra&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/krizhan&quot;&gt;Krizhan Mariampillai&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/mrozieres&quot;&gt;Mathieu Rozieres&lt;/a&gt; and
      &lt;a href=&quot;https://github.com/ShubhamRwt&quot;&gt;Shubham Rawat&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;outlook&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#outlook&quot;&gt;&lt;/a&gt;Outlook&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release is planned to be the last in the 0.9 line.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re now going to focus on Debezium 0.10, whose main topic will be to clean up a few things:
      we’d like to remove a few deprecated options and features (e.g. the legacy DDL parser in the MySQL connector).
      We’re also planning to do a thorough review of the event structure of the different connectors;
      for instance in the &lt;code&gt;source&lt;/code&gt; block of CDC messages there are a some field names that should be unified.
      We believe users will benefit from a more consistent experience across the connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Another focus area will be to migrate the existing Postgres connector to the framework classes established for the SQL Server and Oracle connectors.
      This will allow to expose some new features for the Postgres connector, e.g. the monitoring capabilities already rolled out for the other two connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/04/18/hello-debezium/</id>
    <title>Debezium&#8217;s Team Grows</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-04-18T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/04/18/hello-debezium/" rel="alternate" type="text/html" />
    <author>
      <name>Chris Cranford</name>
    </author>
    <category term="community"></category>
    <category term="news"></category>
    <summary>
      
      Hello everyone, my name is Chris Cranford and I recently joined the Debezium team.
      
      
      My journey at Red Hat began just over three years ago; however I have been in this line of work for nearly
      twenty years.  All throughout my career, I have advocated and supported open source software.  Many of my
      initial software endeavors were based on open source software, several which are still heavily used today
      such as Hibernate ORM.
      
      
      When I first joined Red Hat, I had the pleasure to work on the Hibernate ORM team. I had been an end user
      of the project since 2.0, so it was...
    </summary>
    <content type="html">
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Hello everyone, my name is Chris Cranford and I recently joined the Debezium team.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;My journey at Red Hat began just over three years ago; however I have been in this line of work for nearly
      twenty years.  All throughout my career, I have advocated and supported open source software.  Many of my
      initial software endeavors were based on open source software, several which are still heavily used today
      such as &lt;a href=&quot;http://www.hibernate.org&quot;&gt;Hibernate ORM&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When I first joined Red Hat, I had the pleasure to work on the Hibernate ORM team. I had been an end user
      of the project since 2.0, so it was an excellent fit to be able to contribute full time to a project that
      had served me well in the corporate world n-times over.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It wasn’t long ago when &lt;a href=&quot;http://twitter.com/gunnarmorling&quot;&gt;@gunnarmorling&lt;/a&gt; and I had a brief exchange about
      Debezium.  I had not heard of the project and I was super stoked because I immediately saw parallel in
      its goals and &lt;a href=&quot;http://www.hibernate.org/orm/envers&quot;&gt;Hibernate Envers&lt;/a&gt;, a change data capture solution that
      is based on Hibernate’s event framework that I was currently maintaining.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I believe one of my first &quot;wow&quot; moments was when I realized how well Debezium fits into the micro-services
      world.  The idea of being able to share data between micro-services in a very decoupled way is a massive
      win for building reusable components and minimizes technical debt.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium just felt like the next logical step.  There are so many new and exciting things to come and
      the team and myself cannot wait to share them.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So lets get started!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;--Chris&lt;/p&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/04/11/debezium-0-9-4-final-released/</id>
    <title>Debezium 0.9.4.Final Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-04-11T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/04/11/debezium-0-9-4-final-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s my pleasure to announce the release of Debezium 0.9.4.Final!
      
      
      This is a drop-in replacement for earlier Debezium 0.9.x versions, containing mostly bug fixes and some improvements related to metrics.
      Overall, 17 issues were resolved.
      
      
      
      
      MySQL Connector Improvements
      
      
      The Debezium connector for MySQL comes with two new metrics:
      
      
      
      
      Whether GTID is enabled for offset tracking or not (DBZ-1221)
      
      
      Number of filtered events (DBZ-1206)
      
      
      
      
      It also supports database connections using TLS 1.2 (DBZ-1208) now.
      
      
      
      
      New Postgres Datatypes
      
      
      The Postgres connector now allows to capture changes to columns of the CIDR and INET types (DBZ-1189).
      
      
      
      
      Bug Fixes
      
      
      The fixed bugs include the following:
      
      
      
      
      Closing connection after snapshotting (DBZ-1218)
      
      
      Can parse ALTER statement affecting enum column...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.9.4.Final&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This is a drop-in replacement for earlier Debezium 0.9.x versions, containing mostly bug fixes and some improvements related to metrics.
      Overall, &lt;a href=&quot;https://issues.jboss.org/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.9.4.Final&quot;&gt;17 issues&lt;/a&gt; were resolved.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mysql_connector_improvements&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mysql_connector_improvements&quot;&gt;&lt;/a&gt;MySQL Connector Improvements&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium connector for MySQL comes with two new metrics:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Whether GTID is enabled for offset tracking or not (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1221&quot;&gt;DBZ-1221&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Number of filtered events (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1206&quot;&gt;DBZ-1206&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It also supports database connections using TLS 1.2 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1208&quot;&gt;DBZ-1208&lt;/a&gt;) now.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;new_postgres_datatypes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_postgres_datatypes&quot;&gt;&lt;/a&gt;New Postgres Datatypes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Postgres connector now allows to capture changes to columns of the &lt;code&gt;CIDR&lt;/code&gt; and &lt;code&gt;INET&lt;/code&gt; types (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1189&quot;&gt;DBZ-1189&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;bug_fixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bug_fixes&quot;&gt;&lt;/a&gt;Bug Fixes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The fixed bugs include the following:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Closing connection after snapshotting (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1218&quot;&gt;DBZ-1218&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Can parse ALTER statement affecting enum column with character set options (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1203&quot;&gt;DBZ-1203&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Avoiding timeout after bootstrapping a new table (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1207&quot;&gt;DBZ-1207&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Check out the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-9-4-final&quot;&gt;release notes&lt;/a&gt; for the complete list of issues fixed in Debezium 0.9.4.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Many thanks to Debezium community members
      &lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/jordanbragg&quot;&gt;Jordan Bragg&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/jgao54&quot;&gt;Joy Gao&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/preethi29&quot;&gt;Preethi Sadagopan&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/sashakovryga&quot;&gt;Sasha Kovryga&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/ShubhamRwt&quot;&gt;Shubham Rawat&lt;/a&gt; and
      &lt;a href=&quot;https://github.com/Crim&quot;&gt;Stephen Powis&lt;/a&gt;
      for their contributions to this release!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/03/26/debezium-0-9-3-final-released/</id>
    <title>Debezium 0.9.3.Final Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-03-26T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/03/26/debezium-0-9-3-final-released/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="sqlserver"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      The Debezium team is happy to announce the release of Debezium 0.9.3.Final!
      
      
      This is mostly a bug-fix release and a drop-in replacement for earlier Debezium 0.9.x versions, but there are few significant new features too.
      Overall, 17 issues were resolved.
      
      
      
      
      
      
      
      
      Container images will be released with a small delay due to some Docker Hub configuration issues.
      
      
      
      
      
      
      
      New Features
      
      
      The 0.9.3 release comes with two larger new features:
      
      
      
      
      A feature request was made to execute a partial recovery of the replication process after losing the replication slot with the PostgreSQL database, e.g. after failing over to a secondary database host (DBZ-1082).
      Instead of adding yet another snapshotting mode,...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium team is happy to announce the release of Debezium &lt;strong&gt;0.9.3.Final&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This is mostly a bug-fix release and a drop-in replacement for earlier Debezium 0.9.x versions, but there are few significant new features too.
      Overall, &lt;a href=&quot;https://issues.jboss.org/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.9.3.Final&quot;&gt;17 issues&lt;/a&gt; were resolved.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;admonitionblock note&quot;&gt;
      &lt;table&gt;
      &lt;tr&gt;
      &lt;td class=&quot;icon&quot;&gt;
      &lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot;&gt;&lt;/i&gt;
      &lt;/td&gt;
      &lt;td class=&quot;content&quot;&gt;
      Container images will be released with a small delay due to some Docker Hub configuration issues.
      &lt;/td&gt;
      &lt;/tr&gt;
      &lt;/table&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;new_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_features&quot;&gt;&lt;/a&gt;New Features&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The 0.9.3 release comes with two larger new features:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;A feature request was made to execute a partial recovery of the replication process after losing the replication slot with the PostgreSQL database, e.g. after failing over to a secondary database host (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1082&quot;&gt;DBZ-1082&lt;/a&gt;).
      Instead of adding yet another snapshotting mode, we took a step back and decided to make the Postgres snapshotting process more customizable by introducing a service provider interface (SPI). This lets you implement and register your own Java class for controlling the snaphotting process.
      See the issue description of DBZ-1082 for one possible custom implementation of this SPI, which is based on Postgres' &lt;code&gt;catalog_xmin&lt;/code&gt; property and selects all records altered after the last known xmin position.
      To learn more about the SPI, see the the &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/debezium-connector-postgres/src/main/java/io/debezium/connector/postgresql/spi/Snapshotter.java&quot;&gt;Snapshotter&lt;/a&gt; contract.
      Note that the feature is still in incubating phase and the SPI should be considered unstable for the time being.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Not long ago we published blogpost about implementing the &lt;a href=&quot;http://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/&quot;&gt;outbox&lt;/a&gt; pattern with Debezium for propagating data changes between microservices.
      Community member &lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt; expanded the idea and created a ready-made implementation of the single message transform (SMT) described in the post for routing events from the outbox table to specific topics.
      This SMT is part of the Debezium core library now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1169&quot;&gt;DBZ-1169&lt;/a&gt;).
      Its usage will be described in the documentation soon; for the time being please refer to the &lt;a href=&quot;https://github.com/debezium/debezium/tree/master/debezium-core/src/main/java/io/debezium/transforms/outbox/EventRouter.java&quot;&gt;EventRouter&lt;/a&gt; type and the accompanying configuration class.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;bug_fixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bug_fixes&quot;&gt;&lt;/a&gt;Bug fixes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We did a couple of fixes related to the &lt;a href=&quot;http://debezium.io/docs/connectors/postgres/&quot;&gt;Debezium Postgres connector&lt;/a&gt;:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;A regression that introduced a deadlock in snapshotting process has been fixed (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1161&quot;&gt;DBZ-1161&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The &lt;code&gt;hstore&lt;/code&gt; datatype works correctly in snapshot phase (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1162&quot;&gt;DBZ-1162&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The &lt;code&gt;wal2json&lt;/code&gt; plug-in processes also empty events (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1181&quot;&gt;DBZ-1181&lt;/a&gt;) as e.g. originating from materialize view updates; this should help to resolve some of the issues where log files in Postgres couldn’t be discarded due to Debezium’s replication slot not advancing.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The commit time is propely converted to microseconds (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1174&quot;&gt;DBZ-1174&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;Debezium MySQL connector&lt;/a&gt; saw a number of fixes especially in SQL parser:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;The &lt;code&gt;SERIAL&lt;/code&gt; datatype and default value is now supported (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1185&quot;&gt;DBZ-1185&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;A specific detail in the MySQL grammar that allows to enumerate table options in &lt;code&gt;ALTER TABLE&lt;/code&gt; without comma works (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1186&quot;&gt;DBZ-1186&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;A false alarm for empty MySQL password is no longer reported (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1188&quot;&gt;DBZ-1188&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;It is no longer necessary to create history topic manually for broker without default topic replication value (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1179&quot;&gt;DBZ-1179&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It is now possible to process multiple schemas with a single Oracle connector (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1166&quot;&gt;DBZ-1166&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Check out the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-9-3-final&quot;&gt;release notes&lt;/a&gt; for the complete list of issues fixed in Debezium 0.9.3.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Many thanks to Debezium community members &lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt;, &lt;a href=&quot;https://github.com/ShubhamRwt&quot;&gt;Shubham Rawat&lt;/a&gt;, &lt;a href=&quot;https://github.com/addisonj&quot;&gt;Addison Higham&lt;/a&gt;, &lt;a href=&quot;https://github.com/jcasstevens&quot;&gt;Jon Casstevens&lt;/a&gt;, &lt;a href=&quot;https://github.com/hashhar&quot;&gt;Ashar Hassan&lt;/a&gt; and &lt;a href=&quot;https://github.com/p5k6&quot;&gt;Josh Stanfield&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/03/14/debezium-meets-quarkus/</id>
    <title>Debezium meets Quarkus</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-03-14T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/03/14/debezium-meets-quarkus/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="quarkus"></category>
    <category term="examples"></category>
    <category term="microservices"></category>
    <category term="apache-kafka"></category>
    <summary>
      
      
      
      
      
      Last week&#8217;s announcement of Quarkus sparked a great amount of interest in the Java community:
      crafted from the best of breed Java libraries and standards, it allows to build Kubernetes-native applications based on GraalVM &amp; OpenJDK HotSpot.
      In this blog post we are going to demonstrate how a Quarkus-based microservice can consume Debezium&#8217;s data change events via Apache Kafka.
      For that purpose, we&#8217;ll see what it takes to convert the shipment microservice from our recent post about the outbox pattern into Quarkus-based service.
      
      
      
      
      Quarkus is a Java stack designed for the development of cloud-native applications based on the Java platform.
      It combines and tightly integrates...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;openblock teaser&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Last week’s announcement of &lt;a href=&quot;https://quarkus.io/&quot;&gt;Quarkus&lt;/a&gt; sparked a great amount of interest in the Java community:
      crafted from the best of breed Java libraries and standards, it allows to build Kubernetes-native applications based on GraalVM &amp;amp; OpenJDK HotSpot.
      In this blog post we are going to demonstrate how a Quarkus-based microservice can consume Debezium’s data change events via Apache Kafka.
      For that purpose, we’ll see what it takes to convert the shipment microservice from our recent post about the &lt;a href=&quot;2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern&quot;&gt;outbox pattern&lt;/a&gt; into Quarkus-based service.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Quarkus is a Java stack designed for the development of cloud-native applications based on the Java platform.
      It combines and tightly integrates mature libraries such Hibernate ORM, Vert.x, Netty, RESTEasy and Apache Camel as well as the APIs from the &lt;a href=&quot;https://microprofile.io/&quot;&gt;Eclipse MicroProfile&lt;/a&gt; initiative,
      such as &lt;a href=&quot;https://github.com/eclipse/microprofile-config&quot;&gt;Config&lt;/a&gt; or &lt;a href=&quot;https://github.com/eclipse/microprofile-reactive-messaging&quot;&gt;Reactive Messaging&lt;/a&gt;.
      Using Quarkus, you can develop applications using both imperative and reactive styles, also combining both approaches as needed.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It is designed for significantly reduced memory consumption and improved startup time.
      Last but not least, Quarkus supports both OpenJDK HotSpot and GraalVM virtual machines.
      With GraalVM it is possible to compile the application into a native binary and thus reduce the resource consumption and startup time even more.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To learn more about Quarkus itself, we recommend to take a look at its excellent &lt;a href=&quot;https://quarkus.io/get-started/&quot;&gt;Getting Started&lt;/a&gt; guide.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;consuming_kafka_messages_with_quarkus&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#consuming_kafka_messages_with_quarkus&quot;&gt;&lt;/a&gt;Consuming Kafka Messages with Quarkus&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In the original &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/outbox&quot;&gt;example application&lt;/a&gt; demonstrating the outbox pattern,
      there was a microservice (&quot;shipment&quot;) based on Thorntail that consumed the events produced by the Debezium connector.
      We’ve extended the example with a new service named &quot;shipment-service-quarkus&quot;.
      It provides the same functionality as the &quot;shipment-service&quot; but is implemented as a microservice based on Quarkus instead of Thorntail.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This makes the overall architecture look like so:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/outbox_pattern_quarkus.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Outbox Pattern Overview&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To retrofit the original service into a Quarkus-based application, only a few changes were needed:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Quarkus right now supports only MariaDB but not MySQL; hence we have included an instance of MariaDB to which the service is writing&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The &lt;a href=&quot;https://javaee.github.io/jsonp/&quot;&gt;JSON-P API&lt;/a&gt; used do deserialize incoming JSON messages can currently not be used without RESTEasy (see &lt;a href=&quot;https://github.com/quarkusio/quarkus/issues/1480&quot;&gt;issue #1480&lt;/a&gt;, which should be fixed soon); so the code has been modified to use the Jackson API instead&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Instead of the Kafka consumer API, the &lt;a href=&quot;https://github.com/eclipse/microprofile-reactive-messaging&quot;&gt;Reactive Messaging API&lt;/a&gt; defined by MicroProfile is used to receive messages from Apache Kafka; as an implementation of that API, the one provided by the &lt;a href=&quot;https://github.com/smallrye/smallrye-reactive-messaging&quot;&gt;SmallRye project&lt;/a&gt; is used, which is bundled as a Quarkus extension&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While the first two steps are mere technicalities,
      the Reactive Messaging API is a nice simplification over the polling loop in the original consumer.
      All that’s needed to consume messages from a Kafka topic is to annotate a method with &lt;code&gt;@Incoming&lt;/code&gt;,
      and it will automatically be invoked when a new message arrives:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
      public class KafkaEventConsumer {
      
          @Incoming(&quot;orders&quot;)
          public CompletionStage&amp;lt;Void&amp;gt; onMessage(KafkaMessage&amp;lt;String, String&amp;gt; message)
                  throws IOException {
              // handle message...
      
              return message.ack();
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &quot;orders&quot; message source is configured via the MicroProfile Config API,
      which resolves it to the &quot;OrderEvents&quot; topic already known from the original outbox example.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;build_process&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#build_process&quot;&gt;&lt;/a&gt;Build Process&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The build process is mostly the same as it was before.
      Instead of using the Thorntail Maven plug-in, the Quarkus Maven plug-in is used now.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The following Quarkus extensions are used:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;em&gt;io.quarkus:quarkus-hibernate-orm&lt;/em&gt;: support for Hibernate ORM and JPA&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;em&gt;io.quarkus:quarkus-jdbc-mariadb&lt;/em&gt;: support for accessing MariaDB through JDBC&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;em&gt;io.quarkus:quarkus-smallrye-reactive-messaging-kafka&lt;/em&gt;: support for accessing Kafka through the MicroProfile Reactive Messaging API&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;They pull in some other extensions too, e.g. &lt;em&gt;quarkus-arc&lt;/em&gt; (the Quarkus CDI runtime) and &lt;em&gt;quarkus-vertx&lt;/em&gt; (used by the reactive messaging support).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In addition, two more changes were needed:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;A new build profile named &lt;code&gt;native&lt;/code&gt; has been added; this is used to compile the service into a native binary image using the Quarkus Maven plug-in&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;the &lt;code&gt;native-image.docker-build&lt;/code&gt; system property is enabled when running the build; this means that the native image build is done inside of a Docker container, so that GraalVM doesn’t have to be installed on the developer’s machine&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;All the heavy-lifting is done by the Quarkus Maven plug-in which is configured in &lt;em&gt;pom.xml&lt;/em&gt; like so:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;  &amp;lt;build&amp;gt;
          &amp;lt;finalName&amp;gt;shipment&amp;lt;/finalName&amp;gt;
          &amp;lt;plugins&amp;gt;
            ...
            &amp;lt;plugin&amp;gt;
              &amp;lt;groupId&amp;gt;io.quarkus&amp;lt;/groupId&amp;gt;
              &amp;lt;artifactId&amp;gt;quarkus-maven-plugin&amp;lt;/artifactId&amp;gt;
              &amp;lt;version&amp;gt;${version.quarkus}&amp;lt;/version&amp;gt;
              &amp;lt;executions&amp;gt;
                &amp;lt;execution&amp;gt;
                  &amp;lt;goals&amp;gt;
                    &amp;lt;goal&amp;gt;build&amp;lt;/goal&amp;gt;
                  &amp;lt;/goals&amp;gt;
                &amp;lt;/execution&amp;gt;
              &amp;lt;/executions&amp;gt;
            &amp;lt;/plugin&amp;gt;
          &amp;lt;/plugins&amp;gt;
        &amp;lt;/build&amp;gt;
        ...
          &amp;lt;profile&amp;gt;
            &amp;lt;id&amp;gt;native&amp;lt;/id&amp;gt;
            &amp;lt;build&amp;gt;
              &amp;lt;plugins&amp;gt;
                &amp;lt;plugin&amp;gt;
                  &amp;lt;groupId&amp;gt;io.quarkus&amp;lt;/groupId&amp;gt;
                  &amp;lt;artifactId&amp;gt;quarkus-maven-plugin&amp;lt;/artifactId&amp;gt;
                  &amp;lt;version&amp;gt;${version.quarkus}&amp;lt;/version&amp;gt;
                  &amp;lt;executions&amp;gt;
                    &amp;lt;execution&amp;gt;
                      &amp;lt;goals&amp;gt;
                        &amp;lt;goal&amp;gt;native-image&amp;lt;/goal&amp;gt;
                      &amp;lt;/goals&amp;gt;
                      &amp;lt;configuration&amp;gt;
                        &amp;lt;enableHttpUrlHandler&amp;gt;true&amp;lt;/enableHttpUrlHandler&amp;gt;
                        &amp;lt;autoServiceLoaderRegistration&amp;gt;false&amp;lt;/autoServiceLoaderRegistration&amp;gt;
                      &amp;lt;/configuration&amp;gt;
                    &amp;lt;/execution&amp;gt;
                  &amp;lt;/executions&amp;gt;
                &amp;lt;/plugin&amp;gt;
              &amp;lt;/plugins&amp;gt;
            &amp;lt;/build&amp;gt;
          &amp;lt;/profile&amp;gt;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;configuration&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#configuration&quot;&gt;&lt;/a&gt;Configuration&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As any Quarkus application, the shipment service is configured via the &lt;em&gt;application.properties&lt;/em&gt; file:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;quarkus.datasource.url: jdbc:mariadb://shipment-db-quarkus:3306/shipmentdb
      quarkus.datasource.driver: org.mariadb.jdbc.Driver
      quarkus.datasource.username: mariadbuser
      quarkus.datasource.password: mariadbpw
      quarkus.hibernate-orm.database.generation=drop-and-create
      quarkus.hibernate-orm.log.sql=true
      
      smallrye.messaging.source.orders.type=io.smallrye.reactive.messaging.kafka.Kafka
      smallrye.messaging.source.orders.topic=OrderEvents
      smallrye.messaging.source.orders.bootstrap.servers=kafka:9092
      smallrye.messaging.source.orders.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
      smallrye.messaging.source.orders.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
      smallrye.messaging.source.orders.group.id=shipment-service-quarkus&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In our case it contains&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;the definition of a datasource (based on MariaDB) to which the shipment service writes its data,&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;the definition of a messaging source, which is backed by the &quot;OrderEvents&quot; Kafka topic, using the given bootstrap server, deserializers and Kafka consumer group id.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;execution&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#execution&quot;&gt;&lt;/a&gt;Execution&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Docker Compose config file has been enriched with two services, MariaDB and the new Quarkus-based shipment service.
      So when &lt;code&gt;docker-compose up&lt;/code&gt; is executed, two shipment services are started side-by-side: the original Thorntail-based one and the new one using Quarkus.
      When the order services receives a new purchase order and exports a corresponding event to Apache Kafka via the outbox table,
      that message is processed by both shipment services, as they are using distinct consumer group ids.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;performance_numbers&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#performance_numbers&quot;&gt;&lt;/a&gt;Performance Numbers&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The numbers are definitely not scientific, but provide a good indication of the order-of-magnitude difference between the native Quarkus-based application and the Thorntail service running on the JVM:&lt;/p&gt;
      &lt;/div&gt;
      &lt;table class=&quot;tableblock frame-all grid-all spread table table-bordered table-striped&quot;&gt;
      &lt;colgroup&gt;
      &lt;col style=&quot;width: 30%;&quot; /&gt;
      &lt;col style=&quot;width: 35%;&quot; /&gt;
      &lt;col style=&quot;width: 35%;&quot; /&gt;
      &lt;/colgroup&gt;
      &lt;thead&gt;
      &lt;tr&gt;
      &lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;/th&gt;
      &lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;Quarkus service&lt;/th&gt;
      &lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;Thorntail service&lt;/th&gt;
      &lt;/tr&gt;
      &lt;/thead&gt;
      &lt;tfoot&gt;
      &lt;tr&gt;
      &lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div&gt;&lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;application package size [MB]&lt;/p&gt;
      &lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
      &lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div&gt;&lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;54&lt;/p&gt;
      &lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
      &lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div&gt;&lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;131&lt;/p&gt;
      &lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;/tfoot&gt;
      &lt;tbody&gt;
      &lt;tr&gt;
      &lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div&gt;&lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;memory [MB]&lt;/p&gt;
      &lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
      &lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div&gt;&lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;33.8&lt;/p&gt;
      &lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
      &lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div&gt;&lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;1257&lt;/p&gt;
      &lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
      &lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div&gt;&lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;start time [ms]&lt;/p&gt;
      &lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
      &lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div&gt;&lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;260&lt;/p&gt;
      &lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
      &lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div&gt;&lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;5746&lt;/p&gt;
      &lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;/tbody&gt;
      &lt;/table&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The memory data were obtained via &lt;code&gt;htop&lt;/code&gt; utility.
      The startup time was measured till the message about application readiness was printed.
      As with all performance measurements, you should run your own comparisons based on your set-up and workload to gain insight into the actual differences for your specific use cases.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot;&gt;&lt;/a&gt;Summary&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In this post we have successfully demonstrated that it is possible to consume Debezium-generated events in a Java application written with the Quarkus Java stack.
      We have also shown that it is possible to provide such application as a binary image and provided back-of-the-envelope performance numbers demonstrating significant savings in resources.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to see the awesomeness of deploying Java microservices as native images by yourself,
      you can find the complete &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/outbox/shipment-service-quarkus&quot;&gt;source code&lt;/a&gt; of the implementation in the Debezium examples repo.
      If you got any questions or feedback, please let us know in the comments below;
      looking forward to hearing from you!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;em&gt;Many thanks to Guillaume Smet for reviewing an earlier version of this post!&lt;/em&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/02/25/debezium-0-9-2-final-released/</id>
    <title>Debezium 0.9.2.Final Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-02-25T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/02/25/debezium-0-9-2-final-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="sqlserver"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      The Debezium team is happy to announce the release of Debezium 0.9.2.Final!
      
      
      This is mostly a bug-fix release and a drop-in replacement for earlier Debezium 0.9.x versions.
      Overall, 18 issues were resolved.
      
      
      A couple of fixes relate to the Debezium Postgres connector:
      
      
      
      
      When not using REPLICA IDENTITY FULL, certain data types could trigger exceptions for update or delete events; those are fixed now
      (DBZ-1141, DBZ-1149)
      
      
      The connector won&#8217;t fail any longer when encountering a change to a row with an unaltered TOAST column value
      (DBZ-1146)
      
      
      
      
      Also the Debezium MySQL connector saw a number of fixes:
      
      
      
      
      The connector works correctly now when using GTIDs and ANSI_QUOTES SQL mode (DBZ-1147)
      
      
      The new...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium team is happy to announce the release of Debezium &lt;strong&gt;0.9.2.Final&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This is mostly a bug-fix release and a drop-in replacement for earlier Debezium 0.9.x versions.
      Overall, &lt;a href=&quot;https://issues.jboss.org/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.9.2.Final&quot;&gt;18 issues&lt;/a&gt; were resolved.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A couple of fixes relate to the &lt;a href=&quot;http://debezium.io/docs/connectors/postgres/&quot;&gt;Debezium Postgres connector&lt;/a&gt;:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;When not using &lt;code&gt;REPLICA IDENTITY FULL&lt;/code&gt;, certain data types could trigger exceptions for update or delete events; those are fixed now
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1141&quot;&gt;DBZ-1141&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1149&quot;&gt;DBZ-1149&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The connector won’t fail any longer when encountering a change to a row with an unaltered TOAST column value
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1146&quot;&gt;DBZ-1146&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;Debezium MySQL connector&lt;/a&gt; saw a number of fixes:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;The connector works correctly now when using GTIDs and &lt;code&gt;ANSI_QUOTES&lt;/code&gt; SQL mode (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1147&quot;&gt;DBZ-1147&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The new Antlr-based DDL parsers can handle column names that are key words such as &lt;code&gt;MEDIUM&lt;/code&gt; (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1150&quot;&gt;DBZ-1150&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;TIME&lt;/code&gt; columns with a default value larger than 23:59:59 can be exported now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1137&quot;&gt;DBZ-1137&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Another important fix was done in the &lt;a href=&quot;http://debezium.io/docs/connectors/sqlserver/&quot;&gt;Debezium connector for SQL Server&lt;/a&gt;,
      where the connector archive deployed to Maven Central accidentally contained all test-scoped and provided-scoped dependencies.
      This has been resolved now, so the connector archive only contains the actually needed JARs and thus is much smaller (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1138&quot;&gt;DBZ-1138&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;new_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_features&quot;&gt;&lt;/a&gt;New Features&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The 0.9.2 release also comes with two small new features:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;You can pass arbitrary parameters to the logical decoding plug-in used by the Postgres connector;
      this can for instance be used with wal2json to limit the number of tables to capture on the server side
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1130&quot;&gt;DBZ-1130&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The MongoDB connector now has the long-awaited snapshotting mode &lt;code&gt;NEVER&lt;/code&gt; (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-867&quot;&gt;DBZ-867&lt;/a&gt;),
      i.e. you can set up a new connector without taking an initial snapshot and instantly beginning streaming changes from the oplog&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;version_updates&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#version_updates&quot;&gt;&lt;/a&gt;Version Updates&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As of this release, Debezium has been upgraded to Apache Kafka 2.1.1.
      Amongst others, this release fixes an issue where the Kafka Connect REST API would expose connector credentials also when those were configured via secrets (&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5117&quot;&gt;KAFKA-5117&lt;/a&gt;).
      We’ve also upgraded the binlog client used by the MySQL connector to version 0.19.0 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1140&quot;&gt;DBZ-1140&lt;/a&gt;),
      which fixes a bug that had caused exceptions during rebalancing the connector before (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1132&quot;&gt;DBZ-1132&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Check out the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-9-2-final&quot;&gt;release notes&lt;/a&gt; for the complete list of issues fixed in Debezium 0.9.2.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Many thanks to Debezium community members &lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;, &lt;a href=&quot;https://github.com/kbarber2&quot;&gt;Keith Barber&lt;/a&gt;, &lt;a href=&quot;https://github.com/krizhan&quot;&gt;Krizhan Mariampillai&lt;/a&gt; and &lt;a href=&quot;https://github.com/taylor-rolison&quot;&gt;Taylor Rolison&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/</id>
    <title>Reliable Microservices Data Exchange With the Outbox Pattern</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-02-19T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="discussion"></category>
    <category term="examples"></category>
    <category term="microservices"></category>
    <category term="apache-kafka"></category>
    <summary>
      
      
      
      
      
      As part of their business logic, microservices often do not only have to update their own local data store,
      but they also need to notify other services about data changes that happened.
      The outbox pattern describes an approach for letting services execute these two tasks in a safe and consistent manner;
      it provides source services with instant "read your own writes" semantics,
      while offering reliable, eventually consistent data exchange across service boundaries.
      
      
      
      
      If you&#8217;ve built a couple of microservices,
      you&#8217;ll probably agree that the hardest part about them is data:
      microservices don&#8217;t exist in isolation and very often they need to propagate data and data changes amongst...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;openblock teaser&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As part of their business logic, microservices often do not only have to update their own local data store,
      but they also need to notify other services about data changes that happened.
      The outbox pattern describes an approach for letting services execute these two tasks in a safe and consistent manner;
      it provides source services with instant &quot;read your own writes&quot; semantics,
      while offering reliable, eventually consistent data exchange across service boundaries.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’ve built a couple of microservices,
      you’ll probably agree that the &lt;a href=&quot;https://blog.christianposta.com/microservices/the-hardest-part-about-microservices-data/&quot;&gt;hardest part about them is data&lt;/a&gt;:
      microservices don’t exist in isolation and very often they need to propagate data and data changes amongst each other.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For instance consider a microservice that manages purchase orders:
      when a new order is placed, information about that order may have to be relayed to a shipment service
      (so it can assemble shipments of one or more orders) and a customer service
      (so it can update things like the customer’s total credit balance based on the new order).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;There are different approaches for letting the order service know the other two about new purchase orders;
      e.g. it could invoke some &lt;a href=&quot;https://en.wikipedia.org/wiki/Representational_state_transfer&quot;&gt;REST&lt;/a&gt;, &lt;a href=&quot;https://grpc.io/&quot;&gt;grpc&lt;/a&gt; or other (synchronous) API provided by these services.
      This might create some undesired coupling, though: the sending service must know which other services to invoke and where to find them.
      It also must be prepared for these services temporarily not being available.
      Service meshes such as &lt;a href=&quot;https://istio.io/&quot;&gt;Istio&lt;/a&gt; can come in helpful here, by providing capabilities like request routing, retries, circuit breakers and much more.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The general issue of any synchronous approach is that one service cannot really function without the other services which it invokes.
      While buffering and retrying might help in cases where other services only need to be &lt;em&gt;notified&lt;/em&gt; of certain events,
      this is not the case if a service actually needs to &lt;em&gt;query&lt;/em&gt; other services for information.
      For instance, when a purchase order is placed, the order service might need to obtain the information how many times the purchased item is on stock from an inventory service.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Another downside of such a synchronous approach is that it lacks re-playability,
      i.e. the possibility for new consumers to arrive after events have been sent and still be able to consume the entire event stream from the beginning.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Both problems can be addressed by using an asynchronous data exchange approach instead:
      i.e having the order, inventory and other services propagate events through a durable message log such as &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Apache Kafka&lt;/a&gt;.
      By subscribing to these event streams, each service will be notified about the data change of other services.
      It can react to these events and, if needed, create a local representation of that data in its own data store,
      using a representation tailored towards its own needs.
      For instance, such view might be denormalized to efficiently support specific access patterns, or it may only contain a subset of the original data that’s relevant to the consuming service.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Durable logs also support re-playability,
      i.e. new consumers can be added as needed, enabling use cases you might not have had in mind originally,
      and without touching the source service.
      E.g. consider a data warehouse which should keep information about all the orders ever placed, or some full-text search functionality on purchase orders based on &lt;a href=&quot;https://www.elastic.co/products/elasticsearch&quot;&gt;Elasticsearch&lt;/a&gt;.
      Once the purchase order events are in a Kafka topic
      (Kafka’s topic’s retention policy settings can be used to ensure that events remain in a topic as long as its needed for the given use cases and business requirements),
      new consumers can subscribe, process the topic from the very beginning and materialize a view of all the data in a microservice’s database, search index, data warehouse etc.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;admonitionblock note&quot;&gt;
      &lt;table&gt;
      &lt;tr&gt;
      &lt;td class=&quot;icon&quot;&gt;
      &lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot;&gt;&lt;/i&gt;
      &lt;/td&gt;
      &lt;td class=&quot;content&quot;&gt;
      &lt;div class=&quot;title&quot;&gt;Dealing with Topic Growth&lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Depending on the amount of data (number and size of records, frequency of changes),
      it may or may not be feasible to keep events in topics for a long or even indefinite time.
      Very often, some or even all events pertaining to a given data item
      (e.g. a specific purchase order) might be eligible for deletion from a business point of view after a given point in time.
      See the box &quot;Deletion of Events from Kafka Topics&quot; further below for some more thoughts on the deletion of events from Kafka topics in order to keep their size within bounds.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/td&gt;
      &lt;/tr&gt;
      &lt;/table&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;the_issue_of_dual_writes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_issue_of_dual_writes&quot;&gt;&lt;/a&gt;The Issue of Dual Writes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In order to provide their functionality, microservices will typically have their own local data store.
      For instance, the order service may use a relational database to persist the information about purchase orders.
      When a new order is placed, this may result in an &lt;code&gt;INSERT&lt;/code&gt; operation in a table &lt;code&gt;PurchaseOrder&lt;/code&gt; in the service’s database.
      At the same time, the service may wish to send an event about the new order to Apache Kafka,
      so to propagate that information to other interested services.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Simply issuing these two requests may lead to potential inconsistencies, though.
      The reason being that we cannot have one shared transaction that would span the service’s database as well as Apache Kafka,
      as the latter doesn’t support to be enlisted in distributed (XA) transactions.
      So in unfortunate circumstances it might happen that we end up with having the new purchase order persisted in the local database,
      but not having sent the corresponding message to Kafka
      (e.g. due to some networking issue).
      Or, the other way around, we might have sent the message to Kafka but failed to persist the purchase order in the local database.
      Both situations are undesirable;
      this may cause no shipment to be created for a seemingly successfully placed order.
      Or a shipment gets created, but then there’d be no trace about the corresponding purchase order in the order service itself.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So how can this situation be avoided?
      The answer is to only modify &lt;em&gt;one&lt;/em&gt; of the two resources (the database &lt;em&gt;or&lt;/em&gt; Apache Kafka) and drive the update of the second one based on that, in an eventually consistent manner.
      Let’s first consider the case of only writing to Apache Kafka.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When receiving a new purchase order, the order service would not do the &lt;code&gt;INSERT&lt;/code&gt; into its database synchronously;
      instead, it would only send an event describing the new order to a Kafka topic.
      So only one resource gets modified at a time, and if something goes wrong with that,
      we’ll find out about it instantly and report back to the caller of the order service that the request failed.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;At the same time, the service itself would subscribe to that Kafka topic.
      That way, it will be notified when a new message arrives in the topic and it can persist the new purchase order in its database.
      There’s one subtle challenge here, though, and that is the lack of &quot;read your own write&quot; semantics.
      E.g. let’s assume the order service also has an API for searching for all the purchase orders of a given customer.
      When invoking that API right after placing a new order, due to the asynchronous nature of processing messages from the Kafka topic,
      it might happen that the purchase order has not yet been persisted in the service’s database and thus will not be returned by that query.
      That can lead to a very confusing user experience, as users for instance may miss newly placed orders in their shopping history.
      There are ways to deal with this situation, e.g. the service could keep newly placed purchase orders in memory and answer subsequent queries based on that.
      This gets quickly non-trivial though when implementing more complex queries or considering that the order service might also comprise multiple nodes in a clustered set-up,
      which would require propagation of that data within the cluster.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now how would things look like when only writing to the database synchronously and driving the export of a message to Apache Kafka based on that?
      This is where the outbox pattern comes in.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;the_outbox_pattern&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_outbox_pattern&quot;&gt;&lt;/a&gt;The Outbox Pattern&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The idea of this approach is to have an &quot;outbox&quot; table in the service’s database.
      When receiving a request for placing a purchase order, not only an &lt;code&gt;INSERT&lt;/code&gt; into the &lt;code&gt;PurchaseOrder&lt;/code&gt; table is done,
      but, as part of the same transaction,
      also a record representing the event to be sent is inserted into that outbox table.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The record describes an event that happened in the service,
      for instance it could be a JSON structure representing the fact that a new purchase order has been placed,
      comprising data on the order itself, its order lines as well as contextual information such as a use case identifier.
      By explicitly emitting events via records in the outbox table,
      it can be ensured that events are structured in a way suitable for external consumers.
      This also helps to make sure that event consumers won’t break
      when for instance altering the internal domain model or the &lt;code&gt;PurchaseOrder&lt;/code&gt; table.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;An asynchronous process monitors that table for new entries.
      If there are any, it propagates the events as messages to Apache Kafka.
      This gives us a very nice balance of characteristics:
      By synchronously writing to the &lt;code&gt;PurchaseOrder&lt;/code&gt; table, the source service benefits from &quot;read your own writes&quot; semantics.
      A subsequent query for purchase orders will return the newly persisted order, as soon as that first transaction has been committed.
      At the same time, we get reliable, asynchronous, eventually consistent data propagation to other services via Apache Kafka.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now, the outbox pattern isn’t actually a new idea.
      It has been in use for quite some time.
      In fact, even when using JMS-style message brokers, which actually could participate in distributed transactions,
      it can be a preferable option to avoid any coupling and potential impact by downtimes of remote resources such as a message broker.
      You can also find a description of the pattern on Chris Richardson’s excellent &lt;a href=&quot;https://microservices.io/patterns/data/application-events.html&quot;&gt;microservices.io&lt;/a&gt; site.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Nevertheless, the pattern gets much less attention than it deserves and it is especially useful in the context of microservices.
      As we’ll see, the outbox pattern can be implemented in a very elegant and efficient way using change data capture and Debezium.
      In the following, let’s explore how.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;an_implementation_based_on_change_data_capture&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#an_implementation_based_on_change_data_capture&quot;&gt;&lt;/a&gt;An Implementation Based on Change Data Capture&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://debezium.io/blog/2018/07/19/advantages-of-log-based-change-data-capture/&quot;&gt;Log-based Change Data Capture&lt;/a&gt; (CDC) is a great fit for capturing new entries in the outbox table and stream them to Apache Kafka.
      As opposed to any polling-based approach, event capture happens with a very low overhead in near-realtime.
      Debezium comes with &lt;a href=&quot;http://debezium.io/docs/connectors/&quot;&gt;CDC connectors&lt;/a&gt; for several databases such as MySQL, Postgres and SQL Server.
      The following example will use the &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql&quot;&gt;Debezium connector for Postgres&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You can find the complete &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/outbox&quot;&gt;source code of the example&lt;/a&gt; on GitHub.
      Refer to the &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/outbox/README.md&quot;&gt;README.md&lt;/a&gt; for details on building and running the example code.
      The example is centered around two microservices,
      &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/outbox/order-service&quot;&gt;order-service&lt;/a&gt; and &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/outbox/shipment-service&quot;&gt;shipment-service&lt;/a&gt;.
      Both are implemented in Java, using &lt;a href=&quot;http://cdi-spec.org/&quot;&gt;CDI&lt;/a&gt; as the component model and JPA/Hibernate for accessing their respective databases.
      The order service runs on &lt;a href=&quot;http://wildfly.org/&quot;&gt;WildFly&lt;/a&gt; and exposes a simple REST API for placing purchase orders and canceling specific order lines.
      It uses a Postgres database as its local data store.
      The shipment service is based on &lt;a href=&quot;http://thorntail.io/&quot;&gt;Thorntail&lt;/a&gt;; via Apache Kafka, it receives events exported by the order service and creates corresponding shipment entries in its own MySQL database.
      Debezium tails the transaction log (&quot;write-ahead log&quot;, WAL) of the order service’s Postgres database in order to capture any new events in the outbox table and propagates them to Apache Kafka.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The overall architecture of the solution can be seen in the following picture:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/outbox_pattern.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Outbox Pattern Overview&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note that the pattern is in no way tied to these specific implementation choices.
      It could equally well be realized using alternative technologies such as Spring Boot
      (e.g. leveraging Spring Data’s &lt;a href=&quot;https://docs.spring.io/spring-data/commons/docs/current/api/index.html?org/springframework/data/domain/DomainEvents.html&quot;&gt;support for domain events&lt;/a&gt;),
      plain JDBC or other programming languages than Java altogether.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now let’s take a closer look at some of the relevant components of the solution.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;the_outbox_table&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_outbox_table&quot;&gt;&lt;/a&gt;The Outbox Table&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;code&gt;outbox&lt;/code&gt; table resides in the database of the order service and has the following structure:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Column        |          Type          | Modifiers
      --------------+------------------------+-----------
      id            | uuid                   | not null
      aggregatetype | character varying(255) | not null
      aggregateid   | character varying(255) | not null
      type          | character varying(255) | not null
      payload       | jsonb                  | not null&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Its columns are these:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;id&lt;/code&gt;: unique id of each message; can be used by consumers to detect any duplicate events, e.g. when restarting to read messages after a failure.
      Generated when creating a new event.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;aggregatetype&lt;/code&gt;: the type of the &lt;em&gt;aggregate root&lt;/em&gt; to which a given event is related;
      the idea being, leaning on the same concept of domain-driven design,
      that exported events should refer to an aggregate
      (&lt;a href=&quot;https://martinfowler.com/bliki/DDD_Aggregate.html&quot;&gt;&quot;a cluster of domain objects that can be treated as a single unit&quot;&lt;/a&gt;),
      where the aggregate root provides the sole entry point for accessing any of the entities within the aggregate.
      This could for instance be &quot;purchase order&quot; or &quot;customer&quot;.&lt;/p&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This value will be used to route events to corresponding topics in Kafka,
      so there’d be a topic for all events related to purchase orders,
      one topic for all customer-related events etc.
      Note that also events pertaining to a child entity contained within one such aggregate should use that same type.
      So e.g. an event representing the cancelation of an individual order line
      (which is part of the purchase order aggregate)
      should also use the type of its aggregate root, &quot;order&quot;,
      ensuring that also this event will go into the &quot;order&quot; Kafka topic.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;aggregateid&lt;/code&gt;: the id of the aggregate root that is affected by a given event; this could for instance be the id of a purchase order or a customer id;
      Similar to the aggregate type, events pertaining to a sub-entity contained within an aggregate should use the id of the containing aggregate root,
      e.g. the purchase order id for an order line cancelation event.
      This id will be used as the key for Kafka messages later on.
      That way, all events pertaining to one aggregate root or any of its contained sub-entities will go into the same partition of that Kafka topic,
      which ensures that consumers of that topic will consume all the events related to one and the same aggregate in the exact order as they were produced.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;type&lt;/code&gt;: the type of event, e.g. &quot;Order Created&quot; or &quot;Order Line Canceled&quot;. Allows consumers to trigger suitable event handlers.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;payload&lt;/code&gt;: a JSON structure with the actual event contents, e.g. containing a purchase order, information about the purchaser, contained order lines, their price etc.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;sending_events_to_the_outbox&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#sending_events_to_the_outbox&quot;&gt;&lt;/a&gt;Sending Events to the Outbox&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In order to &quot;send&quot; events to the outbox, code in the order service could in general just do an &lt;code&gt;INSERT&lt;/code&gt; into the outbox table.
      However, it’s a good idea to go for a slightly more abstract API, allowing to adjust implementation details of the outbox later on more easily, if needed.
      &lt;a href=&quot;https://docs.jboss.org/weld/reference/latest/en-US/html/events.html&quot;&gt;CDI events&lt;/a&gt; come in very handy for this.
      They can be raised in the application code and will be processed &lt;em&gt;synchronously&lt;/em&gt; by the outbox event sender,
      which will do the required &lt;code&gt;INSERT&lt;/code&gt; into the outbox table.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;All outbox event types should implement the following contract, resembling the structure of the outbox table shown before:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public interface ExportedEvent {
      
          String getAggregateId();
          String getAggregateType();
          JsonNode getPayload();
          String getType();
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To produce such event, application code uses an injected &lt;code&gt;Event&lt;/code&gt; instance, as e.g. here in the &lt;code&gt;OrderService&lt;/code&gt; class:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
      public class OrderService {
      
          @PersistenceContext
          private EntityManager entityManager;
      
          @Inject
          private Event&amp;lt;ExportedEvent&amp;gt; event;
      
          @Transactional
          public PurchaseOrder addOrder(PurchaseOrder order) {
              order = entityManager.merge(order);
      
              event.fire(OrderCreatedEvent.of(order));
              event.fire(InvoiceCreatedEvent.of(order));
      
              return order;
          }
      
          @Transactional
          public PurchaseOrder updateOrderLine(long orderId, long orderLineId,
                  OrderLineStatus newStatus) {
              // ...
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In the &lt;code&gt;addOrder()&lt;/code&gt; method, the JPA entity manager is used to persist the incoming order in the database
      and the injected &lt;code&gt;event&lt;/code&gt; is used to fire a corresponding &lt;code&gt;OrderCreatedEvent&lt;/code&gt; and an &lt;code&gt;InvoiceCreatedEvent&lt;/code&gt;.
      Again, keep in mind that, despite the notion of &quot;event&quot;, these two things happen within one and the same transaction.
      i.e. within this transaction, three records will be inserted into the database:
      one in the table with purchase orders and two in the outbox table.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Actual event implementations are straight-forward;
      as an example, here’s the &lt;code&gt;OrderCreatedEvent&lt;/code&gt; class:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public class OrderCreatedEvent implements ExportedEvent {
      
          private static ObjectMapper mapper = new ObjectMapper();
      
          private final long id;
          private final JsonNode order;
      
          private OrderCreatedEvent(long id, JsonNode order) {
              this.id = id;
              this.order = order;
          }
      
          public static OrderCreatedEvent of(PurchaseOrder order) {
              ObjectNode asJson = mapper.createObjectNode()
                      .put(&quot;id&quot;, order.getId())
                      .put(&quot;customerId&quot;, order.getCustomerId())
                      .put(&quot;orderDate&quot;, order.getOrderDate().toString());
      
              ArrayNode items = asJson.putArray(&quot;lineItems&quot;);
      
              for (OrderLine orderLine : order.getLineItems()) {
              items.add(
                      mapper.createObjectNode()
                      .put(&quot;id&quot;, orderLine.getId())
                      .put(&quot;item&quot;, orderLine.getItem())
                      .put(&quot;quantity&quot;, orderLine.getQuantity())
                      .put(&quot;totalPrice&quot;, orderLine.getTotalPrice())
                      .put(&quot;status&quot;, orderLine.getStatus().name())
                  );
              }
      
              return new OrderCreatedEvent(order.getId(), asJson);
          }
      
          @Override
          public String getAggregateId() {
              return String.valueOf(id);
          }
      
          @Override
          public String getAggregateType() {
              return &quot;Order&quot;;
          }
      
          @Override
          public String getType() {
              return &quot;OrderCreated&quot;;
          }
      
          @Override
          public JsonNode getPayload() {
              return order;
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note how &lt;a href=&quot;https://github.com/FasterXML/jackson&quot;&gt;Jackson’s&lt;/a&gt; &lt;code&gt;ObjectMapper&lt;/code&gt; is used to create a JSON representation of the event’s payload.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now let’s take a look at the code that consumes any fired &lt;code&gt;ExportedEvent&lt;/code&gt; and does the corresponding write to the outbox table:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
      public class EventSender {
      
          @PersistenceContext
          private EntityManager entityManager;
      
          public void onExportedEvent(@Observes ExportedEvent event) {
              OutboxEvent outboxEvent = new OutboxEvent(
                      event.getAggregateType(),
                      event.getAggregateId(),
                      event.getType(),
                      event.getPayload()
              );
      
              entityManager.persist(outboxEvent);
              entityManager.remove(outboxEvent);
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s rather simple: for each event the CDI runtime will invoke the &lt;code&gt;onExportedEvent()&lt;/code&gt; method.
      An instance of the &lt;code&gt;OutboxEvent&lt;/code&gt; entity is persisted in the database — and removed right away!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This might be surprising at first.
      But it makes sense when remembering how log-based CDC works:
      it doesn’t examine the actual contents of the table in the database, but instead it tails the append-only transaction log.
      The calls to &lt;code&gt;persist()&lt;/code&gt; and &lt;code&gt;remove()&lt;/code&gt; will create an &lt;code&gt;INSERT&lt;/code&gt; and a &lt;code&gt;DELETE&lt;/code&gt; entry in the log once the transaction commits.
      After that, Debezium will process these events:
      for any &lt;code&gt;INSERT&lt;/code&gt;, a message with the event’s payload will be sent to Apache Kafka.
      &lt;code&gt;DELETE&lt;/code&gt; events on the other hand can be ignored,
      as the removal from the outbox table is a mere technicality that doesn’t require any propagation to the message broker.
      So we are able to capture the event added to the outbox table by means of CDC,
      but when looking at the contents of the table itself, it will always be empty.
      This means that no additional disk space is needed for the table
      (apart from the log file elements which will automatically be discarded at some point)
      and also no separate house-keeping process is required to stop it from growing indefinitely.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;registering_the_debezium_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#registering_the_debezium_connector&quot;&gt;&lt;/a&gt;Registering the Debezium Connector&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With the outbox implementation in place, it’s time to register the Debezium Postgres connector,
      so it can capture any new events in the outbox table and relay them to Apache Kafka.
      That can be done by POST-ing the following JSON request to the REST API of Kafka Connect:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
          &quot;name&quot;: &quot;outbox-connector&quot;,
          &quot;config&quot;: {
              &quot;connector.class&quot; : &quot;io.debezium.connector.postgresql.PostgresConnector&quot;,
              &quot;tasks.max&quot; : &quot;1&quot;,
              &quot;database.hostname&quot; : &quot;order-db&quot;,
              &quot;database.port&quot; : &quot;5432&quot;,
              &quot;database.user&quot; : &quot;postgresuser&quot;,
              &quot;database.password&quot; : &quot;postgrespw&quot;,
              &quot;database.dbname&quot; : &quot;orderdb&quot;,
              &quot;database.server.name&quot; : &quot;dbserver1&quot;,
              &quot;schema.whitelist&quot; : &quot;inventory&quot;,
              &quot;table.whitelist&quot; : &quot;inventory.outboxevent&quot;,
              &quot;tombstones.on.delete&quot; : &quot;false&quot;,
              &quot;transforms&quot; : &quot;router&quot;,
              &quot;transforms.router.type&quot; : &quot;io.debezium.examples.outbox.routingsmt.EventRouter&quot;
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This sets up an instance of &lt;code&gt;io.debezium.connector.postgresql.PostgresConnector&lt;/code&gt;,
      capturing changes from the specified Postgres instance.
      Note that by means of a table whitelist, only changes from the &lt;code&gt;outboxevent&lt;/code&gt; table are captured.
      It also applies a single message transform (SMT) named &lt;code&gt;EventRouter&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;admonitionblock note&quot;&gt;
      &lt;table&gt;
      &lt;tr&gt;
      &lt;td class=&quot;icon&quot;&gt;
      &lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot;&gt;&lt;/i&gt;
      &lt;/td&gt;
      &lt;td class=&quot;content&quot;&gt;
      &lt;div class=&quot;title&quot;&gt;Deletion of Events from Kafka Topics&lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;By setting the &lt;code&gt;tombstones.on.delete&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt;, no deletion markers (&quot;tombstones&quot;) will be emitted by the connector when an event record gets deleted from the outbox table.
      That makes sense, as the deletion from the outbox table shouldn’t affect the retention of events in the corresponding Kafka topics.
      Instead, a specific retention time for the event topics may be configured in Kafka,
      e.g. to retain all purchase order events for 30 days.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Alternatively, one could work with &lt;a href=&quot;https://kafka.apache.org/documentation/#compaction&quot;&gt;compacted topics&lt;/a&gt;.
      This would require some changes to the design of events in the outbox table:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;they must describe the entire aggregate;
      so for instance also an event representing the cancelation of a single order line should describe the complete current state of the containing purchase order;
      that way consumers will be able to obtain the entire state of the purchase order also when only seeing the last event pertaining to a given order, after log compaction ran.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;they must have one more &lt;code&gt;boolean&lt;/code&gt; attribute indicating whether a particular event represents the deletion of the event’s aggregate root.
      Such an event (e.g. of type &lt;code&gt;OrderDeleted&lt;/code&gt;) could then be used by the event routing SMT described in the next section to produce a deletion marker for that aggregate root.
      Log compaction would then remove all events pertaining to the given purchase order when its &lt;code&gt;OrderDeleted&lt;/code&gt; event has been written to the topic.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Naturally, when deleting events, the event stream will not be re-playable from its very beginning any longer.
      Depending on the specific business requirements, it might be sufficient to just keep the final state of a given purchase order, customer etc.
      This could be achieved using compacted topics and a sufficiently value for the topic’s &lt;code&gt;delete.retention.ms&lt;/code&gt; setting.
      Another option could be to move historic events to some sort of cold storage (e.g. an Amazon S3 bucket),
      from where they can be retrieved if needed, followed by reading the latest events from the Kafka topics.
      Which approach to follow depends on the specific requirements, expected amount of data and expertise in the team developing and operating the solution.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/td&gt;
      &lt;/tr&gt;
      &lt;/table&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;topic_routing&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#topic_routing&quot;&gt;&lt;/a&gt;Topic Routing&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;By default, the Debezium connectors will send all change events originating from one given table to the same topic,
      i.e. we’d end up with a single Kafka topic named &lt;code&gt;dbserver1.inventory.outboxevent&lt;/code&gt; which would contain all events,
      be it order events, customer events etc.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To simplify the implementation of consumers which are only interested in specific event types it makes more sense, though,
      to have multiple topics, e.g. &lt;code&gt;OrderEvents&lt;/code&gt;, &lt;code&gt;CustomerEvents&lt;/code&gt; and so on.
      For instance the shipment service might not be interested in any customer events.
      By only subscribing to the &lt;code&gt;OrderEvents&lt;/code&gt; topic, it will be sure to never receive any customer events.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In order to route the change events captured from the outbox table to different topics, that custom SMT &lt;code&gt;EventRouter&lt;/code&gt; is used.
      Here is the code of its &lt;code&gt;apply()&lt;/code&gt; method, which will be invoked by Kafka Connect for each record emitted by the Debezium connector:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Override
      public R apply(R record) {
          // Ignoring tombstones just in case
          if (record.value() == null) {
              return record;
          }
      
          Struct struct = (Struct) record.value();
          String op = struct.getString(&quot;op&quot;);
      
          // ignoring deletions in the outbox table
          if (op.equals(&quot;d&quot;)) {
              return null;
          }
          else if (op.equals(&quot;c&quot;)) {
              Long timestamp = struct.getInt64(&quot;ts_ms&quot;);
              Struct after = struct.getStruct(&quot;after&quot;);
      
              String key = after.getString(&quot;aggregateid&quot;);
              String topic = after.getString(&quot;aggregatetype&quot;) + &quot;Events&quot;;
      
              String eventId = after.getString(&quot;id&quot;);
              String eventType = after.getString(&quot;type&quot;);
              String payload = after.getString(&quot;payload&quot;);
      
              Schema valueSchema = SchemaBuilder.struct()
                  .field(&quot;eventType&quot;, after.schema().field(&quot;type&quot;).schema())
                  .field(&quot;ts_ms&quot;, struct.schema().field(&quot;ts_ms&quot;).schema())
                  .field(&quot;payload&quot;, after.schema().field(&quot;payload&quot;).schema())
                  .build();
      
              Struct value = new Struct(valueSchema)
                  .put(&quot;eventType&quot;, eventType)
                  .put(&quot;ts_ms&quot;, timestamp)
                  .put(&quot;payload&quot;, payload);
      
              Headers headers = record.headers();
              headers.addString(&quot;eventId&quot;, eventId);
      
              return record.newRecord(topic, null, Schema.STRING_SCHEMA, key, valueSchema, value,
                      record.timestamp(), headers);
          }
          // not expecting update events, as the outbox table is &quot;append only&quot;,
          // i.e. event records will never be updated
          else {
              throw new IllegalArgumentException(&quot;Record of unexpected op type: &quot; + record);
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When receiving a delete event (&lt;code&gt;op&lt;/code&gt; = &lt;code&gt;d&lt;/code&gt;), it will discard that event,
      as that deletion of event records from the outbox table is not relevant to downstream consumers.
      Things get more interesting, when receiving a create event (&lt;code&gt;op&lt;/code&gt; = &lt;code&gt;c&lt;/code&gt;).
      Such record will be propagated to Apache Kafka.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium’s change events have a complex structure, that contain the old (&lt;code&gt;before&lt;/code&gt;) and new (&lt;code&gt;after&lt;/code&gt;) state of the represented row.
      The event structure to propagate is obtained from the &lt;code&gt;after&lt;/code&gt; state.
      The &lt;code&gt;aggregatetype&lt;/code&gt; value from the captured event record is used to build the name of the topic to send the event to.
      For instance, events with &lt;code&gt;aggregatetype&lt;/code&gt; set to &lt;code&gt;Order&lt;/code&gt; will be sent to the &lt;code&gt;OrderEvents&lt;/code&gt; topic.
      &lt;code&gt;aggregateid&lt;/code&gt; is used as the message key, making sure all messages of that aggregate will go into the same partition of that topic.
      The message value is a structure comprising the original event payload (encoded as JSON),
      the timestamp indicating when the event was produced and the event type.
      Finally, the event UUID is propagated as a Kafka header field.
      This allows for efficient duplicate detection by consumers, without having to examine the actual message contents.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;events_in_apache_kafka&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#events_in_apache_kafka&quot;&gt;&lt;/a&gt;Events in Apache Kafka&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now let’s take a look into the &lt;code&gt;OrderEvents&lt;/code&gt; and &lt;code&gt;CustomerEvents&lt;/code&gt; topics.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you have checked out the example sources and started all the components via Docker Compose
      (see the &lt;em&gt;README.md&lt;/em&gt; file in the example project for more details),
      you can place purchase orders via the order service’s REST API like so:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cat resources/data/create-order-request.json | http POST http://localhost:8080/order-service/rest/orders&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Similarly, specific order lines can be canceled:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cat resources/data/cancel-order-line-request.json | http PUT http://localhost:8080/order-service/rest/orders/1/lines/2&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When using a tool such as the very practical &lt;a href=&quot;https://github.com/edenhill/kafkacat&quot;&gt;kafkacat&lt;/a&gt; utility,
      you should now see messages like these in the &lt;code&gt;OrderEvents&lt;/code&gt; topic:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kafkacat -b kafka:9092 -C -o beginning -f 'Headers: %h\nKey: %k\nValue: %s\n' -q -t OrderEvents&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Headers: eventId=d03dfb18-8af8-464d-890b-09eb8b2dbbdd
      Key: &quot;4&quot;
      Value: {&quot;eventType&quot;:&quot;OrderCreated&quot;,&quot;ts_ms&quot;:1550307598558,&quot;payload&quot;:&quot;{\&quot;id\&quot;: 4, \&quot;lineItems\&quot;: [{\&quot;id\&quot;: 7, \&quot;item\&quot;: \&quot;Debezium in Action\&quot;, \&quot;status\&quot;: \&quot;ENTERED\&quot;, \&quot;quantity\&quot;: 2, \&quot;totalPrice\&quot;: 39.98}, {\&quot;id\&quot;: 8, \&quot;item\&quot;: \&quot;Debezium for Dummies\&quot;, \&quot;status\&quot;: \&quot;ENTERED\&quot;, \&quot;quantity\&quot;: 1, \&quot;totalPrice\&quot;: 29.99}], \&quot;orderDate\&quot;: \&quot;2019-01-31T12:13:01\&quot;, \&quot;customerId\&quot;: 123}&quot;}
      Headers: eventId=49f89ea0-b344-421f-b66f-c635d212f72c
      Key: &quot;4&quot;
      Value: {&quot;eventType&quot;:&quot;OrderLineUpdated&quot;,&quot;ts_ms&quot;:1550308226963,&quot;payload&quot;:&quot;{\&quot;orderId\&quot;: 4, \&quot;newStatus\&quot;: \&quot;CANCELLED\&quot;, \&quot;oldStatus\&quot;: \&quot;ENTERED\&quot;, \&quot;orderLineId\&quot;: 7}&quot;}&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;code&gt;payload&lt;/code&gt; field with the message values is the string-ified JSON representation of the original events.
      The Debezium Postgres connector emits &lt;code&gt;JSONB&lt;/code&gt; columns as a string
      (using the &lt;code&gt;io.debezium.data.Json&lt;/code&gt; logical type name),
      which is why the quotes are escaped.
      The &lt;a href=&quot;https://stedolan.github.io/jq/&quot;&gt;jq&lt;/a&gt; utility, and more specifically,
      its &lt;code&gt;fromjson&lt;/code&gt; operator, come in handy for displaying the event payload in a more readable way:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kafkacat -b kafka:9092 -C -o beginning -t Order | jq '.payload | fromjson'&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
        &quot;id&quot;: 4,
        &quot;lineItems&quot;: [
          {
            &quot;id&quot;: 7,
            &quot;item&quot;: &quot;Debezium in Action&quot;,
            &quot;status&quot;: &quot;ENTERED&quot;,
            &quot;quantity&quot;: 2,
            &quot;totalPrice&quot;: 39.98
          },
          {
            &quot;id&quot;: 8,
            &quot;item&quot;: &quot;Debezium for Dummies&quot;,
            &quot;status&quot;: &quot;ENTERED&quot;,
            &quot;quantity&quot;: 1,
            &quot;totalPrice&quot;: 29.99
          }
        ],
        &quot;orderDate&quot;: &quot;2019-01-31T12:13:01&quot;,
        &quot;customerId&quot;: 123
      }
      {
        &quot;orderId&quot;: 4,
        &quot;newStatus&quot;: &quot;CANCELLED&quot;,
        &quot;oldStatus&quot;: &quot;ENTERED&quot;,
        &quot;orderLineId&quot;: 7
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You can also take a look at the &lt;code&gt;CustomerEvents&lt;/code&gt; topic to inspect the events representing the creation of an invoice when a purchase order is added.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;duplicate_detection_in_the_consuming_service&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#duplicate_detection_in_the_consuming_service&quot;&gt;&lt;/a&gt;Duplicate Detection in the Consuming Service&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;At this point, our implementation of the outbox pattern is fully functional;
      when the order service receives a request to place an order
      (or cancel an order line),
      it will persist the corresponding state in the &lt;code&gt;purchaseorder&lt;/code&gt; and &lt;code&gt;orderline&lt;/code&gt; tables of its database.
      At the same time, within the same transaction, corresponding event entries will be added to the outbox table in the same database.
      The Debezium Postgres connector captures any insertions into that table
      and routes the events into the Kafka topic corresponding to the aggregate type represented by a given event.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To wrap things up, let’s explore how another microservice such as the shipment service can consume these messages.
      The entry point into that service is a regular Kafka consumer implementation,
      which is not too exciting and hence omitted here for the sake of brevity.
      You can find its &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/outbox/shipment-service/src/main/java/io/debezium/examples/outbox/shipment/facade/KafkaEventConsumer.java&quot;&gt;source code&lt;/a&gt; in the example repository.
      For each incoming message on the &lt;code&gt;Order&lt;/code&gt; topic, the consumer calls the &lt;code&gt;OrderEventHandler&lt;/code&gt;:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
      public class OrderEventHandler {
      
          private static final Logger LOGGER = LoggerFactory.getLogger(OrderEventHandler.class);
      
          @Inject
          private MessageLog log;
      
          @Inject
          private ShipmentService shipmentService;
      
          @Transactional
          public void onOrderEvent(UUID eventId, String key, String event) {
              if (log.alreadyProcessed(eventId)) {
                  LOGGER.info(&quot;Event with UUID {} was already retrieved, ignoring it&quot;, eventId);
                  return;
              }
      
              JsonObject json = Json.createReader(new StringReader(event)).readObject();
              JsonObject payload = json.containsKey(&quot;schema&quot;) ? json.getJsonObject(&quot;payload&quot;) :json;
      
              String eventType = payload.getString(&quot;eventType&quot;);
              Long ts = payload.getJsonNumber(&quot;ts_ms&quot;).longValue();
              String eventPayload = payload.getString(&quot;payload&quot;);
      
              JsonReader payloadReader = Json.createReader(new StringReader(eventPayload));
              JsonObject payloadObject = payloadReader.readObject();
      
              if (eventType.equals(&quot;OrderCreated&quot;)) {
                  shipmentService.orderCreated(payloadObject);
              }
              else if (eventType.equals(&quot;OrderLineUpdated&quot;)) {
                  shipmentService.orderLineUpdated(payloadObject);
              }
              else {
                  LOGGER.warn(&quot;Unkown event type&quot;);
              }
      
              log.processed(eventId);
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The first thing done by &lt;code&gt;onOrderEvent()&lt;/code&gt; is to check whether the event with the given UUID has been processed before.
      If so, any further calls for that same event will be ignored.
      This is to prevent any duplicate processing of events caused by the &quot;at least once&quot; semantics of this data pipeline.
      For instance it could happen that the Debezium connector or the consuming service fail
      before acknowledging the retrieval of a specific event with the source database or the messaging broker, respectively.
      In that case, after a restart of Debezium or the consuming service,
      a few events may be processed a second time.
      Propagating the event UUID as a Kafka message header allows for an efficient detection and exclusion of duplicates in the consumer.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If a message is received for the first time, the message value is parsed and the business method of the &lt;code&gt;ShippingService&lt;/code&gt; method corresponding to the specific event type is invoked with the event payload.
      Finally, the message is marked as processed with the message log.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This &lt;code&gt;MessageLog&lt;/code&gt; simply keeps track of all consumed events in a table within the service’s local database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
      public class MessageLog {
      
          @PersistenceContext
          private EntityManager entityManager;
      
          @Transactional(value=TxType.MANDATORY)
          public void processed(UUID eventId) {
              entityManager.persist(new ConsumedMessage(eventId, Instant.now()));
          }
      
          @Transactional(value=TxType.MANDATORY)
          public boolean alreadyProcessed(UUID eventId) {
              return entityManager.find(ConsumedMessage.class, eventId) != null;
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;That way, should the transaction be rolled back for some reason, also the original message will not be marked as processed and an exception would bubble up to the Kafka event consumer loop.
      This allows for re-trying to process the message later on.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note that a more complete implementation should take care of re-trying given messages only for a certain number of times,
      before re-routing any unprocessable messages to a dead-letter queue or similar.
      Also there should be some house-keeping on the message log table;
      periodically, all events older than the consumer’s current offset committed with the broker may be deleted,
      as it’s ensured that such messages won’t be propagated to the consumer another time.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot;&gt;&lt;/a&gt;Summary&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The outbox pattern is a great way for propagating data amongst different microservices.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;By only modifying a single resource - the source service’s own database -
      it avoids any potential inconsistencies of altering multiple resources at the same time which don’t share one common transactional context
      (the database and Apache Kafka).
      By writing to the database first, the source service has instant &quot;read your own writes&quot; semantics,
      which is important for a consistent user experience, allowing query methods invoked following to a write to instantly reflect any data changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;At the same time, the pattern enables asynchronous event propagation to other microservices.
      Apache Kafka acts as a highly scalable and reliable backbone for the messaging amongst the services.
      Given the right topic retention settings, new consumers may come up long after an event has been originally produced,
      and build up their own local state based on the event history.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Putting Apache Kafka into the center of the overall architecture also ensures a decoupling of involved services.
      If for instance single components of the solution fail or are not available for some time, e.g. during an update,
      events will simply be processed later on: after a restart,
      the Debezium connector will continue to tail the outbox table from the point where it left off before.
      Similarly, any consumer will continue to process topics from its previous offset.
      By keeping track of already successfully processed messages, duplicates can be detected and excluded from repeated handling.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Naturally, such event pipeline between different services is eventually consistent,
      i.e. consumers such as the shipping service may lag a bit behind producers such as the order service.
      Usually, that’s just fine, though, and can be handled in terms of the application’s business logic.
      For instance there’ll typically be no need to create a shipment within the very same second as an order has been placed.
      Also, end-to-end delays of the overall solution are typically low (seconds or even sub-second range),
      thanks to log-based change data capture which allows for emission of events in near-realtime.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;One last thing to keep in mind is that the structure of the events exposed via the outbox should be considered a part of the emitting service’s API.
      I.e. when needed, their structure should be adjusted carefully and with compatibility considerations in mind.
      This is to ensure to not accidentally break any consumers when upgrading the producing service.
      At the same time, consumers should be lenient when handling messages and for instance not fail when encountering unknown attributes within received events.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;em&gt;Many thanks to Hans-Peter Grahsl, Jiri Pechanec, Justin Holmes and René Kerner for their feedback while writing this post!&lt;/em&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/02/13/debezium-webinar-at-devnation-live/</id>
    <title>Debezium at DevNation Live</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-02-13T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/02/13/debezium-webinar-at-devnation-live/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="introduction"></category>
    <category term="presentation"></category>
    <summary>
      
      
      
      Last week I had the pleasure to do a webinar on change data streaming patterns for microservices with the fabulous Burr Sutter at DevNation Live.
      
      
      The recording of that 30 min session is available on YouTube now.
      It also contains a demo that shows how to set-up a data streaming pipeline with Debezium and Apache Kafka,
      running on OpenShift.
      The demo begins at 12 min 40 into the recording.
      
      
      Enjoy!
      
      
      
      -->
      
      
      &#160;
      You can also find the slide deck (in a slightly extended version) on Speaker Deck:
      &#160;
      &#160;
      
      
      
      
      
      
      
      About Debezium
      
      
      Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Last week I had the pleasure to do a &lt;a href=&quot;https://developers.redhat.com/videos/youtube/QYbXDp4Vu-8/&quot;&gt;webinar on change data streaming patterns for microservices&lt;/a&gt; with the fabulous Burr Sutter at DevNation Live.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The recording of that 30 min session is available on YouTube now.
      It also contains a demo that shows how to set-up a data streaming pipeline with Debezium and Apache Kafka,
      running on OpenShift.
      The demo begins at 12 min 40 into the recording.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Enjoy!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;responsive-video&quot;&gt;
      &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/QYbXDp4Vu-8&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
      &lt;!--&lt;iframe width=&quot;1600&quot; height=&quot;900&quot; src=&quot;https://www.youtube.com/embed/IOZ2Um6e430?rel=0&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;--&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt; &lt;br /&gt;
      You can also find the &lt;a href=&quot;https://speakerdeck.com/gunnarmorling/change-data-streaming-patterns-for-microservices-with-debezium-apache-kafka-meetup-hamburg&quot;&gt;slide deck&lt;/a&gt; (in a slightly extended version) on Speaker Deck:
       &lt;br /&gt;
       &lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div style=&quot;text-align-center&quot;&gt;
      &lt;script async=&quot;&quot; class=&quot;speakerdeck-embed&quot; data-id=&quot;c390d77e50464c99916ede7368a279c2&quot; data-ratio=&quot;1.77777777777778&quot; src=&quot;//speakerdeck.com/assets/embed.js&quot;&gt;&lt;/script&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/02/13/debezium-0-9-1-final-released/</id>
    <title>Debezium 0.9.1.Final Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-02-13T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/02/13/debezium-0-9-1-final-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="sqlserver"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      Quickly following up to last week&#8217;s release of Debezium 0.9, it&#8217;s my pleasure today to announce the release of Debezium 0.9.1.Final!
      
      
      This release fixes a couple of bugs which were reported after the 0.9 release.
      Most importantly, there are two fixes to the new Debezium connector for SQL Server,
      which deal with correct handling of LSNs after connector restarts (DBZ-1128, DBZ-1131).
      The connector also uses more reasonable defaults for the selectMethod and fetchSize options of the SQL Server JDBC driver (DBZ-1065),
      which can help to significantly increase through-put and reduce memory consumption of the connector.
      
      
      The MySQL connector supports GENERATED columns now with the new Antlr-based...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Quickly following up to last week’s release of Debezium 0.9, it’s my pleasure today to announce the release of Debezium &lt;strong&gt;0.9.1.Final&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release fixes a couple of bugs which were reported after the 0.9 release.
      Most importantly, there are two fixes to the new &lt;a href=&quot;http://debezium.io/docs/connectors/sqlserver/&quot;&gt;Debezium connector for SQL Server&lt;/a&gt;,
      which deal with correct handling of LSNs after connector restarts (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1128&quot;&gt;DBZ-1128&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1131&quot;&gt;DBZ-1131&lt;/a&gt;).
      The connector also uses more reasonable defaults for the &lt;code&gt;selectMethod&lt;/code&gt; and &lt;code&gt;fetchSize&lt;/code&gt; options of the SQL Server JDBC driver (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1065&quot;&gt;DBZ-1065&lt;/a&gt;),
      which can help to significantly increase through-put and reduce memory consumption of the connector.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The MySQL connector supports &lt;code&gt;GENERATED&lt;/code&gt; columns now with the new Antlr-based DDL parser (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1123&quot;&gt;DBZ-1123&lt;/a&gt;),
      and for the Postgres connector the handling of primary key column definition changes was improved (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-997&quot;&gt;DBZ-997&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In terms of new features, there is a new container image provided on Docker Hub now:
      the &lt;a href=&quot;https://hub.docker.com/r/debezium/tooling&quot;&gt;debezium/tooling&lt;/a&gt; image contains a couple of open-source CLI tools
      (currently &lt;a href=&quot;https://github.com/edenhill/kafkacat&quot;&gt;kafkacat&lt;/a&gt;, &lt;a href=&quot;https://github.com/jakubroztocil/httpie&quot;&gt;httpie&lt;/a&gt;, &lt;a href=&quot;https://github.com/stedolan/jq&quot;&gt;jq&lt;/a&gt;, &lt;a href=&quot;https://github.com/dbcli/mycli&quot;&gt;mycli&lt;/a&gt; and &lt;a href=&quot;https://github.com/dbcli/pgcli&quot;&gt;pqcli&lt;/a&gt;)
      which greatly help when working with Debezium connectors, Apache Kafka and Kafka Connect on the command line
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1125&quot;&gt;DBZ-1125&lt;/a&gt;).
      A big thank you to the respective authors these fantastic tools!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/debezium_shell.gif&quot; style=&quot;max-width:100%; margin-bottom:10px; margin-top:10px;&quot; class=&quot;responsive-image&quot; alt=&quot;CLI tools for working with Debezium&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Altogether, &lt;a href=&quot;https://issues.jboss.org/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.9.1.Final&quot;&gt;12 issues&lt;/a&gt; were resolved in this release.
      Please refer to the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-9-1-final&quot;&gt;release notes&lt;/a&gt; to learn more about all fixed bugs, update procedures etc.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks a lot to community members &lt;a href=&quot;https://github.com/ivan-lorenz&quot;&gt;Ivan Lorenz&lt;/a&gt; and &lt;a href=&quot;https://github.com/tomazlemos&quot;&gt;Tomaz Lemos Fernandes&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/02/05/debezium-0-9-0-final-released/</id>
    <title>Debezium 0.9.0.Final Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-02-05T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/02/05/debezium-0-9-0-final-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="sqlserver"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m delighted to announce the release of Debezium 0.9 Final!
      
      
      This release only adds a small number of changes since last week&#8217;s CR1 release;
      most prominently there&#8217;s some more metrics for the SQL Server connector
      (lag behind master, number of transactions etc.)
      and two bug fixes related to the handling of partitioned tables in MySQL (DBZ-1113) and Postgres (DBZ-1118).
      
      
      Having been in the works for six months after the initial Alpha release,
      Debezium 0.9 comes with a brand new connector for SQL Server,
      lots of new features and improvements for the existing connectors,
      updates to the latest versions of Apache Kafka and the supported databases
      as well as a...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m delighted to announce the release of Debezium &lt;strong&gt;0.9 Final&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release only adds a small number of changes since last week’s CR1 release;
      most prominently there’s some more metrics for the SQL Server connector
      (lag behind master, number of transactions etc.)
      and two bug fixes related to the handling of partitioned tables in MySQL (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1113&quot;&gt;DBZ-1113&lt;/a&gt;) and Postgres (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1118&quot;&gt;DBZ-1118&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Having been in the works for six months after the initial Alpha release,
      Debezium 0.9 comes with a brand new &lt;a href=&quot;http://debezium.io/docs/connectors/sqlserver/&quot;&gt;connector for SQL Server&lt;/a&gt;,
      lots of new features and improvements for the existing connectors,
      updates to the latest versions of Apache Kafka and the supported databases
      as well as a wide range of bug fixes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Some key features of the release besides the aforementioned CDC connector for SQL Server are:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Initial snapshotting for the Oracle connector (which remains to be a &quot;tech preview&quot; at this point)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Brand-new metrics for the SQL Server and Oracle connectors and extended metrics for the MySQL connector&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Field filtering and renaming for MongoDB&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;A new handler interface for the embedded engine&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Lots of improvements around the &quot;event flattening&quot; SMT for MongoDB&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;More detailed source info in CDC events and optional metadata such as a column’s source type&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Option to delay snapshots for a given time&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Support for HSTORE columns in Postgres&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Incubating support for picking up changes to the whitelist/blacklist configuration of the MySQL connector&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As a teaser on the connector metrics support, here’s a screenshot of &lt;a href=&quot;https://openjdk.java.net/projects/jmc/&quot;&gt;Java Mission Control&lt;/a&gt;
      displaying the SQL Server connector metrics:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/java_mission_control.png&quot; style=&quot;max-width:100%; margin-bottom:10px; margin-top:10px;&quot; class=&quot;responsive-image&quot; alt=&quot;Monitoring the Debezium SQL Server connector&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The list above is far from being exhaustive; please take a look at the preview release announcements
      (&lt;a href=&quot;http://debezium.io/blog/2018/07/26/debezium-0-9-0-alpha1-released/&quot;&gt;Alpha1&lt;/a&gt;,
      &lt;a href=&quot;http://debezium.io/blog/2018/10/04/debezium-0-9-0-alpha2-released/&quot;&gt;Alpha2&lt;/a&gt;,
      &lt;a href=&quot;http://debezium.io/blog/2018/11/22/debezium-0-9-0-beta1-released/&quot;&gt;Beta1&lt;/a&gt;,
      &lt;a href=&quot;http://debezium.io/blog/2018/12/19/debezium-0-9-0-beta2-released/&quot;&gt;Beta2&lt;/a&gt; and
      &lt;a href=&quot;http://debezium.io/blog/2019/01/28/debezium-0-9-0-cr1-released/&quot;&gt;CR 1&lt;/a&gt;)
      as well as the full list of a whopping &lt;a href=&quot;https://issues.jboss.org/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20in%20(0.9.0.Alpha1%2C%200.9.0.Alpha2%2C%200.9.0.Beta1%2C%200.9.0.Beta2%2C%200.9.0.CR1%2C%200.9.0.Final)%20ORDER%20BY%20issuetype%20ASC&amp;amp;startIndex=120&quot;&gt;176 fixed issues&lt;/a&gt; in JIRA.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s hard to say which of the changes and new features I’m most excited about,
      but one thing surely sticking out is the tremendous amount of community work on this release.
      Not less than 34 different members of Debezium’s outstanding community have contributed to this release.
      A huge and massive &quot;Thank You!&quot; to all of you:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/addisonj&quot;&gt;Addison Higham&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/anton-martynov&quot;&gt;Anton Martynov&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/amitsela&quot;&gt;Amit Sela&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/artiship&quot;&gt;Artiship Artiship&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/CliffWheadon&quot;&gt;Cliff Wheadon&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/deepakbarr&quot;&gt;Deepak Barr&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/pimpelsang&quot;&gt;Eero Koplimets&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/gaganpaytm&quot;&gt;Gagan Agrawal&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/grzegorz8&quot;&gt;Grzegorz Kołakowski&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/ian-axelrod&quot;&gt;Ian Axelrod&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/Ipshin&quot;&gt;Ilia Bogdanov&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/ivankovbas&quot;&gt;Ivan Kovbas&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/jgao54&quot;&gt;Joy Gao&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/kppullin&quot;&gt;Kevin Pullin&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/sweat123&quot;&gt;Lao Mei&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/ooooorz&quot;&gt;Liu Hanlin&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/maver1ck&quot;&gt;Maciej Bryński&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/mtagle&quot;&gt;Moira Tagle&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/olavim&quot;&gt;Olavi Mustanoja&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/olivierlemasle&quot;&gt;Olivier Lemasle&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/oripwk&quot;&gt;Ori Popowski&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/PengLyu&quot;&gt;Peng Lyu&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/plarsson&quot;&gt;Peter Larsson&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/PSanetra&quot;&gt;Philip Sanetra&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/sagarrao&quot;&gt;Sagar Rao&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/shivamsharma&quot;&gt;Shivam Sharma&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/SyedMuhammadSufyian&quot;&gt;Syed Muhammad Sufyian&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/tautautau&quot;&gt;Tautvydas Januskevicius&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/Tapppi&quot;&gt;Tapani Moilanen&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/trizko&quot;&gt;Tony Rizko&lt;/a&gt;
      &lt;a href=&quot;https://github.com/wscheep&quot;&gt;Wout Scheepers&lt;/a&gt; and
      &lt;a href=&quot;https://github.com/wangzheng422&quot;&gt;Zheng Wang&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When upgrading from earlier Debezium releases,
      please make sure to read the information regarding update procedures and breaking changes in the &lt;a href=&quot;http://debezium.io/docs/releases/&quot;&gt;release notes&lt;/a&gt;.
      One relevant change to the users of the Debezium connector for MySQL is that our new Antlr-based DDL parser is used by default now.
      After lots of honing we felt it’s time for using the new parser by default now.
      While the existing parser can still be used as a fallback as of Debezium 0.9,
      it will be phased out in 0.10.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;next_steps&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#next_steps&quot;&gt;&lt;/a&gt;Next Steps&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;After some drinks to celebrate this release, the plan is to do a 0.9.1 release rather quickly
      (probably in two weeks from now),
      providing improvements and potential bug fixes to the features and changes done in 0.9.
      We’ll also begin the work on Debezium 0.10,
      stay tuned for the details on that!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For further plans beyond that, take a look at our &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;road map&lt;/a&gt;.
      Any suggestions and ideas are very welcomed on &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; or in the comments below.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’re just about to begin using Debezium for streaming changes out of your database,
      you might be interested in join us for the &lt;a href=&quot;https://www.redhat.com/en/events/webinar/change-data-streaming-patterns-microservices-kafka-and-debezium&quot;&gt;upcoming webinar&lt;/a&gt; on February 7th.
      After a quick overview, you’ll see Debezium in action, as it streams changes to a browser-based dashboard and more.
      You can also find lots of resources around Debezium and change data capture such as blog posts and presentations in our curated &lt;a href=&quot;http://debezium.io/docs/online-resources/&quot;&gt;list of online resources&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2019/01/28/debezium-0-9-0-cr1-released/</id>
    <title>Debezium 0.9.0.CR1 Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2019-01-28T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2019/01/28/debezium-0-9-0-cr1-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="sqlserver"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      Reaching the home stretch towards Debezium 0.9, it&#8217;s with great pleasure that I&#8217;m announcing the first release of Debezium in 2019, 0.9.0.CR1!
      
      
      For this release we&#8217;ve mainly focused on sorting out remaining issues in the Debezium connector for SQL Server;
      the connector comes with greatly improved performance and has received a fair number of bug fixes.
      
      
      Other changes include a new interface for event handlers of Debezium&#8217;s embedded engine,
      which allows for bulk handling of change events, an option to export the scale of numeric columns as schema parameter,
      as well as a wide range of bug fixes for the Debezium connectors for MySQL, Postgres...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Reaching the home stretch towards Debezium 0.9, it’s with great pleasure that I’m announcing the first release of Debezium in 2019, &lt;strong&gt;0.9.0.CR1&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For this release we’ve mainly focused on sorting out remaining issues in the Debezium &lt;a href=&quot;http://debezium.io/docs/connectors/sqlserver/&quot;&gt;connector for SQL Server&lt;/a&gt;;
      the connector comes with greatly improved performance and has received a fair number of bug fixes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Other changes include a new interface for event handlers of Debezium’s &lt;a href=&quot;http://debezium.io/docs/embedded/&quot;&gt;embedded engine&lt;/a&gt;,
      which allows for bulk handling of change events, an option to export the scale of numeric columns as schema parameter,
      as well as a wide range of bug fixes for the Debezium connectors for MySQL, Postgres and Oracle.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;sql_server_connector_improvements&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#sql_server_connector_improvements&quot;&gt;&lt;/a&gt;SQL Server Connector Improvements&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The SQL Server connector supports blacklisting of specific columns now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1067&quot;&gt;DBZ-1067&lt;/a&gt;).
      That’s useful in cases where you’d like to exclude specific columns from emitted change data messages, e.g. to data protection considerations.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &quot;snapshot locking mode&quot; option has been reworked (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-947&quot;&gt;DBZ-947&lt;/a&gt;) and is named &quot;snapshot isolation mode&quot; now,
      better reflecting its semantics.
      A new mode &quot;repeatable_read&quot; has been added, and &quot;none&quot; has been renamed to &quot;read_uncommitted&quot;.
      Please see the connector documentation and the &lt;a href=&quot;http://debezium.io/docs/releases/#breaking_changes&quot;&gt;migration notes&lt;/a&gt; for more details.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The connector allows for a much higher through-put now, thanks to caching of timestamps for the same LSN (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1078&quot;&gt;DBZ-1078&lt;/a&gt;).
      Please refer to the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-9-0-cr1&quot;&gt;change log&lt;/a&gt; for details on bugs fixed in this connector.
      A massive &quot;Thank You&quot; is in order to Grzegorz Kołakowski, for his tireless work on and testing of this connector!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;new_embedded_engine_handler_interface&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_embedded_engine_handler_interface&quot;&gt;&lt;/a&gt;New Embedded Engine Handler Interface&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium’s embedded engine now comes with a new interface &lt;code&gt;ChangeConsumer&lt;/code&gt;,
      which event handlers can implement if they’d like to process change events in bulks (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1080&quot;&gt;DBZ-1080&lt;/a&gt;).
      That can result in substantial performance improvements when pushing change events to APIs that apply batch semantics themselves,
      such as the Kinesis Producer Library.
      You can learn more in the &lt;a href=&quot;http://debezium.io/docs/embedded/&quot;&gt;embedded engine&lt;/a&gt; docs.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;misc_changes_and_bug_fixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#misc_changes_and_bug_fixes&quot;&gt;&lt;/a&gt;Misc. Changes and Bug Fixes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;All the relational connectors allow now to propagate the scale of numeric columns as a schema parameter
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1073&quot;&gt;DBZ-1073&lt;/a&gt;).
      This is controlled via the &lt;code&gt;column.propagate.source.type&lt;/code&gt; option and builds on the exposure of type name and width added in Debezium 0.8.
      All these schema parameters can be used when creating the schema of corresponding tables in sink databases.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium’s container image for Apache Kafka allows to create and watch topics now
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1057&quot;&gt;DBZ-1057&lt;/a&gt;).
      You also can specify a clean-up policy when creating a topic
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1038&quot;&gt;DBZ-1038&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium MySQL connector handles unsigned &lt;code&gt;SMALLINT&lt;/code&gt; columns as expected now.
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1063&quot;&gt;DBZ-1063&lt;/a&gt;).
      For nullable columns with a default value, &lt;code&gt;NULL&lt;/code&gt; values are correctly exported
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1064&quot;&gt;DBZ-1064&lt;/a&gt;; previously, the default value would have been exported in that case).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Postgres connector handles tables without a primary key correctly now
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1029&quot;&gt;DBZ-1029&lt;/a&gt;).
      We’ve also applied a fix to make sure that the connector works with Postgres on Amazon RDS,
      which recently was broken due to an update of wal2json in RDS
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1083&quot;&gt;DBZ-1083&lt;/a&gt;).
      Going forward, we’re planning to set-up CI jobs to test against Postgres on RDS in all the versions supported by the Debezium connector.
      This will help us to spot similar issues early on and react quickly.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-9-0-cr1&quot;&gt;change log&lt;/a&gt; for the complete list of all addressed issues.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release wouldn’t have been possible without all the contributions by the following members of the Debezium community:
      &lt;a href=&quot;https://github.com/addisonj&quot;&gt;Addison Higham&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/amitsela&quot;&gt;Amit Sela&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/gaganpaytm&quot;&gt;Gagan Agrawal&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/grzegorz8&quot;&gt;Grzegorz Kołakowski&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/Ipshin&quot;&gt;Ilia Bogdanov&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/ivankovbas&quot;&gt;Ivan Kovbas&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/mtagle&quot;&gt;Moira Tagle&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt; and
      &lt;a href=&quot;https://github.com/trizko&quot;&gt;Tony Rizko&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks a lot!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;next_steps&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#next_steps&quot;&gt;&lt;/a&gt;Next Steps&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The CR1 release took us a bit longer than anticipated.
      The release of Debezium 0.9.0.Final will therefore be moved to early February.
      Rather quickly thereafter we’re planning to release Debezium 0.9.1,
      which will provide improvements and potential bugfixes to the features added in 0.9.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For further plans beyond that, check out our &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;road map&lt;/a&gt;.
      If you got any feedback or suggestions for future additions, please get in touch via the &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; or in the comments below.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/12/19/debezium-0-9-0-beta2-released/</id>
    <title>Debezium 0.9.0.Beta2 Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-12-19T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/12/19/debezium-0-9-0-beta2-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="sqlserver"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      With only a few days left for the year, it&#8217;s about time for another Debezium release;
      so it&#8217;s with great pleasure that I&#8217;m announcing Debezium 0.9.0.Beta2!
      
      
      This release comes with support for MySQL 8 and Oracle 11g;
      it includes a first cut of metrics for monitoring the SQL Server and Oracle connectors,
      several improvements to the MongoDB event flattening SMT as well as a wide range of bug fixes.
      Overall, not less than 42 issues were addressed;
      very clearly, there has to be some deeper sense in that ;)
      
      
      A big shout out goes to the following members Debezium&#8217;s amazing community, who contributed to this release:
      Eero Koplimets,...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With only a few days left for the year, it’s about time for another Debezium release;
      so it’s with great pleasure that I’m announcing Debezium &lt;strong&gt;0.9.0.Beta2&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release comes with support for MySQL 8 and Oracle 11g;
      it includes a first cut of metrics for monitoring the SQL Server and Oracle connectors,
      several improvements to the MongoDB event flattening SMT as well as a wide range of bug fixes.
      Overall, not less than &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-9-0-beta2&quot;&gt;42 issues&lt;/a&gt; were addressed;
      very clearly, there has to be &lt;a href=&quot;https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#Answer_to_the_Ultimate_Question_of_Life%2C_the_Universe%2C_and_Everything_%2842%29&quot;&gt;some deeper sense&lt;/a&gt; in that ;)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A big shout out goes to the following members Debezium’s amazing community, who contributed to this release:
      &lt;a href=&quot;https://github.com/pimpelsang&quot;&gt;Eero Koplimets&lt;/a&gt;, &lt;a href=&quot;https://github.com/grzegorz8&quot;&gt;Grzegorz Kołakowski&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/ooooorz&quot;&gt;Hanlin Liu&lt;/a&gt;, &lt;a href=&quot;https://github.com/sweat123&quot;&gt;Lao Mei&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt;, &lt;a href=&quot;https://github.com/tautautau&quot;&gt;Tautvydas Januskevicius&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/wscheep&quot;&gt;Wout Scheepers&lt;/a&gt; and &lt;a href=&quot;https://github.com/wangzheng422&quot;&gt;Zheng Wang&lt;/a&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In the following, let’s take a closer look at some of the changes coming with the 0.9 Beta2 release.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;monitoring_and_metrics_for_the_sql_server_and_oracle_connectors&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#monitoring_and_metrics_for_the_sql_server_and_oracle_connectors&quot;&gt;&lt;/a&gt;Monitoring and Metrics for the SQL Server and Oracle Connectors&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Following the example of the MySQL connector, the connectors for &lt;a href=&quot;http://debezium.io/docs/connectors/sqlserver/&quot;&gt;SQL Server&lt;/a&gt; and &lt;a href=&quot;http://debezium.io/docs/connectors/oracle/&quot;&gt;Oracle&lt;/a&gt; now expose a range of metrics for monitoring purposes via JMX (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-978&quot;&gt;DBZ-978&lt;/a&gt;).
      This includes values like the time since the last CDC event, offset of the last event, the total number of events, remaining and already scanned tables while doing a snapshot and much more.
      Please see &lt;a href=&quot;http://debezium.io/docs/monitoring/&quot;&gt;the monitoring documentation&lt;/a&gt; for details on how to enable JMX.
      The following image shows an example of displaying the values in OpenJDK’s &lt;a href=&quot;https://openjdk.java.net/projects/jmc/&quot;&gt;Mission Control&lt;/a&gt; tool:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/monitoring_mission_control.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Monitoring the Debezium SQL Server connector&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re planning to expand the set of exposed metrics in future versions and also make them available for Postgres and MongoDB.
      Please let us know about the metrics you’d like to see by commenting on JIRA issue &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1040&quot;&gt;DBZ-1040&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As a bonus, we’ve also created a Grafana dashboard for visualizing all the relevant metrics:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/monitoring_dashboard.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Connector metrics in Grafana&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll blog about monitoring and the dashboard in more detail soon;
      but if you are interested, you already can take a look at &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/monitoring&quot;&gt;this demo&lt;/a&gt; in our examples repository.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;misc_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#misc_features&quot;&gt;&lt;/a&gt;Misc. Features&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &quot;snapshot.delay.ms&quot; option already known from the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;Debezium MySQL connector&lt;/a&gt; is now available for all other Debezium connectors, too (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-966&quot;&gt;DBZ-966&lt;/a&gt;).
      This comes in handy when deploying multiple connectors to a Kafka Connect cluster,
      which may cause rebalancing the connectors in the cluster,
      interrupting and restarting running snapshots of already deployed connector instances.
      This can be avoided by specifying a delay which allows to wait with the snapshotting until the rebalancing phase is completed.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;a href=&quot;http://debezium.io/docs/configuration/mongodb-event-flattening/&quot;&gt;MongoDB CDC Event Flattening&lt;/a&gt; transformation received a number of improvements:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Support for MongoDB’s &lt;code&gt;$unset&lt;/code&gt; operator (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-612&quot;&gt;DBZ-612&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Support for full document updates (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-987&quot;&gt;DBZ-987&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;New option for dropping delete and tombstone messages (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-563&quot;&gt;DBZ-563&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Option to convey the original type of operation as a header parameter (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-971&quot;&gt;DBZ-971&lt;/a&gt;);
      that option is also available for the &lt;a href=&quot;http://debezium.io/docs/configuration/event-flattening/&quot;&gt;Flattening SMT&lt;/a&gt; for the relational connectors and can be useful in case sink connectors need to differentiate between inserts and updates&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;bug_fixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bug_fixes&quot;&gt;&lt;/a&gt;Bug fixes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As always, we’ve also fixed a good number of bugs reported by Debezium users.
      The set of fixed issues includes:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Several bugs related to streaming changes from MySQL in GTID mode (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-923&quot;&gt;DBZ-923&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1005&quot;&gt;DBZ-1005&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1008&quot;&gt;DBZ-1008&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Handling of tables with reserved names in the SQL Server connector (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1031&quot;&gt;DBZ-1031&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Potential event loss after MySQL connector restart (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1033&quot;&gt;DBZ-1033&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Unchanged values of TOASTed columns caused the Postgres connector to fail (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-842&quot;&gt;DBZ-842&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-9-0-beta2&quot;&gt;change log&lt;/a&gt; for the complete list of addressed issues.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;next_steps&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#next_steps&quot;&gt;&lt;/a&gt;Next Steps&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re planning to do a candidate release of Debezium 0.9 in early January.
      Provided no critical issues show up, Debezium 0.9.0.Final should be out by the end of January.
      For the CR we’ve mostly scheduled a number of further bug fixes, improvements to the SQL Server connector and the addition of further metrics.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In parallel, we’ll focus our attention on the Oracle connector again, finally getting back to the long-awaited LogMiner-based capture implementation (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-137&quot;&gt;DBZ-137&lt;/a&gt;).
      This will be a primary feature of Debezium 0.10.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In addition, we’ll spend some cycles on the blogging and demo side of things;
      namely we’re thinking about writing on and demoing the new monitoring and metrics support,
      HA architectures including failover with MySQL, HAProxy and Debezium,
      as well as enriching CDC events with contextual information such as the current user or use case identifiers.
      Stay tuned!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also going beyond 0.10, we got some &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;great plans&lt;/a&gt; for Debezium in the coming year.
      If you’d like to bring in your ideas, too, please let us know on the &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; or in the comments below,
      we’re looking forward to hearing from you.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;And with that, all that remains to be said, is &lt;a href=&quot;https://en.wikipedia.org/wiki/Festivus&quot;&gt;&quot;Happy Festivus for the rest of us!&quot;&lt;/a&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Happy change data streaming and see you in 2019!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/12/05/automating-cache-invalidation-with-change-data-capture/</id>
    <title>Automating Cache Invalidation With Change Data Capture</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-12-05T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/12/05/automating-cache-invalidation-with-change-data-capture/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="discussion"></category>
    <category term="examples"></category>
    <summary>
      
      
      
      The second-level cache of Hibernate ORM / JPA is a proven and efficient way to increase application performance:
      caching read-only or rarely modified entities avoids roundtrips to the database,
      resulting in improved response times of the application.
      
      
      Unlike the first-level cache, the second-level cache is associated with the session factory (or entity manager factory in JPA terms),
      so its contents are shared across transactions and concurrent sessions.
      Naturally, if a cached entity gets modified, the corresponding cache entry must be updated (or purged from the cache), too.
      As long as the data changes are done through Hibernate ORM, this is nothing to worry about: the ORM...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;a href=&quot;https://docs.jboss.org/hibernate/stable/orm/userguide/html_single/Hibernate_User_Guide.html#caching-config&quot;&gt;second-level cache&lt;/a&gt; of Hibernate ORM / JPA is a proven and efficient way to increase application performance:
      caching read-only or rarely modified entities avoids roundtrips to the database,
      resulting in improved response times of the application.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Unlike the first-level cache, the second-level cache is associated with the session factory (or entity manager factory in JPA terms),
      so its contents are shared across transactions and concurrent sessions.
      Naturally, if a cached entity gets modified, the corresponding cache entry must be updated (or purged from the cache), too.
      As long as the data changes are done through Hibernate ORM, this is nothing to worry about: the ORM will update the cache automatically.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Things get tricky, though, when bypassing the application, e.g. when modifying records directly in the database.
      Hibernate ORM then has no way of knowing that the cached data has become stale, and it’s necessary to invalidate the affected items explicitly.
      A common way for doing so is to foresee some admin functionality that allows to clear  an application’s caches.
      For this to work, it’s vital to not forget about calling that invalidation functionality, or the application will keep working with outdated cached data.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In the following we’re going to explore an alternative approach for cache invalidation, which works in a reliable and fully automated way:
      by employing Debezium and its &lt;a href=&quot;http://debezium.io/blog/2018/07/19/advantages-of-log-based-change-data-capture/&quot;&gt;change data capture&lt;/a&gt; (CDC) capabilities, you can track data changes in the database itself and react to any applied change.
      This allows to invalidate affected cache entries in near-realtime,
      without the risk of stale data due to missed changes.
      If an entry has been evicted from the cache, Hibernate ORM will load the latest version of the entity from the database the next time is requested.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;the_example_application&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_example_application&quot;&gt;&lt;/a&gt;The Example Application&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As an example, consider this simple model of two entities, &lt;code&gt;PurchaseOrder&lt;/code&gt; and &lt;code&gt;Item&lt;/code&gt;:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/cache_invalidation_class_diagram.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Example domain model&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A purchase order represents the order of an item, where its total price is the ordered quantity times the item’s base price.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;admonitionblock note&quot;&gt;
      &lt;table&gt;
      &lt;tr&gt;
      &lt;td class=&quot;icon&quot;&gt;
      &lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot;&gt;&lt;/i&gt;
      &lt;/td&gt;
      &lt;td class=&quot;content&quot;&gt;
      &lt;div class=&quot;title&quot;&gt;Source Code&lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/cache-invalidation/&quot;&gt;source code&lt;/a&gt; of this example is provided on GitHub.
      If you want to follow along and try out all the steps described in the following,
      clone the repo and follow the instructions in &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/cache-invalidation/_README.md&quot;&gt;README.md&lt;/a&gt; for building the project.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/td&gt;
      &lt;/tr&gt;
      &lt;/table&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Modelling order and item as JPA entities is straight-forward:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Entity
      public class PurchaseOrder {
      
          @Id
          @GeneratedValue(generator = &quot;sequence&quot;)
          @SequenceGenerator(
              name = &quot;sequence&quot;, sequenceName = &quot;seq_po&quot;, initialValue = 1001, allocationSize = 50
          )
          private long id;
          private String customer;
          @ManyToOne private Item item;
          private int quantity;
          private BigDecimal totalPrice;
      
          // ...
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As changes to items are rare, the &lt;code&gt;Item&lt;/code&gt; entity should be cached.
      This can be done by simply specifying JPA’s &lt;a href=&quot;https://docs.oracle.com/javaee/7/api/javax/persistence/Cacheable.html&quot;&gt;@Cacheable&lt;/a&gt; annotation:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Entity
      @Cacheable
      public class Item {
      
          @Id
          private long id;
          private String description;
          private BigDecimal price;
      
          // ...
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You also need to enable the second-level cache in the &lt;em&gt;META-INF/persistence.xml&lt;/em&gt; file.
      The property &lt;code&gt;hibernate.cache.use_second_level_cache&lt;/code&gt; activates the cache itself, and the &lt;code&gt;ENABLE_SELECTIVE&lt;/code&gt; cache mode
      causes only those entities to be put into the cache which are annotated with &lt;code&gt;@Cacheable&lt;/code&gt;.
      It’s also a good idea to enable SQL query logging and cache access statistics.
      That way you’ll be able to verify whether things work as expected by examining the application log:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt;
      &amp;lt;persistence xmlns=&quot;http://xmlns.jcp.org/xml/ns/persistence&quot;
          xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
          xsi:schemaLocation=&quot;...&quot;
          version=&quot;2.2&quot;&amp;gt;
      
          &amp;lt;persistence-unit name=&quot;orders-PU-JTA&quot; transaction-type=&quot;JTA&quot;&amp;gt;
              &amp;lt;jta-data-source&amp;gt;java:jboss/datasources/OrderDS&amp;lt;/jta-data-source&amp;gt;
              &amp;lt;shared-cache-mode&amp;gt;ENABLE_SELECTIVE&amp;lt;/shared-cache-mode&amp;gt;
              &amp;lt;properties&amp;gt;
                  &amp;lt;property name=&quot;hibernate.cache.use_second_level_cache&quot; value=&quot;true&quot; /&amp;gt;
      
                  &amp;lt;property name=&quot;hibernate.show_sql&quot; value=&quot;true&quot; /&amp;gt;
                  &amp;lt;property name=&quot;hibernate.format_sql&quot; value=&quot;true&quot; /&amp;gt;
                  &amp;lt;property name=&quot;hibernate.generate_statistics&quot; value=&quot;true&quot; /&amp;gt;
      
                  &amp;lt;!-- dialect etc. ... --&amp;gt;
              &amp;lt;/properties&amp;gt;
          &amp;lt;/persistence-unit&amp;gt;
      &amp;lt;/persistence&amp;gt;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When running on a &lt;a href=&quot;https://www.oracle.com/technetwork/java/javaee/overview/index.html&quot;&gt;Java EE&lt;/a&gt; application server
      (or &lt;a href=&quot;https://jakarta.ee/&quot;&gt;Jakarta EE&lt;/a&gt; how the stack is called after it has been donated to the Eclipse Foundation),
      that’s all you need to enable second-level caching.
      In the case of &lt;a href=&quot;http://wildfly.org/&quot;&gt;WildFly&lt;/a&gt; (which is what’s used in the example project), the &lt;a href=&quot;http://infinispan.org/&quot;&gt;Infinispan&lt;/a&gt; key/value store is used as the cache provider by default.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now try and see what happens when modifying an item’s price by running some SQL in the database,
      bypassing the application layer.
      If you’ve checked out the example source code, comment out the &lt;code&gt;DatabaseChangeEventListener&lt;/code&gt; class and start the application as described in the &lt;em&gt;README.md&lt;/em&gt;.
      You then can place purchase orders using curl like this
      (a couple of example items have been persisted at application start-up):&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&amp;gt; curl -H &quot;Content-Type: application/json&quot; \
        -X POST \
        --data '{ &quot;customer&quot; : &quot;Billy-Bob&quot;, &quot;itemId&quot; : 10003, &quot;quantity&quot; : 2 }' \
        http://localhost:8080/cache-invalidation/rest/orders&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;{
          &quot;id&quot; : 1002,
          &quot;customer&quot; : &quot;Billy-Bob&quot;,
          &quot;item&quot; : {
              &quot;id&quot; :10003,
              &quot;description&quot; : &quot;North By Northwest&quot;,
              &quot;price&quot; : 14.99
          },
          &quot;quantity&quot; : 2,
          &quot;totalPrice&quot; : 29.98
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The response is the expected one, as the item price is 14.99.
      Now update the item’s price directly in the database.
      The example uses Postgres, so you can use the &lt;em&gt;psql&lt;/em&gt; CLI utility to do so:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec postgres bash -c 'psql -U $POSTGRES_USER $POSTGRES_DB -c &quot;UPDATE item SET price = 20.99 where id = 10003&quot;'&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Placing another purchase order for the same item using curl,
      you’ll see that the calculated total price doesn’t reflect the update.
      Not good!
      But it’s not too surprising, given that the price update was applied completely bypassing the application layer and Hibernate ORM.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;the_change_event_handler&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_change_event_handler&quot;&gt;&lt;/a&gt;The Change Event Handler&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now let’s explore how to use Debezium and CDC to react to changes in the &lt;code&gt;item&lt;/code&gt; table and invalidate corresponding cache entries.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While Debezium most of the times is deployed into &lt;a href=&quot;https://kafka.apache.org/documentation/#connect&quot;&gt;Kafka Connect&lt;/a&gt; (thus streaming change events into Apache Kafka topics),
      it has another mode of operation that comes in very handy for the use case at hand.
      Using the &lt;a href=&quot;http://debezium.io/docs/embedded/&quot;&gt;embedded engine&lt;/a&gt;, you can run the Debezium connectors as a library directly within your application.
      For each change event received from the database, a configured callback method will be invoked, which in the case at hand will evict the affected item from the second-level cache.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The following picture shows the design of this approach:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/cache_invalidation_architecture.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Architecture Overview&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While this doesn’t come with the scalability and fault tolerance provided by Apache Kafka,
      it nicely fits the given requirements.
      As the second-level cache is bound to the application lifecycle, there is for instance no need for the offset management and restarting capabilities provided by the Kafka Connect framework.
      For the given use case it is enough to receive data change events while the application is running, and using the embedded engine enables exactly that.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;admonitionblock note&quot;&gt;
      &lt;table&gt;
      &lt;tr&gt;
      &lt;td class=&quot;icon&quot;&gt;
      &lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot;&gt;&lt;/i&gt;
      &lt;/td&gt;
      &lt;td class=&quot;content&quot;&gt;
      &lt;div class=&quot;title&quot;&gt;Clustered Applications&lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note that it still might make sense to use Apache Kafka and the regular deployment of Debezium into Kafka Connect when running a clustered application where each node has a local cache.
      Instead of registering a connector on each node, Kafka and Connect would allow you to deploy a single connector instance and have the application nodes listen to the topic(s) with the change events.
      This would result in less resource utilization in the database.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/td&gt;
      &lt;/tr&gt;
      &lt;/table&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Having added the dependencies of the Debezium embedded engine (&lt;em&gt;io.debezium:debezium-embedded:0.9.0.Beta1&lt;/em&gt;) and the Debezium Postgres connector (&lt;em&gt;io.debezium:debezium-connector-postgres:0.9.0.Beta1&lt;/em&gt;) to your project,
      a class &lt;code&gt;DatabaseChangeEventListener&lt;/code&gt; for listening to any changes in the database can be implemented like this:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
      public class DatabaseChangeEventListener {
      
          @Resource
          private ManagedExecutorService executorService;
      
          @PersistenceUnit private EntityManagerFactory emf;
      
          @PersistenceContext
          private EntityManager em;
      
          private EmbeddedEngine engine;
      
          public void startEmbeddedEngine(@Observes @Initialized(ApplicationScoped.class) Object init) {
              Configuration config = Configuration.empty()
                      .withSystemProperties(Function.identity()).edit()
                      .with(EmbeddedEngine.CONNECTOR_CLASS, PostgresConnector.class)
                      .with(EmbeddedEngine.ENGINE_NAME, &quot;cache-invalidation-engine&quot;)
                      .with(EmbeddedEngine.OFFSET_STORAGE, MemoryOffsetBackingStore.class)
                      .with(&quot;name&quot;, &quot;cache-invalidation-connector&quot;)
                      .with(&quot;database.hostname&quot;, &quot;postgres&quot;)
                      .with(&quot;database.port&quot;, 5432)
                      .with(&quot;database.user&quot;, &quot;postgresuser&quot;)
                      .with(&quot;database.password&quot;, &quot;postgrespw&quot;)
                      .with(&quot;database.server.name&quot;, &quot;dbserver1&quot;)
                      .with(&quot;database.dbname&quot;, &quot;inventory&quot;)
                      .with(&quot;database.whitelist&quot;, &quot;public&quot;)
                      .with(&quot;snapshot.mode&quot;, &quot;never&quot;)
                      .build();
      
              this.engine = EmbeddedEngine.create()
                      .using(config)
                      .notifying(this::handleDbChangeEvent)
                      .build();
      
              executorService.execute(engine);
          }
      
          @PreDestroy
          public void shutdownEngine() {
              engine.stop();
          }
      
          private void handleDbChangeEvent(SourceRecord record) {
              if (record.topic().equals(&quot;dbserver1.public.item&quot;)) {
                  Long itemId = ((Struct) record.key()).getInt64(&quot;id&quot;);
                  Struct payload = (Struct) record.value();
                  Operation op = Operation.forCode(payload.getString(&quot;op&quot;));
      
                  if (op == Operation.UPDATE || op == Operation.DELETE) {
                      emf.getCache().evict(Item.class, itemId);
                  }
              }
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Upon application start-up, this configures an instance of the &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql/&quot;&gt;Debezium Postgres connector&lt;/a&gt; and sets up the embedded engine for running the connector.
      The &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql/#connector-properties&quot;&gt;connector options&lt;/a&gt; (host name, credentials etc.) are mostly the same as when deploying the connector into Kafka Connect.
      There is no need for doing an initial snapshot of the existing data, hence the &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql/#snapshots&quot;&gt;snapshot mode&lt;/a&gt; is set to &quot;never&quot;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The offset storage option is used for controlling how connector offsets should be persisted.
      As it’s not necessary to process any change events occurring while the connector is not running
      (instead you’d just begin to read the log from the current location after the restart),
      the in-memory implementation provided by Kafka Connect is used.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once configured, the embedded engine must be run via an &lt;code&gt;Executor&lt;/code&gt; instance.
      As the example runs in WildFly, a managed executor can simply be obtained through &lt;code&gt;@Resource&lt;/code&gt; injection for that purpose (see &lt;a href=&quot;https://www.jcp.org/en/jsr/detail?id=236&quot;&gt;JSR 236&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The embedded engine is configured to invoke the &lt;code&gt;handleDbChangeEvent()&lt;/code&gt; method for each received data change event.
      In this method it first is checked whether the incoming event originates from the &lt;code&gt;item&lt;/code&gt; table.
      If that’s the case, and if the change event represents an &lt;code&gt;UPDATE&lt;/code&gt; or &lt;code&gt;DELETE&lt;/code&gt; statement,
      the affected &lt;code&gt;Item&lt;/code&gt; instance is evicted from the second-level cache.
      JPA 2.0 provides a &lt;a href=&quot;https://javaee.github.io/javaee-spec/javadocs/index.html?javax/persistence/Cache.html&quot;&gt;simple API&lt;/a&gt; for this purpose which is accessible via the &lt;code&gt;EntityManagerFactory&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With the &lt;code&gt;DatabaseChangeEventListener&lt;/code&gt; class in place, the cache entry will now automatically be evicted when doing another item update via &lt;em&gt;psql&lt;/em&gt;.
      When placing the first purchase order for that item after the update, you’ll see in the application log how Hibernate ORM executes a query &lt;code&gt;SELECT ... FROM item ...&lt;/code&gt; in order to load the item referenced by the order.
      Also the cache statistics will report one &quot;L2C miss&quot;.
      Upon subsequent orders of that same item it will be obtained from the cache again.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;admonitionblock note&quot;&gt;
      &lt;table&gt;
      &lt;tr&gt;
      &lt;td class=&quot;icon&quot;&gt;
      &lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot;&gt;&lt;/i&gt;
      &lt;/td&gt;
      &lt;td class=&quot;content&quot;&gt;
      &lt;div class=&quot;title&quot;&gt;Eventual Consistency&lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While the event handling happens in near-realtime, it’s important to point out that it still applies eventual consistency semantics.
      This means that there is a very short time window between the point in time where a transaction is committed
      and the point in time where the change event is streamed from the log to the event handler and the cache entry is invalidated.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/td&gt;
      &lt;/tr&gt;
      &lt;/table&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;avoiding_cache_invalidations_after_application_triggered_data_changes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#avoiding_cache_invalidations_after_application_triggered_data_changes&quot;&gt;&lt;/a&gt;Avoiding Cache Invalidations After Application-triggered Data Changes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The change event listener shown above satisfies the requirement of invalidating cached items after external data changes.
      But in its current form it is evicting cache items a bit too aggressively:
      cached items will also be purged when updating an &lt;code&gt;Item&lt;/code&gt; instance through the application itself.
      This is not only not needed (as the cached item already is the current version), but it’s even counter-productive:
      the superfluous cache evictions will cause additional database roundtrips, resulting in longer response times.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It is therefore necessary to distinguish between data changes performed by the application itself and external data changes.
      Only in the latter case the affected items should be evicted from the cache.
      In order to do so, you can leverage the fact that each Debezium data change event contains the id of the originating transaction.
      Keeping track of all transactions run by the application itself allows to trigger the cache eviction only for those items altered by external transactions.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Accounting for this change, the overall architecture looks like so:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/cache_invalidation_architecture_tx_registry.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Architecture Overview with Transaction Registry&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The first thing to implement is the transaction registry, i.e. a class for the transaction book keeping:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
      public class KnownTransactions {
      
          private final DefaultCacheManager cacheManager;
          private final Cache&amp;lt;Long, Boolean&amp;gt; applicationTransactions;
      
          public KnownTransactions() {
              cacheManager = new DefaultCacheManager();
              cacheManager.defineConfiguration(
                      &quot;tx-id-cache&quot;,
                      new ConfigurationBuilder()
                          .expiration()
                              .lifespan(60, TimeUnit.SECONDS)
                          .build()
                      );
      
              applicationTransactions = cacheManager.getCache(&quot;tx-id-cache&quot;);
          }
      
          @PreDestroy
          public void stopCacheManager() {
              cacheManager.stop();
          }
      
          public void register(long txId) {
              applicationTransactions.put(txId, true);
          }
      
          public boolean isKnown(long txId) {
              return Boolean.TRUE.equals(applicationTransactions.get(txId));
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This uses the Infinispan &lt;code&gt;DefaultCacheManager&lt;/code&gt; for creating and maintaining an in-memory cache of transaction ids encountered by the application.
      As data change events arrive in near-realtime, the TTL of the cache entries can be rather short
      (in fact, the value of one minute shown in the example is chosen very conservatively, usually events should be received within seconds).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The next step is to retrieve the current transaction id whenever a request is processed by the application and register it within &lt;code&gt;KnownTransactions&lt;/code&gt;.
      This should happen once per transaction.
      There are multiple ways for implementing this logic; in the following a Hibernate ORM &lt;code&gt;FlushEventListener&lt;/code&gt; is used for this purpose:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;class TransactionRegistrationListener implements FlushEventListener {
      
          private volatile KnownTransactions knownTransactions;
      
          public TransactionRegistrationListener() {
          }
      
          @Override
          public void onFlush(FlushEvent event) throws HibernateException {
              event.getSession().getActionQueue().registerProcess( session -&amp;gt; {
                  Number txId = (Number) event.getSession().createNativeQuery(&quot;SELECT txid_current()&quot;)
                          .setFlushMode(FlushMode.MANUAL)
                          .getSingleResult();
      
                  getKnownTransactions().register(txId.longValue());
              } );
          }
      
          private  KnownTransactions getKnownTransactions() {
              KnownTransactions value = knownTransactions;
      
              if (value == null) {
                  knownTransactions = value = CDI.current().select(KnownTransactions.class).get();
              }
      
              return value;
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As there’s no portable way to obtain the transaction id, this is done using a native SQL query.
      In the case of Postgres, the &lt;code&gt;txid_current()&lt;/code&gt; function can be called for that.
      Hibernate ORM event listeners are not subject to dependency injection via CDI.
      Hence the static &lt;code&gt;current()&lt;/code&gt; method is used to obtain a handle to the application’s CDI container and get a reference to the &lt;code&gt;KnownTransactions&lt;/code&gt; bean.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This listener will be invoked whenever Hibernate ORM is synchronizing its persistence context with the database (&quot;flushing&quot;),
      which usually happens exactly once when the transaction is committed.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;admonitionblock note&quot;&gt;
      &lt;table&gt;
      &lt;tr&gt;
      &lt;td class=&quot;icon&quot;&gt;
      &lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot;&gt;&lt;/i&gt;
      &lt;/td&gt;
      &lt;td class=&quot;content&quot;&gt;
      &lt;div class=&quot;title&quot;&gt;Manual Flushes&lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The session / entity manager can also be flushed manually, in which case the &lt;code&gt;txid_current()&lt;/code&gt; function would be invoked multiple times.
      That’s neglected here for the sake of simplicity.
      The actual code in the example repo contains a slightly extended version of this class which makes sure that the transaction id is obtained only once.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/td&gt;
      &lt;/tr&gt;
      &lt;/table&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To register the flush listener with Hibernate ORM, an &lt;code&gt;Integrator&lt;/code&gt; implementation must be created and declared in the &lt;em&gt;META-INF/services/org.hibernate.integrator.spi.Integrator&lt;/em&gt; file:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public class TransactionRegistrationIntegrator implements Integrator {
      
          @Override
          public void integrate(Metadata metadata, SessionFactoryImplementor sessionFactory,
                  SessionFactoryServiceRegistry serviceRegistry) {
              serviceRegistry.getService(EventListenerRegistry.class)
                  .appendListeners(EventType.FLUSH, new TransactionRegistrationListener());
          }
      
          @Override
          public void disintegrate(SessionFactoryImplementor sessionFactory,
                  SessionFactoryServiceRegistry serviceRegistry) {
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;io.debezium.examples.cacheinvalidation.persistence.TransactionRegistrationIntegrator&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;During bootstrap, Hibernate ORM will detect the integrator class (by means of the &lt;a href=&quot;https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/ServiceLoader.html&quot;&gt;Java service loader&lt;/a&gt;),
      invoke its &lt;code&gt;integrate()&lt;/code&gt; method which in turn will register the listener class for the &lt;code&gt;FLUSH&lt;/code&gt; event.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The last step is to exclude any events stemming from transactions run by the application itself in the database change event handler:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
      public class DatabaseChangeEventListener {
      
          // ...
      
          @Inject
          private KnownTransactions knownTransactions;
      
          private void handleDbChangeEvent(SourceRecord record) {
              if (record.topic().equals(&quot;dbserver1.public.item&quot;)) {
                  Long itemId = ((Struct) record.key()).getInt64(&quot;id&quot;);
                  Struct payload = (Struct) record.value();
                  Operation op = Operation.forCode(payload.getString(&quot;op&quot;));
                  Long txId = ((Struct) payload.get(&quot;source&quot;)).getInt64(&quot;txId&quot;);
      
                  if (!knownTransactions.isKnown(txId) &amp;amp;&amp;amp;
                          (op == Operation.UPDATE || op == Operation.DELETE)) {
                      emf.getCache().evict(Item.class, itemId);
                  }
              }
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;And with that, you got all the pieces in place: cached &lt;code&gt;Item&lt;/code&gt;s will only be evicted after external data changes, but not after changes done by the application itself.
      To confirm, you can invoke the example’s &lt;code&gt;items&lt;/code&gt; resource using curl:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&amp;gt; curl -H &quot;Content-Type: application/json&quot; \
        -X PUT \
        --data '{ &quot;description&quot; : &quot;North by Northwest&quot;, &quot;price&quot; : 20.99}' \
        http://localhost:8080/cache-invalidation/rest/items/10003&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When placing the next order for the item after this update, you should see that the &lt;code&gt;Item&lt;/code&gt; entity is obtained from the cache,
      i.e. the change event will not have caused the item’s cache entry to be evicted.
      In contrast, if you update the item’s price via &lt;em&gt;psql&lt;/em&gt; another time,
      the item should be removed from the cache and the order request will produce a cache miss, followed by a &lt;code&gt;SELECT&lt;/code&gt; against the &lt;code&gt;item&lt;/code&gt; table in the database.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot;&gt;&lt;/a&gt;Summary&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In this blog post we’ve explored how Debezium and change data capture can be employed to invalidate application-level caches after external data changes.
      Compared to manual cache invalidation, this approach works very reliably
      (by capturing changes directly from the database log, no events will be missed) and fast
      (cache eviction happens in near-realtime after the data changes).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As you have seen, not too much glue code is needed in order to implement this.
      While the shown implementation is somewhat specific to the entities of the example,
      it should be possible to implement the change event handler in a more generic fashion,
      so that it can handle a set of configured entity types
      (essentially, the database change listener would have to convert the primary key field(s) from the change events into the primary key type of the corresponding entities in a generic way).
      Also such generic implementation would have to provide the logic for obtaining the current transaction id for the most commonly used databases.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please let us know whether you think this would be an interesting extension to have for Debezium and Hibernate ORM.
      For instance this could be a new module under the Debezium umbrella,
      and it could also be a very great project to work on, should you be interested in contributing to Debezium.
      If you got any thoughts on this idea, please post a comment below or come to our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Many thanks to Guillaume Smet, Hans-Peter Grahsl and Jiri Pechanec for their feedback while writing this post!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/11/22/debezium-0-9-0-beta1-released/</id>
    <title>Debezium 0.9.0.Beta1 Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-11-22T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/11/22/debezium-0-9-0-beta1-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="sqlserver"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s my pleasure to announce the release of Debezium 0.9.0.Beta1!
      Oh, and to those of you who are celebrating it&#8201;&#8212;&#8201;Happy Thanksgiving!
      
      
      This new Debezium release comes with several great improvements to our work-in-progress SQL Server connector:
      
      
      
      
      Initial snapshots can be done using the snapshot isolation level if enabled in the DB (DBZ-941)
      
      
      Changes to the structures of captured tables after the connector has been set up are supported now (DBZ-812)
      
      
      New connector option decimal.handling.mode (DBZ-953) and pass-through of any database.* option to the JDBC driver (DBZ-964)
      
      
      
      
      Besides that, we spent some time on supporting the latest versions of the different databases.
      The Debezium connectors now support Postgres...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.9.0.Beta1&lt;/strong&gt;!
      Oh, and to those of you who are celebrating it — Happy Thanksgiving!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This new Debezium release comes with several great improvements to our work-in-progress SQL Server connector:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Initial snapshots can be done using the &lt;code&gt;snapshot&lt;/code&gt; isolation level if enabled in the DB (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-941&quot;&gt;DBZ-941&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Changes to the structures of captured tables after the connector has been set up are supported now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-812&quot;&gt;DBZ-812&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;New connector option &lt;code&gt;decimal.handling.mode&lt;/code&gt; (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-953&quot;&gt;DBZ-953&lt;/a&gt;) and pass-through of any &lt;code&gt;database.*&lt;/code&gt; option to the JDBC driver (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-964&quot;&gt;DBZ-964&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Besides that, we spent some time on supporting the latest versions of the different databases.
      The Debezium connectors now support Postgres 11 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-955&quot;&gt;DBZ-955&lt;/a&gt;) and MongoDB 4.0 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-974&quot;&gt;DBZ-974&lt;/a&gt;).
      We are also working on supporting MySQL 8.0, which should be completed in the next 0.9.x release.
      The Debezium container images have been updated to Kafka 2.0.1 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-979&quot;&gt;DBZ-979&lt;/a&gt;)
      and the Kafka Connect image now supports the &lt;code&gt;STATUS_STORAGE_TOPIC&lt;/code&gt; environment variable,
      bringing consistency with &lt;code&gt;CONFIG_STORAGE_TOPIC&lt;/code&gt; and &lt;code&gt;OFFSET_STORAGE_TOPIC&lt;/code&gt; that already were supported before (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-893&quot;&gt;DBZ-893&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As usual, several bugs were fixed, too.
      Several of them dealt with the new Antlr-based DDL parser for the MySQL connector.
      By now we feel confident about its implementation, so it’s the default DDL parser as of this release (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-757&quot;&gt;DBZ-757&lt;/a&gt;).
      If you would like to continue to use the legacy parser for some reason, you can do so by setting the &lt;code&gt;ddl.parser.mode&lt;/code&gt; connector option to &quot;legacy&quot;.
      This implementation will remain available in the lifetime of Debezium 0.9.x and is scheduled for removal after that.
      So please make sure to log issues in JIRA should you run into any problems with the Antlr parser.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Overall, this release contains &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-9-0-beta1&quot;&gt;21 fixes&lt;/a&gt;.
      Thanks a lot to all the community members who helped with making this happen:
      &lt;a href=&quot;https://github.com/anton-martynov&quot;&gt;Anton Martynov&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/deepakbarr&quot;&gt;Deepak Barr&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/grzegorz8&quot;&gt;Grzegorz Kołakowski&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/olavim&quot;&gt;Olavi Mustanoja&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/vamossagar12&quot;&gt;Sagar Rao&lt;/a&gt; and
      &lt;a href=&quot;https://github.com/shivamsharma&quot;&gt;Shivam Sharma&lt;/a&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_else&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_else&quot;&gt;&lt;/a&gt;What else?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While the work towards Debezium 0.9 continues, we’ve lately been quite busy with presenting Debezium at multiple conferences.
      You can find the slides and recordings from &lt;a href=&quot;https://kafka-summit.org/sessions/change-data-streaming-patterns-microservices-debezium/&quot;&gt;Kafka Summit&lt;/a&gt; San Francisco and &lt;a href=&quot;https://vxdms2018.confinabox.com/talk/INI-9172/Data_Streaming_for_Microservices_using_Debezium&quot;&gt;Voxxed Days Microservices&lt;/a&gt; on our list of &lt;a href=&quot;http://debezium.io/docs/online-resources/&quot;&gt;online resources&lt;/a&gt; around Debezium.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;There you also can find the links to the slides of the great talk &quot;The Why’s and How’s of Database Streaming&quot; by Joy Gao of WePay, a Debezium user of the first hour,
      as well as the link to a blog post by Hans-Peter Grahsl about setting up a CDC pipeline from MySQL into Cosmos DB running on Azure.
      If you know about other great articles, session recordings or similar on Debezium and change data capture which should be added there, please let us know.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/10/04/debezium-0-9-0-alpha2-released/</id>
    <title>Debezium 0.9.0.Alpha2 Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-10-04T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/10/04/debezium-0-9-0-alpha2-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="sqlserver"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s my pleasure to announce the release of Debezium 0.9.0.Alpha2!
      
      
      While the work on the connectors for SQL Server and Oracle continues, we decided to do another Alpha release,
      as lots of fixes and new features - many of them contributed by community members - have piled up,
      which we wanted to get into your hands as quickly as possible.
      
      
      This release supports Apache Kafka 2.0, comes with support for Postgres' HSTORE column type, allows to rename and filter fields from change data messages for MongoDB
      and contains multiple bug fixes and performance improvements.
      Overall, this release contains 55 fixes
      (note that a few of these have...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.9.0.Alpha2&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While the work on the connectors for SQL Server and Oracle continues, we decided to do another Alpha release,
      as lots of fixes and new features - many of them contributed by community members - have piled up,
      which we wanted to get into your hands as quickly as possible.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release supports Apache Kafka 2.0, comes with support for Postgres' HSTORE column type, allows to rename and filter fields from change data messages for MongoDB
      and contains multiple bug fixes and performance improvements.
      Overall, this release contains &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-9-0-alpha2&quot;&gt;55 fixes&lt;/a&gt;
      (note that a few of these have been merged back to 0.8.x and are contained in earlier 0.8 releases, too).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A big &quot;Thank You&quot; is in order to community members
      &lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/artiship&quot;&gt;Artiship Artiship&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/CliffWheadon&quot;&gt;Cliff Wheadon&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/deepakbarr&quot;&gt;Deepak Barr&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/ian-axelrod&quot;&gt;Ian Axelrod&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/ooooorz&quot;&gt;Liu Hanlin&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/maver1ck&quot;&gt;Maciej Bryński&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/oripwk&quot;&gt;Ori Popowski&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/PengLyu&quot;&gt;Peng Lyu&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/PSanetra&quot;&gt;Philip Sanetra&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/sagarrao&quot;&gt;Sagar Rao&lt;/a&gt; and
      &lt;a href=&quot;https://github.com/SyedMuhammadSufyian&quot;&gt;Syed Muhammad Sufyian&lt;/a&gt;
      for their contributions to this release.
      We salute you!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;kafka_upgrade&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#kafka_upgrade&quot;&gt;&lt;/a&gt;Kafka Upgrade&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium runs with and has been tested on top of the recently released Apache Kafka 2.0 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-858&quot;&gt;DBZ-858&lt;/a&gt;).
      The widely used version Kafka 1.x continues to be supported as well.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note that 0.10.x is not supported due to Debezium’s usage of the admin client API which is only available in later versions.
      It shouldn’t be too hard to work around this, so if someone is interested in helping out with this,
      this would be a great contribution (see &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-883&quot;&gt;DBZ-883&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;support_for_hstore_columns_in_postgres&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#support_for_hstore_columns_in_postgres&quot;&gt;&lt;/a&gt;Support for HSTORE columns in Postgres&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Postgres is an amazingly powerful and flexible RDBMS, not the least due to its wide range of column types which go far beyond what’s defined by the SQL standard.
      One of these types being &lt;a href=&quot;https://www.postgresql.org/docs/current/static/hstore.html&quot;&gt;HSTORE&lt;/a&gt;, which is a string-to-string map essentially.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium can capture changes to columns of this type now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-898&quot;&gt;DBZ-898&lt;/a&gt;).
      By default, the field values will be represented using Kafka Connect’s map data type.
      As this may not be supported by all sink connectors,
      you might alternatively represent them as a string-ified JSON by setting the new &lt;code&gt;hstore.handling.mode&lt;/code&gt; connector option to &lt;code&gt;json&lt;/code&gt;.
      In this case, you’d see HSTORE columns represented as values in change messages like so: &lt;code&gt;{ &quot;key1&quot; : &quot;val1&quot;, &quot;key2&quot; : &quot;val2&quot; }&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;field_filtering_and_renaming_for_mongodb&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#field_filtering_and_renaming_for_mongodb&quot;&gt;&lt;/a&gt;Field filtering and renaming for MongoDB&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Unlike the connectors for MySQL and Postgres, the Debezium MongoDB connector so far didn’t allow to exclude single fields of captured collections from CDC messages.
      Also renaming them wasn’t supported e.g. by means of Kafka’s &lt;code&gt;ReplaceField&lt;/code&gt; SMT.
      The reason being that MongoDB doesn’t mandate a fixed schema for the documents of a given collection,
      and documents therefore are represented in change messages using a single string-ified JSON field.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to the fantastic work of community member Andrey Pustovetov,
      this finally has changed, i.e. you can remove given fields (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-633&quot;&gt;DBZ-633&lt;/a&gt;) now from the CDC messages of given collections or have them renamed (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-881&quot;&gt;DBZ-881&lt;/a&gt;).
      Please refer to the description of the new connector options &lt;code&gt;field.blacklist&lt;/code&gt; and &lt;code&gt;field.renames&lt;/code&gt; in the &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/&quot;&gt;MongoDB connector documentation&lt;/a&gt; to learn more.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;extended_source_info&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#extended_source_info&quot;&gt;&lt;/a&gt;Extended source info&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Another contribution by Andrey is the new optional &lt;code&gt;connector&lt;/code&gt; field within the source info block of CDC messages
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-918&quot;&gt;DBZ-918&lt;/a&gt;).
      This tells the type of source connector that produced the messages (&quot;mysql&quot;, &quot;postgres&quot; etc.),
      which can come in handy in cases where specific semantics need to be applied on the consumer side depending on the type of source database.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;bug_fixes_and_version_upgrades&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bug_fixes_and_version_upgrades&quot;&gt;&lt;/a&gt;Bug fixes and version upgrades&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The new release contains a good number of bug fixes and other smaller improvements.
      Amongst them are&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;correct handling of invalid temporal default values with MySQL (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-927&quot;&gt;DBZ-927&lt;/a&gt;),&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;support for table/collection names with special characters for MySQL (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-878&quot;&gt;DBZ-878&lt;/a&gt;) and MongoDB (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-865&quot;&gt;DBZ-865&lt;/a&gt;) and&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;fixed handling of blacklisted tables with the new Antlr-based DDL parser (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-872&quot;&gt;DBZ-872&lt;/a&gt;).&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Community member Ian Axelrod provided a fix for a potential performance issue,
      where changes to tables with TOAST columns in Postgres would cause repeated updates to the connector’s internal schema metadata,
      which can be a costly operation (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-911&quot;&gt;DBZ-911&lt;/a&gt;).
      Please refer to the &lt;a href=&quot;http://debezium.io/docs/connectors/postgres/&quot;&gt;Postgres connector documentation&lt;/a&gt; for details on the new &lt;code&gt;schema.refresh.mode&lt;/code&gt; option,
      which deals with this issue.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In terms of version upgrades we migrated to the latest releases of the MySQL (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-763&quot;&gt;DBZ-763&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-764&quot;&gt;DBZ-764&lt;/a&gt;) and Postgres drivers (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-912&quot;&gt;DBZ-912&lt;/a&gt;).
      The former is part of a longer stream of work leading towards support of MySQL 8 which should be finished in one of the next Debezium releases.
      For Postgres we provide a Docker image with Debezium’s supported logical decoding plug-ins based on Alpine now,
      which might be interesting to those concerned about container size (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-705&quot;&gt;DBZ-705&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the change log for the complete list of fixed issues.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The work towards Debezium 0.9 continues, and we’ll focus mostly on improvements to the SQL Server and Oracle connectors.
      Other potential topics include support for MySQL 8 and native logical decoding as introduced with Postgres 10,
      which should greatly help with using the Debezium Postgres connectors in cloud environments such as Amazon RDS.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll also be talking about Debezium at the following conferences:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://kafka-summit.org/sessions/change-data-streaming-patterns-microservices-debezium/&quot;&gt;Kafka Summit&lt;/a&gt;; San Francisco, Cal.; Oct. 17&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://vxdms2018.confinabox.com/talk/INI-9172/Data_Streaming_for_Microservices_using_Debezium&quot;&gt;VoxxedDays Microservices&lt;/a&gt;; Paris, France; Oct. 29 - 31&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://cfp.devoxx.ma/2018/talk/AEY-4477/Change_Data_Streaming_Patterns_for_Microservices_With_Debezium&quot;&gt;Devoxx Morocco&lt;/a&gt;; Marrakesh, Morocco; Nov. 27 - 29&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Already last week I had the opportunity to present Debezium at &lt;a href=&quot;https://jug-saxony-day.org/programm/#!/P31&quot;&gt;JUG Saxony Day&lt;/a&gt;.
      If you are interested, you can find the (German) &lt;a href=&quot;https://speakerdeck.com/gunnarmorling/streaming-von-datenbankanderungen-mit-debezium-jug-saxony-day&quot;&gt;slideset of that talk&lt;/a&gt; on Speaker Deck.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/09/20/materializing-aggregate-views-with-hibernate-and-debezium/</id>
    <title>Materializing Aggregate Views With Hibernate and Debezium</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-09-20T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/09/20/materializing-aggregate-views-with-hibernate-and-debezium/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="discussion"></category>
    <category term="examples"></category>
    <summary>
      
      
      
      Updating external full text search indexes (e.g. Elasticsearch) after data changes is a very popular use case for change data capture (CDC).
      
      
      As we&#8217;ve discussed in a blog post a while ago,
      the combination of Debezium&#8217;s CDC source connectors and Confluent&#8217;s sink connector for Elasticsearch makes it straight forward to capture data changes in MySQL, Postgres etc. and push them towards Elastisearch in near real-time.
      This results in a 1:1 relationship between tables in the source database and a corresponding search index in Elasticsearch,
      which is perfectly fine for many use cases.
      
      
      It gets more challenging though if you&#8217;d like to put entire aggregates into...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Updating external full text search indexes (e.g. &lt;a href=&quot;https://www.elastic.co/products/elasticsearch&quot;&gt;Elasticsearch&lt;/a&gt;) after data changes is a very popular use case for change data capture (CDC).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As we’ve discussed in a &lt;a href=&quot;http://debezium.io/blog/2018/01/17/streaming-to-elasticsearch/&quot;&gt;blog post&lt;/a&gt; a while ago,
      the combination of Debezium’s CDC source connectors and Confluent’s &lt;a href=&quot;https://docs.confluent.io/current/connect/connect-elasticsearch/docs/index.html&quot;&gt;sink connector for Elasticsearch&lt;/a&gt; makes it straight forward to capture data changes in MySQL, Postgres etc. and push them towards Elastisearch in near real-time.
      This results in a 1:1 relationship between tables in the source database and a corresponding search index in Elasticsearch,
      which is perfectly fine for many use cases.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It gets more challenging though if you’d like to put entire aggregates into a single index.
      An example could be a customer and all their addresses;
      those would typically be stored in two separate tables in an RDBMS, linked by a foreign key,
      whereas you’d like to have just one index in Elasticsearch,
      containing documents of customers with their addresses embedded,
      allowing you to efficiently search for customers based on their address.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Following up to the &lt;a href=&quot;http://debezium.io/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/&quot;&gt;KStreams-based solution&lt;/a&gt; to this we described recently,
      we’d like to present in this post an alternative for materializing such aggregate views driven by the application layer.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;overview&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The idea is to materialize views in a separate table in the source database,
      right in the moment the original data is altered.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Aggregates are serialized as JSON structures (which naturally can represent any nested object structure) and stored in a specific table.
      This is done within the actual transaction altering the data,
      which means the aggregate view is always consistent with the primary data.
      In particular this approach isn’t prone to exposing intermediary aggregations as the KStreams-based solution discussed in the post linked above.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The following picture shows the overall architecture:&lt;/p&gt;
      &lt;/div&gt;
      &lt;img src=&quot;http://debezium.io/images/jpa_aggregations.png&quot; style=&quot;max-width:100%; margin-bottom:10px;&quot; class=&quot;responsive-image&quot; alt=&quot;Streaming Materialized Aggregate Views to Elastisearch&quot; /&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Here the aggregate views are materialized by means of a small extension to &lt;a href=&quot;http://hibernate.org/orm/&quot;&gt;Hibernate ORM&lt;/a&gt;,
      which stores the JSON aggregates within the source database
      (note &quot;aggregate views&quot; can be considered conceptually the same as &quot;materialized views&quot; as known from different RDBMS,
      as in that they materialize the result of a &quot;join&quot; operation,
      but technically we’re not using the latter to store aggregate views, but a regular table).
      Changes to that aggregate table are then captured by Debezium and streamed to one topic per aggregate type.
      The Elasticsearch sink connector can subscribe to these topics and update corresponding full-text indexes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You can find a proof-of-concept implementation (said Hibernate extension and related code) of this idea in our &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/jpa-aggregations&quot;&gt;examples repository&lt;/a&gt;.
      Of course the general idea isn’t limited to Hibernate ORM or JPA,
      you could implement something similar with any other API you’re using to access your data.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;creating_aggregate_views_via_hibernate_orm&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#creating_aggregate_views_via_hibernate_orm&quot;&gt;&lt;/a&gt;Creating Aggregate Views via Hibernate ORM&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For the following let’s assume we’re persisting a simple domain model
      (comprising a &lt;code&gt;Customer&lt;/code&gt; entity and a few related ones like &lt;code&gt;Address&lt;/code&gt;, (customer) &lt;code&gt;Category&lt;/code&gt; etc.) in a database.
      Using Hibernate for that allows us to make the creation of aggregates fully transparent to the actual application code using a &lt;a href=&quot;http://docs.jboss.org/hibernate/orm/current/userguide/html_single/Hibernate_User_Guide.html#events-events&quot;&gt;Hibernate event listener&lt;/a&gt;.
      Thanks to its extensible architecture, we can plug such listener into Hibernate just by adding it to the classpath,
      from where it will be picked up automatically when bootstrapping the entity manager / session factory.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Our example listener reacts to an annotation, &lt;code&gt;@MaterializeAggregate&lt;/code&gt;,
      which marks those entity types that should be the roots of materialized aggregates.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Entity
      @MaterializeAggregate(aggregateName=&quot;customers-complete&quot;)
      public class Customer {
      
          @Id
          private long id;
      
          private String firstName;
      
          @OneToMany(mappedBy = &quot;customer&quot;, fetch = FetchType.EAGER, cascade = CascadeType.ALL)
          private Set&amp;lt;Address&amp;gt; addresses;
      
          @ManyToOne
          private Category category;
      
          ...
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now if any entity annotated with &lt;code&gt;@MaterializeAggregate&lt;/code&gt; is inserted, updated or deleted via Hibernate,
      the listener will kick in and materialize a JSON view of the aggregate root (customer) and its associated entities (addresses, category).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Under the hood the &lt;a href=&quot;https://github.com/FasterXML/jackson&quot;&gt;Jackson API&lt;/a&gt; is used for serializing the model into JSON.
      This means you can use any of its annotations to customize the JSON output, e.g. &lt;code&gt;@JsonIgnore&lt;/code&gt;  to exclude the inverse relationship from &lt;code&gt;Address&lt;/code&gt; to &lt;code&gt;Customer&lt;/code&gt;:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Entity
      public class Address {
      
          @Id
          private long id;
      
          @ManyToOne
          @JoinColumn(name = &quot;customer_id&quot;)
          @JsonIgnore
          private Customer customer;
      
          private String street;
      
          private String city;
      
          ...
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note that &lt;code&gt;Address&lt;/code&gt; itself isn’t marked with &lt;code&gt;@MaterializeAggregate&lt;/code&gt;, i.e. it won’t be materialized into an aggregate view by itself.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;After using JPA’s &lt;code&gt;EntityManager&lt;/code&gt; to insert or update a few customers,
      let’s take a look at the &lt;code&gt;aggregates&lt;/code&gt; table which has been populated by the listener
      (value schema omitted for the sake of brevity):&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;&amp;gt; select * from aggregates;
      
      | rootType | keySchema | rootId | materialization | valueSchema |
      
      | customers-complete
      
      | {
        &quot;schema&quot; : {
          &quot;type&quot; : &quot;struct&quot;,
          &quot;fields&quot; : [ {
            &quot;type&quot; : &quot;int64&quot;,
            &quot;optional&quot; : false,
            &quot;field&quot; : &quot;id&quot;
          } ],
          &quot;optional&quot; : false,
          &quot;name&quot; : &quot;customers-complete.Key&quot;
        }
      }
      
      | { &quot;id&quot; : 1004 }
      
      | { &quot;schema&quot; : { ... } }
      
      | {
        &quot;id&quot; : 1004,
        &quot;firstName&quot; : &quot;Anne&quot;,
        &quot;lastName&quot; : &quot;Kretchmar&quot;,
        &quot;email&quot; : &quot;annek@noanswer.org&quot;,
        &quot;tags&quot; : [ &quot;long-term&quot;, &quot;vip&quot; ],
        &quot;birthday&quot; : 5098,
        &quot;category&quot; : {
          &quot;id&quot; : 100001,
          &quot;name&quot; : &quot;Retail&quot;
        },
        &quot;addresses&quot; : [ {
          &quot;id&quot; : 16,
          &quot;street&quot; : &quot;1289 University Hill Road&quot;,
          &quot;city&quot; : &quot;Canehill&quot;,
          &quot;state&quot; : &quot;Arkansas&quot;,
          &quot;zip&quot; : &quot;72717&quot;,
          &quot;type&quot; : &quot;SHIPPING&quot;
        } ]
      } |&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The table contains these columns:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;rootType&lt;/code&gt;: The name of the aggregate as given in the &lt;code&gt;@MaterializeAggregate&lt;/code&gt; annotation&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;rootId&lt;/code&gt;: The aggregate’s id as serialized JSON&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;materialization&lt;/code&gt;: The aggregate itself as serialized JSON; in this case a customer and their addresses, category etc.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;keySchema&lt;/code&gt;: The Kafka Connect schema of the row’s key&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;valueSchema&lt;/code&gt;: The Kafka Connect schema of the materialization&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s talk about the two schema columns for a bit.
      JSON itself is quite limited as far as its supported data types are concerned.
      So for instance we’d loose information about a numeric field’s value range (int vs. long etc.) without any additional information.
      Therefore the listener derives the corresponding schema information for key and aggregate view from the entity model and stores it within the aggregate records.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now Jackson itself only supports JSON Schema, which would be a bit too limited for our purposes.
      Hence the example implementation provides custom serializers for Jackson’s schema system,
      which allow us to emit Kafka Connect’s schema representation (with more precise type information) instead of plain JSON Schema.
      This will come in handy in the following when we’d like to expand the string-based JSON representations of key and value into properly typed Kafka Connect records.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;capturing_changes_to_the_aggregate_table&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#capturing_changes_to_the_aggregate_table&quot;&gt;&lt;/a&gt;Capturing Changes to the Aggregate Table&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We now have a mechanism in place which transparently persists aggregates into a separate table within the source database,
      whenever the application data is changed through Hibernate.
      Note that this happens within the boundaries of the source transaction,
      so if the same would be rolled back for some reason, also the aggregate view would not be updated.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Hibernate listener uses insert-or-update semantics when writing an aggregate view,
      i.e. for a given aggregate root there’ll always be exactly one corresponding entry in the aggregate table which reflects its current state.
      If an aggregate root entity is deleted, the listener will also drop the entry from the aggregate table.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So let’s set up Debezium now to capture any changes to the &lt;code&gt;aggregates&lt;/code&gt; table:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;curl -i -X POST \
        -H &quot;Accept:application/json&quot; \
        -H &quot;Content-Type:application/json&quot; \
        http://localhost:8083/connectors/ -d @- &amp;lt;&amp;lt;-EOF
        {
            &quot;name&quot;: &quot;inventory-connector&quot;,
            &quot;config&quot;: {
                &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
                &quot;tasks.max&quot;: &quot;1&quot;,
                &quot;database.hostname&quot;: &quot;mysql&quot;,
                &quot;database.port&quot;: &quot;3306&quot;,
                &quot;database.user&quot;: &quot;debezium&quot;,
                &quot;database.password&quot;: &quot;dbz&quot;,
                &quot;database.server.id&quot;: &quot;184054&quot;,
                &quot;database.server.name&quot;: &quot;dbserver1&quot;,
                &quot;database.whitelist&quot;: &quot;inventory&quot;,
                &quot;table.whitelist&quot;: &quot;.*aggregates&quot;,
                &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
                &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot;
            }
        }
      EOF&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This registers the MySQL connector with the &quot;inventory&quot; database
      (we’re using an expanded version of the schema from the &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;Debezium tutorial&lt;/a&gt;),
      capturing any changes to the &quot;aggregates&quot; table.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;expanding_json&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#expanding_json&quot;&gt;&lt;/a&gt;Expanding JSON&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If we now were to browse the corresponding Kafka topic, we’d see data change events in the known Debezium format for all the changes to the &lt;code&gt;aggregates&lt;/code&gt; table.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &quot;materialization&quot; field with the records' &quot;after&quot; state still is a single field containing a JSON string, though.
      What we’d rather like to have is a strongly typed Kafka Connect record, whose schema exactly describes the aggregate structure and the types of its fields.
      For that purpose the example project provides an SMT (single message transform) which takes the JSON materialization and the corresponding &lt;code&gt;valueSchema&lt;/code&gt; and converts this into a full-blown Kafka Connect record.
      The same is done for keys.
      DELETE events are rewritten into tombstone events.
      Finally, the SMT re-routes every record to a topic named after the aggregate root,
      allowing consumers to subscribe just to changes to specific aggregate types.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So let’s add that SMT when registering the Debezium CDC connector:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;...
      &quot;transforms&quot;:&quot;expandjson&quot;,
      &quot;transforms.expandjson.type&quot;:&quot;io.debezium.aggregation.smt.ExpandJsonSmt&quot;,
      ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When now browsing the &quot;customers-complete&quot; topic, we’ll see the strongly typed Kafka Connect records we’d expect:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
          &quot;schema&quot;: {
              &quot;type&quot;: &quot;struct&quot;,
              &quot;fields&quot;: [
                  {
                      &quot;type&quot;: &quot;int64&quot;,
                      &quot;optional&quot;: false,
                      &quot;field&quot;: &quot;id&quot;
                  }
              ],
              &quot;optional&quot;: false,
              &quot;name&quot;: &quot;customers-complete.Key&quot;
          },
          &quot;payload&quot;: {
              &quot;id&quot;: 1004
          }
      }
      {
          &quot;schema&quot;: {
              &quot;type&quot;: &quot;struct&quot;,
              &quot;fields&quot;: [ ... ],
              &quot;optional&quot;: true,
              &quot;name&quot;: &quot;urn:jsonschema:com:example:domain:Customer&quot;
          },
          &quot;payload&quot;: {
              &quot;id&quot;: 1004,
              &quot;firstName&quot;: &quot;Anne&quot;,
              &quot;lastName&quot;: &quot;Kretchmar&quot;,
              &quot;email&quot;: &quot;annek@noanswer.org&quot;,
              &quot;active&quot;: true,
              &quot;tags&quot; : [ &quot;long-term&quot;, &quot;vip&quot; ],
              &quot;birthday&quot; : 5098,
              &quot;category&quot;: {
                  &quot;id&quot;: 100001,
                  &quot;name&quot;: &quot;Retail&quot;
              },
              &quot;addresses&quot;: [
                  {
                      &quot;id&quot;: 16,
                      &quot;street&quot;: &quot;1289 University Hill Road&quot;,
                      &quot;city&quot;: &quot;Canehill&quot;,
                      &quot;state&quot;: &quot;Arkansas&quot;,
                      &quot;zip&quot;: &quot;72717&quot;,
                      &quot;type&quot;: &quot;LIVING&quot;
                  }
              ]
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To confirm that these are actual typed Kafka Connect records and not just a single JSON string field,
      you could for instance use the &lt;a href=&quot;http://debezium.io/docs/configuration/avro/&quot;&gt;Avro message converter&lt;/a&gt; and examine the message schemas in the schema registry.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;sinking_aggregate_messages_into_elasticsearch&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#sinking_aggregate_messages_into_elasticsearch&quot;&gt;&lt;/a&gt;Sinking Aggregate Messages Into Elasticsearch&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The last missing step is to register the Confluent Elasticsearch sink connector, hooking it up with the &quot;customers-complete&quot; topic and letting it push any changes to the corresponding index:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;curl -i -X POST \
        -H &quot;Accept:application/json&quot; \
        -H &quot;Content-Type:application/json&quot; \
        http://localhost:8083/connectors/ -d @- &amp;lt;&amp;lt;-EOF
        {
            &quot;name&quot;: &quot;es-customers&quot;,
            &quot;config&quot;: {
                &quot;connector.class&quot;: &quot;io.confluent.connect.elasticsearch.ElasticsearchSinkConnector&quot;,
                &quot;tasks.max&quot;: &quot;1&quot;,
                &quot;topics&quot;: &quot;customers-complete&quot;,
                &quot;connection.url&quot;: &quot;http://elastic:9200&quot;,
                &quot;key.ignore&quot;: &quot;false&quot;,
                &quot;schema.ignore&quot; : &quot;false&quot;,
                &quot;behavior.on.null.values&quot; : &quot;delete&quot;,
                &quot;type.name&quot;: &quot;customer-with-addresses&quot;,
                &quot;transforms&quot; : &quot;key&quot;,
                &quot;transforms.key.type&quot;: &quot;org.apache.kafka.connect.transforms.ExtractField$Key&quot;,
                &quot;transforms.key.field&quot;: &quot;id&quot;
            }
        }
      EOF&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This uses Connect’s &lt;code&gt;ExtractField&lt;/code&gt; transformation to obtain just the actual id value from the key struct and use it as key for the corresponding Elasticsearch documents.
      Specifying the &quot;behavior.on.null.values&quot; option will let the connector delete the corresponding document from the index when encountering a tombstone message (i.e. a message with a key but without value).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, we can use the Elasticsearch REST API to browse the index and of course use its powerful full-text query language to find customers by the address or any other property embedded into the aggregate structure:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&amp;gt; curl -X GET -H &quot;Accept:application/json&quot; \
        http://localhost:9200/customers-complete/_search?pretty
      
        {
            &quot;_shards&quot;: {
                &quot;failed&quot;: 0,
                &quot;successful&quot;: 5,
                &quot;total&quot;: 5
            },
            &quot;hits&quot;: {
                &quot;hits&quot;: [
                    {
                        &quot;_id&quot;: &quot;1004&quot;,
                        &quot;_index&quot;: &quot;customers-complete&quot;,
                        &quot;_score&quot;: 1.0,
                        &quot;_source&quot;: {
                            &quot;active&quot;: true,
                            &quot;addresses&quot;: [
                                {
                                    &quot;city&quot;: &quot;Canehill&quot;,
                                    &quot;id&quot;: 16,
                                    &quot;state&quot;: &quot;Arkansas&quot;,
                                    &quot;street&quot;: &quot;1289 University Hill Road&quot;,
                                    &quot;type&quot;: &quot;LIVING&quot;,
                                    &quot;zip&quot;: &quot;72717&quot;
                                }
                            ],
                            &quot;tags&quot; : [ &quot;long-term&quot;, &quot;vip&quot; ],
                            &quot;birthday&quot; : 5098,
                            &quot;category&quot;: {
                                &quot;id&quot;: 100001,
                                &quot;name&quot;: &quot;Retail&quot;
                            },
                            &quot;email&quot;: &quot;annek@noanswer.org&quot;,
                            &quot;firstName&quot;: &quot;Anne&quot;,
                            &quot;id&quot;: 1004,
                            &quot;lastName&quot;: &quot;Kretchmar&quot;,
                            &quot;scores&quot;: [],
                            &quot;someBlob&quot;: null,
                            &quot;tags&quot;: []
                        },
                        &quot;_type&quot;: &quot;customer-with-addresses&quot;
                    }
                ],
                &quot;max_score&quot;: 1.0,
                &quot;total&quot;: 1
            },
            &quot;timed_out&quot;: false,
            &quot;took&quot;: 11
        }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;And there you have it: a customer’s complete data, including their addresses, categories, tags etc., materialized into a single document within Elasticsearch.
      If you’re using JPA to update the customer, you’ll see the data in the index being updated accordingly in near-realtime.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;pros_and_cons&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#pros_and_cons&quot;&gt;&lt;/a&gt;Pros and Cons&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So what are the advantages and disadvantages of this approach for materializing aggregates from multiple source tables compared to the &lt;a href=&quot;http://debezium.io/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/&quot;&gt;KStreams-based approach&lt;/a&gt;?&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The big advantage is consistency and awareness of transactional boundaries,
      whereas the KStreams-based solution in its suggested form was prone to exposing intermediary aggregates.
      For instance, if you’re storing a customer and three addresses, it might happen that the streaming query first creates an aggregation of the customer and the two addresses inserted first, and shortly thereafter the complete aggregate with all three addresses.
      This not the case for the approach discussed here, as you’ll only ever stream complete aggregates to Kafka.
      Also this approach feels a bit more &quot;light-weight&quot;, i.e. a simple marker annotation (together with some Jackson annotations for fine-tuning the emitted JSON structures) is enough in order to materialize aggregates from your domain model,
      whereas some more effort was needed to set up the required streams, temporary tables etc. with the KStreams solution.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The downside of driving aggregations through the application layer is that it’s not fully agnostic to the way you access the primary data.
      If you bypass the application, e.g. by patching data directly in the database, naturally these updates would be missed, requiring a refresh of affected aggregates.
      Although this again could be done through change data capture and Debezium:
      change events to source tables could be captured and consumed by the application itself, allowing it to re-materialize aggregates after external data changes.
      You also might argue that running JSON serializations within source transactions and storing aggregates within the source database represents some overhead.
      This often may be acceptable, though.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Another question to ask is what’s the advantage of using change data capture on an intermediary aggregate table over simply posting REST requests to Elasticsearch.
      The answer is the highly increased robustness and fault tolerance.
      If the Elasticsearch cluster can’t be accessed for some reason, the machinery of Kafka and Kafka Connect will ensure that any change events will be propagated eventually, once the sink is up again.
      Also other consumers than Elasticsearch can subscribe to the aggregate topic, the log can be replayed from the beginning etc.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note that while we’ve been talking primarily about using Elasticsearch as a data sink, there are also other datastores and connectors that support complexly structured records.
      One example would be MongoDB and the &lt;a href=&quot;https://github.com/hpgrahsl/kafka-connect-mongodb&quot;&gt;sink connector&lt;/a&gt; maintained by Hans-Peter Grahsl,
      which one could use to sink customer aggregates into MongoDB, for instance enabling efficient retrieval of a customer and all their associated data with a single primary key look-up.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;outlook&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#outlook&quot;&gt;&lt;/a&gt;Outlook&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Hibernate ORM extension as well as the SMT discussed in this post can be found in our &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/jpa-aggregations&quot;&gt;examples repository&lt;/a&gt;.
      They should be considered to be at &quot;proof-of-concept&quot; level currently.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;That being said, we’re considering to make this a Debezium component proper,
      allowing you to employ this aggregation approach within your Hibernate-based applications just by pulling in this new component.
      For that we’d have to improve a few things first, though.
      Most importantly, an API is needed which will let you (re-)create aggregates on demand,
      e.g. for existing data or for data updated by bulk updates via the Criteria API / JPQL (which will be missed by listeners).
      Also aggregates should be re-created automatically, if any of the referenced entities change
      (with the current PoC, only a change to the customer instance itself will trigger its aggregate view to be rebuilt, but not a change to one of its addresses).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you like this idea, then let us know about it,
      so we can gauge the general interest in this.
      Also, this would be a great item to work on, if you’re interested in contributing to the Debezium project.
      Looking forward to hearing from you, e.g. in the comment section below or on our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks a lot to Hans-Peter Grahsl for his feedback on an earlier version of this post!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/09/19/debezium-0-8-3-final-released/</id>
    <title>Debezium 0.8.3.Final Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-09-19T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/09/19/debezium-0-8-3-final-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      As temperatures are cooling off, the Debezium team is getting into full swing again and we&#8217;re happy to announce the release of Debezium 0.8.3.Final!
      
      
      This is a bugfix release to the current stable release line of Debezium, 0.8.x, while the work on Debezium 0.9 goes on in parallel.
      There are 14 fixes in this release.
      As in earlier 0.8.x releases, we&#8217;ve further improved the new Antlr-based DDL parser used by the MySQL connector (see DBZ-901, DBZ-903 and DBZ-910).
      
      
      The Postgres connector saw a huge improvement to its start-up time for databases with lots of custom types (DBZ-899).
      The user reporting this issue had nearly 200K...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As temperatures are cooling off, the Debezium team is getting into full swing again and we’re happy to announce the release of Debezium &lt;strong&gt;0.8.3.Final&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This is a bugfix release to the current stable release line of Debezium, 0.8.x, while the work on Debezium 0.9 goes on in parallel.
      There are &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-8-3-final&quot;&gt;14 fixes&lt;/a&gt; in this release.
      As in earlier 0.8.x releases, we’ve further improved the new Antlr-based DDL parser used by the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL connector&lt;/a&gt; (see &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-901&quot;&gt;DBZ-901&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-903&quot;&gt;DBZ-903&lt;/a&gt; and &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-910&quot;&gt;DBZ-910&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;a href=&quot;http://debezium.io/docs/connectors/postgres/&quot;&gt;Postgres connector&lt;/a&gt; saw a huge improvement to its start-up time for databases with lots of custom types (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-899&quot;&gt;DBZ-899&lt;/a&gt;).
      The user reporting this issue had nearly 200K entries in pg_catalog.pg_type, and due to an N + 1 SELECT issue within the Postgres driver itself, this caused the connector to take 24 minutes to start.
      By using a custom query for obtaining the type metadata, we were able to cut down this time to 5 seconds!
      Right now we’re working with the maintainers of the Postgres driver to get this issue fixed upstream, too.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;more_flexible_propagation_of_deletes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#more_flexible_propagation_of_deletes&quot;&gt;&lt;/a&gt;More Flexible Propagation of DELETEs&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Besides those bug fixes we decided to also merge one new feature from the 0.9.x branch into the 0.8.3.Final release,
      which those of you may find useful who are using the &lt;a href=&quot;http://debezium.io/docs/configuration/event-flattening/&quot;&gt;SMT for extracting the &quot;after&quot; state&lt;/a&gt; from change events (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-857&quot;&gt;DBZ-857&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This SMT can be employed to stream changes to sink connectors which expect just a &quot;flat&quot; row representation of data instead of Debezium’s complex event structure.
      Not all sink connectors support the handling of deletions, though.
      E.g. some connectors will fail when encountering tombstone events.
      Therefore the SMT can now optionally rewrite delete events into updates of a special &quot;deleted&quot; marker field.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For that, set the &lt;code&gt;delete.handling.mode&lt;/code&gt; option of the SMT to &quot;rewrite&quot;:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;...
      &quot;transforms&quot; : &quot;unwrap&quot;,
      &quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.UnwrapFromEnvelope&quot;,
      &quot;transforms.unwrap.delete.handling.mode&quot; : &quot;rewrite&quot;,
      ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When a DELETE event is propagated, the &quot;__deleted&quot; field of outgoing records will be set to true.
      So when for instance consuming the events with the JDBC sink connector, you’d see this being reflected in a corresponding column in the sink tables:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;__deleted | last_name |  id  | first_name |         email
      -----------+-----------+------+------------+-----------------------
      false     | Thomas    | 1001 | Sally      | sally.thomas@acme.com
      false     | Bailey    | 1002 | George     | gbailey@foobar.com
      false     | Kretchmar | 1004 | Anne       | annek@noanswer.org
      true      | Walker    | 1003 | Edward     | ed@walker.com&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You then for instance can use a batch job running on your sink to remove all records flagged as deleted.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re continuing the work on Debezium 0.9, which will mostly be about improvements to the SQL Server and Oracle connectors.
      The current plan is to do the next 0.9 release (either Alpha2 or Beta1) in two weeks from now.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also it’s the beginning of the conference season, so we’ll spend some time with preparing demos and presenting Debezium at multiple locations.
      There will be sessions on change data capture with Debezium a these conferences:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://jug-saxony-day.org/programm/#!/P31&quot;&gt;JUG Saxony Day&lt;/a&gt;; Dresden, Germany; Sept. 28&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://kafka-summit.org/sessions/change-data-streaming-patterns-microservices-debezium/&quot;&gt;Kafka Summit&lt;/a&gt;; San Francisco, Cal.; Oct. 17&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://vxdms2018.confinabox.com/talk/INI-9172/Data_Streaming_for_Microservices_using_Debezium&quot;&gt;VoxxedDays Microservices&lt;/a&gt;; Paris, France; Oct. 29 - 31&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://cfp.devoxx.ma/2018/talk/AEY-4477/Change_Data_Streaming_Patterns_for_Microservices_With_Debezium&quot;&gt;Devoxx Morocco&lt;/a&gt;; Marrakesh, Morocco; Nov. 27 - 29&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you are at any of these conferences, come and say Hi;
      we’d love to exchange with you about your use cases, feature requests, feedback on our &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;roadmap&lt;/a&gt; and any other ideas around Debezium.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, a big &quot;Thank You&quot; goes to our fantastic community members &lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;, &lt;a href=&quot;https://github.com/maver1ck&quot;&gt;Maciej Bryński&lt;/a&gt; and &lt;a href=&quot;https://github.com/PengLyu&quot;&gt;Peng Lyu&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/08/30/streaming-mysql-data-changes-into-kinesis/</id>
    <title>Streaming MySQL Data Changes to Amazon Kinesis</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-08-30T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/08/30/streaming-mysql-data-changes-into-kinesis/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="discussion"></category>
    <category term="examples"></category>
    <summary>
      
      
      
      Most of the times Debezium is used to stream data changes into Apache Kafka.
      What though if you&#8217;re using another streaming platform such as Apache Pulsar or a cloud-based solution such as Amazon Kinesis, Azure Event Hubs and the like?
      Can you still benefit from Debezium&#8217;s powerful change data capture (CDC) capabilities  and ingest changes from databases such as MySQL, Postgres, SQL Server etc.?
      
      
      Turns out, with just a bit of glue code, you can!
      In the following we&#8217;ll discuss how to use Debezium to capture changes in a MySQL database and stream the change events into Kinesis,
      a fully-managed data streaming service available...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Most of the times Debezium is used to stream data changes into &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Apache Kafka&lt;/a&gt;.
      What though if you’re using another streaming platform such as &lt;a href=&quot;https://pulsar.incubator.apache.org/&quot;&gt;Apache Pulsar&lt;/a&gt; or a cloud-based solution such as &lt;a href=&quot;https://aws.amazon.com/kinesis/&quot;&gt;Amazon Kinesis&lt;/a&gt;, &lt;a href=&quot;https://azure.microsoft.com/services/event-hubs/&quot;&gt;Azure Event Hubs&lt;/a&gt; and the like?
      Can you still benefit from Debezium’s powerful change data capture (CDC) capabilities  and ingest changes from databases such as MySQL, Postgres, SQL Server etc.?&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Turns out, with just a bit of glue code, you can!
      In the following we’ll discuss how to use Debezium to capture changes in a MySQL database and stream the change events into Kinesis,
      a fully-managed data streaming service available on the Amazon cloud.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;introducing_the_debezium_embedded_engine&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#introducing_the_debezium_embedded_engine&quot;&gt;&lt;/a&gt;Introducing the Debezium Embedded Engine&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is implemented as a set of connectors for Kafka and thus usually is run via &lt;a href=&quot;https://kafka.apache.org/documentation/#connect&quot;&gt;Kafka Connect&lt;/a&gt;.
      But there’s one little gem in Debezium which isn’t as widely known yet, which is the &lt;a href=&quot;http://debezium.io/docs/embedded/&quot;&gt;embedded engine&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When using this engine, the Debezium connectors are not executed within Kafka Connect, but as a library embedded into your own Java application.
      For this purpose, the &lt;em&gt;debezium-embedded&lt;/em&gt; module provides a small runtime environment which performs the tasks that’d otherwise be handled by the Kafka Connect framework:
      requesting change records from the connector, committing offsets etc.
      Each change record produced by the connector is passed to a configured event handler method,
      which in our case will convert the record into its JSON representation and submit it to a Kinesis stream, using the Kinesis Java API.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The overall architecture looks like so:&lt;/p&gt;
      &lt;/div&gt;
      &lt;img src=&quot;http://debezium.io/images/debezium-embedded.png&quot; style=&quot;max-width:100%; margin-bottom:10px;&quot; class=&quot;responsive-image&quot; alt=&quot;Debezium Embedded Engine Streaming to Amazon Kinesis&quot; /&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now let’s walk through the relevant parts of the code required for that.
      A complete executable example can be found in the &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/kinesis&quot;&gt;debezium-examples&lt;/a&gt; repo on GitHub.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;set_up&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#set_up&quot;&gt;&lt;/a&gt;Set-Up&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In order to use Debezium’s embedded engine, add the &lt;em&gt;debezium-embedded&lt;/em&gt; dependency as well as the Debezium connector of your choice to your project’s &lt;em&gt;pom.xml&lt;/em&gt;.
      In the following we’re going to use the connector for MySQL.
      We also need to add a dependency to the &lt;a href=&quot;https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/kinesis/package-summary.html&quot;&gt;Kinesis Client API&lt;/a&gt;, so these are the dependencies needed:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;...
      &amp;lt;dependency&amp;gt;
          &amp;lt;groupId&amp;gt;io.debezium&amp;lt;/groupId&amp;gt;
          &amp;lt;artifactId&amp;gt;debezium-embedded&amp;lt;/artifactId&amp;gt;
          &amp;lt;version&amp;gt;0.8.3.Final&amp;lt;/version&amp;gt;
      &amp;lt;/dependency&amp;gt;
      &amp;lt;dependency&amp;gt;
          &amp;lt;groupId&amp;gt;io.debezium&amp;lt;/groupId&amp;gt;
          &amp;lt;artifactId&amp;gt;debezium-connector-mysql&amp;lt;/artifactId&amp;gt;
          &amp;lt;version&amp;gt;0.8.3.Final&amp;lt;/version&amp;gt;
      &amp;lt;/dependency&amp;gt;
      &amp;lt;dependency&amp;gt;
          &amp;lt;groupId&amp;gt;com.amazonaws&amp;lt;/groupId&amp;gt;
          &amp;lt;artifactId&amp;gt;amazon-kinesis-client&amp;lt;/artifactId&amp;gt;
          &amp;lt;version&amp;gt;1.9.0&amp;lt;/version&amp;gt;
      &amp;lt;/dependency&amp;gt;
      ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;configuring_the_embedded_engine&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#configuring_the_embedded_engine&quot;&gt;&lt;/a&gt;Configuring the Embedded Engine&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium embedded engine is configured through an instance of &lt;code&gt;io.debezium.config.Configuration&lt;/code&gt;.
      This class can obtain values from system properties or from a given config file,
      but for the sake of the example we’ll simply pass all required values via its fluent builder API:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;Configuration config = Configuration.create()
          .with(EmbeddedEngine.CONNECTOR_CLASS, &quot;io.debezium.connector.mysql.MySqlConnector&quot;)
          .with(EmbeddedEngine.ENGINE_NAME, &quot;kinesis&quot;)
          .with(MySqlConnectorConfig.SERVER_NAME, &quot;kinesis&quot;)
          .with(MySqlConnectorConfig.SERVER_ID, 8192)
          .with(MySqlConnectorConfig.HOSTNAME, &quot;localhost&quot;)
          .with(MySqlConnectorConfig.PORT, 3306)
          .with(MySqlConnectorConfig.USER, &quot;debezium&quot;)
          .with(MySqlConnectorConfig.PASSWORD, &quot;dbz&quot;)
          .with(MySqlConnectorConfig.DATABASE_WHITELIST, &quot;inventory&quot;)
          .with(MySqlConnectorConfig.TABLE_WHITELIST, &quot;inventory.customers&quot;)
          .with(EmbeddedEngine.OFFSET_STORAGE,
              &quot;org.apache.kafka.connect.storage.MemoryOffsetBackingStore&quot;)
          .with(MySqlConnectorConfig.DATABASE_HISTORY,
              MemoryDatabaseHistory.class.getName())
          .with(&quot;schemas.enable&quot;, false)
          .build();&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’ve ever set up the Debezium MySQL connector in Kafka Connect, most of the properties will look familiar to you.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;But let’s talk about the &lt;code&gt;OFFSET_STORAGE&lt;/code&gt; and &lt;code&gt;DATABASE_HISTORY&lt;/code&gt; options in a bit more detail.
      They deal with how connector offsets and the database history should be persisted.
      When running the connector via Kafka Connect, both would typically be stored in specific Kafka topics.
      But that’s not an option here, so an alternative is needed.
      For this example we’re simply going to keep the offsets and database history in memory.
      I.e. if the engine is restarted, this information will be lost and the connector will start from scratch, e.g. with a new initial snapshot.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While out of scope for this blog post, it wouldn’t be too difficult to create alternative implementations of the &lt;code&gt;OffsetBackingStore&lt;/code&gt; and &lt;code&gt;DatabaseHistory&lt;/code&gt; contracts, respectively.
      For instance if you’re fully committed into the AWS cloud services, you could think of storing offsets and database history in the DynamoDB NoSQL store.
      Note that, different from Kafka, a Kinesis stream wouldn’t be suitable for storing the database history.
      The reason being, that the maximum retention period for Kinesis data streams is seven days, whereas the database history must be kept for the entire lifetime of the connector.
      Another alternative could be to use the existing filesystem based implementations &lt;code&gt;FileOffsetBackingStore&lt;/code&gt; and &lt;code&gt;FileDatabaseHistory&lt;/code&gt;, respectively.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The next step is to build an &lt;code&gt;EmbeddedEngine&lt;/code&gt; instance from the configuration.
      Again this is done using a fluent API:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;EmbeddedEngine engine = EmbeddedEngine.create()
          .using(config)
          .using(this.getClass().getClassLoader())
          .using(Clock.SYSTEM)
          .notifying(this::sendRecord)
          .build();&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The most interesting part here is the &lt;code&gt;notifying&lt;/code&gt; call.
      The method passed here is the one which will be invoked by the engine for each emitted data change record.
      So let’s take a look at the implementation of this method.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;sending_change_records_to_kinesis&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#sending_change_records_to_kinesis&quot;&gt;&lt;/a&gt;Sending Change Records to Kinesis&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;code&gt;sendRecord()&lt;/code&gt; method is where the magic happens.
      We’ll convert the incoming &lt;code&gt;SourceRecord&lt;/code&gt; into an equivalent JSON representation and propagate it to a Kinesis stream.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For that, it’s important to understand some conceptual differences between Apache Kafka and Kinesis.
      Specifically, messages in Kafka have a &lt;em&gt;key&lt;/em&gt; and a &lt;em&gt;value&lt;/em&gt; (which both are arbitrary byte arrays).
      In case of Debezium, the key of data change events represents the primary key of the affected record and the value is a structure comprising of old and new row state as well as some additional metadata.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In Kinesis on the other hand a message contains a &lt;em&gt;data blob&lt;/em&gt; (again an arbitrary byte sequence) and a &lt;em&gt;partition key&lt;/em&gt;.
      Kinesis streams can be split up into multiple shards and the partition key is used to determine into which shard a given message should go.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now one could think of mapping the key from Debezium’s change data events to the Kinesis partition key,
      but partition keys are limited to a length of 256 bytes.
      Depending on the length of primary key column(s) in the captured tables, this might not be enough.
      So a safer option is to create a hash value from the change message key and use that as the partition key.
      This in turn means that the change message key structure should be added next to the actual value to the Kinesis message’s data blob.
      While the key column values themselves are part of the value structure, too, a consumer otherwise wouldn’t know which column(s) make up the primary key.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With that in mind, let’s take a look at the &lt;code&gt;sendRecord()&lt;/code&gt; implementation:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;private void sendRecord(SourceRecord record) {
          // We are interested only in data events not schema change events
          if (record.topic().equals(&quot;kinesis&quot;)) {
              return;
          }
      
          // create schema for container with key *and* value
          Schema schema = SchemaBuilder.struct()
              .field(&quot;key&quot;, record.keySchema())
              .field(&quot;value&quot;, record.valueSchema())
              .build();
      
          Struct message = new Struct(schema);
          message.put(&quot;key&quot;, record.key());
          message.put(&quot;value&quot;, record.value());
      
          // create partition key by hashing the record's key
          String partitionKey = String.valueOf(
              record.key() != null ? record.key().hashCode() : -1);
      
          // create data blob representing the container by using Kafka Connect's
          // JSON converter
          final byte[] payload = valueConverter.fromConnectData(
              &quot;dummy&quot;, schema, message);
      
          // Assemble the put-record request ...
          PutRecordRequest putRecord = new PutRecordRequest();
      
          putRecord.setStreamName(record.topic());
          putRecord.setPartitionKey(partitionKey);
          putRecord.setData(ByteBuffer.wrap(payload));
      
          // ... and execute it
          kinesisClient.putRecord(putRecord);
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The code is quite straight-forward; as discussed above it’s first creating a container structure containing key &lt;em&gt;and&lt;/em&gt; value of the incoming source record.
      This structure then is converted into a binary representation using the JSON converter provided by Kafka Connect (an instance of &lt;code&gt;JsonConverter&lt;/code&gt;).
      Then a &lt;code&gt;PutRecordRequest&lt;/code&gt; is assembled from that blob, the partition key and the change record’s topic name, which finally is sent to Kinesis.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Kinesis client object can be re-used and is set up once like so:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;// Uses the credentials from the local &quot;default&quot; AWS profile
      AWSCredentialsProvider credentialsProvider =
          new ProfileCredentialsProvider(&quot;default&quot;);
      
      this.kinesisClient = AmazonKinesisClientBuilder.standard()
          .withCredentials(credentialsProvider)
          .withRegion(&quot;eu-central-1&quot;) // use your AWS region here
          .build();&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With that, we’ve set up an instance of Debezium’s &lt;code&gt;EmbeddedEngine&lt;/code&gt; which runs the configured MySQL connector and passes each emitted change event to Amazon Kinesis.
      The last missing step is to actually run the engine.
      This is done on a separate thread using an &lt;code&gt;Executor&lt;/code&gt;, e.g. like so:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;ExecutorService executor = Executors.newSingleThreadExecutor();
      executor.execute(engine);&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note you also should make sure to properly shut down the engine eventually.
      How that can be done &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/kinesis/src/main/java/io/debezium/examples/kinesis/ChangeDataSender.java#L83-L88&quot;&gt;is shown&lt;/a&gt; in the accompanying example in the &lt;em&gt;debezium-examples&lt;/em&gt; repo.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;running_the_example&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#running_the_example&quot;&gt;&lt;/a&gt;Running the Example&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally let’s take a look at running the complete example and consuming the Debezium CDC events from the Kinesis stream.
      Start by cloning the examples repository and go to the &lt;em&gt;kinesis&lt;/em&gt; directory:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;git clone https://github.com/debezium/debezium-examples.git
      cd debezium-examples/kinesis&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Make sure you’ve met the &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/kinesis#prerequisites&quot;&gt;prerequisites&lt;/a&gt; described in the example’s &lt;em&gt;README.md&lt;/em&gt;;
      most notably you should have a local Docker installation and you’ll need to have set up an AWS account as well as have the AWS client tools installed.
      Note that Kinesis isn’t part of the free tier when registering with AWS, i.e. you’ll pay a (small) amount of money when executing the example.
      Don’t forget to delete the streams you’ve set up once done, we won’t pay your AWS bills :)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now run Debezium’s MySQL example database to have some data to play with:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker run -it --rm --name mysql -p 3306:3306 \
        -e MYSQL_ROOT_PASSWORD=debezium \
        -e MYSQL_USER=mysqluser \
        -e MYSQL_PASSWORD=mysqlpw \
        debezium/example-mysql:0.8&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Create a Kinesis stream for change events from the &lt;code&gt;customers&lt;/code&gt; table:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;aws kinesis create-stream --stream-name kinesis.inventory.customers \
        --shard-count 1&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Execute the Java application that runs the Debezium embedded engine
      (if needed, adjust the value of the &lt;code&gt;kinesis.region&lt;/code&gt; property in &lt;em&gt;pom.xml&lt;/em&gt; to your own region first):&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;mvn exec:java&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This will start up the engine and the MySQL connector, which takes an initial snapshot of the captured database.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In order to take a look at the CDC events in the Kinesis stream, the AWS CLI can be used
      (usually, you’d implement a Kinesis Streams application for consuming the events).
      To do so, set up a &lt;a href=&quot;https://docs.aws.amazon.com/streams/latest/dev/developing-consumers-with-sdk.html#kinesis-using-sdk-java-get-data-shard-iterators&quot;&gt;shard iterator&lt;/a&gt; first:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;ITERATOR=$(aws kinesis get-shard-iterator --stream-name kinesis.inventory.customers --shard-id 0 --shard-iterator-type TRIM_HORIZON | jq '.ShardIterator')&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note how the &lt;a href=&quot;https://stedolan.github.io/jq/&quot;&gt;jq&lt;/a&gt; utility is used to obtain the generated id of the iterator from the JSON structure returned by the Kinesis API.
      Next that iterator can be used to examine the stream:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;aws kinesis get-records --shard-iterator $ITERATOR&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You should receive an array of records like this:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
          &quot;Records&quot;: [
              {
                  &quot;SequenceNumber&quot;:
                      &quot;49587760482547027816046765529422807492446419903410339842&quot;,
                  &quot;ApproximateArrivalTimestamp&quot;: 1535551896.475,
                  &quot;Data&quot;: &quot;eyJiZWZvcm...4OTI3MzN9&quot;,
                  &quot;PartitionKey&quot;: &quot;eyJpZCI6MTAwMX0=&quot;
              },
              ...
          ]
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;code&gt;Data&lt;/code&gt; element is a Base64-encoded representation of the message’s data blob.
      Again &lt;em&gt;jq&lt;/em&gt; comes in handy: we can use it to just extract the &lt;code&gt;Data&lt;/code&gt; part of each record and decode the Base64 representation
      (make sure to use jq 1.6 or newer):&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;aws kinesis get-records --shard-iterator $ITERATOR | \
        jq -r '.Records[].Data | @base64d' | jq .&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now you should see the change events as JSON, each one with key and value:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
        &quot;key&quot;: {
          &quot;id&quot;: 1001
        },
        &quot;value&quot;: {
          &quot;before&quot;: null,
          &quot;after&quot;: {
            &quot;id&quot;: 1001,
            &quot;first_name&quot;: &quot;Sally&quot;,
            &quot;last_name&quot;: &quot;Thomas&quot;,
            &quot;email&quot;: &quot;sally.thomas@acme.com&quot;
          },
          &quot;source&quot;: {
            &quot;version&quot;: &quot;0.8.1.Final&quot;,
            &quot;name&quot;: &quot;kinesis&quot;,
            &quot;server_id&quot;: 0,
            &quot;ts_sec&quot;: 0,
            &quot;gtid&quot;: null,
            &quot;file&quot;: &quot;mysql-bin.000003&quot;,
            &quot;pos&quot;: 154,
            &quot;row&quot;: 0,
            &quot;snapshot&quot;: true,
            &quot;thread&quot;: null,
            &quot;db&quot;: &quot;inventory&quot;,
            &quot;table&quot;: &quot;customers&quot;,
            &quot;query&quot;: null
          },
          &quot;op&quot;: &quot;c&quot;,
          &quot;ts_ms&quot;: 1535555325628
        }
      }
      ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Next let’s try and update a record in MySQL:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;# Start MySQL CLI client
      docker run -it --rm --name mysqlterm --link mysql --rm mysql:5.7 \
        sh -c 'exec mysql -h&quot;$MYSQL_PORT_3306_TCP_ADDR&quot; \
        -P&quot;$MYSQL_PORT_3306_TCP_PORT&quot; -uroot -p&quot;$MYSQL_ENV_MYSQL_ROOT_PASSWORD&quot;'
      
      # In the MySQL client
      use inventory;
      update customers set first_name = 'Trudy' where id = 1001;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you now fetch the iterator again, you should see one more data change event representing that update:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;...
      
      {
        &quot;key&quot;: {
          &quot;id&quot;: 1001
        },
        &quot;value&quot;: {
          &quot;before&quot;: {
            &quot;id&quot;: 1001,
            &quot;first_name&quot;: &quot;Sally&quot;,
            &quot;last_name&quot;: &quot;Thomas&quot;,
            &quot;email&quot;: &quot;sally.thomas@acme.com&quot;
          },
          &quot;after&quot;: {
            &quot;id&quot;: 1001,
            &quot;first_name&quot;: &quot;Trudy&quot;,
            &quot;last_name&quot;: &quot;Thomas&quot;,
            &quot;email&quot;: &quot;sally.thomas@acme.com&quot;
          },
          &quot;source&quot;: {
            &quot;version&quot;: &quot;0.8.1.Final&quot;,
            &quot;name&quot;: &quot;kinesis&quot;,
            &quot;server_id&quot;: 223344,
            &quot;ts_sec&quot;: 1535627629,
            &quot;gtid&quot;: null,
            &quot;file&quot;: &quot;mysql-bin.000003&quot;,
            &quot;pos&quot;: 364,
            &quot;row&quot;: 0,
            &quot;snapshot&quot;: false,
            &quot;thread&quot;: 10,
            &quot;db&quot;: &quot;inventory&quot;,
            &quot;table&quot;: &quot;customers&quot;,
            &quot;query&quot;: null
          },
          &quot;op&quot;: &quot;u&quot;,
          &quot;ts_ms&quot;: 1535627622546
        }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once you’re done, stop the embedded engine application by hitting Ctrl + C,
      stop the MySQL server by running &lt;code&gt;docker stop mysql&lt;/code&gt; and delete the &lt;em&gt;kinesis.inventory.customers&lt;/em&gt; stream in Kinesis.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;summary_and_outlook&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary_and_outlook&quot;&gt;&lt;/a&gt;Summary and Outlook&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In this blog post we’ve demonstrated that Debezium cannot only be used to stream data changes into Apache Kafka, but also into other streaming platforms such as Amazon Kinesis.
      Leveraging its embedded engine and by implementing a bit of glue code, you can benefit from &lt;a href=&quot;http://debezium.io/docs/connectors/&quot;&gt;all the CDC connectors&lt;/a&gt; provided by Debezium and their capabilities and connect them to the streaming solution of your choice.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;And we’re thinking about even further simplifying this usage of Debezium.
      Instead of requiring you to implement your own application that invokes the embedded engine API,
      we’re considering to provide a small self-contained Debezium runtime which you can simply execute.
      It’d be configured with the source connector to run and make use of an outbound plug-in SPI with ready-to-use implementations for Kinesis, Apache Pulsar and others.
      Of course such runtime would also provide suitable implementations for safely persisting offsets and database history,
      and it’d offer means of monitoring, health checks etc.
      Meaning you could connect the Debezium source connectors with your preferred streaming platform in a robust and reliable way, without any manual coding required!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you like this idea, then please check out JIRA issue &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-651&quot;&gt;DBZ-651&lt;/a&gt; and let us know about your thoughts,
      e.g. by leaving a comment on the issue, in the comment section below or on our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/08/30/debezium-0-8-2-released/</id>
    <title>Debezium 0.8.2 Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-08-30T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/08/30/debezium-0-8-2-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      The Debezium team is back from summer holidays and we&#8217;re happy to announce the release of Debezium 0.8.2!
      
      
      This is a bugfix release to the current stable release line of Debezium, 0.8.x, while the work on Debezium 0.9 is continuing.
      
      
      Note: By accident the version of the release artifacts is 0.8.2 instead of 0.8.2.Final.
      This is not in line with our recently established convention of always letting release versions end with qualifiers such as Alpha1, Beta1, CR1 or Final.
      The next version in the 0.8 line will be 0.8.3.Final and we&#8217;ll improve our release pipeline to make sure that this situation doesn&#8217;t occur again.
      
      
      The...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium team is back from summer holidays and we’re happy to announce the release of Debezium &lt;strong&gt;0.8.2&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This is a bugfix release to the current stable release line of Debezium, 0.8.x, while the work on Debezium 0.9 is continuing.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; By accident the version of the release artifacts is &lt;em&gt;0.8.2&lt;/em&gt; instead of &lt;em&gt;0.8.2.Final&lt;/em&gt;.
      This is not in line with our recently established convention of always letting release versions end with qualifiers such as &lt;em&gt;Alpha1&lt;/em&gt;, &lt;em&gt;Beta1&lt;/em&gt;, &lt;em&gt;CR1&lt;/em&gt; or &lt;em&gt;Final&lt;/em&gt;.
      The next version in the 0.8 line will be &lt;em&gt;0.8.3.Final&lt;/em&gt; and we’ll improve our release pipeline to make sure that this situation doesn’t occur again.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The 0.8.2 release contains &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-8-2&quot;&gt;10 fixes&lt;/a&gt; overall, most of them dealing with issues related to DDL parsing as done by the Debezium &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL connector&lt;/a&gt;.
      For instance, implicit non-nullable primary key columns will be handled correctly now using the new Antlr-based DDL parser (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-860&quot;&gt;DBZ-860&lt;/a&gt;).
      Also the &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/&quot;&gt;MongoDB connector&lt;/a&gt; saw a bug fix (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-838&quot;&gt;DBZ-838&lt;/a&gt;): initial snapshots will be interrupted now if the connector is requested to stop
      (e.g. when shutting down Kafka Connect).
      More a useful improvement rather than a bug fix is the &lt;a href=&quot;http://debezium.io/docs/connectors/postgres/&quot;&gt;Postgres connector’s&lt;/a&gt; capability to add the table, schema and database names to the &lt;code&gt;source&lt;/code&gt; block of emitted CDC events (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-866&quot;&gt;DBZ-866&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks a lot to community members &lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;, &lt;a href=&quot;https://github.com/CliffWheadon&quot;&gt;Cliff Wheadon&lt;/a&gt; and &lt;a href=&quot;https://github.com/oripwk&quot;&gt;Ori Popowski&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re continuing the work on Debezium 0.9, which will mostly be about improvements to the SQL Server and Oracle connectors.
      Both will get support for handling structural changes to captured tables while the connectors are running.
      Also the exploration of alternatives to using the XStream API for the Oracle connector continues.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, a recurring theme of our work is to further consolidate the code bases of the different connectors,
      which will allow us to roll out new and improved features more quickly across all the Debezium connectors.
      The recently added Oracle and SQL Server connectors already share a lot of code,
      and in the next step we’ve planned to move the existing Postgres connector to the new basis established for these two connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to learn more about some middle and long term ideas, please check out our &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;roadmap&lt;/a&gt;.
      Also please get in touch with us if you got any ideas or suggestions for future development.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/07/26/debezium-0-9-0-alpha1-released/</id>
    <title>Debezium 0.9 Alpha1 and 0.8.1 Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-07-26T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/07/26/debezium-0-9-0-alpha1-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="postgres"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <category term="sqlserver"></category>
    <summary>
      
      
      
      Just two weeks after the Debezium 0.8 release, I&#8217;m very happy to announce the release of Debezium 0.9.0.Alpha1!
      
      
      The main feature of the new version is a first work-in-progress version of the long-awaited Debezium connector for MS SQL Server.
      Based on the CDC functionality available in the Enterprise and Standard editions,
      the new connector lets you stream data changes out of Microsoft&#8217;s popular RDBMS.
      
      
      Besides that we&#8217;ve continued the work on the Debezium Oracle connector.
      Most notably, it supports initial snapshots of captured tables now.
      We&#8217;ve also upgraded Apache Kafka in our Docker images to 1.1.1 (DBZ-829).
      
      
      Please take a look at the change log for the...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Just two weeks after the Debezium 0.8 release, I’m very happy to announce the release of Debezium &lt;strong&gt;0.9.0.Alpha1&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The main feature of the new version is a first work-in-progress version of the long-awaited Debezium connector for &lt;a href=&quot;https://www.microsoft.com/en-us/sql-server&quot;&gt;MS SQL Server&lt;/a&gt;.
      Based on the &lt;a href=&quot;https://docs.microsoft.com/en-us/sql/relational-databases/track-changes/about-change-data-capture-sql-server?view=sql-server-2017&quot;&gt;CDC functionality&lt;/a&gt; available in the Enterprise and Standard editions,
      the new connector lets you stream data changes out of Microsoft’s popular RDBMS.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Besides that we’ve continued the work on the Debezium &lt;a href=&quot;http://debezium.io/docs/connectors/oracle/&quot;&gt;Oracle connector&lt;/a&gt;.
      Most notably, it supports initial snapshots of captured tables now.
      We’ve also upgraded Apache Kafka in our Docker images to 1.1.1 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-829&quot;&gt;DBZ-829&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please take a look at the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-9-0-alpha1&quot;&gt;change log&lt;/a&gt; for the complete list of changes in 0.9.0.Alpha1 and general upgrade notes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; &lt;span class=&quot;line-through&quot;&gt;At the time of writing (2018-07-26), the release artifacts (connector archives) are available on &lt;a href=&quot;http://central.maven.org/maven2/io/debezium/&quot;&gt;Maven Central&lt;/a&gt;.
      We’ll upload the Docker images for 0.9.0.Alpha1 to &lt;a href=&quot;https://hub.docker.com/u/debezium/&quot;&gt;Docker Hub&lt;/a&gt; as soon as possible.&lt;/span&gt;
      The Docker images are already uplodaded and ready for use under tags &lt;code&gt;0.9.0.Alpha1&lt;/code&gt; and rolling &lt;code&gt;0.9&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;sql_server_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#sql_server_connector&quot;&gt;&lt;/a&gt;SQL Server Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Support for SQL Server had been on the wish list of Debezium users for a long time (the original issue was &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-40&quot;&gt;DBZ-40&lt;/a&gt;).
      Thanks to lots of basic infrastructure created while working on the Oracle connector,
      we were finally able to come up with a first preview of this new connector in comparatively short time of development.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Just as the Oracle connector, the one for SQL Server is under active development and should be considered an incubating feature at this point.
      So for instance the structure of emitted change messages may change in upcoming releases.
      In terms of features, it supports initial snapshotting and capturing changes via SQL Server’s CDC functionality.
      There’s support for the most common column types, table whitelisting/blacklisting and more.
      The most significant feature missing is support for structural changes of tables while the connector is running.
      This is the next feature we’ll work on and it’s planned to be delivered as part of the next 0.9 release (see &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-812&quot;&gt;DBZ-812&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’d be very happy to learn about any feedback you may have on this newest connector of the Debezium family.
      If you spot any bugs or have feature requests for it, please create a report in our &lt;a href=&quot;https://issues.jboss.org/browse/DBZ&quot;&gt;JIRA tracker&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;oracle_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#oracle_connector&quot;&gt;&lt;/a&gt;Oracle Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium connector for Oracle is able to take initial snapshots now.
      By means of the new connector option &lt;code&gt;snapshot.mode&lt;/code&gt; you can control whether &lt;em&gt;read&lt;/em&gt; events for all the records of all the captured tables should be emitted.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In addition the support for numeric data types has been honed (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-804&quot;&gt;DBZ-804&lt;/a&gt;);
      any integer columns (i.e. &lt;code&gt;NUMBER&lt;/code&gt; with a scale &amp;lt;\= 0) will be emitted using the corresponding &lt;code&gt;int8&lt;/code&gt;/&lt;code&gt;int16&lt;/code&gt;/&lt;code&gt;int32&lt;/code&gt;/&lt;code&gt;int64&lt;/code&gt; field type,
      if the columns precision allows for that.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also spent some time on expanding the Oracle &lt;a href=&quot;http://debezium.io/docs/connectors/oracle/&quot;&gt;connector documentation&lt;/a&gt;,
      which covers the structure of emitted change events and all the data type mappings in detail now.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;debezium_0_8_1_final&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#debezium_0_8_1_final&quot;&gt;&lt;/a&gt;Debezium 0.8.1.Final&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Together with Debezium 0.9.0.Alpha1 we also did another release of the current stable Debezium version 0.8.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While 0.9 at this point is more interesting to those eager to try out the latest developments in the Oracle and SQL Server connectors,
      0.8.1.Final is a recommended upgrade especially to the users of the Postgres connector.
      This release fixes an issue where it could happen that WAL segments on the server were retained longer than necessary,
      in case only records of non-whitelisted tables changed for a while.
      This has been addressed by means of supporting heartbeat messages (as already known from the MySQL connector) also for Postgres (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-800&quot;&gt;DBZ-800&lt;/a&gt;).
      This lets the connector regularly commit offsets to Kafka Connect which also serves as the hook to acknowledge processed LSNs with the Postgres server.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You can find the list of all changes done in Debezium 0.8.1.Final in the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-8-1-final&quot;&gt;change log&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As discussed above, we’ll work on supporting structural changes to captured tables while the SQL Server connector is running.
      The same applies to the Oracle connector.
      This will require some work on our DDL parsers, but thanks to the foundations provided by our recent migration of the MySQL DDL parser to Antlr, this should be manageable.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The other big focus of work with be to provide an alternative implementation for getting changes from Oracle which isn’t based on the XStream API.
      We’ve done some experiments with LogMiner and are also actively exploring further alternatives.
      While some details are still unclear, we are optimistic to have something to release in this area soon.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to learn more about some middle and long term ideas, please check out our &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;roadmap&lt;/a&gt;.
      Also please get in touch with us if you got any ideas or suggestions for future development.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/07/19/advantages-of-log-based-change-data-capture/</id>
    <title>Five Advantages of Log-Based Change Data Capture</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-07-19T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/07/19/advantages-of-log-based-change-data-capture/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="discussion"></category>
    <summary>
      
      
      
      Yesterday I had the opportunity to present Debezium and the idea of change data capture (CDC) to the Darmstadt Java User Group.
      It was a great evening with lots of interesting discussions and questions.
      One of the questions being the following: what is the advantage of using a log-based change data capturing tool such as Debezium over simply polling for updated records?
      
      
      So first of all, what&#8217;s the difference between the two approaches?
      With polling-based (or query-based) CDC you repeatedly run queries (e.g. via JDBC) for retrieving any newly inserted or updated rows from the tables to be captured.
      Log-based CDC in contrast works by...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Yesterday I had the opportunity to present Debezium and the idea of change data capture (CDC) to the &lt;a href=&quot;https://twitter.com/JUG_DA/status/1019634941020332032&quot;&gt;Darmstadt Java User Group&lt;/a&gt;.
      It was a great evening with lots of interesting discussions and questions.
      One of the questions being the following: what is the advantage of using a log-based change data capturing tool such as Debezium over simply polling for updated records?&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So first of all, what’s the difference between the two approaches?
      With polling-based (or query-based) CDC you repeatedly run queries (e.g. via JDBC) for retrieving any newly inserted or updated rows from the tables to be captured.
      Log-based CDC in contrast works by reacting to any changes to the database’s log files (e.g. MySQL’s binlog or MongoDB’s op log).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As this wasn’t the first time this question came up, I thought I could provide a more extensive answer also here on the blog.
      That way I’ll be able to refer to this post in the future, should the question come up again :)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So without further ado, here’s my list of five advantages of log-based CDC over polling-based approaches.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;dlist&quot;&gt;
      &lt;dl&gt;
      &lt;dt class=&quot;hdlist1&quot;&gt;All Data Changes Are Captured&lt;/dt&gt;
      &lt;dd&gt;
      &lt;p&gt;By reading the database’s log, you get the complete list of all data changes in their exact order of application.
      This is vital for many use cases where you are interested in the complete history of record changes.
      In contrast, with a polling-based approach you might miss intermediary data changes that happen between two runs of the poll loop.
      For instance it could happen that a record is inserted and deleted between two polls,
      in which case this record would never be captured by poll-based CDC.&lt;/p&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Related to this is the aspect of downtimes, e.g. when updating the CDC tool.
      With poll-based CDC, only the latest state of a given record would be captured once the CDC tool is back online,
      missing any earlier changes to the record that occurred during the downtime.
      A log-based CDC tool will be able to resume reading the database log from the point where it left off before it was shut down,
      causing the complete history of data changes to be captured.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/dd&gt;
      &lt;dt class=&quot;hdlist1&quot;&gt;Low Delays of Events While Avoiding Increased CPU Load&lt;/dt&gt;
      &lt;dd&gt;
      &lt;p&gt;With polling, you might be tempted to increase the frequency of polling attempts in order to reduce the chances of missing intermediary updates.
      While this works to some degree, polling too frequently may cause performance issues (as the queries used for polling cause load on the source database).
      On the other hand, expanding the polling interval will reduce the CPU load but may not only result in missed change events but also in a longer delay for propagating data changes.
      Log-based CDC allows you to react to data changes in near real-time without paying the price of spending CPU time on running polling queries repeatedly.&lt;/p&gt;
      &lt;/dd&gt;
      &lt;dt class=&quot;hdlist1&quot;&gt;No Impact on Data Model&lt;/dt&gt;
      &lt;dd&gt;
      &lt;p&gt;Polling requires some indicator to identify those records that have been changed since the last poll.
      So all the captured tables need to have some column like &lt;code&gt;LAST_UPDATE_TIMESTAMP&lt;/code&gt; which can be used to find changed rows.
      This can be fine in some cases, but in others such requirement might not be desirable.
      Specifically, you’ll need to make sure that the update timestamps are maintained correctly on all tables to be captured by the writing applications or e.g. through triggers.&lt;/p&gt;
      &lt;/dd&gt;
      &lt;dt class=&quot;hdlist1&quot;&gt;Can Capture Deletes&lt;/dt&gt;
      &lt;dd&gt;
      &lt;p&gt;Naturally, polling will not allow you to identify any records that have been deleted since the last poll.
      Often times that’s a problem for replication-like use cases where you’d like to have an identical data set on the source database and the replication targets,
      meaning you’d also like to delete records on the sink side if they have been removed in the source database.&lt;/p&gt;
      &lt;/dd&gt;
      &lt;dt class=&quot;hdlist1&quot;&gt;Can Capture Old Record State And Further Meta Data&lt;/dt&gt;
      &lt;dd&gt;
      &lt;p&gt;Depending on the source database’s capabilities, log-based CDC can provide the old record state for update and delete events.
      Whereas with polling, you’ll only get the current row state.
      Having the old row state handy in a single change event can be interesting for many use cases, e.g. if you’d like to display the complete data change with old and new column values to an application user for auditing purposes.&lt;/p&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In addition, log-based approaches often can provide streams of schema changes (e.g. in form of applied DDL statements) and expose additional metadata such as transaction ids or the user applying a certain change.
      These things may generally be doable with query-based approaches, too (depending on the capabilities of the database), I haven’t really seen it being done in practice, though.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/dd&gt;
      &lt;/dl&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot;&gt;&lt;/a&gt;Summary&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;And that’s it, five advantages of log-based change data capture.
      Note that this is not to say that polling-based CDC doesn’t have its applications.
      If for instance your use case can be satisfied by propagating changes once per hour and it’s not a problem to miss intermediary versions of records that were valid in between, it can be perfectly fine.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;But if you’re interested in capturing data changes in near real-time, making sure you don’t miss any change events (including deletions), then I’d recommend very much to explore the possibilities of log-based CDC as enabled by Debezium.
      The Debezium connectors do all the heavy-lifting for you, i.e. you don’t have to deal with all the low-level specifics of the individual databases and the means of getting changes from their logs.
      Instead, you can consume the generic and largely unified change data events produced by Debezium.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/07/12/debezium-0-8-0-final-released/</id>
    <title>Debezium 0.8 Final Is Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-07-12T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/07/12/debezium-0-8-0-final-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="mongodb"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m very happy to announce the release of Debezium 0.8.0.Final!
      
      
      The key features of Debezium 0.8 are the first work-in-progress version of our Oracle connector
      (based on the XStream API) and a brand-new parser for MySQL DDL statements.
      Besides that, there are plenty of smaller new features (e.g. propagation of default values to corresponding Connect schemas,
      optional propagation of source queries in CDC messages and a largely improved SMT for sinking changes from MongoDB into RDBMS)
      as well as lots of bug fixes (e.g. around temporal and numeric column types, large transactions with Postgres).
      
      
      Please see the previous announcements (Beta 1, CR 1)
      to learn about all...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m very happy to announce the release of Debezium &lt;strong&gt;0.8.0.Final&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The key features of Debezium 0.8 are the first work-in-progress version of our &lt;a href=&quot;http://debezium.io/docs/connectors/oracle/&quot;&gt;Oracle connector&lt;/a&gt;
      (based on the XStream API) and a brand-new parser for MySQL DDL statements.
      Besides that, there are plenty of smaller new features (e.g. propagation of default values to corresponding Connect schemas,
      optional propagation of source queries in CDC messages and a largely improved SMT for sinking changes from MongoDB into RDBMS)
      as well as lots of bug fixes (e.g. around temporal and numeric column types, large transactions with Postgres).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the previous announcements (&lt;a href=&quot;http://debezium.io/blog/2018/06/21/debezium-0-8-0-beta1-released/&quot;&gt;Beta 1&lt;/a&gt;, &lt;a href=&quot;http://debezium.io/blog/2018/07/04/debezium-0-8-0-cr1-released/&quot;&gt;CR 1&lt;/a&gt;)
      to learn about all the changes in more depth.
      The Final release largely resembles CR1;
      apart from further improvements to the Oracle connector (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-762&quot;&gt;DBZ-792&lt;/a&gt;) there’s one nice addition to the MySQL connector contributed by &lt;a href=&quot;https://github.com/pgoranss&quot;&gt;Peter Goransson&lt;/a&gt;:
      when doing a snapshot, it will now expose information about the processed rows via JMX (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-789&quot;&gt;DBZ-789&lt;/a&gt;), which is very handy when snapshotting larger tables.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please take a look at the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-8-0-final&quot;&gt;change log&lt;/a&gt; for the complete list of changes in 0.8.0.Final and general upgrade notes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re continuing our work on the Oracle connector.
      The work on initial snapshotting is well progressing and it should be part of the next release.
      Other improvements will be support for structural changes to captured tables after the initial snapshot has been made,
      more extensive source info metadata and more.
      Please track &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-716&quot;&gt;DBZ-716&lt;/a&gt; for this work; the improvements are planned to be released incrementally in the upcoming versions of Debezium.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also started to explore ingesting changes via LogMiner.
      This is more involved in terms of engineering efforts than using XStream, but it comes with the huge advantage of not requiring a separate license
      (LogMiner comes with the Oracle database itself).
      It’s not quite clear yet when we can release something on this front, and we’re also actively exploring further alternatives.
      But we are quite optimistic and hope to have something some time soon.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The other focus of work is a connector for SQL Server (see &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-40&quot;&gt;DBZ-40&lt;/a&gt;).
      Work on this has started as well, and there should be an Alpha1 release of Debezium 0.9 with a first drop of that connector within the next few weeks.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To find out about some more long term ideas, please check out our &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;roadmap&lt;/a&gt; and get in touch with us, if you got any ideas or suggestions for future development.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/07/04/debezium-0-8-0-cr1-released/</id>
    <title>Debezium 0.8.0.CR1 Is Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-07-04T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/07/04/debezium-0-8-0-cr1-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="mongodb"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      A fantastic Independence Day to all the Debezium users in the U.S.!
      But that&#8217;s not the only reason to celebrate: it&#8217;s also with great happiness that I&#8217;m announcing the release of Debezium 0.8.0.CR1!
      
      
      Following our new release scheme,
      the focus for this candidate release of Debezium 0.8 has been to fix bug reported for last week&#8217;s Beta release,
      accompanied by a small number of newly implemented features.
      
      
      Thanks a lot to everyone testing the new Antlr-based DDL parser for the MySQL connector;
      based on the issues you reported, we were able to fix a few bugs in it.
      As announced recently, for 0.8 the legacy parser will...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A fantastic Independence Day to all the Debezium users in the U.S.!
      But that’s not the only reason to celebrate: it’s also with great happiness that I’m announcing the release of Debezium &lt;strong&gt;0.8.0.CR1&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Following our new &lt;a href=&quot;http://debezium.io/blog/2018/06/21/debezium-0-8-0-beta1-released/&quot;&gt;release scheme&lt;/a&gt;,
      the focus for this candidate release of Debezium 0.8 has been to fix bug reported for last week’s Beta release,
      accompanied by a small number of newly implemented features.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks a lot to everyone testing the new Antlr-based DDL parser for the MySQL connector;
      based on the issues you reported, we were able to fix a few bugs in it.
      As announced recently, for 0.8 the legacy parser will remain the default implementation,
      but you are strongly encouraged to test out the new one
      (by setting the connector option &lt;code&gt;ddl.parser.mode&lt;/code&gt; to &lt;code&gt;antlr&lt;/code&gt;) and report any findings you may have.
      We’ve planned to switch to the new implementation by default in Debezium 0.9.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In terms of new features, the CR1 release brings support for &lt;code&gt;CITEXT&lt;/code&gt; columns in the Postgres connector (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-762&quot;&gt;DBZ-762&lt;/a&gt;).
      All the relational connectors support it now to convey the original name and length of captured columns using schema parameters in the emitted change messages (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-644&quot;&gt;DBZ-644&lt;/a&gt;).
      This can come in handy to properly size columns in a sink database for types such as &lt;code&gt;VARCHAR&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks a lot to the following community members who contributed to this release:
      &lt;a href=&quot;https://github.com/abergmeier&quot;&gt;Andreas Bergmeier&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/olavim&quot;&gt;Olavi Mustanoja&lt;/a&gt; and
      &lt;a href=&quot;https://github.com/orrganani&quot;&gt;Orr Ganani&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please take a look at the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-8-0-cr-1&quot;&gt;change log&lt;/a&gt; for the complete list of changes in 0.8.0.CR1 and general upgrade notes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Barring any unforeseen issues and critical bug reports, we’ll release Debezium 0.8.0.Final next week.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once that’s out, we’ll continue work on the Oracle connector (e.g. exploring alternatives to using XStream for ingesting changes from the database as well as initial snapshotting),
      which remains a &quot;tech preview&quot; component as of 0.8.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll also work towards a connector for SQL Server (see &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-40&quot;&gt;DBZ-40&lt;/a&gt;),
      for which the first steps just have been made today by preparing a Docker-based setup with a CDC-enabled SQL Server instance,
      allowing to implement and test the connector in the following.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To find out about some more long term ideas, please check out our &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;roadmap&lt;/a&gt; and get in touch with us, if you got any ideas or suggestions for future development.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/06/21/debezium-0-8-0-beta1-released/</id>
    <title>Debezium 0.8.0.Beta1 Is Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-06-21T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/06/21/debezium-0-8-0-beta1-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="mongodb"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s with great excitement that I&#8217;m announcing the release of Debezium 0.8.0.Beta1!
      
      
      This release brings many exciting new features as well as bug fixes,
      e.g. the first drop of our new Oracle connector,
      a brand new DDL parser for the MySQL connector,
      support for MySQL default values and the update to Apache Kafka 1.1.
      
      
      Due to the big number of changes (the release contains exactly 42 issues overall),
      we decided to alter our versioning schema a little bit:
      going forward we may do one or more Beta and CR ("candidate release") releases before doing a final one.
      This will allow us to get feedback from the community early...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s with great excitement that I’m announcing the release of Debezium &lt;strong&gt;0.8.0.Beta1&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release brings many exciting new features as well as bug fixes,
      e.g. the first drop of our new Oracle connector,
      a brand new DDL parser for the MySQL connector,
      support for MySQL default values and the update to Apache Kafka 1.1.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Due to the big number of changes (the release contains exactly &lt;a href=&quot;https://issues.jboss.org/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.8.0.Beta1&quot;&gt;42 issues&lt;/a&gt; overall),
      we decided to alter our versioning schema a little bit:
      going forward we may do one or more Beta and CR (&quot;candidate release&quot;) releases before doing a final one.
      This will allow us to get feedback from the community early on,
      while still completing and polishing specific features.
      Final (stable) releases will be named like 0.8.0.Final etc.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release would not have been possible without our outstanding community;
      a huge &quot;thank you&quot; goes out to the following open source enthusiasts who all contributed to the new version:
      &lt;a href=&quot;https://github.com/echo-xu&quot;&gt;Echo Xu&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/vuckooo&quot;&gt;Ivan Vucina&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/glistman&quot;&gt;Listman Gamboa&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/omarsmak&quot;&gt;Omar Al-Safi&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/pgoranss&quot;&gt;Peter Goransson&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/kucharo2&quot;&gt;Roman Kuchar&lt;/a&gt; (who did a tremendous job with the new DDL parser implementation!),
      &lt;a href=&quot;https://github.com/sagarrao&quot;&gt;Sagar Rao&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/sauliusvl&quot;&gt;Saulius Valatka&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/sairam881990&quot;&gt;Sairam Polavarapu&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/Crim&quot;&gt;Stephen Powis&lt;/a&gt; and
      &lt;a href=&quot;https://github.com/sweat123&quot;&gt;WenZe Hu&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thank you all very much for your help!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now let’s take a closer look at some of the features new in Debezium 0.8.0.Beta1;
      as always, you can find the complete list of changes of this release in the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-8-0-beta-1&quot;&gt;change log&lt;/a&gt;.
      Plese take a special look at the breaking changes and the upgrade notes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;xstream_based_oracle_connector_tech_preview&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#xstream_based_oracle_connector_tech_preview&quot;&gt;&lt;/a&gt;XStream-based Oracle Connector (Tech Preview)&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Support for a Debezium Oracle connector has been one of the most asked for features for a long time
      (its original issue number is &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-20&quot;&gt;DBZ-20&lt;/a&gt;!).
      So we are very happy that we eventually can release a first work-in-progress version of that connector.
      At this point this code is still very much evolving, so it should be considered as a first tech preview.
      This means it’s not feature complete (most notably, there’s no support for initial snapshots yet),
      the emitted message format may still change etc.
      So while we don’t recommend using it in production quite yet,
      you should definitely give it a try and report back about your experiences.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;One challenge for the Oracle connector is how to get the actual change events out of the database.
      Unlike with MySQL and Postgres, there’s unfortunately no free-to-use and easy-to-work-with API which would allow to do the same for Oracle.
      After some exploration we decided to base this first version of the connector on the &lt;a href=&quot;https://docs.oracle.com/database/121/XSTRM/xstrm_intro.htm#XSTRM72647&quot;&gt;Oracle XStream&lt;/a&gt; API.
      While this (kinda) checks the box for &quot;easy-to-work-with&quot;, it doesn’t do so for &quot;free-to-use&quot;:
      using this API requires you to have a license for Oracle’s separate GoldenGate product.
      We’re fully aware of this being not ideal, but we decided to still go this route as a first step,
      allowing us to get some experiences with Oracle and also get a connector into the hands of those with the required license handy.
      Going forward, we are going to explore alternative approaches.
      We already have some ideas and discussions around this, so please stay tuned (the issue to track is &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-137&quot;&gt;DBZ-137&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Oracle connector is going to evolve within the next 0.8.x releases.
      To learn more about it, please check its &lt;a href=&quot;http://debezium.io/docs/connectors/oracle/&quot;&gt;connector documentation page&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;antlr_based_mysql_ddl_parser&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#antlr_based_mysql_ddl_parser&quot;&gt;&lt;/a&gt;Antlr-based MySQL DDL Parser&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In order to build up an internal meta-model of the captured database’s structure,
      the Debezium MySQL connector needs to parse all issued DDL statements (&lt;code&gt;CREATE TABLE&lt;/code&gt; etc.).
      This used to be done with a hand-written DDL parser which worked reasonably well,
      but over time it also revealed some shortcomings; as the DDL language is quite extensive,
      we saw repeatedly bug reports caused by some specific DDL constructs not being parseable.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So we decided to go back to the drawing board and came up with a brand new parser design.
      Thanks to the great work of Roman Kuchar, we now have a completely new DDL parser
      which is based on the proven and very mature &lt;a href=&quot;http://antlr.org/&quot;&gt;Antlr&lt;/a&gt; parser generator
      (luckily, the Antlr project provides a complete MySQL grammar).
      So we should see much less issue reports related to DDL parsing going forward.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For the time being, the old parser still is in place and remains to be the default parser for Debezium 0.8.x.
      You are very encouraged though to test the new implementation by setting the connector option &lt;code&gt;ddl.parser.mode&lt;/code&gt; to &lt;code&gt;antlr&lt;/code&gt;
      and report back if you run into any issues doing so.
      We plan to improve and polish the Antlr parser during the 0.8.x release line
      (specifically we’re going to measure its performance and optimize as needed)
      and switch to it by default as of Debezium 0.9.
      Eventually, the old parser will be removed in a future release after that.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;further_mysql_connector_changes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#further_mysql_connector_changes&quot;&gt;&lt;/a&gt;Further MySQL Connector Changes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The MySQL Connector propagates column default values to corresponding Kafka Connect schemas now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-191&quot;&gt;DBZ-191&lt;/a&gt;).
      That’s beneficial when using Avro as serialization format and the schema registry with compatibility checking enabled.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;By setting the &lt;code&gt;include.query&lt;/code&gt; connector option to true, you can add the original query that caused a data change to the corresponding CDC events (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-706&quot;&gt;DBZ-706&lt;/a&gt;).
      While disabled by default, this feature can be a useful tool for analyzing and interpreting data changes captured with Debezium.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Some other changes in the MySQL connector include configurability of the heartbeat topic name (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-668&quot;&gt;DBZ-668&lt;/a&gt;),
      fixes around timezone handling for &lt;code&gt;TIMESTAMP&lt;/code&gt; (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-578&quot;&gt;DBZ-578&lt;/a&gt;) and &lt;code&gt;DATETIME&lt;/code&gt; columns (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-741&quot;&gt;DBZ-741&lt;/a&gt;)
      and correct handling of &lt;code&gt;NUMERIC&lt;/code&gt; column without an explicit scale value (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-727&quot;&gt;DBZ-727&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;postgres_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#postgres_connector&quot;&gt;&lt;/a&gt;Postgres Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium Connector for Postgres has seen quite a number of bugfixes, including the following ones:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;wal2json can handle transactions now that are bigger than 1Gb (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-638&quot;&gt;DBZ-638&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;the transaction ID is consistently handled as long now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-673&quot;&gt;DBZ-673&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;multiple fixes related to temporal column types (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-681&quot;&gt;DBZ-681&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-696&quot;&gt;DBZ-696&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;OIDs are handled correctly as unsigned int now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-697&quot;&gt;DBZ-697&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-701&quot;&gt;DBZ-701&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mongodb_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mongodb_connector&quot;&gt;&lt;/a&gt;MongoDB Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also for the MongoDB Connector a number of small feature implementations and bugfixes has been done:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Tested against MongoDB 3.6 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-529&quot;&gt;DBZ-529&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Nested documents can be flattened using a provided SMT now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-561&quot;&gt;DBZ-561&lt;/a&gt;), which is useful when sinking changes from MongoDB into a relational database&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The &lt;a href=&quot;http://debezium.io/docs/configuration/mongodb-event-flattening/&quot;&gt;unwrapping SMT&lt;/a&gt; can be used together with Avro now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-650&quot;&gt;DBZ-650&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The unwrapping SMT can handle arrays with mixed element types (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-649&quot;&gt;DBZ-649&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;When interrupted during snapshotting before completion, the connector will redo the snapshot after restarting (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-712&quot;&gt;DBZ-712&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As per the new Beta/CR/Final release scheme, we hope to get some feedback by the community (i.e. you :) on this Beta release.
      Depending on the number of issues reported, we’ll either release another Beta or go to CR1 with the next version.
      The 0.8.0.Final version will be released within a few weeks.
      Note that the Oracle connector will remain a &quot;tech preview&quot; component also in the final version.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;After that, we’ve planned to do a few 0.8.x releases with bug fixes mostly,
      while work on Debezium 0.9 will commence in parallel.
      For that we’ve planned to work on a connector for SQL Server (see &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-40&quot;&gt;DBZ-40&lt;/a&gt;).
      We’d also like to explore means of creating consistent materializations of joins from multiple tables' CDC streams,
      based on the ids of originating transactions.
      Also there’s the idea and a first prototype of exposing Debezium change events as a reactive event stream (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-566&quot;&gt;DBZ-566&lt;/a&gt;),
      which might be shipped eventually.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please take a look at the &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;roadmap&lt;/a&gt; for some more long term ideas and get in touch with us,
      if you got thoughts around that.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/05/24/querying-debezium-change-data-eEvents-with-ksql/</id>
    <title>Querying Debezium Change Data Events With KSQL</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-05-24T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/05/24/querying-debezium-change-data-eEvents-with-ksql/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="mysql"></category>
    <category term="ksql"></category>
    <category term="example"></category>
    <summary>
      
      
      
      Last updated at Nov 21st 2018 (adjusted to new KSQL Docker images).
      
      
      Last year we have seen the inception of a new open-source project in the Apache Kafka universe, KSQL,
      which is a streaming SQL engine build on top of Kafka Streams.
      In this post, we are going to try out KSQL querying with data change events generated by Debezium from a MySQL database.
      
      
      As a source of data we will use the database and setup from our tutorial.
      The result of this exercise should be similar to the recent post about aggregation of events into domain driven aggregates.
      
      
      
      
      Entity diagram
      
      
      First let&#8217;s look at the entities...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;em&gt;Last updated at Nov 21st 2018 (adjusted to new KSQL Docker images)&lt;/em&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Last year we have seen the inception of a new open-source project in the &lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Apache Kafka&lt;/a&gt; universe, &lt;a href=&quot;https://github.com/confluentinc/ksql&quot;&gt;KSQL&lt;/a&gt;,
      which is a streaming SQL engine build on top of &lt;a href=&quot;https://kafka.apache.org/documentation/streams/&quot;&gt;Kafka Streams&lt;/a&gt;.
      In this post, we are going to try out KSQL querying with data change events generated by Debezium from a MySQL database.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As a source of data we will use the database and setup from our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;.
      The result of this exercise should be similar to the recent &lt;a href=&quot;http://debezium.io/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/&quot;&gt;post&lt;/a&gt; about aggregation of events into &lt;a href=&quot;https://martinfowler.com/bliki/DDD_Aggregate.html&quot;&gt;domain driven aggregates&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;entity_diagram&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#entity_diagram&quot;&gt;&lt;/a&gt;Entity diagram&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;First let’s look at the entities in the database and the relations between them.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;http://debezium.io/images/tutorial-erd.svg&quot; alt=&quot;Entity diagram&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;title&quot;&gt;Figure 1: Entity diagram of the example entities&lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt; &lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The picture above shows the full ER diagram for the inventory database in the example MySQL instance.
      We are going to focus on two entities:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;customers&lt;/code&gt; - the list of customers in the system&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;orders&lt;/code&gt; - the list of orders in the system&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;There is a &lt;code&gt;1:n&lt;/code&gt; relation between &lt;code&gt;customers&lt;/code&gt; and &lt;code&gt;orders&lt;/code&gt;, modelled by the &lt;code&gt;purchaser&lt;/code&gt; column in the &lt;code&gt;orders&lt;/code&gt; table, which is a foreign key to the &lt;code&gt;customers&lt;/code&gt; table.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;configuration&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#configuration&quot;&gt;&lt;/a&gt;Configuration&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We are going to use a &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/ksql/docker-compose.yaml&quot;&gt;Docker Compose file&lt;/a&gt; for the deployment of the environment.
      The deployment consists of the following Docker images:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://hub.docker.com/r/debezium/zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://hub.docker.com/r/debezium/kafka/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Kafka Connect including the Debezium connectors &lt;a href=&quot;https://hub.docker.com/r/debezium/connect/&quot;&gt;image&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;A pre-populated MySQL database as used in our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The &lt;a href=&quot;https://hub.docker.com/r/confluentinc/cp-ksql-server/&quot;&gt;KSQL server&lt;/a&gt; and &lt;a href=&quot;https://hub.docker.com/r/confluentinc/cp-ksql-cli/&quot;&gt;CLI client&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;example&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#example&quot;&gt;&lt;/a&gt;Example&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;First we need to start the Debezium and Kafka infrastructure.
      To do so, clone the &lt;a href=&quot;https://github.com/debezium/debezium-examples/&quot;&gt;debezium-examples&lt;/a&gt; GitHub repository and start the required components using the provided Compose file:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;export DEBEZIUM_VERSION=0.8
      git clone https://github.com/debezium/debezium-examples.git
      cd debezium-examples/ksql/
      docker-compose up&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Next we must register an instance of the Debezium MySQL connector to listen to changes in the database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ -d @- &amp;lt;&amp;lt;-EOF
      {
          &quot;name&quot;: &quot;inventory-connector&quot;,
          &quot;config&quot;: {
              &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
              &quot;tasks.max&quot;: &quot;1&quot;,
              &quot;database.hostname&quot;: &quot;mysql&quot;,
              &quot;database.port&quot;: &quot;3306&quot;,
              &quot;database.user&quot;: &quot;debezium&quot;,
              &quot;database.password&quot;: &quot;dbz&quot;,
              &quot;database.server.id&quot;: &quot;184055&quot;,
              &quot;database.server.name&quot;: &quot;dbserver&quot;,
              &quot;database.whitelist&quot;: &quot;inventory&quot;,
              &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
              &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot;,
              &quot;transforms&quot;: &quot;unwrap&quot;,
              &quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.UnwrapFromEnvelope&quot;,
              &quot;key.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;,
              &quot;key.converter.schemas.enable&quot;: &quot;false&quot;,
              &quot;value.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;,
              &quot;value.converter.schemas.enable&quot;: &quot;false&quot;
          }
      }
      EOF&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now we should have all components up and running and initial data change events are already streamed into Kafka topics.
      There are multiple properties that are especially important for our use case:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;The &lt;a href=&quot;http://debezium.io/docs/configuration/event-flattening/&quot;&gt;UnwrapFromEnvelope SMT&lt;/a&gt; is used.
      This allows us to directly map fields from the &lt;code&gt;after&lt;/code&gt; part of change records into KSQL statements.
      Without it, we would need to use &lt;code&gt;EXTRACTJSONFIELD&lt;/code&gt; for each field to be extracted from the &lt;code&gt;after&lt;/code&gt; part of messages.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Schemas are disabled for the JSON converter.
      The reason is the same as above.
      With schemas enabled, for JSON the record is encapsulated in a JSON structure that contains the fields &lt;code&gt;schema&lt;/code&gt; (with schema information) and &lt;code&gt;payload&lt;/code&gt; (with the actual data itself).
      We would again need to use &lt;code&gt;EXTRACTJSONFIELD&lt;/code&gt; to get to the relevant fields.
      There is no such issue with Avro converter so this option does not need to be set when Avro is used.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Next we are going to start the KSQL command shell.
      We will run a local engine in the CLI.
      Also please note &lt;code&gt;--net&lt;/code&gt; parameter. This guarantees that KSQL container runs in the same network as Debezium containers and allows proper DNS resolution.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec ksql-cli ksql http://ksql-server:8088&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;First we will list all Kafka topics that exist in the broker:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; LIST TOPICS;
      
       Kafka Topic                         | Registered | Partitions | Partition Replicas
      ------------------------------------------------------------------------------------
       connect-status                      | false      | 5          | 1
       dbserver                            | false      | 1          | 1
       dbserver.inventory.addresses        | false      | 1          | 1
       dbserver.inventory.customers        | false      | 1          | 1
       dbserver.inventory.orders           | false      | 1          | 1
       dbserver.inventory.products         | false      | 1          | 1
       dbserver.inventory.products_on_hand | false      | 1          | 1
       ksql__commands                      | true       | 1          | 1
       my_connect_configs                  | false      | 1          | 1
       my_connect_offsets                  | false      | 25         | 1
       schema-changes.inventory            | false      | 1          | 1&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The topics we are interested in are &lt;code&gt;dbserver.inventory.orders&lt;/code&gt; and &lt;code&gt;dbserver.inventory.customers&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;KSQL processing by default starts with &lt;code&gt;latest&lt;/code&gt; offsets.
      We want to process the events already in the topics so we switch processing from &lt;code&gt;earliest&lt;/code&gt; offsets.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; SET 'auto.offset.reset' = 'earliest';
      Successfully changed local property 'auto.offset.reset' from 'null' to 'earliest'&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;First we need to create streams from the topics containing the Debezium data change events.
      A &lt;em&gt;stream&lt;/em&gt; in KSQL and Kafka Streams terminology is an unbounded incoming data set with no state.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; CREATE STREAM orders_from_debezium (order_number integer, order_date string, purchaser integer, quantity integer, product_id integer) WITH (KAFKA_TOPIC='dbserver.inventory.orders',VALUE_FORMAT='json');
      
       Message
      ----------------
       Stream created
      ksql&amp;gt;
      ksql&amp;gt; CREATE STREAM customers_from_debezium (id integer, first_name string, last_name string, email string) WITH (KAFKA_TOPIC='dbserver.inventory.customers',VALUE_FORMAT='json');
      
       Message
      ----------------
       Stream created&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;partitioning&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#partitioning&quot;&gt;&lt;/a&gt;Partitioning&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Our deployment uses only one partition per topic.
      In a production system there will likely be multiple partitions per topic and we need to ensure that all events belonging to our aggregated object end up in the same partition.
      The natural partioning in our case is per customer id.
      We are going to repartition the &lt;code&gt;orders_from_debezium&lt;/code&gt; stream according to the &lt;code&gt;purchaser&lt;/code&gt; field that contains the customer id.
      The repartitioned data are written into a new topic &lt;code&gt;ORDERS_REPART&lt;/code&gt;:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; CREATE STREAM orders WITH (KAFKA_TOPIC='ORDERS_REPART',VALUE_FORMAT='json',PARTITIONS=1) as SELECT * FROM orders_from_debezium PARTITION BY PURCHASER;
      
       Message
      ----------------------------
       Stream created and running
      ksql&amp;gt; LIST TOPICS;
      
       Kafka Topic                         | Registered | Partitions | Partition Replicas
      ------------------------------------------------------------------------------------
      ...
       ORDERS_REPART                       | true       | 1          | 1
      ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We are going to execute the same operation for customers too.
      It is necessary for two reasons:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;The current key is a struct that contains a field named &lt;code&gt;id&lt;/code&gt; with the customer id.
      This is different from the repartitioned order topic which contains only the &lt;code&gt;id&lt;/code&gt; value as the key, so the partitions would not match.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;When we will create a JOIN later, there is a limitation that requires the key to have the same value as a key field in the table.
      The table field contains a plain value but the key contains a struct so they would not match.
      See &lt;a href=&quot;https://github.com/confluentinc/ksql/issues/749&quot;&gt;this KSQL issue&lt;/a&gt; for more details.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; CREATE STREAM customers_stream WITH (KAFKA_TOPIC='CUSTOMERS_REPART',VALUE_FORMAT='json',PARTITIONS=1) as SELECT * FROM customers_from_debezium PARTITION BY ID;
      
       Message
      ----------------------------
       Stream created and running
      ksql&amp;gt; LIST TOPICS;
      
       Kafka Topic                         | Registered | Partitions | Partition Replicas
      ------------------------------------------------------------------------------------
      ...
       CUSTOMERS_REPART                    | true       | 1          | 1
      ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To verify that records have a new key and are thus repartioned we can issue few statements to compare the results:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; SELECT * FROM orders_from_debezium LIMIT 1;
      1524034842810 | {&quot;order_number&quot;:10001} | 10001 | 16816 | 1001 | 1 | 102
      LIMIT reached for the partition.
      Query terminated
      ksql&amp;gt; SELECT * FROM orders LIMIT 1;
      1524034842810 | 1001 | 10001 | 16816 | 1001 | 1 | 102
      LIMIT reached for the partition.
      Query terminated&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The second column contains &lt;code&gt;ROWKEY&lt;/code&gt; which is the key of the message.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect3&quot;&gt;
      &lt;h4 id=&quot;customer_order_join&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#customer_order_join&quot;&gt;&lt;/a&gt;Customer/order join&lt;/h4&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So far we were only declaring streams as an unbounded stateless data set.
      In our use case the &lt;code&gt;order&lt;/code&gt; is really an event that comes and goes.
      But &lt;code&gt;customer&lt;/code&gt; is an entity that can be updated and generally is a part of a state fo the system.
      Such quality is represented in KSQL or Kafka Streams as table.
      We are going to create a table of customers from the topic containing repartitioned customers.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; CREATE TABLE customers (id integer, first_name string, last_name string, email string) WITH (KAFKA_TOPIC='CUSTOMERS_REPART',VALUE_FORMAT='json',KEY='id');
      
       Message
      ---------------
       Table created&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now we have everything in place to make a join between customer and its orders and create a query that will monitor incoming orders and list them with associated customer fields.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; SELECT order_number,quantity,customers.first_name,customers.last_name FROM orders left join customers on orders.purchaser=customers.id;
      10001 | 1 | Sally | Thomas
      10002 | 2 | George | Bailey
      10003 | 2 | George | Bailey
      10004 | 1 | Edward | Walker&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s apply a few changes to the database, which will result in corresponding CDC events being emitted by Debezium:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER -p$MYSQL_PASSWORD inventory'
      
      mysql&amp;gt; INSERT INTO orders VALUES(default,NOW(), 1003,5,101);
      Query OK, 1 row affected, 1 warning (0.02 sec)
      
      mysql&amp;gt; UPDATE customers SET first_name='Annie' WHERE id=1004;
      Query OK, 1 row affected (0.02 sec)
      Rows matched: 1  Changed: 1  Warnings: 0
      
      mysql&amp;gt; UPDATE orders SET quantity=20 WHERE order_number=10004;
      Query OK, 1 row affected (0.02 sec)
      Rows matched: 1  Changed: 1  Warnings: 0&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You may notice that only changes in the &lt;code&gt;orders&lt;/code&gt; table have triggered changes in the joined stream.
      This is a product of the stream/table join.
      We would need a stream/stream join to trigger changes if any of input streams is modified.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So the final result of the select after the database is modified is&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;10001 | 1 | Sally | Thomas
      10002 | 2 | George | Bailey
      10003 | 2 | George | Bailey
      10004 | 1 | Edward | Walker
      10005 | 5 | Edward | Walker
      10004 | 20 | Edward | Walker&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot;&gt;&lt;/a&gt;Summary&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We have successfully started a KSQL instance. We have mapped KSQL streams to Debezium topics filled by Debezium and made a join between them.
      We have also discussed the problem of repartioning in streaming applications.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to try out this example with Avro encoding and schema registry then you can use our &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/tutorial/docker-compose-mysql-avro.yaml&quot;&gt;Avro example&lt;/a&gt;.
      Also for further details and more advanced usages just refer to the KSQL &lt;a href=&quot;https://github.com/confluentinc/ksql/blob/master/docs/syntax-reference.md&quot;&gt;syntax reference&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In case you need help, have feature requests or would like to share your experiences with this example, please let us know in the comments below.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/03/20/debezium-0-7-5-released/</id>
    <title>Debezium 0.7.5 Is Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-03-20T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/03/20/debezium-0-7-5-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="mongodb"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s my pleasure to announce the release of Debezium 0.7.5!
      
      
      This is a bugfix release to the 0.7 release line, which we decided to do while working towards Debezium 0.8.
      Most notably it fixes an unfortunate bug introduced in 0.7.3 (DBZ-663),
      where the internal database history topic of the Debezium MySQL connector could be partly deleted under some specific conditions.
      Please see the dedicated blog post on this issue to find out whether this affects you and what you should do to prevent this issue.
      
      
      Together with this, we released a couple of other fixes and improvements.
      Thanks to Maciej Brynski, the performance of the logical...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.7.5&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This is a bugfix release to the 0.7 release line, which we decided to do while working towards Debezium 0.8.
      Most notably it fixes an unfortunate bug introduced in 0.7.3 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-663&quot;&gt;DBZ-663&lt;/a&gt;),
      where the internal database history topic of the Debezium MySQL connector could be partly deleted under some specific conditions.
      Please see the &lt;a href=&quot;http://debezium.io/2018/03/16/note-on-database-history-topic-configuration/&quot;&gt;dedicated blog post&lt;/a&gt; on this issue to find out whether this affects you and what you should do to prevent this issue.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Together with this, we released a couple of other fixes and improvements.
      Thanks to &lt;a href=&quot;https://github.com/maver1ck&quot;&gt;Maciej Brynski&lt;/a&gt;, the performance of the &lt;a href=&quot;http://debezium.io/docs/configuration/topic-routing/&quot;&gt;logical table routing SMT&lt;/a&gt; has been improved significantly (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-655&quot;&gt;DBZ-655&lt;/a&gt;).
      Another fix contributed by Maciej is for &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-646&quot;&gt;DBZ-646&lt;/a&gt; which lets the MySQL connector handle &lt;code&gt;CREATE TABLE&lt;/code&gt; statements for the TokuDB storage engine now.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;And we got some more bugfixes by our fantastic community:
      Long-term community member &lt;a href=&quot;https://github.com/pgoranss&quot;&gt;Peter Goransson&lt;/a&gt; fixed an issue about the snapshot JMX metrics of the MySQL connector,
      which are now also accessible after the snapshot has been completed (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-640&quot;&gt;DBZ-640&lt;/a&gt;).
      &lt;a href=&quot;https://github.com/atongen&quot;&gt;Andrew Tongen&lt;/a&gt; spotted and fixed an issue for the Debezium embedded engine (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-665&quot;&gt;DBZ-665&lt;/a&gt;) which caused offsets to be committed more often than needed.
      And &lt;a href=&quot;https://github.com/matzew&quot;&gt;Matthias Wessendorf&lt;/a&gt; upgraded the Debezium dependencies and Docker images to Apache Kafka 1.0.1 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-647&quot;&gt;DBZ-647&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thank you all for your help!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please refer to the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-4&quot;&gt;change log&lt;/a&gt; for the complete list of changes in Debezium 0.7.5.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/blog/2018/03/07/debezium-0-7-4-released/&quot;&gt;previous release announcement&lt;/a&gt; for the next planned features.
      Due to the unplanned 0.7.5 release, though, the schedule of the next one will likely be extended a little bit.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/03/16/note-on-database-history-topic-configuration/</id>
    <title>A Note On Database History Topic Configuration</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-03-16T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/03/16/note-on-database-history-topic-configuration/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="mysql"></category>
    <summary>
      
      
      
      A user of the Debezium connector for MySQL informed us about a potential issue with the configuration of the connector&#8217;s internal database history topic,
      which may cause the deletion of parts of that topic (DBZ-663).
      Please continue reading if you&#8217;re using the Debezium MySQL connector in versions 0.7.3 or 0.7.4.
      
      
      
      
      What is the issue about?
      
      
      In Debezium 0.7.3 we rolled out a feature for creating the database history automatically if it doesn&#8217;t exist yet (DBZ-278).
      While this feature sets the retention time for the topic to an "infinite" period, it doesn&#8217;t specify the "retention.bytes" option for the history topic.
      This may cause parts of the history...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A user of the Debezium connector for MySQL informed us about a potential issue with the configuration of the connector’s internal database history topic,
      which may cause the deletion of parts of that topic (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-663&quot;&gt;DBZ-663&lt;/a&gt;).
      Please continue reading if you’re using the Debezium MySQL connector in versions 0.7.3 or 0.7.4.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_is_the_issue_about&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_is_the_issue_about&quot;&gt;&lt;/a&gt;What is the issue about?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In Debezium 0.7.3 we rolled out a feature for creating the database history automatically if it doesn’t exist yet (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-278&quot;&gt;DBZ-278&lt;/a&gt;).
      While this feature sets the retention time for the topic to an &quot;infinite&quot; period, it doesn’t specify the &quot;retention.bytes&quot; option for the history topic.
      This may cause parts of the history topic to be deleted in case all of the following conditions are met:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;You are using versions 0.7.3 or 0.7.4 of the Debezium connector for MySQL&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The database history topic has been created by the connector (i.e. you haven’t created it yourself)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The broker level option &quot;log.retention.bytes&quot; is set to another value than -1
      (note that the default &lt;strong&gt;is&lt;/strong&gt; -1, in which case things work as intended)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The database history topic grows beyond the threshold configured via &quot;log.retention.bytes&quot;&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If the history topic is incomplete, the connector will fail to recover the database history after a restart of the connector and will not continue with reading the MySQL binlog.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;how_to_prevent_the_issue&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#how_to_prevent_the_issue&quot;&gt;&lt;/a&gt;How to prevent the issue?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You should either create the database history topic yourself with an infinite retention
      or alternatively override the &quot;retention.bytes&quot; configuration for the history topic created by the connector:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&amp;lt;KAFKA_DIR&amp;gt;/bin/kafka-configs.sh \
        --zookeeper zookeeper:2181 \
        --entity-type topics \
        --entity-name &amp;lt;DB_HISTORY_TOPIC&amp;gt; \
        --alter \
        --add-config retention.bytes=-1&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In case parts of the history topic were removed already,
      you can use the snapshot mode &lt;code&gt;schema_only_recovery&lt;/code&gt; for re-creating the history topic in case no schema changes have happened since the last committed offset of the connector.
      Alternatively, a complete new snapshot should be taken, e.g. by setting up a new connector instance.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;next_steps&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#next_steps&quot;&gt;&lt;/a&gt;Next steps&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll release Debezium 0.7.5 with a fix for this issue early next week.
      Note that previously created database history topics should be re-configured as described above.
      Please don’t hesitate to get in touch in the comments below, the chat room or the mailing list in case you have any further questions on this issue.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/</id>
    <title>Creating DDD aggregates with Debezium and Kafka Streams</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-03-08T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/" rel="alternate" type="text/html" />
    <author>
      <name>Hans-Peter Grahsl, Gunnar Morling</name>
    </author>
    <category term="discussion"></category>
    <category term="examples"></category>
    <summary>
      
      
      
      Microservice-based architectures can be considered an industry trend and are thus
      often found in enterprise applications lately. One possible way to keep data
      synchronized across multiple services and their backing data stores is to make us of an approach
      called change data capture, or CDC for short.
      
      
      Essentially CDC allows to listen to any modifications which are occurring at one end of a data flow (i.e. the data source)
      and communicate them as change events to other interested parties or storing them into a data sink.
      Instead of doing this in a point-to-point fashion, it&#8217;s advisable to decouple this flow of events
      between data sources and data...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Microservice-based architectures can be considered an industry trend and are thus
      often found in enterprise applications lately. One possible way to keep data
      synchronized across multiple services and their backing data stores is to make us of an approach
      called &lt;a href=&quot;https://vladmihalcea.com/a-beginners-guide-to-cdc-change-data-capture/&quot;&gt;change data capture&lt;/a&gt;, or CDC for short.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Essentially CDC allows to listen to any modifications which are occurring at one end of a data flow (i.e. the data source)
      and communicate them as change events to other interested parties or storing them into a data sink.
      Instead of doing this in a point-to-point fashion, it’s advisable to decouple this flow of events
      between data sources and data sinks. Such a scenario can be implemented based on &lt;a href=&quot;http://debezium.io/&quot;&gt;Debezium&lt;/a&gt;
      and &lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Apache Kafka&lt;/a&gt; with relative ease and effectively no coding.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As an example, consider the following microservice-based architecture of an order management system:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;http://debezium.io/images/msa_streaming.png&quot; alt=&quot;Microservice-based architecture of an order management system&quot; /&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This system comprises three services, &lt;em&gt;Order&lt;/em&gt;, &lt;em&gt;Item&lt;/em&gt; and &lt;em&gt;Stock&lt;/em&gt;.
      If the &lt;em&gt;Order&lt;/em&gt; service receives an order request, it will need information from the other two,
      such as item definitions or the stock count for specific items.
      Instead of making synchronous calls to these services to obtain this information,
      CDC can be used to set up change event streams for the data managed by the &lt;em&gt;Item&lt;/em&gt; and &lt;em&gt;Stock&lt;/em&gt; services.
      The &lt;em&gt;Order&lt;/em&gt; service can subscribe to these event streams and keep a local copy of the relevant item and stock data in its own database.
      This approach helps to decouple the services
      (e.g. no direct impact by service outages)
      and can also be beneficial for overall performance,
      as each service can hold optimized views just of those data items owned by other services which it is interested in.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;how_to_handle_aggregate_objects&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#how_to_handle_aggregate_objects&quot;&gt;&lt;/a&gt;How to Handle Aggregate Objects?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;There are use cases however, where things are a bit more tricky. It is sometimes
      useful to share information across services and data stores by means of so-called
      aggregates, which are a concept/pattern defined by domain-driven design (DDD).
      In general, a &lt;a href=&quot;https://martinfowler.com/bliki/DDD_Aggregate.html&quot;&gt;DDD aggregate&lt;/a&gt; is used
      to transfer state which can be comprised of multiple different domain objects that are
      together treated as a single unit of information.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Concrete examples are:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;strong&gt;customers and their addresses&lt;/strong&gt; which are represented as a customer record &lt;em&gt;aggregate&lt;/em&gt;
      storing a customer and a list of addresses&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;strong&gt;orders and corresponding line items&lt;/strong&gt; which are represented as an order record
      &lt;em&gt;aggregate&lt;/em&gt; storing an order and all its line items&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Chances are that the data of the involved domain objects backing these DDD aggregates are stored in
      separate relations of an RDBMS. When making use of the CDC capabilities currently found
      in Debezium, all changes to domain objects will be independently captured and by default eventually
      reflected in separate Kafka topics, one per RDBMS relation. While this behaviour
      is tremendously helpful for a lot of use cases it can be pretty limiting to others,
      like the DDD aggregate scenario described above.
      Therefore, this blog post explores how DDD aggregates can be built based on Debezium CDC events,
      using the &lt;a href=&quot;https://kafka.apache.org/documentation/streams/&quot;&gt;Kafka Streams API&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;capturing_change_events_from_a_data_source&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#capturing_change_events_from_a_data_source&quot;&gt;&lt;/a&gt;Capturing Change Events from a Data Source&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The complete source code for this blog post is provided in the Debezium &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/kstreams&quot;&gt;examples repository&lt;/a&gt; on GitHub.
      Begin by cloning this repository and changing into the &lt;em&gt;kstreams&lt;/em&gt; directory:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;git clone https://github.com/debezium/debezium-examples.git
      cd kstreams&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The project provides a Docker Compose file with services for all the components you may already know from the &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;Debezium tutorial&lt;/a&gt;:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://zookeeper.apache.org/&quot;&gt;Apache ZooKeeper&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;A &lt;a href=&quot;https://kafka.apache.org/documentation/#connect&quot;&gt;Kafka Connect&lt;/a&gt; instance with the Debezium CDC connectors&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.mysql.com/&quot;&gt;MySQL&lt;/a&gt; (populated with some test data)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In addition it declares the following services:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.mongodb.com/&quot;&gt;MongoDB&lt;/a&gt; which will be used as a data sink&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Another Kafka Connect instance which will host the MongoDB sink connector&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;A service for running the DDD aggregation process we’re going to build in the following&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll get to those three in a bit, for now let’s prepare the source side of our pipeline:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;export DEBEZIUM_VERSION=0.7
      docker-compose up mysql zookeeper kafka connect_source&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once all services have been started, register an instance of the Debezium MySQL connector by submitting the following JSON document:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
          &quot;name&quot;: &quot;mysql-source&quot;,
          &quot;config&quot;: {
              &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
              &quot;tasks.max&quot;: &quot;1&quot;,
              &quot;database.hostname&quot;: &quot;mysql&quot;,
              &quot;database.port&quot;: &quot;3306&quot;,
              &quot;database.user&quot;: &quot;debezium&quot;,
              &quot;database.password&quot;: &quot;dbz&quot;,
              &quot;database.server.id&quot;: &quot;184054&quot;,
              &quot;database.server.name&quot;: &quot;dbserver1&quot;,
              &quot;table.whitelist&quot;: &quot;inventory.customers,inventory.addresses&quot;,
              &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
              &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot;,
              &quot;transforms&quot;: &quot;unwrap&quot;,
              &quot;transforms.unwrap.type&quot;:&quot;io.debezium.transforms.UnwrapFromEnvelope&quot;,
              &quot;transforms.unwrap.drop.tombstones&quot;:&quot;false&quot;
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To do so, run the following curl command:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ -d @mysql-source.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This sets up the connector for the specified database, using the given credentials.
      For our purposes we’re only interested in changes to the &lt;code&gt;customers&lt;/code&gt; and &lt;code&gt;addresses&lt;/code&gt; tables,
      hence the &lt;code&gt;table.whitelist&lt;/code&gt; property is given to just select these two tables.
      Another noteworthy thing is the &quot;unwrap&quot; transform that is applied.
      By default, Debezium’s CDC events would contain the old and new state of changed rows and some additional metadata on the source of the change.
      By applying the &lt;a href=&quot;http://debezium.io/docs/configuration/event-flattening/&quot;&gt;UnwrapFromEnvelope&lt;/a&gt; SMT (single message transformation),
      only the new state will be propagated into the corresponding Kafka topics.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We can take a look at them once the connector has been deployed and finished its initial snapshot of the two captured tables:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker-compose exec kafka /kafka/bin/kafka-console-consumer.sh \
          --bootstrap-server kafka:9092 \
          --from-beginning \
          --property print.key=true \
          --topic dbserver1.inventory.customers # or dbserver1.inventory.addresses&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;E.g. you should see the following output&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;(formatted and omitting the schema information for the sake of readability) for the topic with customer changes:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;{
          &quot;schema&quot;: { ... },
          &quot;payload&quot;: {
              &quot;id&quot;: 1001
          }
      }
      {
          &quot;schema&quot;: { ... },
          &quot;payload&quot;: {
              &quot;id&quot;: 1001,
              &quot;first_name&quot;: &quot;Sally&quot;,
              &quot;last_name&quot;: &quot;Thomas&quot;,
              &quot;email&quot;: &quot;sally.thomas@acme.com&quot;
          }
      }
      ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;building_ddd_aggregates&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#building_ddd_aggregates&quot;&gt;&lt;/a&gt;Building DDD Aggregates&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The KStreams application is going to process data from the two Kafka topics. These topics
      receive CDC events based on the customers and addresses relations found in MySQL, each of which has its
      corresponding Jackson-annotated POJO (Customer and Address), enriched by a field holding the CDC event type (i.e. UPSERT/DELETE).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Since the Kafka topic records are in Debezium JSON format with unwrapped envelopes, a special &lt;strong&gt;SerDe&lt;/strong&gt;
      has been written in order to be able to read/write these records using their POJO or Debezium event representation respectively.
      While the serializer simply converts the POJOs into JSON using Jackson, the deserializer is a &quot;hybrid&quot;
      one, being able to deserialize from either Debezium CDC events or jsonified POJOs.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With that in place, the KStreams topology to create and maintain DDD aggregates on-the-fly can be built as follows:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;customers_topic_parent&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#customers_topic_parent&quot;&gt;&lt;/a&gt;Customers Topic (&quot;parent&quot;)&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;All the customer records are simply read from the customer topic into a &lt;strong&gt;KTable&lt;/strong&gt; which will automatically maintain
      the latest state per customer according to the record key (i.e. the customer’s PK)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;KTable&amp;lt;DefaultId, Customer&amp;gt; customerTable =
              builder.table(parentTopic, Consumed.with(defaultIdSerde,customerSerde));&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;addresses_topic_children&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#addresses_topic_children&quot;&gt;&lt;/a&gt;Addresses Topic (&quot;children&quot;)&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For the address records the processing is a bit more involved and needs several steps. First, all the address
      records are read into a &lt;strong&gt;KStream&lt;/strong&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;KStream&amp;lt;DefaultId, Address&amp;gt; addressStream = builder.stream(childrenTopic,
              Consumed.with(defaultIdSerde, addressSerde));&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Second, a 'pseudo' grouping of these address records is done based on their keys (the original primary key in the relation),
      During this step the relationships towards the corresponding customer records are maintained. This effectively allows to keep
      track which address record belongs to which customer record, even in the light of address record deletions.
      To achieve this an additional &lt;em&gt;LatestAddress&lt;/em&gt; POJO is introduced which allows to store the latest known PK &amp;lt;→ FK
      relation in addition to the &lt;em&gt;Address&lt;/em&gt; record itself.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;KTable&amp;lt;DefaultId,LatestAddress&amp;gt; tempTable = addressStream
              .groupByKey(Serialized.with(defaultIdSerde, addressSerde))
              .aggregate(
                      () -&amp;gt; new LatestAddress(),
                      (DefaultId addressId, Address address, LatestAddress latest) -&amp;gt; {
                          latest.update(
                              address, addressId, new DefaultId(address.getCustomer_id()));
                          return latest;
                      },
                      Materialized.&amp;lt;DefaultId,LatestAddress,KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;
                              as(childrenTopic+&quot;_table_temp&quot;)
                                  .withKeySerde(defaultIdSerde)
                                      .withValueSerde(latestAddressSerde)
              );&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Third, the intermediate &lt;strong&gt;KTable&lt;/strong&gt; is again converted to a &lt;strong&gt;KStream&lt;/strong&gt;. The &lt;em&gt;LatestAddress&lt;/em&gt; records are transformed
      to have the customer id (FK relationship) as their new key in order to group them per customer.
      During the grouping step, customer specific addresses are updated which can result in an address
      record being added or deleted. For this purpose, another POJO called &lt;em&gt;Addresses&lt;/em&gt; is introduced, which
      holds a map of address records that gets updated accordingly. The result is a &lt;strong&gt;KTable&lt;/strong&gt; holding the
      most recent &lt;em&gt;Addresses&lt;/em&gt; per customer id.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;KTable&amp;lt;DefaultId, Addresses&amp;gt; addressTable = tempTable.toStream()
              .map((addressId, latestAddress) -&amp;gt;
                  new KeyValue&amp;lt;&amp;gt;(latestAddress.getCustomerId(),latestAddress))
              .groupByKey(Serialized.with(defaultIdSerde,latestAddressSerde))
              .aggregate(
                      () -&amp;gt; new Addresses(),
                      (customerId, latestAddress, addresses) -&amp;gt; {
                          addresses.update(latestAddress);
                          return addresses;
                      },
                      Materialized.&amp;lt;DefaultId,Addresses,KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;
                              as(childrenTopic+&quot;_table_aggregate&quot;)
                                  .withKeySerde(defaultIdSerde)
                                      .withValueSerde(addressesSerde)
              );&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;combining_customers_with_addresses&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#combining_customers_with_addresses&quot;&gt;&lt;/a&gt;Combining Customers With Addresses&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, it’s easy to bring customers and addresses together by &lt;strong&gt;joining the customers KTable with
      the addresses KTable&lt;/strong&gt; and thereby building the DDD aggregates which are represented by the &lt;em&gt;CustomerAddressAggregate&lt;/em&gt; POJO.
      At the end, the KTable changes are written to a KStream, which in turn gets saved into a kafka topic.
      This allows to make use of the resulting DDD aggregates in manifold ways.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;KTable&amp;lt;DefaultId,CustomerAddressAggregate&amp;gt; dddAggregate =
                customerTable.join(addressTable, (customer, addresses) -&amp;gt;
                    customer.get_eventType() == EventType.DELETE ?
                            null :
                            new CustomerAddressAggregate(customer,addresses.getEntries())
                );
      
        dddAggregate.toStream().to(&quot;final_ddd_aggregates&quot;,
                                    Produced.with(defaultIdSerde,(Serde)aggregateSerde));&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;admonitionblock note&quot;&gt;
      &lt;table&gt;
      &lt;tr&gt;
      &lt;td class=&quot;icon&quot;&gt;
      &lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot;&gt;&lt;/i&gt;
      &lt;/td&gt;
      &lt;td class=&quot;content&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Records in the customers KTable might receive a CDC delete event. If so, this can be detected by
      checking the event type field of the customer POJO and e.g. return 'null' instead of a DDD aggregate.
      Such a convention can be helpful whenever consuming parties also need to act to deletions accordingly._&lt;/p&gt;
      &lt;/div&gt;
      &lt;/td&gt;
      &lt;/tr&gt;
      &lt;/table&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;running_the_aggregation_pipeline&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#running_the_aggregation_pipeline&quot;&gt;&lt;/a&gt;Running the Aggregation Pipeline&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Having implemented the aggregation pipeline, it’s time to give it a test run.
      To do so, build the &lt;em&gt;poc-ddd-aggregates&lt;/em&gt; Maven project which contains the complete implementation:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;mvn clean package -f poc-ddd-aggregates/pom.xml&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Then run the &lt;code&gt;aggregator&lt;/code&gt; service from the Compose file which takes the JAR built by this project
      and launches it using the &lt;a href=&quot;https://hub.docker.com/r/fabric8/java-jboss-openjdk8-jdk/&quot;&gt;java-jboss-openjdk8-jdk&lt;/a&gt; base image:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker-compose up -d aggregator&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once the aggregation pipeline is running, we can take a look at the aggregated events using the console consumer:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker-compose exec kafka /kafka/bin/kafka-console-consumer.sh \
          --bootstrap-server kafka:9092 \
          --from-beginning \
          --property print.key=true \
          --topic final_ddd_aggregates&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;transferring_ddd_aggregates_to_data_sinks&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#transferring_ddd_aggregates_to_data_sinks&quot;&gt;&lt;/a&gt;Transferring DDD Aggregates to Data Sinks&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We originally set out to build these DDD aggregates in order to transfer data and synchronize changes between
      a data source (MySQL tables in this case) and a convenient data sink. By definition,
      DDD aggregates are typically complex data structures and therefore it makes perfect sense to write them
      to data stores which offer flexible ways and means to query and/or index them. Talking about NoSQL databases, a
      document store seems the most natural choice with &lt;a href=&quot;https://www.mongodb.com/&quot;&gt;MongoDB&lt;/a&gt; being the leading database
      for such use cases.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to &lt;a href=&quot;https://kafka.apache.org/documentation/#connect&quot;&gt;Kafka Connect&lt;/a&gt; and numerous turn-key ready
      &lt;a href=&quot;https://www.confluent.io/product/connectors/&quot;&gt;connectors&lt;/a&gt; it is almost effortless to get this done.
      Using a &lt;a href=&quot;https://github.com/hpgrahsl/kafka-connect-mongodb&quot;&gt;MongoDB sink connector&lt;/a&gt; from the open-source community,
      it is easy to have the DDD aggregates written into MongoDB. All it needs is a proper configuration which can be posted
      to the &lt;a href=&quot;https://docs.confluent.io/current/connect/restapi.html&quot;&gt;REST API&lt;/a&gt; of Kafka Connect in order to run the connector.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So let’s start MongoDb and another Kafka Connect instance for hosting the sink connector:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker-compose up -d mongodb connect_sink&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In case the DDD aggregates should get written unmodified into MongoDB, a configuration may look as simple as follows:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
          &quot;name&quot;: &quot;mongodb-sink&quot;,
          &quot;config&quot;: {
              &quot;connector.class&quot;: &quot;at.grahsl.kafka.connect.mongodb.MongoDbSinkConnector&quot;,
              &quot;tasks.max&quot;: &quot;1&quot;,
              &quot;topics&quot;: &quot;final_ddd_aggregates&quot;,
              &quot;mongodb.connection.uri&quot;: &quot;mongodb://mongodb:27017/inventory?w=1&amp;amp;journal=true&quot;,
              &quot;mongodb.collection&quot;: &quot;customers_with_addresses&quot;,
              &quot;mongodb.document.id.strategy&quot;: &quot;at.grahsl.kafka.connect.mongodb.processor.id.strategy.FullKeyStrategy&quot;,
              &quot;mongodb.delete.on.null.values&quot;: true
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As with the source connector, deploy the connector using curl:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; -H  &quot;Content-Type:application/json&quot; http://localhost:8084/connectors/ -d @mongodb-sink.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This connector will consume messages from the &quot;final_ddd_aggregates&quot; Kafka topic and
      write them as &lt;strong&gt;MongoDB documents&lt;/strong&gt; into the &quot;customers_with_addresses&quot; collection.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You can take a look by firing up a Mongo shell and querying the collection’s contents:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker-compose exec mongodb bash -c 'mongo inventory'
      
      &amp;gt; db.customers_with_addresses.find().pretty()&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
          &quot;_id&quot;: {
              &quot;id&quot;: &quot;1001&quot;
          },
          &quot;addresses&quot;: [
              {
                  &quot;zip&quot;: &quot;76036&quot;,
                  &quot;_eventType&quot;: &quot;UPSERT&quot;,
                  &quot;city&quot;: &quot;Euless&quot;,
                  &quot;street&quot;: &quot;3183 Moore Avenue&quot;,
                  &quot;id&quot;: &quot;10&quot;,
                  &quot;state&quot;: &quot;Texas&quot;,
                  &quot;customer_id&quot;: &quot;1001&quot;,
                  &quot;type&quot;: &quot;SHIPPING&quot;
              },
              {
                  &quot;zip&quot;: &quot;17116&quot;,
                  &quot;_eventType&quot;: &quot;UPSERT&quot;,
                  &quot;city&quot;: &quot;Harrisburg&quot;,
                  &quot;street&quot;: &quot;2389 Hidden Valley Road&quot;,
                  &quot;id&quot;: &quot;11&quot;,
                  &quot;state&quot;: &quot;Pennsylvania&quot;,
                  &quot;customer_id&quot;: &quot;1001&quot;,
                  &quot;type&quot;: &quot;BILLING&quot;
              }
          ],
          &quot;customer&quot;: {
              &quot;_eventType&quot;: &quot;UPSERT&quot;,
              &quot;last_name&quot;: &quot;Thomas&quot;,
              &quot;id&quot;: &quot;1001&quot;,
              &quot;first_name&quot;: &quot;Sally&quot;,
              &quot;email&quot;: &quot;sally.thomas@acme.com&quot;
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Due to the combination of the data in a single document some parts aren’t needed or redundant. To get rid of any
      unwanted data (e.g. _eventType, customer_id of each address sub-document) it would also be possible
      to adapt the configuration in order to blacklist said fields.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, you update some customer or address data in the MySQL source database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER -p$MYSQL_PASSWORD inventory'
      
      mysql&amp;gt; update customers set first_name= &quot;Sarah&quot; where id = 1001;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Shortly thereafter, you should see that the corresponding aggregate document in MongoDB has been updated accordingly.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;drawbacks_and_limitations&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#drawbacks_and_limitations&quot;&gt;&lt;/a&gt;Drawbacks and Limitations&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While this first version for creating DDD aggregates from table-based CDC events basically works, it is very important to understand its current limitations:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;not generically applicable thus needs custom code for POJOs and intermediate types&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;cannot be scaled across multiple instances as is due to missing but necessary data repartitioning prior to processing&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;limited to building aggregates based on a single JOIN between 1:N relationships&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;resulting DDD aggregates are eventually consistent, meaning that it is possible for them to temporarily exhibit intermediate state before converging&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The first few can be addressed with a reasonable amount of work on the KStreams application. The last one,
      dealing with the eventually consistent nature of resulting DDD aggregates is much harder to correct
      and will require some efforts at Debezium’s own CDC mechanism.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;outlook&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#outlook&quot;&gt;&lt;/a&gt;Outlook&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In this post we described an approach for creating aggregated events from Debezium’s CDC events.
      In a follow-up blog post we may dive a bit more into the topic of how to be able to horizontally scale
      the DDD creation by running multiple KStreams aggregator instances. For that purpose, the data needs proper
      re-partitioning before running the topology. In addition, it could be interesting to look into
      a somewhat more generic version which only needs custom classes to the describe the two main POJOs involved.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We also thought about providing a ready-to-use component which would work in a generic way
      (based on Connect records, i.e. not tied to a specific serialization format such as JSON) and
      could be set up as a configurable stand-alone process running given aggregations.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also on the topic of dealing with eventual consistency we got some ideas,
      but those will need some more exploration and investigation for sure.
      Stay tuned!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’d love to hear about your feedback on the topic of event aggreation.
      If you got any ideas or thoughts on the subject,
      please get in touch by posting a comment below or sending a message to our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/03/07/debezium-0-7-4-released/</id>
    <title>Debezium 0.7.4 Is Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-03-07T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/03/07/debezium-0-7-4-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s my pleasure to announce the release of Debezium 0.7.4!
      
      
      Continuing the 0.7 release line, this new version brings several bug fixes and a handful of new features.
      We recommend this upgrade to all users.
      When upgrading from earlier versions,
      please check out the release notes of all versions between the one you&#8217;re currently on and 0.7.4 in order to learn about any steps potentially required for upgrading.
      
      
      
      
      New features
      
      
      In terms of new features, there&#8217;s a new mode for handling decimal columns in Postgres and MySQL (DBZ-611).
      By setting the decimal.handling.mode connector option to string, Debezium will emit decimal and numeric columns as Strings.
      That oftentimes is...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.7.4&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Continuing the 0.7 release line, this new version brings several bug fixes and a handful of new features.
      We recommend this upgrade to all users.
      When upgrading from earlier versions,
      please check out the &lt;a href=&quot;http://debezium.io/docs/releases/&quot;&gt;release notes&lt;/a&gt; of all versions between the one you’re currently on and 0.7.4 in order to learn about any steps potentially required for upgrading.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;new_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_features&quot;&gt;&lt;/a&gt;New features&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In terms of new features, there’s a new mode for handling decimal columns in Postgres and MySQL (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-611&quot;&gt;DBZ-611&lt;/a&gt;).
      By setting the &lt;code&gt;decimal.handling.mode&lt;/code&gt; connector option to &lt;code&gt;string&lt;/code&gt;, Debezium will emit decimal and numeric columns as Strings.
      That oftentimes is easier to handle for consumers than the byte-array based representation used by default, while keeping the full precision.
      As a bonus, &lt;code&gt;string&lt;/code&gt; also allows to convey the special numeric values &lt;code&gt;NaN&lt;/code&gt; and &lt;code&gt;Infinity&lt;/code&gt; as supported by Postgres.
      Note that this functionality required an update to Debezium’s logical decoding plug-in which runs within the Postgres database server.
      This plug-in must be upgraded to the new version &lt;em&gt;before&lt;/em&gt; upgrading the Debezium Postgres connector.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Speaking of byte arrays, the &lt;code&gt;BYTEA&lt;/code&gt; column type in Postgres is now also supported (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-605&quot;&gt;DBZ-605&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For the MySQL connector, there’s a new option to the snapshotting routine: &lt;code&gt;snapshot.locking.mode&lt;/code&gt; (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-602&quot;&gt;DBZ-602&lt;/a&gt;).
      By setting this to &lt;code&gt;NONE&lt;/code&gt;, this option allows to skip any table locks during snapshotting.
      This should be used if and only if you’re absolutely sure that the tables don’t undergo structural changes (columns added, removed etc.)
      while the snapshot is taken.
      But if that’s guaranteed, the new mode can be a useful tool for increasing overall system performance, as writes by concurrent processes won’t be blocked.
      That’s especially useful on environments such as Amazon RDS, where the connector otherwise would be required to keep a lock for the entirety of the snapshot.
      The new option supersedes the existing &lt;code&gt;snapshot.minimal.locks&lt;/code&gt; option.
      Please see the connector documentation for &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/#connector-properties&quot;&gt;the details&lt;/a&gt;.
      This feature was contributed by our community member &lt;a href=&quot;https://github.com/Crim&quot;&gt;Stephen Powis&lt;/a&gt;; many thanks to you!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;bug_fixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bug_fixes&quot;&gt;&lt;/a&gt;Bug Fixes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;0.7.4 brings multiple fixes related to how numeric columns are handled.
      E.g. columns without scale couldn’t correctly be processed by the MySQL connector during binlog reading (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-615&quot;&gt;DBZ-615&lt;/a&gt;).
      That’s fixed now.
      And when using the Postgres connector, arbitrary precision column values are correctly converted into change data message fields now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-351&quot;&gt;DBZ-351&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We also noticed a regression introduced in Debezium 0.6:
      the field schema for &lt;code&gt;NUMERIC&lt;/code&gt; columns was always marked as optional, also if that column was actually declared as &lt;code&gt;NOT NULL&lt;/code&gt;.
      The same affected geo-spatial array types on Postgres as supported as of Debezium 0.7.
      This has been fixed with &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-635&quot;&gt;DBZ-635&lt;/a&gt;.
      We don’t expect any impact on consumers by this change
      (just as before, they’ll always get a value for such field, only its schema won’t be incorrectly marked as optional any more).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-4&quot;&gt;full change log&lt;/a&gt; for more details and the complete list of issues fixed in Debezium 0.7.4.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Following our three weeks release cadence, the next Debezium release is planned for March 28th.
      We got some exciting changes in the works for that:
      if things go as planned, we’ll release the first version of our Oracle connector (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-20&quot;&gt;DBZ-20&lt;/a&gt;).
      This will be based on the Oracle XStream API in the first iteration and not support snapshots yet.
      But we felt it’d make sense to roll out this connector incrementally, so to get out the new feature early on and collect feedback on it.
      We’ve also planned to explore alternatives to using the XStream API in future releases.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Another great new feature will be &lt;a href=&quot;http://www.reactive-streams.org/&quot;&gt;Reactive Streams&lt;/a&gt; support (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-566&quot;&gt;DBZ-566&lt;/a&gt;).
      Based on top of the existing &lt;a href=&quot;http://debezium.io/docs/embedded/&quot;&gt;embedded mode&lt;/a&gt;,
      this will make it very easy to consume change data events using Reactive Streams implementations such as RxJava 2, the Java 9 Flow API and many more.
      It’ll also be very useful to consume change events in reactive frameworks such as Vert.x.
      We’re really looking forward to shipping this feature and already have a pending &lt;a href=&quot;https://github.com/debezium/debezium/pull/458&quot;&gt;pull request&lt;/a&gt; for it.
      If you like, take a look and let us know about your feedback!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please also check out our &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;roadmap&lt;/a&gt; for the coming months of Debezium’s development.
      This is our current plan for the things we’ll work on,
      but it’s not cast in stone, so please tell us about your feature requests by sending a message to our Google group.
      We’re looking forward to your feedback!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/02/15/debezium-0-7-3-released/</id>
    <title>Debezium 0.7.3 Is Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-02-15T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/02/15/debezium-0-7-3-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m very happy to announce the release of Debezium 0.7.3!
      
      
      This is primarily a bugfix release, but we&#8217;ve also added a handful of smaller new features.
      It&#8217;s a recommended upgrade for all users.
      When upgrading from earlier versions,
      please check out the release notes of all versions between the one your&#8217;re currently on and 0.7.3 in order to learn about any steps potentially required for upgrading.
      
      
      Let&#8217;s take a closer look at some of the new features.
      
      
      
      
      All Connectors
      
      
      Using the new connector option tombstones.on.delete you can now control whether upon record deletions a tombstone event should be emitted or not
      (DBZ-582).
      Doing so is usually the right thing...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m very happy to announce the release of Debezium &lt;strong&gt;0.7.3&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This is primarily a bugfix release, but we’ve also added a handful of smaller new features.
      It’s a recommended upgrade for all users.
      When upgrading from earlier versions,
      please check out the &lt;a href=&quot;http://debezium.io/docs/releases/&quot;&gt;release notes&lt;/a&gt; of all versions between the one your’re currently on and 0.7.3 in order to learn about any steps potentially required for upgrading.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s take a closer look at some of the new features.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;all_connectors&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#all_connectors&quot;&gt;&lt;/a&gt;All Connectors&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Using the new connector option &lt;code&gt;tombstones.on.delete&lt;/code&gt; you can now control whether upon record deletions a tombstone event should be emitted or not
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-582&quot;&gt;DBZ-582&lt;/a&gt;).
      Doing so is usually the right thing and thus remains the default behaviour.
      But disabling tombstones may be desirable in certain situations,
      and this gets a bit easier now using that option
      (before you’d have to use an SMT - single message transform -, which for instance isn’t supported when using Debezium’s embedded mode).
      This feature was contributed by our community member &lt;a href=&quot;https://github.com/rliwoch&quot;&gt;Raf Liwoch&lt;/a&gt;. Thanks!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also spent some time on a few operational aspects:
      The &lt;code&gt;sourceInfo&lt;/code&gt; element of Debezium’s change data messages contains a new field representing the version of the connector that created the message
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-593&quot;&gt;DBZ-593&lt;/a&gt;).
      This lets message consumers take specific action based on the version.
      For instance this can be helpful where a new Debezium release fixes a bug, which consumers could work around so far.
      Now, after the update to that new Debezium version, that workaround should not be applied anymore.
      The version field will allow consumers to decide whether to apply the workaround or not.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The names of all the threads managed by Debezium are now structured in the form of &quot;debezium-&amp;lt;connector&amp;gt;-…​&quot;
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-587&quot;&gt;DBZ-587&lt;/a&gt;).
      This helps with identifying Debezium’s threads when analyzing thread dumps for instance.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;postgres_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#postgres_connector&quot;&gt;&lt;/a&gt;Postgres Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Here we’ve focused on improving the support for array types:
      besides fixing a bug related to numeric arrays (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-577&quot;&gt;DBZ-577&lt;/a&gt;)
      we’ve also completed the support for the PostGIS types (which was introduced in 0.7.2),
      allowing you to capture array columns of types &lt;code&gt;GEOMETRY&lt;/code&gt; and &lt;code&gt;GEOGRAPHY&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Snapshots are now correctly interruptable (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-586&quot;&gt;DBZ-586&lt;/a&gt;)
      and the connector will correctly handle the case where after a restart it should continue from a WAL position which isn’t available any more:
      it’ll stop, requiring you to do a new snapshot (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-590&quot;&gt;DBZ-590&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mysql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mysql_connector&quot;&gt;&lt;/a&gt;MySQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The MySQL connector can create the DB history topic automatically, if needed
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-278&quot;&gt;DBZ-278&lt;/a&gt;).
      This means you don’t have to create that topic yourself and you also don’t need to rely on Kafka’s automatic topic creation any longer
      (any change data topics will automatically be created by Kafka Connect).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also the connector can optionally emit messages to a dedicated heartbeat topic in a configurable interval
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-220&quot;&gt;DBZ-220&lt;/a&gt;).
      This comes in handy in situations where you only want to capture tables with low traffic,
      while other tables in the database are changed more frequently.
      In that case, no messages would have been emitted to Kafka Connect for a long time,
      and thus no offset would have been committed either.
      This could have caused trouble when restarting the connector: it wanted to resume from the last comitted offset,
      which may not be available in the binlogs any longer.
      But as the captured tables didn’t change, it actually wouldn’t be necessary to resume from such old binlog position.
      This all is avoided by emitting messages to the heartbeat topic regularly, which causes the last offset the connector has seen to be committed.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll roll out this change to the other connectors, too, in future releases.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-3&quot;&gt;full change log&lt;/a&gt; for more details and the complete list of issues fixed in Debezium 0.7.3.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The next release is scheduled for March 7th.
      We’ll still have to decide whether that will be 0.7.4 or 0.8.0, depending on how far we are by then with our work on the Oracle connector
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-137&quot;&gt;DBZ-137&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please also our &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;roadmap&lt;/a&gt; describing our ideas for future development of Debezium.
      This is our current thinking of the things we’d like to tackle in the coming months,
      but it’s not cast in stone, so please let us know about your feature requests by sending a message to our Google group.
      We’re looking forward to your feedback!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/01/25/debezium-0-7-2-released/</id>
    <title>Debezium 0.7.2 Is Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-01-25T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/01/25/debezium-0-7-2-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s my pleasure to announce the release of Debezium 0.7.2!
      
      
      Amongst the new features there&#8217;s support for geo-spatial types,
      a new snapshotting mode for recovering a lost DB history topic for the MySQL connector,
      and a message transformation for converting MongoDB change events into a structure which can be consumed by many more sink connectors.
      And of course we fixed a whole lot of bugs, too.
      
      
      Debezium 0.7.2 is a drop-in replacement for previous 0.7.x versions.
      When upgrading from versions earlier than 0.7.0,
      please check out the release notes of all 0.7.x releases to learn about any steps potentially required for upgrading.
      
      
      A big thank you goes out...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.7.2&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Amongst the new features there’s support for geo-spatial types,
      a new snapshotting mode for recovering a lost DB history topic for the MySQL connector,
      and a message transformation for converting MongoDB change events into a structure which can be consumed by many more sink connectors.
      And of course we fixed a whole lot of bugs, too.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium 0.7.2 is a drop-in replacement for previous 0.7.x versions.
      When upgrading from versions earlier than 0.7.0,
      please check out the &lt;a href=&quot;http://debezium.io/docs/releases/&quot;&gt;release notes&lt;/a&gt; of all 0.7.x releases to learn about any steps potentially required for upgrading.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A big thank you goes out to our fantastic community members for their hard work on this release:
      &lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/notxcain&quot;&gt;Denis Mikhaylov&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/pgoranss&quot;&gt;Peter Goransson&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/rcoup&quot;&gt;Robert Coup&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/sairam881990&quot;&gt;Sairam Polavarapu&lt;/a&gt; and
      &lt;a href=&quot;https://github.com/tombentley&quot;&gt;Tom Bentley&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now let’s take a closer look at some of new features.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mysql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mysql_connector&quot;&gt;&lt;/a&gt;MySQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The biggest change of the MySQL connector is &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-507&quot;&gt;support for geo-spatial column types&lt;/a&gt; such as &lt;code&gt;GEOMETRY&lt;/code&gt;, &lt;code&gt;POLYGON&lt;/code&gt;, &lt;code&gt;MULTIPOINT&lt;/code&gt; etc.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;There are two new logical field types — &lt;code&gt;io.debezium.data.geometry.Geometry&lt;/code&gt; and &lt;code&gt;io.debezium.data.geometry.Geography&lt;/code&gt; — for representing geo-spatial columns in change data messages.
      These types represent geo-spatial data via WKB (&quot;well-known binary&quot;) and SRID (coordinate reference system identifier),
      allowing downstream consumers to interpret the change events using any existing library with support for parsing WKB.
      A blog post with more details on this will follow soon.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-443&quot;&gt;new snapshotting mode&lt;/a&gt; &lt;code&gt;schema_only_recovery&lt;/code&gt; comes in handy
      when for some reason you lost (parts of) the DB history topic used by the MySQL connector.
      It’s also useful if you’d like to compact that topic by re-creating it.
      Please refer to the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;connector documentation&lt;/a&gt; for the details of this mode,
      esp. when it’s safe (and when not) to make use of it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Another new feature related to managing the size of the DB history topic is &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-541&quot;&gt;the option&lt;/a&gt; to control
      whether to include all DDL events or only those pertaining to tables captured as per the whitelist/blacklist configuration.
      Again, check out the connector docs to learn more about the specifics of that setting.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, we fixed a few shortcomings of the MySQL DDL parser (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-524&quot;&gt;DBZ-524&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-530&quot;&gt;DBZ-530&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;postgresql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#postgresql_connector&quot;&gt;&lt;/a&gt;PostgreSQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Similar to the MySQL connector, there’s largely improved support for geo-spatial columns in Postgres now.
      More specifically, PostGIS column types can be represented in change data events now.
      Thanks a lot for Robert Coup who contributed this feature!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also the support for Postgres &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-506&quot;&gt;array columns&lt;/a&gt; has been expanded,
      e.g. we now support to track changes to &lt;code&gt;VARCHAR&lt;/code&gt; and &lt;code&gt;DATE&lt;/code&gt; array columns.
      Note that the connector doesn’t yet work with  geo-spatial array columns (should you ever have those),
      but this &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-571&quot;&gt;should be added&lt;/a&gt; soon, too.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to include just a subset of the rows of a captured table in snapshots, you may like the ability to &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-506&quot;&gt;specify
      dedicated SELECT statements&lt;/a&gt; to do so.
      For instance this can be used to exclude any logically deleted records — which you can recognize based on some flag in that table — from the snapshot.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A few bugs in this connector where reported and fixed by community members, too,
      e.g. the connector can be &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-519&quot;&gt;correctly paused&lt;/a&gt; now (thanks, Andrey Pustovetov),
      and we fixed an issue which could potentially have committed an &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-521&quot;&gt;incorrect offset&lt;/a&gt; to Kafka Connect (thanks, Thon Mekathikom).&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mongodb_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mongodb_connector&quot;&gt;&lt;/a&gt;MongoDB Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’ve ever compared the structures of change events emitted by the Debezium RDBMS connectors (MySQL, Postgres) and the MongoDB connector,
      you’ll know that the message structure of the latter is a bit different than the others.
      Due to the schemaless nature of MongoDB, the change events essentially contain a String with a JSON representation of the applied insert or patch.
      This structure cannot be consumed by existing sink connectors, such as the Confluent connectors for JDBC or Elasticsearch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This gets &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-409&quot;&gt;possible now&lt;/a&gt; by means of a newly added single message transformation (SMT),
      which parses these JSON strings and creates a structured Kafka Connect record from it (thanks, Sairam Polavarapu!).
      When applying this SMT to the JDBC sink connector, you can now stream data changes from MongoDB to any supported relational database.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note that this SMT is work-in-progress, details of its emitted message structure may still change.
      Also there are some inherent limitations to what can be achieved with it, if you e.g. have arrays in your MongoDB documents,
      the record created by this SMT will be structured accordingly, but many sink connectors cannot process such structure.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We have some ideas for further development here, e.g. there could be &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-561&quot;&gt;an option&lt;/a&gt; for flattening out (non-array) nested structures,
      so that e.g. &lt;code&gt;{ &quot;address&quot; { &quot;street&quot; : &quot;...&quot; } }&lt;/code&gt; would be represented as &lt;code&gt;address_street&lt;/code&gt;,
      which then could be consumed by sink connectors expecting a flat structure.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The new SMT is described in detail in &lt;a href=&quot;http://debezium.io/docs/configuration/mongodb-event-flattening/&quot;&gt;our docs&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-2&quot;&gt;full change log&lt;/a&gt; for more details and the complete list of issues fixed in Debezium 0.7.2.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The 0.7.3 release is scheduled for February 14th.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll focus on some more bug fixes, also we’re working on having Debezium regulary emit &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-409&quot;&gt;heartbeat messages&lt;/a&gt; to a dedicated topic.
      This will be practical for diagnostic purposes but also help to regularly trigger commits of the offset in Kafka Connect.
      That’s beneficial in certain situations when capturing tables which only very infrequently change.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also worked out &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;a roadmap&lt;/a&gt; describing our ideas for future work on Debezium, going beyond the next bugfix releases.
      While nothing is cast in stone, this is our idea of the features to add in the coming months.
      If you miss anything important on this roadmap, please tell us either in the comments below or send a message to our Google group.
      Looking forward to your feedback!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/01/17/streaming-to-elasticsearch/</id>
    <title>Streaming Data Changes from Your Database to Elasticsearch</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2018-01-17T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/01/17/streaming-to-elasticsearch/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="elasticsearch"></category>
    <category term="smt"></category>
    <category term="example"></category>
    <summary>
      
      
      
      We wish all the best to the Debezium community for 2018!
      
      
      While we&#8217;re working on the 0.7.2 release, we thought we&#8217;d publish another post describing an end-to-end data streaming use case based on Debezium.
      We have seen how to set up a change data stream to a downstream database a few weeks ago.
      In this blog post we will follow the same approach to stream the data to an Elasticsearch server to leverage its excellent capabilities for full-text search on our data.
      But to make the matter a little bit more interesting, we will stream the data to both, a PostgreSQL database and Elasticsearch,...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We wish all the best to the Debezium community for 2018!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While we’re working on the 0.7.2 release, we thought we’d publish another post describing an end-to-end data streaming use case based on Debezium.
      We have seen how to set up a change data stream to a downstream database &lt;a href=&quot;http://debezium.io/blog/2017/09/25/streaming-to-another-database/&quot;&gt;a few weeks ago&lt;/a&gt;.
      In this blog post we will follow the same approach to stream the data to an &lt;a href=&quot;https://www.elastic.co/&quot;&gt;Elasticsearch&lt;/a&gt; server to leverage its excellent capabilities for full-text search on our data.
      But to make the matter a little bit more interesting, we will stream the data to both, a PostgreSQL database and Elasticsearch, so we will optimize access to the data via the SQL query language as well as via full-text search.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;topology&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#topology&quot;&gt;&lt;/a&gt;Topology&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Here’s a diagram that shows how the data is flowing through our distributed system.
      First, the Debezium MySQL connector is continuously capturing the changes from the MySQL database, and sending the changes for each table to separate Kafka topics.
      Then, the Confluent &lt;a href=&quot;https://docs.confluent.io/current/connect/connect-jdbc/docs/sink_connector.html&quot;&gt;JDBC sink connector&lt;/a&gt; is continuously reading those topics and writing the events into the PostgreSQL database.
      And, at the same time, the Confluent &lt;a href=&quot;https://github.com/confluentinc/kafka-connect-elasticsearch&quot;&gt;Elasticsearch connector&lt;/a&gt; is continuously reading those same topics and writing the events into Elasticsearch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt; &lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div id=&quot;img-general&quot; class=&quot;imageblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;http://debezium.io/images/dbz-to-multiple.svg&quot; alt=&quot;Scenario topology&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;title&quot;&gt;Figure 1: A general topology&lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt; &lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We are going to deploy these components into several different processes.
      In this example, we’ll deploy all three connectors to a single Kafka Connect instance that will write to and read from Kafka on behalf of all of the connectors
      (in production you might need to keep the connectors separated to achieve better performance).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt; &lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div id=&quot;img-general&quot; class=&quot;imageblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;http://debezium.io/images/dbz-to-multiple-simplified.svg&quot; alt=&quot;Scenario topology&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;title&quot;&gt;Figure 2: A simplified topology&lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;configuration&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#configuration&quot;&gt;&lt;/a&gt;Configuration&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We will use this &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/unwrap-smt&quot;&gt;Docker Compose file&lt;/a&gt; for a fast deployment of the demo.
      The deployment consists of the following Docker images:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://hub.docker.com/r/debezium/zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://hub.docker.com/r/debezium/kafka/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;An &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/unwrap-smt/debezium-jdbc&quot;&gt;enriched&lt;/a&gt; Kafka Connect / Debezium &lt;a href=&quot;https://hub.docker.com/r/debezium/connect/&quot;&gt;image&lt;/a&gt; with a few changes:&lt;/p&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;PostgreSQL JDBC driver placed into &lt;em&gt;/kafka/libs&lt;/em&gt; directory&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The Confluent JDBC connector placed into &lt;em&gt;/kafka/connect/kafka-connect-jdbc&lt;/em&gt; directory&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Pre-populated MySQL as used in our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Empty PostgreSQL&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Empty Elasticsearch&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The message format is not the same for the Debezium source connector and the JDBC and Elasticsearch connectors as they are developed separately and each focuses on slightly different objectives.
      Debezium emits a more complex event structure so that it captures all of the information available.
      In particular, the change events contain the old and the new state of a changed record.
      Both sink connectors on the other hand expect a simple message that just represents the record state to be written.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium’s &lt;a href=&quot;http://debezium.io/docs/configuration/event-flattening/&quot;&gt;UnwrapFromEnvelope&lt;/a&gt; single message transformation (SMT) collapses the complex change event structure into the same row-based format expected by the two sink connectors and effectively acts as a &lt;a href=&quot;http://www.enterpriseintegrationpatterns.com/patterns/messaging/MessageTranslator.html&quot;&gt;message translator&lt;/a&gt; between the two aforementioned formats.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;example&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#example&quot;&gt;&lt;/a&gt;Example&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s move directly to our example as that’s where the changes are visible.
      First of all we need to deploy all components:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;export DEBEZIUM_VERSION=0.7
      docker-compose up&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When all components are started we are going to register the Elasticsearch Sink connector writing into the Elasticsearch instance.
      We want to use the same key (primary id) in the source and both PostgreSQL and Elasticsearch:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; \
          -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ \
          -d @es-sink.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re using this registration request:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
        {
          &quot;name&quot;: &quot;elastic-sink&quot;,
          &quot;config&quot;: {
            &quot;connector.class&quot;:
                &quot;io.confluent.connect.elasticsearch.ElasticsearchSinkConnector&quot;,
            &quot;tasks.max&quot;: &quot;1&quot;,
            &quot;topics&quot;: &quot;customers&quot;,
            &quot;connection.url&quot;: &quot;http://elastic:9200&quot;,
            &quot;transforms&quot;: &quot;unwrap,key&quot;,
            &quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.UnwrapFromEnvelope&quot;,        (1)
            &quot;transforms.key.type&quot;: &quot;org.apache.kafka.connect.transforms.ExtractField$Key&quot;,(2)
            &quot;transforms.key.field&quot;: &quot;id&quot;,                                                 (2)
            &quot;key.ignore&quot;: &quot;false&quot;,                                                        (3)
            &quot;type.name&quot;: &quot;customer&quot;                                                       (4)
          }
        }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The request configures these options:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;extracting only the new row’s state from Debezium’s change data message&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;extracting the &lt;code&gt;id&lt;/code&gt; field from the key &lt;code&gt;struct&lt;/code&gt;, then the same key is used for the source and both destinations.
      This is to address the fact that the Elasticsearch connector only supports numeric types and &lt;code&gt;string&lt;/code&gt; as keys. If we do not extract the &lt;code&gt;id&lt;/code&gt; the messages will be filtered out by the connector because of unknown key type.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;use key from the event instead of generating a synthetic one&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;type under which the events will be registered in Elasticsearch&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Next we are going to register the JDBC Sink connector writing into PostgreSQL database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; \
          -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ \
          -d @jdbc-sink.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, the source connector must be set up:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; \
          -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ \
          -d @source.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s check if the databases and the search server are synchronized.
      All the rows of the &lt;code&gt;customers&lt;/code&gt; table should be found in the source database (MySQL) as well as the target database (Postgres) and Elasticsearch:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER  -p$MYSQL_PASSWORD inventory -e &quot;select * from customers&quot;'
      +------+------------+-----------+-----------------------+
      | id   | first_name | last_name | email                 |
      +------+------------+-----------+-----------------------+
      | 1001 | Sally      | Thomas    | sally.thomas@acme.com |
      | 1002 | George     | Bailey    | gbailey@foobar.com    |
      | 1003 | Edward     | Walker    | ed@walker.com         |
      | 1004 | Anne       | Kretchmar | annek@noanswer.org    |
      +------+------------+-----------+-----------------------+&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec postgres bash -c 'psql -U $POSTGRES_USER $POSTGRES_DB -c &quot;select * from customers&quot;'
       last_name |  id  | first_name |         email
      -----------+------+------------+-----------------------
       Thomas    | 1001 | Sally      | sally.thomas@acme.com
       Bailey    | 1002 | George     | gbailey@foobar.com
       Walker    | 1003 | Edward     | ed@walker.com
       Kretchmar | 1004 | Anne       | annek@noanswer.org&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl 'http://localhost:9200/customers/_search?pretty'
      {
        &quot;took&quot; : 42,
        &quot;timed_out&quot; : false,
        &quot;_shards&quot; : {
          &quot;total&quot; : 5,
          &quot;successful&quot; : 5,
          &quot;failed&quot; : 0
        },
        &quot;hits&quot; : {
          &quot;total&quot; : 4,
          &quot;max_score&quot; : 1.0,
          &quot;hits&quot; : [
            {
              &quot;_index&quot; : &quot;customers&quot;,
              &quot;_type&quot; : &quot;customer&quot;,
              &quot;_id&quot; : &quot;1001&quot;,
              &quot;_score&quot; : 1.0,
              &quot;_source&quot; : {
                &quot;id&quot; : 1001,
                &quot;first_name&quot; : &quot;Sally&quot;,
                &quot;last_name&quot; : &quot;Thomas&quot;,
                &quot;email&quot; : &quot;sally.thomas@acme.com&quot;
              }
            },
            {
              &quot;_index&quot; : &quot;customers&quot;,
              &quot;_type&quot; : &quot;customer&quot;,
              &quot;_id&quot; : &quot;1004&quot;,
              &quot;_score&quot; : 1.0,
              &quot;_source&quot; : {
                &quot;id&quot; : 1004,
                &quot;first_name&quot; : &quot;Anne&quot;,
                &quot;last_name&quot; : &quot;Kretchmar&quot;,
                &quot;email&quot; : &quot;annek@noanswer.org&quot;
              }
            },
            {
              &quot;_index&quot; : &quot;customers&quot;,
              &quot;_type&quot; : &quot;customer&quot;,
              &quot;_id&quot; : &quot;1002&quot;,
              &quot;_score&quot; : 1.0,
              &quot;_source&quot; : {
                &quot;id&quot; : 1002,
                &quot;first_name&quot; : &quot;George&quot;,
                &quot;last_name&quot; : &quot;Bailey&quot;,
                &quot;email&quot; : &quot;gbailey@foobar.com&quot;
              }
            },
            {
              &quot;_index&quot; : &quot;customers&quot;,
              &quot;_type&quot; : &quot;customer&quot;,
              &quot;_id&quot; : &quot;1003&quot;,
              &quot;_score&quot; : 1.0,
              &quot;_source&quot; : {
                &quot;id&quot; : 1003,
                &quot;first_name&quot; : &quot;Edward&quot;,
                &quot;last_name&quot; : &quot;Walker&quot;,
                &quot;email&quot; : &quot;ed@walker.com&quot;
              }
            }
          ]
        }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With the connectors still running, we can add a new row to the MySQL database and then check that it was replicated into both the PostgreSQL database and Elasticsearch:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER  -p$MYSQL_PASSWORD inventory'
      
      mysql&amp;gt; insert into customers values(default, 'John', 'Doe', 'john.doe@example.com');
      Query OK, 1 row affected (0.02 sec)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec -postgres bash -c 'psql -U $POSTGRES_USER $POSTGRES_DB -c &quot;select * from customers&quot;'
       last_name |  id  | first_name |         email
      -----------+------+------------+-----------------------
      ...
      Doe        | 1005 | John       | john.doe@example.com
      (5 rows)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl 'http://localhost:9200/customers/_search?pretty'
      ...
      {
        &quot;_index&quot; : &quot;customers&quot;,
        &quot;_type&quot; : &quot;customer&quot;,
        &quot;_id&quot; : &quot;1005&quot;,
        &quot;_score&quot; : 1.0,
        &quot;_source&quot; : {
          &quot;id&quot; : 1005,
          &quot;first_name&quot; : &quot;John&quot;,
          &quot;last_name&quot; : &quot;Doe&quot;,
          &quot;email&quot; : &quot;john.doe@example.com&quot;
        }
      }
      ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot;&gt;&lt;/a&gt;Summary&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We set up a complex streaming data pipeline to synchronize a MySQL database with another database and also with an Elasticsearch instance.
      We managed to keep the same identifier across all systems which allows us to correlate records across the system as a whole.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Propagating data changes from a primary database in near realtime to a search engine such as Elasticsearch enables many interesting use cases.
      Besides different applications of fulltext search one could for instance also think about creating dashboards and all kinds of visualizations using &lt;a href=&quot;https://www.elastic.co/de/products/kibana&quot;&gt;Kibana&lt;/a&gt;, to gain further insight into the data.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to try out this set-up yourself, just clone the project from our &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/unwrap-smt&quot;&gt;examples repo&lt;/a&gt;.
      In case you need help, have feature requests or would like to share your experiences with this pipeline, please let us know in the comments below.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/12/20/debezium-0-7-1-released/</id>
    <title>Debezium 0.7.1 Is Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2017-12-20T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/12/20/debezium-0-7-1-released/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      Just last few days before Christmas we are releasing Debezium  0.7.1!
      This is a bugfix release that fixes few annoying issues that were found during first rounds of use of Debezium 0.7 by our community.
      All issues relate to either newly provided wal2json support or reduced risk of internal race condition improvement.
      
      
      Robert Coup has found a performance regression in situations when 0.7.0 was used with old version of Protobuf decoder.
      
      
      Suraj Savita (and others) has found an issue when our code failed to correctly detect it runs with Amazon RDS wal2json plug-in.
      We are outsmarted by the JDBC driver internals and included a...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Just last few days before Christmas we are releasing Debezium  &lt;strong&gt;0.7.1&lt;/strong&gt;!
      This is a bugfix release that fixes few annoying issues that were found during first rounds of use of Debezium 0.7 by our community.
      All issues relate to either newly provided wal2json support or reduced risk of internal race condition improvement.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/rcoup&quot;&gt;Robert Coup&lt;/a&gt; has found a &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-512&quot;&gt;performance regression&lt;/a&gt; in situations when 0.7.0 was used with old version of Protobuf decoder.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Suraj Savita (and others) has found an issue when our code failed to &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-513&quot;&gt;correctly detect&lt;/a&gt; it runs with Amazon RDS wal2json plug-in.
      We are outsmarted by the JDBC driver internals and included a distinct plugin decoder name &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-517&quot;&gt;wal2json_rds&lt;/a&gt; that bypasses detection routine and by default expects it runs against Amazon RDS instance. This mode should be used only with RDS instances.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We have also gathered feedback from first tries to run with Amazon RDS and included &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql/#amazon-rds&quot;&gt;a short section&lt;/a&gt; in our documentation on this topic.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/12/15/debezium-0-7-0-released/</id>
    <title>Debezium 0.7.0 Is Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2017-12-15T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/12/15/debezium-0-7-0-released/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s not Christmas yet, but we already got a present for you: Debezium  0.7.0 is here, full of new features as well as many bug fixes!
      A big thank you goes out to all the community members who contributed to this release.
      It is very encouraging for us to see not only more and more issues and feature requests being reported, but also pull requests coming in.
      
      
      Note that this release comes with a small number of changes to the default mappings for some data types.
      We try to avoid this sort of changes as far as possible, but in some cases it...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s not Christmas yet, but we already got a present for you: Debezium  &lt;strong&gt;0.7.0&lt;/strong&gt; is here, full of new features as well as many bug fixes!
      A big thank you goes out to all the community members who contributed to this release.
      It is very encouraging for us to see not only more and more issues and feature requests being reported, but also pull requests coming in.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note that this release comes with a small number of changes to the default mappings for some data types.
      We try to avoid this sort of changes as far as possible, but in some cases it is required,
      e.g. if the previous mapping could have caused potential value losses.
      Please see below for the details and also make sure to check out the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-0&quot;&gt;full change log&lt;/a&gt; which describes these changes in detail.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now let’s take a closer look at some of new features.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;based_on_apache_kafka_1_0&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#based_on_apache_kafka_1_0&quot;&gt;&lt;/a&gt;Based on Apache Kafka 1.0&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A few weeks ago the Apache Kafka team has &lt;a href=&quot;https://www.confluent.io/blog/apache-kafka-goes-1-0/&quot;&gt;released version 1.0.0&lt;/a&gt;.
      This was an important milestone for the Kafka community,
      and we now can happily declare that Debezium is &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-432&quot;&gt;built&lt;/a&gt; against and runs on that Apache Kafka version.
      Our Docker images were also &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-433&quot;&gt;promoted&lt;/a&gt; to contain Apache Kafka and Kafka Connect 1.0.0.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;postgresql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#postgresql_connector&quot;&gt;&lt;/a&gt;PostgreSQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The big news for the PostgreSQL connector is that it now &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-258&quot;&gt;supports&lt;/a&gt; the &lt;a href=&quot;https://github.com/eulerto/wal2json&quot;&gt;wal2json&lt;/a&gt; logical decoding plugin as an alternative to the existing &lt;a href=&quot;https://github.com/debezium/postgres-decoderbufs&quot;&gt;DecoderBufs plug-in&lt;/a&gt;.
      This means that you now can use Debezium to stream changes out of PostgreSQL on &lt;a href=&quot;https://aws.amazon.com/rds/postgresql/&quot;&gt;Amazon RDS&lt;/a&gt;, as wal2json is the logical decoding plugin used in this environment.
      Many thanks to &lt;a href=&quot;https://github.com/rcoup&quot;&gt;Robert Coup&lt;/a&gt; who significantly contributed to this feature.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Working on this plug-in, we noticed that there was a &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-379&quot;&gt;potential race condition&lt;/a&gt; when it comes to applying changes to the schema of captured tables.
      In that case it could have happened that a number of messages pertaining to data changes done before the schema change were emitted using the new schema.
      With the exception of a few corner cases (which are described &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-0&quot;&gt;here&lt;/a&gt;), this has been addressed when using Debezium’s own DecoderBufs plug-in.
      So it’s highly recommended to upgrade the DecoderBufs plug-in to the new version before upgrading the Debezium connector.
      We’ve also worked closely with the author of the wal2json plug-in (big thanks for the quick help!) to prevent the issue when using the wal2json plug-in.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While the Debezium Docker images for Postgres already come with the latest version of DecoderBufs and wal2json,
      RDS for now is still using an older version of wal2json.
      Until this has been updated, special attention must be paid when applying schema changes to captured tables.
      Please see &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-0&quot;&gt;the changelog&lt;/a&gt; for a in-depth description of this issue and ways to mitigate it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;There are new daily running &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-495&quot;&gt;CI jobs&lt;/a&gt; that verify that the wal2json plugin passes our test suite.
      For the foreseeable future we’ll support both, wal2json as well as the existing DecoderBufs plug-in.
      The latter should be more efficient due to the usage of the Protocol Buffers binary format,
      whereas the former comes in handy for RDS or other cloud environments where you don’t have control over the installed logical decoding plug-ins, but wal2json is available.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In other news on the Postgres connector, &lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt; discovered and proposed a fix for a &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-501&quot;&gt;multi-threading bug&lt;/a&gt; that could have put the connector into an undefined state if a rebalance in the Connect cluster was triggered during snapshotting.
      Thanks, Andrey!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mysql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mysql_connector&quot;&gt;&lt;/a&gt;MySQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In the MySQL connector we’ve fixed two issues which affect the default mapping of certain column types.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Following up to the new &lt;code&gt;BIGINT UNSIGNED&lt;/code&gt; mapping introduced in &lt;a href=&quot;http://debezium.io/blog/2017/10/26/debezium-0-6-1-released/&quot;&gt;Debezium 0.6.1&lt;/a&gt;, this type is now encoded as &lt;code&gt;int64&lt;/code&gt; in Debezium messages &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-461&quot;&gt;by default&lt;/a&gt; as it is easier for (polyglot) clients to work with.
      This is a reasonable mapping for the vast majority of cases.
      Only when using values &amp;gt; 2^63, you should switch it back to the &lt;code&gt;Decimal&lt;/code&gt; logical type
      which is a bit more cumbersome to handle, though.
      This should be a rare situation, as MySQL &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/numeric-type-overview.html&quot;&gt;advices against&lt;/a&gt; using unsigned values &amp;gt; 2^63 due to potential value losses when performing DB-side calculations.
      Please see the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;connector documentation&lt;/a&gt; for the details.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/rk3rn3r&quot;&gt;Rene Kerner&lt;/a&gt; has improved the support for the MySQL &lt;code&gt;TIME&lt;/code&gt; type.
      MySQL allows to store values larger than &lt;code&gt;23:59:59&lt;/code&gt; in such columns, and the type &lt;code&gt;int32&lt;/code&gt; which was previously used for &lt;code&gt;TIME(0-3)&lt;/code&gt; columns isn’t enough to convey the entire possible value range.
      Therefore all &lt;code&gt;TIME&lt;/code&gt; columns in MySQL are by default represented as &lt;code&gt;int64&lt;/code&gt; now,
      using the &lt;code&gt;io.debezium.time.MicroTime&lt;/code&gt; logical type, i.e. the value represents micro-seconds.
      If needed, you can switch to the previous mapping by setting &lt;code&gt;time.precision.mode&lt;/code&gt; to &lt;code&gt;adaptive&lt;/code&gt;,
      but you should only do so if you’re sure that you only ever will have values that fit into &lt;code&gt;int32&lt;/code&gt;.
      This option is only kept for a transitioning period and will be removed in a future release.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Recently we got a &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-390&quot;&gt;report&lt;/a&gt; that MySQL’s binlog can contain &lt;code&gt;ROLLBACK&lt;/code&gt; statements and thus transactions that are actually not committed.
      Of course no data change messages should be emitted in this situation.
      This e.g. can be the case when temporary tables are dropped.
      So we introduced a &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-406&quot;&gt;look-ahead buffer&lt;/a&gt; functionality that reads the binlog by transaction and excludes those that were rolled back.
      This feature should be considered incubating and is disabled by default for the time being.
      We’d like to gather your feedback on this, so if you’d benefit from this feature, please give it a try and let us know if you run into any issues.
      For further details please refer to the &lt;code&gt;binlog.buffer.size&lt;/code&gt; setting in the MySQL connector docs.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/ainagy&quot;&gt;Andras Istvan Nagy&lt;/a&gt; came with the idea and &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-349&quot;&gt;implemented&lt;/a&gt; a way for explicitly selecting the rows from each table that will be part of the snapshotting process.
      This can for instance be very useful if you work with soft deletes and would like to exclude all logically deleted records from snapshotting.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-0&quot;&gt;full change log&lt;/a&gt; for more details and the complete list of fixed issues.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium 0.7.1 release is planned to be out roughly two weeks after Christmas.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It will contain a new SMT that will unwind MongoDB change events into a regular JSON consumable by sink connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A big overhaul of &lt;code&gt;GEOMETRY&lt;/code&gt; types is in progress.
      When completed, all &lt;code&gt;GEOMETRY&lt;/code&gt; types will be supported by both MySQL and PostgreSQL connectors and they will be available in standard &lt;code&gt;WKB&lt;/code&gt; format for easy consumption by polyglot clients.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;There is ongoing work for the MySQL connector to allow dynamic update of &lt;code&gt;table.whitelist&lt;/code&gt; option.
      This will allow the user to re-configure the set of tables captured without need to re-create connector.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to contribute, please let us know.
      We’re happy about any help and will work with you to get you started quickly.
      Check out the details below on how to get in touch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/11/15/debezium-0-6-2-released/</id>
    <title>Debezium 0.6.2 Is Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2017-11-15T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/11/15/debezium-0-6-2-released/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We are accelerating! Three weeks after the 0.6.1 release, the Debezium team is bringing Debezium 0.6.2 to you!
      
      
      This release revolves mostly around bug fixes, but there are a few new features, too.
      Let&#8217;s take a closer look at some of the changes.
      
      
      
      
      PostgreSQL Connector
      
      
      The big news for the Postgres connector is that Debezium now runs against PostgreSQL 10 thanks to a contribution from Scofield Xu.
      As a part of this change we are providing a Docker Image with PostgreSQL 10, too, and we have set up a daily run of our integration tests against it.
      
      
      If you are building Postgres yourself using the Debezium...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We are accelerating! Three weeks after the 0.6.1 release, the Debezium team is bringing &lt;strong&gt;Debezium 0.6.2&lt;/strong&gt; to you!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release revolves mostly around bug fixes, but there are a few new features, too.
      Let’s take a closer look at some of the changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;postgresql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#postgresql_connector&quot;&gt;&lt;/a&gt;PostgreSQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The big news for the Postgres connector is that Debezium now runs against &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-424&quot;&gt;PostgreSQL 10&lt;/a&gt; thanks to a contribution from &lt;a href=&quot;https://github.com/ScofieldXu&quot;&gt;Scofield Xu&lt;/a&gt;.
      As a part of this change we are providing a &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-426&quot;&gt;Docker Image&lt;/a&gt; with PostgreSQL 10, too, and we have set up a &lt;a href=&quot;http://ci.hibernate.org/view/Debezium/job/debezium-postgresql-10-test/&quot;&gt;daily run&lt;/a&gt; of our integration tests against it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you are building Postgres yourself using the Debezium &lt;a href=&quot;https://github.com/debezium/postgres-decoderbufs&quot;&gt;logical decoding plug-in&lt;/a&gt;,
      you can save quite some megabytes if you don’t need the PostGIS geometric extension:
      thanks to the work by &lt;a href=&quot;https://github.com/QazerLab&quot;&gt;Danila Kiver&lt;/a&gt;, it’s now possible to omit that extension.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mysql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mysql_connector&quot;&gt;&lt;/a&gt;MySQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve received multiple reports related to parsing MySQL DDL statements, e.g. there were a few specific invocations of the &lt;code&gt;ALTER TABLE&lt;/code&gt; statement which weren’t handled correctly.
      Those as well as a few other parser bugs have been fixed.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you work with the &lt;code&gt;TIMESTAMP&lt;/code&gt; column type and your Kafka Connect server isn’t using UTC as timezone, then the fix for &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-260&quot;&gt;DBZ-260&lt;/a&gt; is applying to you.
      In that case, the ISO 8601 formatted String emitted by Debezium would have, incorrectly, contained the UTC date and time plus the zone offset (as per the time zone the Kafka Connect server is located in) before.
      Whereas now it will contain the date and time adjusted to the zone offset.
      This may require adjustments to to downstream consumers if they were relying on the previous, incorrect behavior.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-217&quot;&gt;DBZ-217&lt;/a&gt; gives you more flexibility for handling corrupt events encountered in the MySQL binlog.
      By default, the connector will stop at the problematic event in such case.
      But you now also have the option to just log the event and its position and continue the processing after it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Another nice improvement for the MySQL connector is a much reduced CPU load after the snapshot has been completed, when using the &quot;snapshot only&quot; mode (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-396&quot;&gt;DBZ-396&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mongodb_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mongodb_connector&quot;&gt;&lt;/a&gt;MongoDB Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This connector received an important fix applying when more than one thread is used to performing the initial snapshot (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-438&quot;&gt;DBZ-438&lt;/a&gt;).
      Before, it could happen that single messages got lost during snapshotting which is fixed now.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;examples_and_docker_images&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#examples_and_docker_images&quot;&gt;&lt;/a&gt;Examples and Docker Images&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We have expanded our examples repository with &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/tutorial#using-mysql-and-the-avro-message-format&quot;&gt;an Avro example&lt;/a&gt;,
      which may be interesting to you if you’d like to not work with JSON messages but rather the compact Avro binary format and the Confluent schema registry.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As a part of our release process we are now creating &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-418&quot;&gt;micro tags&lt;/a&gt; for our Docker images for every released version.
      While tags in the format &lt;code&gt;x.y.z&lt;/code&gt; are fixed in time, tags in the format &lt;code&gt;x.y&lt;/code&gt; are rolling updates and always point to the latest micro release of that image.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-6-2&quot;&gt;full change log&lt;/a&gt; for more details and the complete list of fixed issues.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium 0.7 release is planned to be out in two to three weeks from now.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It will contain the move to Apache Kafka 1.0.0 and bring support for the wal2json logical decoding plug-in for Postgres.
      This will eventually allow to use the Debezium Postgres connector on Amazon RDS (once the correct wal2json version is available there).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In parallel, the work around handling updates to the whitelist configuration of the MySQL connector continues (it may be ready for 0.7.0),
      and so does the work on the Oracle connector (which will be shipping in a future release).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to contribute, please let us know.
      We’re happy about any help and will work with you to get you started quickly.
      Check out the details below on how to get in touch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/11/11/debezium-at-devoxx-belgium/</id>
    <title>Debezium at Devoxx Belgium</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2017-11-11T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/11/11/debezium-at-devoxx-belgium/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="introduction"></category>
    <category term="presentation"></category>
    <summary>
      
      
      
      Debezium&#8217;s project lead Gunnar Morling gave a few talks during recent Devoxx Belgium 2017.
      One of his talks was dedicated to Debezium and change data capture in general.
      
      
      If you are interested in those topics and you want to obtain a fast and simple introduction to it, do not hesitate and watch the talk.
      Batteries and demo included!
      
      
      
      
      
      The slide deck is available, too:
      
      
      
      
      
      
      
      About Debezium
      
      
      Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of Kafka and provides Kafka Connect...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium’s project lead &lt;a href=&quot;https://twitter.com/gunnarmorling&quot;&gt;Gunnar Morling&lt;/a&gt; gave a few talks during recent &lt;a href=&quot;https://cfp.devoxx.be/2017/index.html&quot;&gt;Devoxx Belgium 2017&lt;/a&gt;.
      One of his talks was dedicated to Debezium and change data capture in general.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you are interested in those topics and you want to obtain a fast and simple introduction to it, do not hesitate and watch the talk.
      Batteries and demo included!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;responsive-video&quot;&gt;
      &lt;iframe width=&quot;1600&quot; height=&quot;900&quot; src=&quot;https://www.youtube.com/embed/IOZ2Um6e430?rel=0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The slide deck is &lt;a href=&quot;https://speakerdeck.com/gunnarmorling/streaming-database-changes-with-debezium&quot;&gt;available&lt;/a&gt;, too:&lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div style=&quot;text-align-center&quot;&gt;
      &lt;script async=&quot;&quot; class=&quot;speakerdeck-embed&quot; data-id=&quot;4fb7aa5af1c54d7ea807c9d46fb5b1fa&quot; data-ratio=&quot;1.77777777777778&quot; src=&quot;//speakerdeck.com/assets/embed.js&quot;&gt;&lt;/script&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/10/26/debezium-0-6-1-released/</id>
    <title>Debezium 0.6.1 Is Released</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2017-10-26T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/10/26/debezium-0-6-1-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      Just shy of a month after the 0.6.0 release, I&#8217;m happy to announce the release of Debezium 0.6.1!
      
      
      This release contains several bugfixes, dependency upgrades and a new option for controlling how BIGINT UNSIGNED columns are conveyed.
      We also expanded the set of Docker images and Docker Compose files accompanying our tutorial, so you can run it now with all the databases we support.
      
      
      Let&#8217;s take a closer look at some of the changes.
      
      
      
      
      New connector option for controlling BIGINT UNSIGNED representation
      
      
      BIGINT UNSIGNED columns from MySQL databases have been represented using Kafka Connect&#8217;s Decimal type until now.
      This type allows to represent all possible values...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Just shy of a month after the 0.6.0 release, I’m happy to announce the release of &lt;strong&gt;Debezium 0.6.1&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release contains several bugfixes, dependency upgrades and a new option for controlling how &lt;code&gt;BIGINT UNSIGNED&lt;/code&gt; columns are conveyed.
      We also expanded the set of Docker images and Docker Compose files accompanying &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;our tutorial&lt;/a&gt;, so you can run it now with all the databases we support.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s take a closer look at some of the changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;new_connector_option_for_controlling_bigint_unsigned_representation&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_connector_option_for_controlling_bigint_unsigned_representation&quot;&gt;&lt;/a&gt;New connector option for controlling BIGINT UNSIGNED representation&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;code&gt;BIGINT UNSIGNED&lt;/code&gt; columns from MySQL databases have been represented using Kafka Connect’s &lt;code&gt;Decimal&lt;/code&gt; type until now.
      This type allows to represent all possible values of such columns, but its based on a byte array, so it can be a bit cumbersome to handle for consumers.
      Therefore we added a new option named &lt;code&gt;bigint.unsigned.handling.mode&lt;/code&gt; to the MySQL connector that allows to represent such columns using &lt;code&gt;long&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For the very most cases that’s the preferable option, only if your column contains values larger than 2^63
      (which &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/numeric-type-overview.html&quot;&gt;MySQL doesn’t recommend&lt;/a&gt; due to potential value losses when performing calculations),
      you should stick to the &lt;code&gt;Decimal&lt;/code&gt; representation.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Using &lt;code&gt;long&lt;/code&gt; will be the default as of Debezium 0.7, for the 0.6.x timeline we decided to go with the previous behavior (i.e. using &lt;code&gt;Decimal&lt;/code&gt;) for the sake of backwards compatibility.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks a lot to &lt;a href=&quot;https://github.com/vultron81&quot;&gt;Ben Williams&lt;/a&gt; who contributed this feature!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;new_example_docker_images_and_docker_compose_files&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_example_docker_images_and_docker_compose_files&quot;&gt;&lt;/a&gt;New example Docker images and Docker Compose files&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In the &lt;a href=&quot;https://github.com/debezium/debezium-examples/&quot;&gt;Debezium examples repository&lt;/a&gt; we now provide &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/tutorial&quot;&gt;Docker Compose files&lt;/a&gt; which let you run the tutorial with all the three databases we currently support, MySQL, Postgres and MongoDB.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Just choose the Compose file for your preferred database and get a all the required components (ZooKeeper, Apache Kafka, Kafka Connect and the database) running within a few seconds.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also deployed Docker images for Postgres and MongoDB to the &lt;a href=&quot;https://hub.docker.com/u/debezium/&quot;&gt;Debezium organization&lt;/a&gt; on Docker Hub, so you got some data to play with.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;version_upgrades&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#version_upgrades&quot;&gt;&lt;/a&gt;Version upgrades&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve upgraded our images from Kafka 0.11.0.0 to &lt;a href=&quot;https://issues.apache.org/jira/projects/KAFKA/versions/12340632&quot;&gt;0.11.0.1&lt;/a&gt;.
      Also the &lt;a href=&quot;https://github.com/shyiko/mysql-binlog-connector-java&quot;&gt;binlog client library&lt;/a&gt; used by the MySQL connector was upgraded from 0.9.0 to 0.13.0.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;bugfixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bugfixes&quot;&gt;&lt;/a&gt;Bugfixes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, several bugs were fixed in 0.6.1.
      E.g. you can now name a column &lt;code&gt;column&lt;/code&gt; in MySQL (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-408&quot;&gt;DBZ-408&lt;/a&gt;),
      generated &lt;code&gt;DROP TEMP TABLE&lt;/code&gt; statements won’t flood the DB history topic (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-395&quot;&gt;DBZ-295&lt;/a&gt;)
      and we’ve fixed a case where the Postgres connector would stop working due to an internal error but fail to report though via the task/connector status (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-380&quot;&gt;DBZ-380&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-6-1&quot;&gt;full change log&lt;/a&gt; for more details and the complete list of fixed issues.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The work on Debezium 0.7 has already begun and we’ve merged the first set of changes.
      You can expect to see support for using the &lt;a href=&quot;https://github.com/eulerto/wal2json&quot;&gt;wal2json&lt;/a&gt; logical decoding plug-in with the Postgres connector, which will finally allow it to use Debezium with Postgres on Amazon RDS!
      We’ve also started our explorations of providing a connector for Oracle (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-20&quot;&gt;DBZ-20&lt;/a&gt;) and hope to report some progress here soon.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While the work on Debezium 0.7 continues, you will likely continue to see one or more 0.6.x bugfix releases.
      We’ve automated the release process as much as possible, making it a breeze to ship a new release and getting fixes into your hands quickly.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to contribute, please let us know.
      We’re happy about any help and will work with you to get you started quickly.
      Check out the details below on how to get in touch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/09/25/streaming-to-another-database/</id>
    <title>Streaming data to a downstream database</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2017-09-25T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/09/25/streaming-to-another-database/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="smt"></category>
    <category term="example"></category>
    <summary>
      
      
      
      In this blog post we will create a simple streaming data pipeline to continuously capture the changes in a MySQL database and replicate them in near real-time into a PostgreSQL database.
      We&#8217;ll show how to do this without writing any code, but instead by using and configuring Kafka Connect, the Debezium MySQL source connector, the Confluent JDBC sink connector, and a few single message transforms (SMTs).
      
      
      This approach of replicating data through Kafka is really useful on its own, but it becomes even more advantageous when we can combine our near real-time streams of data changes with other streams, connectors, and stream...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In this blog post we will create a simple streaming data pipeline to continuously capture the changes in a MySQL database and replicate them in near real-time into a PostgreSQL database.
      We’ll show how to do this without writing any code, but instead by using and configuring Kafka Connect, the Debezium MySQL source connector, the Confluent JDBC sink connector, and a few single message transforms (SMTs).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This approach of replicating data through Kafka is really useful on its own, but it becomes even more advantageous when we can combine our near real-time streams of data changes with other streams, connectors, and stream processing applications.
      A recent &lt;a href=&quot;https://www.confluent.io/blog/simplest-useful-kafka-connect-data-pipeline-world-thereabouts-part-1/&quot;&gt;Confluent blog post series&lt;/a&gt; shows a similar streaming data pipeline but using different connectors and SMTs.
      What’s great about Kafka Connect is that you can mix and match connectors to move data between multiple systems.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We will also demonstrate a new functionality that was released with &lt;a href=&quot;2017/09/21/debezium-0-6-0-released/&quot;&gt;Debezium 0.6.0&lt;/a&gt;: a single message transform for &lt;a href=&quot;http://debezium.io/docs/configuration/event-flattening/&quot;&gt;CDC Event Flattening&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;topology&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#topology&quot;&gt;&lt;/a&gt;Topology&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The general topology for this scenario is displayed on the following picture:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div id=&quot;img-general&quot; class=&quot;imageblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;http://debezium.io/images/dbz-to-jdbc.svg&quot; alt=&quot;Scenario topology&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;title&quot;&gt;Figure 1: A General topology&lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt; &lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To simplify the setup a little bit, we will use only one Kafka Connect instance that will contain all connectors.
      I.e. this instance will serve as an event producer and an event consumer:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt; &lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div id=&quot;img-general&quot; class=&quot;imageblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;http://debezium.io/images/dbz-to-jdbc-simplified.svg&quot; alt=&quot;Scenario topology&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;title&quot;&gt;Figure 2: A Simplified topology&lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;configuration&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#configuration&quot;&gt;&lt;/a&gt;Configuration&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We will use this &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/unwrap-smt&quot;&gt;compose&lt;/a&gt; for a fast deployment of the demo.
      The deployment consists of following Docker images:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://hub.docker.com/r/debezium/zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://hub.docker.com/r/debezium/kafka/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;An &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/unwrap-smt/debezium-jdbc&quot;&gt;enriched&lt;/a&gt; Kafka Connect / Debezium &lt;a href=&quot;https://hub.docker.com/r/debezium/connect/&quot;&gt;image&lt;/a&gt; with changes&lt;/p&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;PostgreSQL JDBC driver placed into &lt;code&gt;/kafka/libs&lt;/code&gt; directory&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://docs.confluent.io/current/connect/connect-jdbc/docs/index.html&quot;&gt;Kafka Connect JDBC Connector&lt;/a&gt; (developed by &lt;a href=&quot;https://www.confluent.io/&quot;&gt;Confluent&lt;/a&gt;) placed into &lt;code&gt;/kafka/connect/kafka-connect-jdbc&lt;/code&gt; directory&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Pre-populated MySQL used in our &lt;a href=&quot;docs/tutorial&quot;&gt;tutorial&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Empty PostgreSQL&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium MySQL Connector was designed to specifically capture database changes and provide as much information as possible about those events beyond just the new state of each row.
      Meanwhile, the Confluent JDBC Sink Connector was designed to simply convert each message into a database insert/upsert based upon the structure of the message.
      So, the two connectors have different structures for the messages, but they also use different topic naming conventions and behavior of representing deleted records.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;These mismatches in structure and behavior will be common when using connectors that were not designed to work together. But this is something that we can easily deal with, and we discuss how in the next few sections.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;event_format&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#event_format&quot;&gt;&lt;/a&gt;Event format&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium emits events in a complex format that contains all of the information about the captured data change:
      the type of operation, source metadata, the timestamp the event was processed by the connector, and state of the row before and after the change was made.
      Debezium calls this structure an &lt;em&gt;&quot;envelope&quot;&lt;/em&gt;:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
      	&quot;op&quot;: &quot;u&quot;,
      	&quot;source&quot;: {
      		...
      	},
      	&quot;ts_ms&quot; : &quot;...&quot;,
      	&quot;before&quot; : {
      		&quot;field1&quot; : &quot;oldvalue1&quot;,
      		&quot;field2&quot; : &quot;oldvalue2&quot;
      	},
      	&quot;after&quot; : {
      		&quot;field1&quot; : &quot;newvalue1&quot;,
      		&quot;field2&quot; : &quot;newvalue2&quot;
      	}
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Many other Kafka Connect source connectors don’t have the luxury of knowing this much about the changes, and instead use a simpler model where each message directly represents the after state of the row.
      This is also what many sink connectors expect, and the Confluent JDBC Sink Connector is not different:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
      	&quot;field1&quot; : &quot;newvalue1&quot;,
      	&quot;field2&quot; : &quot;newvalue2&quot;
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While we think it’s actually a great thing that Debezium CDC connectors provide as much detail as possible, we also make it easy for you to transform Debezium’s &lt;em&gt;&quot;envelope&quot;&lt;/em&gt; format into the &lt;em&gt;&quot;row&quot;&lt;/em&gt; format that is expected by many other connectors.
      Debezium provides a bridge between those two formats in a form of a &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-66%3A+Single+Message+Transforms+for+Kafka+Connect&quot;&gt;single message transform&lt;/a&gt;.
      The &lt;code&gt;UnwrapFromEnvelope&lt;/code&gt; transformation automatically extracts a new row record and thus effectively &lt;em&gt;flattens&lt;/em&gt; the complex record into a simple one consumable by other connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You can use this SMT on the source connector to transform the message &lt;em&gt;before&lt;/em&gt; it is written to Kafka, or you can instead store the source connector’s richer &lt;em&gt;&quot;envelope&quot;&lt;/em&gt; form of the message in Kafka and use this SMT on the sink connector to transform the message &lt;em&gt;after&lt;/em&gt; it is read from Kafka and before it is passed to the sink connector.
      Both options work, and it just depends on whether you find the envelope form of the message useful for other purposes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In our example we apply the SMT at the sink connector using these configuration properties:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;transforms&quot;: &quot;unwrap&quot;,
      &quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.UnwrapFromEnvelope&quot;,&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;delete_records&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#delete_records&quot;&gt;&lt;/a&gt;Delete records&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When the Debezium connector detects a row is deleted, it creates two event messages: a &lt;em&gt;delete&lt;/em&gt; event and a &lt;em&gt;tombstone&lt;/em&gt; message.
      The &lt;em&gt;delete&lt;/em&gt; message has an envelope with the state of the deleted row in the &lt;code&gt;before&lt;/code&gt; field, and an &lt;code&gt;after&lt;/code&gt; field that is &lt;code&gt;null&lt;/code&gt;.
      The &lt;em&gt;tombstone&lt;/em&gt; message contains same key as the &lt;em&gt;delete&lt;/em&gt; message, but the entire message value is &lt;code&gt;null&lt;/code&gt;, and Kafka’s log compaction utilizes this to know that it can remove any earlier messages with the same key.
      A number of sink connectors, including the Confluent’s JDBC Sink Connector, are not expecting these messages and will instead fail if they see either kind of message.
      The &lt;code&gt;UnwrapFromEnvelope&lt;/code&gt; SMT will by default filter out both &lt;em&gt;delete&lt;/em&gt; and &lt;em&gt;tombstone&lt;/em&gt; records, though you can change this if you’re using the SMT and want to keep one or both of these kinds of messages.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;topic_naming&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#topic_naming&quot;&gt;&lt;/a&gt;Topic naming&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Last but not least there is a difference in naming of topics.
      Debezium uses fully qualified naming for target topics representing each table it manages.
      The naming follows the pattern &lt;code&gt;&amp;lt;logical-name&amp;gt;.&amp;lt;database-name&amp;gt;.&amp;lt;table-name&amp;gt;&lt;/code&gt;.
      Kafka Connect JDBC Connector works with simple names &lt;code&gt;&amp;lt;table-name&amp;gt;&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In more complex scenarios the user may deploy the &lt;a href=&quot;https://kafka.apache.org/documentation/streams/&quot;&gt;Kafka Streams&lt;/a&gt; framework to establish elaborated routing between source and target routes.
      In our example we will use a stock &lt;code&gt;RegexRouter&lt;/code&gt; SMT that would route records created by Debezium into topics named according to JDBC Connector schema.
      Again, we could use this SMT in either the source or sink connectors, but for this example we’re going to use it in the source connector so we can choose the names of the Kafka topics where the records will be written.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;transforms&quot;: &quot;route&quot;,
      &quot;transforms.route.type&quot;: &quot;org.apache.kafka.connect.transforms.RegexRouter&quot;,
      &quot;transforms.route.regex&quot;: &quot;([^.]+)\\.([^.]+)\\.([^.]+)&quot;,
      &quot;transforms.route.replacement&quot;: &quot;$3&quot;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;example&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#example&quot;&gt;&lt;/a&gt;Example&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kick the tires and let’s try our example!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;First of all we need to deploy all components.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;export DEBEZIUM_VERSION=0.6
      docker-compose up&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When all components are started we are going to register the JDBC Sink connector writing into PostgreSQL database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ -d @jdbc-sink.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Using this registration request:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
          &quot;name&quot;: &quot;jdbc-sink&quot;,
          &quot;config&quot;: {
              &quot;connector.class&quot;: &quot;io.confluent.connect.jdbc.JdbcSinkConnector&quot;,
              &quot;tasks.max&quot;: &quot;1&quot;,
              &quot;topics&quot;: &quot;customers&quot;,
              &quot;connection.url&quot;: &quot;jdbc:postgresql://postgres:5432/inventory?user=postgresuser&amp;amp;password=postgrespw&quot;,
              &quot;transforms&quot;: &quot;unwrap&quot;,                                                  (1)
              &quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.UnwrapFromEnvelope&quot;,   (1)
              &quot;auto.create&quot;: &quot;true&quot;,                                                   (2)
              &quot;insert.mode&quot;: &quot;upsert&quot;,                                                 (3)
              &quot;pk.fields&quot;: &quot;id&quot;,                                                       (4)
              &quot;pk.mode&quot;: &quot;record_value&quot;                                                (4)
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The request configures these options:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;unwrapping Debezium’s complex format into a simple one&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;automatically create target tables&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;insert a row if it does not exist or update an existing one&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;identify the primary key stored in Kafka’s record value field&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Then the source connector must be set up:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ -d @source.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Using this registration request:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
          &quot;name&quot;: &quot;inventory-connector&quot;,
          &quot;config&quot;: {
              &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
              &quot;tasks.max&quot;: &quot;1&quot;,
              &quot;database.hostname&quot;: &quot;mysql&quot;,
              &quot;database.port&quot;: &quot;3306&quot;,
              &quot;database.user&quot;: &quot;debezium&quot;,
              &quot;database.password&quot;: &quot;dbz&quot;,
              &quot;database.server.id&quot;: &quot;184054&quot;,
              &quot;database.server.name&quot;: &quot;dbserver1&quot;,                                         (1)
              &quot;database.whitelist&quot;: &quot;inventory&quot;,                                           (2)
              &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
              &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot;,
              &quot;transforms&quot;: &quot;route&quot;,                                                       (3)
              &quot;transforms.route.type&quot;: &quot;org.apache.kafka.connect.transforms.RegexRouter&quot;,  (3)
              &quot;transforms.route.regex&quot;: &quot;([^.]+)\\.([^.]+)\\.([^.]+)&quot;,                     (3)
              &quot;transforms.route.replacement&quot;: &quot;$3&quot;                                         (3)
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The request configures these options:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;logical name of the database&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;the database we want to monitor&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;an SMT which defines a regular expression matching the topic name &lt;code&gt;&amp;lt;logical-name&amp;gt;.&amp;lt;database-name&amp;gt;.&amp;lt;table-name&amp;gt;&lt;/code&gt; and extracts the third part of it as the final topic name&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s check if the databases are synchronized.
      All the rows of the &lt;code&gt;customers&lt;/code&gt; table should be found in the source database (MySQL) as well as the target database (Postgres):&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER  -p$MYSQL_PASSWORD inventory -e &quot;select * from customers&quot;'
      +------+------------+-----------+-----------------------+
      | id   | first_name | last_name | email                 |
      +------+------------+-----------+-----------------------+
      | 1001 | Sally      | Thomas    | sally.thomas@acme.com |
      | 1002 | George     | Bailey    | gbailey@foobar.com    |
      | 1003 | Edward     | Walker    | ed@walker.com         |
      | 1004 | Anne       | Kretchmar | annek@noanswer.org    |
      +------+------------+-----------+-----------------------+
      
      docker-compose exec postgres bash -c 'psql -U $POSTGRES_USER $POSTGRES_DB -c &quot;select * from customers&quot;'
       last_name |  id  | first_name |         email
      -----------+------+------------+-----------------------
       Thomas    | 1001 | Sally      | sally.thomas@acme.com
       Bailey    | 1002 | George     | gbailey@foobar.com
       Walker    | 1003 | Edward     | ed@walker.com
       Kretchmar | 1004 | Anne       | annek@noanswer.org&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With the connectors still running, we can add a new row to the MySQL database and then check that it was replicated into the PostgreSQL database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER  -p$MYSQL_PASSWORD inventory'
      mysql&amp;gt; insert into customers values(default, 'John', 'Doe', 'john.doe@example.com');
      Query OK, 1 row affected (0.02 sec)
      
      docker-compose exec -postgres bash -c 'psql -U $POSTGRES_USER $POSTGRES_DB -c &quot;select * from customers&quot;'
       last_name |  id  | first_name |         email
      -----------+------+------------+-----------------------
      ...
      Doe        | 1005 | John       | john.doe@example.com
      (5 rows)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot;&gt;&lt;/a&gt;Summary&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We set up a simple streaming data pipeline to replicate data in near real-time from a MySQL database to a PostgreSQL database. We accomplished this using Kafka Connect, the Debezium MySQL source connector, the Confluent JDBC sink connector, and a few SMTs — all without having to write any code.
      And since it is a streaming system, it will continue to capture all changes made to the MySQL database and replicating them in near real time.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In a future blog post we will reproduce the same scenario with Elasticsearch as a target for events.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/09/21/debezium-0-6-0-released/</id>
    <title>Debezium 0.6 Is Out</title>
    <updated>2019-08-01T03:22:58+00:00</updated>
    <published>2017-09-21T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/09/21/debezium-0-6-0-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      What&#8217;s better than getting Java 9?
      Getting Java 9 and a new version of Debezium at the same time!
      So it&#8217;s with great happiness that I&#8217;m announcing the release of Debezium 0.6 today.
      
      
      
      
      What&#8217;s in it?
      
      
      Debezium is now built against and tested with Apache Kafka 0.11.0.
      Also the Debezium Docker images have been updated do that version (DBZ-305).
      You should make sure to read the Kafka update guide when upgrading from an earlier version.
      
      
      To improve integration with existing Kafka sink connectors such as the JDBC sink connector or the Elasticsearch connector,
      Debezium provides a new single message transformation (DBZ-226).
      This SMT converts Debezium&#8217;s CDC event structure into...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;What’s better than getting &lt;a href=&quot;http://openjdk.java.net/projects/jdk9/&quot;&gt;Java 9&lt;/a&gt;?
      Getting Java 9 and a new version of Debezium at the same time!
      So it’s with great happiness that I’m announcing the release of &lt;strong&gt;Debezium 0.6&lt;/strong&gt; today.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_in_it&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_in_it&quot;&gt;&lt;/a&gt;What’s in it?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is now built against and tested with Apache Kafka 0.11.0.
      Also the Debezium Docker images have been updated do that version (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-305&quot;&gt;DBZ-305&lt;/a&gt;).
      You should make sure to read the Kafka &lt;a href=&quot;https://kafka.apache.org/documentation/#upgrade&quot;&gt;update guide&lt;/a&gt; when upgrading from an earlier version.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To improve integration with existing Kafka sink connectors such as the &lt;a href=&quot;https://docs.confluent.io/current/connect/connect-jdbc/docs/sink_connector.html&quot;&gt;JDBC sink connector&lt;/a&gt; or the &lt;a href=&quot;https://docs.confluent.io/current/connect/connect-elasticsearch/docs/elasticsearch_connector.html&quot;&gt;Elasticsearch&lt;/a&gt; connector,
      Debezium provides a new &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/transforms/UnwrapFromEnvelope.java&quot;&gt;single message transformation&lt;/a&gt; (&lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-226&quot;&gt;DBZ-226&lt;/a&gt;).
      This SMT converts Debezium’s CDC event structure into a more conventional structure commonly used in other sink and non-CDC source connectors where the message represents the state of the inserted or updated row, or null in the case of a deleted row.
      This lets your for instance capture the changes from a table in MySQL and update a corresponding table in a Postgres database accordingly.
      We’ll provide a complete example showing the usage of that new SMT in the next few days.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you are doing the Debezium &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;, you will like the new &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/tutorial&quot;&gt;Docker Compose set-up&lt;/a&gt; provided in the examples repo (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-127&quot;&gt;DBZ-127&lt;/a&gt;).
      This lets you start all the required Docker containers with a single command.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;new_connector_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_connector_features&quot;&gt;&lt;/a&gt;New connector features&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now let’s take a look at some of the changes around the specific Debezium connectors.
      The &lt;strong&gt;MySQL connector&lt;/strong&gt; has seen multiple improvements, e.g.:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Snapshot consistency wasn’t guaranteed before in some corner cases (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-210&quot;&gt;DBZ-210&lt;/a&gt;); that’s fixed now&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;DEC and FIXED types supported in the DDL parser (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-359&quot;&gt;DBZ-359&lt;/a&gt;; thanks to &lt;a href=&quot;https://github.com/ooooorz&quot;&gt;Liu Hanlin&lt;/a&gt;!)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;UNION clause supported for ALTER TABLE (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-346&quot;&gt;DBZ-346&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For the &lt;strong&gt;MongoDB connector&lt;/strong&gt;, the way of serializing ids into the key payload of CDC events has changed (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-306&quot;&gt;DBZ-306&lt;/a&gt;).
      The new format allows to read back ids into the correct type.
      We also took the opportunity and made the id field name consistent with the other connectors, i.e. it’s &quot;id&quot; now.
      &lt;strong&gt;Note:&lt;/strong&gt; that change may break existing consumers, so some work on your end may be required, depending on the implementation of your consumer.
      The details are discussed in the &lt;a href=&quot;http://debezium.io/docs/releases/#_breaking_changes&quot;&gt;release notes&lt;/a&gt; and the format of message keys is described in depth in the &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/#change-events-key&quot;&gt;connector documentation&lt;/a&gt;.
      Kudos to &lt;a href=&quot;https://github.com/hpgrahsl&quot;&gt;Hans-Peter Grahsl&lt;/a&gt; who contributed on this feature!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Another nice improvement for this connector is support for SSL connections (&lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-343&quot;&gt;DBZ-343&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, the &lt;strong&gt;Postgres connector&lt;/strong&gt; learned some new tricks, too:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Support for variable-width numeric columns (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-318&quot;&gt;DBZ-318&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Views won’t stop the connector any more (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-319&quot;&gt;DBZ-319&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Warnings and notifications emitted by the server are correctly forwarded to the log (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-279&quot;&gt;DBZ-279&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please refer to the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-6-0&quot;&gt;changelog&lt;/a&gt; for an overview of all the 20 issues fixed in Debezium 0.6.0.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;High on our agenda is exploring support for Oracle (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-20&quot;&gt;DBZ-20&lt;/a&gt;).
      We are also looking into using another logical decoding plug-in (wal2json) for the Postgres connector, which would enable to use Debezium with Postgres instances running on Amazon RDS.
      Another feature being worked on by community member &lt;a href=&quot;https://github.com/mtagle&quot;&gt;Moira Tagle&lt;/a&gt; is support for updates to the &lt;code&gt;table.whitelist&lt;/code&gt; for existing connector instances.
      Finally, we’ve planned to test and adapt the existing MySQL connector for providing CDC functionality to MariaDB.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium 0.7 with one or more out of those features as well as hopefully some others will be released later this year.
      We’ll likely also do further 0.6.x releases with bug fixes as required.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You’d like to contribute?
      That’s great - let us know and we’ll get you started.
      Check out the details below on how to get in touch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
</feed>
