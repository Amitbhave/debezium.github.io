<?xml version="1.0" encoding="utf-8" ?>
<feed xml:lang="en-US" xmlns="http://www.w3.org/2005/Atom">
  <id>http://debezium.io/</id>
  <title>Debezium Blog</title>
  <updated>2017-09-21T07:35:59+00:00</updated>
  <link href="/blog.atom" rel="self" type="application/atom+xml" />
  <link href="http://debezium.io/" rel="alternate" type="text/html" />
  <entry>
    <id>/blog/2017/08/17/debezium-0-5-2-is-out/</id>
    <title>Debezium 0.5.2 Is Out</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2017-08-17T00:00:00+00:00</published>
    <link href="/blog/2017/08/17/debezium-0-5-2-is-out/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m very happy to announce the release of Debezium 0.5.2!
      
      
      As the previous release, the 0.5.2 release fixes several bugs in the MySQL, Postgres and MongoDB connectors.
      But there are also several new features and options:
      
      
      
      
      The decimal.handling.mode option already known from the MySQL connector is now also supported for PostgreSQL (DBZ-337).
      It lets you control how NUMERIC and DECIMAL columns are represented in change events (either using Kafka&#8217;s Decimal type or as double).
      
      
      The MongoDB connector supports the options database.whitelist and database.blacklist now (DBZ-302)
      
      
      The PostgreSQL connector can deal with array-typed columns as well as with quoted identifiers for tables, schemas etc. (DBZ-297, DBZ-298)
      
      
      The Debezium...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m very happy to announce the release of &lt;strong&gt;Debezium 0.5.2&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As the previous release, the 0.5.2 release fixes several bugs in the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL&lt;/a&gt;, &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql/&quot;&gt;Postgres&lt;/a&gt; and &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/&quot;&gt;MongoDB&lt;/a&gt; connectors.
      But there are also several new features and options:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;The &lt;code&gt;decimal.handling.mode&lt;/code&gt; option already known from the MySQL connector is now also supported for PostgreSQL (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-337&quot;&gt;DBZ-337&lt;/a&gt;).
      It lets you control how &lt;code&gt;NUMERIC&lt;/code&gt; and &lt;code&gt;DECIMAL&lt;/code&gt; columns are represented in change events (either using Kafka’s &lt;code&gt;Decimal&lt;/code&gt; type or as &lt;code&gt;double&lt;/code&gt;).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The MongoDB connector supports the options &lt;code&gt;database.whitelist&lt;/code&gt; and &lt;code&gt;database.blacklist&lt;/code&gt; now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-302&quot;&gt;DBZ-302&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The PostgreSQL connector can deal with array-typed columns as well as with quoted identifiers for tables, schemas etc. (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-297&quot;&gt;DBZ-297&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-298&quot;&gt;DBZ-298&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The Debezium Docker images run on Red Hat’s &lt;a href=&quot;https://www.openshift.com/&quot;&gt;OpenShift&lt;/a&gt; cloud environment (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-267&quot;&gt;DBZ-267&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Speaking about the Docker images, we’ve set up &lt;em&gt;nightly&lt;/em&gt; tags for the &lt;a href=&quot;https://hub.docker.com/u/debezium/&quot;&gt;Debezium images on Docker Hub&lt;/a&gt;,
      allowing you to grab the latest improvements even before an official release has been cut.
      The connector archives are also deployed to the &lt;a href=&quot;https://oss.sonatype.org/content/repositories/snapshots/io/debezium/&quot;&gt;Sonatype OSS Maven repository&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, we’ve spent some time to extend the documentation on some things not covered before:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;http://debezium.io/docs/configuration/avro/&quot;&gt;Avro Serialization&lt;/a&gt; describes how to use the use the Avro converter and the Confluent Schema Registry instead of the JSON converter instead of the default JSON converter for serializing change events, resulting in much smaller message sizes;
      The Avro converter itself has also been added to the Debezium Docker image for Kafka Connect, so you can use it right away&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;http://debezium.io/docs/configuration/topic-routing/&quot;&gt;Topic Routing&lt;/a&gt; describes how to use Debezium’s &lt;code&gt;ByLogicalTableRouter&lt;/code&gt; single message transformation (SMT) for routing the change events from multiple tables into a single topic, which for instance is very useful when working with sharded tables&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please refer to the &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/CHANGELOG.md#052&quot;&gt;changelog&lt;/a&gt; for an overview of all the 19 issues fixed in Debezium 0.5.2.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The following people from the community have sent in pull requests for this release:
      &lt;a href=&quot;https://github.com/emrul&quot;&gt;Emrul Islam&lt;/a&gt;, &lt;a href=&quot;https://github.com/ekreiser&quot;&gt;Eric S. Kreiser&lt;/a&gt;, &lt;a href=&quot;https://github.com/xenji&quot;&gt;Mario Mueller&lt;/a&gt;, &lt;a href=&quot;https://github.com/mcapitanio&quot;&gt;Matteo Capitanio&lt;/a&gt;, &lt;a href=&quot;https://github.com/omarsmak&quot;&gt;Omar Al-Safi&lt;/a&gt; and &lt;a href=&quot;https://github.com/Satyajitv&quot;&gt;Satyajit Vegesna&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks a lot to you and everyone else in the community for contributing to Debezium via feature requests, bug reports, discussions and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_what_s_next&quot;&gt;What’s next&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The next version of Debezium will be 0.6 (planned for September).
      This release is planned to bring the upgrade to Kafka 0.11.
      We’ll also look into an SMT for transforming the change events emitted by Debezium into a flat representation, which for instance will be very useful in conjunction with the JDBC sink connector.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While 0.6 is planned to be more of a &quot;stabilization release&quot;, 0.7 should bring a long-awaited major feature:
      we’ve planned to explore support for Oracle and hopefully will do an initial release of a Debezium connector for that database.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In other words, exciting times are ahead!
      If you’d like to get involved, let us know.
      Check out the details below on how to get in touch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2017/06/12/debezium-0-5-1-released/</id>
    <title>Debezium 0.5.1 Released</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2017-06-12T00:00:00+00:00</published>
    <link href="/blog/2017/06/12/debezium-0-5-1-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s my pleasure to announce the release of Debezium 0.5.1!
      
      
      This release fixes several bugs in the MySQL, Postgres and MongoDB connectors.
      There&#8217;s also support for some new datatypes: POINT on MySQL (DBZ-222) and TSTZRANGE on Postgres (DBZ-280).
      This release is a drop-in replacement for 0.5.0, upgrading is recommended to all users.
      
      
      Note that in the&#8201;&#8212;&#8201;rather unlikely&#8201;&#8212;&#8201;case that you happened to enable Debezium for all the system tables of MySQL,
      any configured table filters will be applied to these system tables now, too (DBZ-242).
      This may require an adjustment of your filters if you indeed wanted to capture all system tables but only selected non-system tables.
      
      
      Please...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s my pleasure to announce the release of &lt;strong&gt;Debezium 0.5.1&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release fixes several bugs in the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL&lt;/a&gt;, &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql/&quot;&gt;Postgres&lt;/a&gt; and &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/&quot;&gt;MongoDB&lt;/a&gt; connectors.
      There’s also support for some new datatypes: &lt;code&gt;POINT&lt;/code&gt; on MySQL (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-222&quot;&gt;DBZ-222&lt;/a&gt;) and &lt;code&gt;TSTZRANGE&lt;/code&gt; on Postgres (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-280&quot;&gt;DBZ-280&lt;/a&gt;).
      This release is a drop-in replacement for 0.5.0, upgrading is recommended to all users.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note that in the — rather unlikely — case that you happened to enable Debezium for all the system tables of MySQL,
      any configured table filters will be applied to these system tables now, too (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-242&quot;&gt;DBZ-242&lt;/a&gt;).
      This may require an adjustment of your filters if you indeed wanted to capture all system tables but only selected non-system tables.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please refer to the &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/CHANGELOG.md#051&quot;&gt;changelog&lt;/a&gt; for an overview of all the 29 issues fixed in Debezium 0.5.1.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Docker image containing &lt;a href=&quot;https://hub.docker.com/r/debezium/connect/&quot;&gt;Kafka Connect and all the Debezium 0.5.x connectors&lt;/a&gt;
      as well as the image containing &lt;a href=&quot;https://hub.docker.com/r/debezium/postgres/&quot;&gt;Postgres and the Debezium logical decoding plug-in&lt;/a&gt; have been updated to 0.5.1, too.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As Debezium continues to evolve and grow, the number of people contributing to the project is also going up.
      The following people have sent in pull requests for this release:
      &lt;a href=&quot;https://github.com/arosenber&quot;&gt;Aaron Rosenberg&lt;/a&gt;, &lt;a href=&quot;https://github.com/CyberDem0n&quot;&gt;Alexander Kukushkin&lt;/a&gt;, &lt;a href=&quot;https://github.com/brendanmaguire&quot;&gt;Brendan Maguire&lt;/a&gt;, &lt;a href=&quot;https://github.com/DuncanSands&quot;&gt;Duncan Sands&lt;/a&gt;, &lt;a href=&quot;https://github.com/dasl-&quot;&gt;David Leibovic&lt;/a&gt;, &lt;a href=&quot;https://github.com/jpechane&quot;&gt;Jiri Pechanec&lt;/a&gt;, &lt;a href=&quot;https://github.com/nacivida&quot;&gt;nacivida&lt;/a&gt;, &lt;a href=&quot;https://github.com/omarsmak&quot;&gt;Omar Al-Safi&lt;/a&gt;, &lt;a href=&quot;https://github.com/rhauch&quot;&gt;Randall Hauch&lt;/a&gt; and &lt;a href=&quot;https://github.com/tombentley&quot;&gt;Tom Bentley&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks a lot to you and everyone else in the community contributing via feature requests, bug reports, discussions and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_what_s_next&quot;&gt;What’s next&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve planned to do further bug fix releases for the 0.5.x line.
      Specifically, we’ll release a fix for &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-217&quot;&gt;DBZ-217&lt;/a&gt; shortly,
      which is about the MySQL connector stumbling when getting across a corrupt event in the binlog.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In parallel we’re looking into Debezium connectors for &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-40&quot;&gt;SQL Server&lt;/a&gt; and &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-137&quot;&gt;Oracle&lt;/a&gt;.
      While we cannot promise anything yet in terms of when these will be ready to be published, we hope to have at least one of them ready some time soon.
      Stay tuned and get involved!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2017/04/27/hello-debezium/</id>
    <title>Hello Debezium!</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2017-04-27T00:00:00+00:00</published>
    <link href="/blog/2017/04/27/hello-debezium/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="community"></category>
    <category term="news"></category>
    <summary>
      
      When I first learned about the Debezium project last year, I was very excited about it right away.
      
      
      I could see how this project would be very useful for many people out there and I was very impressed by the professional way it was set up:
      a solid architecture for change data capture based on Apache Kafka, a strong focus on robustness and correctness also in the case of failures, the overall idea of creating a diverse eco-system of CDC connectors.
      All that based on the principles of open source, combined with extensive documentation from day one, a friendly and welcoming web site...
    </summary>
    <content type="html">
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When I first learned about the Debezium project last year, I was very excited about it right away.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I could see how this project would be very useful for many people out there and I was very impressed by the professional way it was set up:
      a solid architecture for change data capture based on Apache Kafka, a strong focus on robustness and correctness also in the case of failures, the overall idea of creating a diverse eco-system of CDC connectors.
      All that based on the principles of open source, combined with extensive documentation from day one, a friendly and welcoming web site and a great getting-started experience.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So you can imagine that I was more than enthusiastic about the opportunity to take over the role of Debezium’s project lead.
      Debezium and CDC have close links to some data-centric projects I’ve been previously working on and also tie in with ideas I’ve been pursuing around CQRS, even sourcing and denormalization.
      As core member of the &lt;a href=&quot;http://hibernate.org/&quot;&gt;Hibernate team&lt;/a&gt; at Red Hat, I’ve implemented the initial Elasticsearch support for &lt;a href=&quot;http://hibernate.org/search/&quot;&gt;Hibernate Search&lt;/a&gt;
      (which deals with full-text index updates via JPA/Hibernate).
      I’ve also contributed to &lt;a href=&quot;http://hibernate.org/ogm/&quot;&gt;Hibernate OGM&lt;/a&gt; - a project which connects JPA and the world of NoSQL.
      One of the plans for OGM is to create a declarative denormalization engine for creating read models optimized for specific use cases.
      It will be very interesting to see how this plays together with the capabilities provided by Debezium.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Currently I am serving as the lead of the &lt;a href=&quot;http://beanvalidation.org/&quot;&gt;Bean Validation 2.0&lt;/a&gt; specification (JSR 380) as well as its reference implementation &lt;a href=&quot;http://hibernate.org/validator/&quot;&gt;Hibernate Validator&lt;/a&gt;.
      Two other projects close to my heart are &lt;a href=&quot;http://mapstruct.org/&quot;&gt;MapStruct&lt;/a&gt; - a code generator for bean-to-bean mappings - and &lt;a href=&quot;https://github.com/moditect/moditect&quot;&gt;ModiTect&lt;/a&gt;, which is tooling for Java 9 modules and their descriptors.
      In general, I’m a strong believer into the idea of open source and I just love it to work with folks from all over the world to create useful tools and libraries.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Joining the Debezium community and working on change data capture is a great next step.
      There are so many things to do: connectors for Oracle, SQL Server and Cassandra,
      but also things like an entity join processor which would allow to step from row-level events to more aggregated business-level events (e.g. for updating a combined search index for an order and its order lines) or tooling for managing and visualizing histories of event schema changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;One thing I’d like to emphasize is that the project’s direction generally isn’t going to change very much.
      Red Hat is fully committed to maintaining and evolving the project together with you, the Debezium community.
      The ride really has just begun!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, let me say a huge thank you to Randall for his excellent work!
      You’ve been a true role model for going from an idea over pitching it - within Red Hat as well as within the wider community - to building a steadily growing and evolving project.
      It’s stating the obvious, but it wouldn’t be for Debezium without you.
      Thanks for everything and looking forward very much to working with you and the community on this great project!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Onwards,&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;--Gunnar&lt;/p&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2017/04/26/Debezium-evolving/</id>
    <title>Debezium Evolving</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2017-04-26T00:00:00+00:00</published>
    <link href="/blog/2017/04/26/Debezium-evolving/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="community"></category>
    <category term="news"></category>
    <summary>
      
      Just before I started the Debezium project in early 2016, Martin Kleppmann gave several presentations about turning the database inside out and how his Bottled Water project demonstrated the importantance that change data capture can play in using Kafka for stream processing. Then Kafka Connect was announced, and at that point it seemed obvious to me that Kafka Connect was the foundation upon which practical and reusable change data capture can be built. As these techniques and technologies were becoming more important to Red Hat, I was given the opportunity to start a new open source project and community around...
    </summary>
    <content type="html">
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Just before I started the Debezium project in early 2016, &lt;a href=&quot;https://martin.kleppmann.com&quot;&gt;Martin Kleppmann&lt;/a&gt; gave several presentations about &lt;a href=&quot;https://martin.kleppmann.com/2015/03/04/turning-the-database-inside-out.html&quot;&gt;turning the database inside out&lt;/a&gt; and how his &lt;a href=&quot;https://martin.kleppmann.com/2015/04/23/bottled-water-real-time-postgresql-kafka.html&quot;&gt;Bottled Water&lt;/a&gt; project demonstrated the importantance that change data capture can play in using Kafka for stream processing. Then Kafka Connect was &lt;a href=&quot;https://www.confluent.io/blog/announcing-kafka-connect-building-large-scale-low-latency-data-pipelines/&quot;&gt;announced&lt;/a&gt;, and at that point it seemed obvious to me that Kafka Connect was the foundation upon which practical and reusable change data capture can be built. As these techniques and technologies were becoming more important to &lt;a href=&quot;https://www.redhat.com/&quot;&gt;Red Hat&lt;/a&gt;, I was given the opportunity to start a new open source project and community around building great CDC connectors for a variety of databases management systems.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Over the past few years, we have created Kafka Connect connectors for &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL&lt;/a&gt;, then &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/&quot;&gt;MongoDB&lt;/a&gt;, and most recently &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql/&quot;&gt;PostgreSQL&lt;/a&gt;. Each were initially limited and had a number of problems and issues, but over time more and more people have tried the connectors, asked questions, answered questions, mentioned &lt;a href=&quot;https://twitter.com/search?vertical=default&amp;amp;q=debezium&amp;amp;src=typd&quot;&gt;Debezium on Twitter&lt;/a&gt;, tested connectors in their own environments, reported problems, fixed bugs, discussed limitations and potential new features, implemented enhancements and new features, improved the documentation, and wrote blog posts. Simply put, people with similar needs and interests have worked together and have formed a community. Additional connectors for &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-137&quot;&gt;Oracle&lt;/a&gt; and &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-40&quot;&gt;SQL Server&lt;/a&gt; are in the works, but could use some help to move things along more quickly.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s really exciting to see how far we’ve come and how the Debezium community continues to evolve and grow. And it’s perhaps as good a time as any to hand the reigns over to someone else. In fact, after nearly 10 wonderful years at Red Hat, I’m making a bigger change and as of today am part of &lt;a href=&quot;https://www.confluent.io&quot;&gt;Confluent’s&lt;/a&gt; engineering team, where I expect to play a more active role in the broader &lt;a href=&quot;https://kafka.apache.org&quot;&gt;Kafka&lt;/a&gt; community and more directly with Kafka Connect and Kafka Streams. I &lt;strong&gt;definitely&lt;/strong&gt; plan to stay involved in the Debezium community, but will no longer be leading the project. That role will instead be filled by &lt;a href=&quot;https://github.com/gunnarmorling/&quot;&gt;Gunnar Morling&lt;/a&gt;, who’s recently joined the Debezium community but has extensive experience in open source, the &lt;a href=&quot;http://in.relation.to/gunnar-morling/&quot;&gt;Hibernate community&lt;/a&gt;, and the &lt;a href=&quot;http://beanvalidation.org&quot;&gt;Bean Validation&lt;/a&gt; specification effort. Gunnar is a great guy and an excellent developer, and will be an excellent lead for the Debezium community.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Will the Debezium project change? To some degree it will always continue to evolve just as it has from the very beginning, and that’s a healthy thing. But a lot is staying the same. Red Hat remains committed to the Debezium project, and will continue its sponsorship and community-oriented governance that has worked so well from the beginning. And just as importantly, we the community are still here and will continue building the best open source CDC connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So keep up the great work, and look for and take advantage of opportunities to &lt;a href=&quot;http://debezium.io/community/&quot;&gt;become more involved&lt;/a&gt; in Debezium. Please give a warm welcome to Gunnar by introducing yourself in the &lt;a href=&quot;https://gitter.im/debezium/dev&quot;&gt;developer&lt;/a&gt; and / or &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;user&lt;/a&gt; chat rooms and mention how you’re using Debezium and what the Debezium community means to you.&lt;/p&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2017/03/27/Debezium-0-5-0-Released/</id>
    <title>Debezium 0.5.0 Released</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2017-03-27T00:00:00+00:00</published>
    <link href="/blog/2017/03/27/Debezium-0-5-0-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.5.0 is now available for use with Kafka Connect 0.10.2.0. This release also includes a few fixes for the MySQL connector. See the release notes for specifics on these changes, and be sure to check out the Kafka documentation for compatibility with the version of the Kafka broker that you are using.
      
      
      Kafka Connect 0.10.2.0 comes with a significant new feature called Single Message Transforms, and you can now use them with Debezium connectors. SMTs allow you to modify the messages produced by Debezium connectors and any oher Kafka Connect source connectors, before those messages...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.5.0&lt;/strong&gt; is now available for use with &lt;strong&gt;Kafka Connect 0.10.2.0&lt;/strong&gt;. This release also includes a few fixes for the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt;. See the &lt;a href=&quot;http://debezium.io/docs/releases&quot;&gt;release notes&lt;/a&gt; for specifics on these changes, and be sure to check out the &lt;a href=&quot;https://kafka.apache.org/documentation/#upgrade&quot;&gt;Kafka documentation&lt;/a&gt; for compatibility with the version of the Kafka broker that you are using.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka Connect 0.10.2.0 comes with a significant new feature called &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-66%3A+Single+Message+Transforms+for+Kafka+Connect&quot;&gt;Single Message Transforms&lt;/a&gt;, and you can now use them with Debezium connectors. SMTs allow you to modify the messages produced by Debezium connectors and any oher Kafka Connect source connectors, before those messages are written to Kafka. SMTs can also be used with Kafka Connect sink connectors to modify the messages &lt;em&gt;before&lt;/em&gt; the sink connectors processes them. You can use SMTs to filter out or mask specific fields, add new fields, modify existing fields, change the topic and/or topic partition to which the messages are written, and even more. And you can even chain multiple SMTs together.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka Connect comes with a number of built-in SMTs that you can simply configure and use, but you can also create your own SMT implementations to do more complex and interesting things. For example, although Debezium connectors normally map all of the changes in each table (or collection) to separate topics, you can write a custom SMT that uses a completely different mapping between tables and topics and even add fields to message keys and/or values. Using your new SMT is also very easy - simply put it on the Kafka Connect classpath and update the connector configuration to use it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also added &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.5&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Sanjay and everyone in the community for their help with this release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_what_s_next&quot;&gt;What’s next&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll continue to improve the MongoDB, MySQL, and PostgreSQL connectors and pushing out 0.5.x releases with fixes. And we’re still working on connectors for &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-40&quot;&gt;SQL Server&lt;/a&gt; and &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-137&quot;&gt;Oracle&lt;/a&gt;. Stay tuned and get involved!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2017/03/17/Debezium-0-4-1-Released/</id>
    <title>Debezium 0.4.1 Released</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2017-03-17T00:00:00+00:00</published>
    <link href="/blog/2017/03/17/Debezium-0-4-1-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="rds"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.4.1 is now available for use with Kafka Connect 0.10.1.1. This release includes several fixes for the MongoDB connector and MySQL connector, including improved support for Amazon RDS and Amazon Aurora (MySQL compatibility). See the release notes for specifics on these changes.
      
      
      We&#8217;ve also updated the Debezium Docker images labelled 0.4 and latest, which we use in our tutorial.
      
      
      Thanks to Jan, Horia, David, Josh, Johan, Sanjay, Saulius, and everyone in the community for their help with this release, issues, discussions, contributions, and questions!
      
      
      
      
      What&#8217;s next
      
      
      Kafka 0.10.2.0 is out, so we plan to release 0.5.0 next week...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.4.1&lt;/strong&gt; is now available for use with Kafka Connect 0.10.1.1. This release includes several fixes for the &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb&quot;&gt;MongoDB connector&lt;/a&gt; and &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt;, including improved support for &lt;a href=&quot;https://aws.amazon.com/rds/mysql/&quot;&gt;Amazon RDS&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/rds/aurora/&quot;&gt;Amazon Aurora (MySQL compatibility)&lt;/a&gt;. See the &lt;a href=&quot;http://debezium.io/docs/releases&quot;&gt;release notes&lt;/a&gt; for specifics on these changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.4&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Jan, Horia, David, Josh, Johan, Sanjay, Saulius, and everyone in the community for their help with this release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_what_s_next&quot;&gt;What’s next&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka 0.10.2.0 is out, so we plan to release 0.5.0 next week with all of the changes/fixes in 0.4.1 but with support for Kafka 0.10.2.0. We’ll then continue to improve the MongoDB, MySQL, and PostgreSQL connectors and pushing out 0.5.x releases. Stay tuned and get involved!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2017/02/22/Debezium-at-WePay/</id>
    <title>Streaming databases in realtime with MySQL, Debezium, and Kafka</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2017-02-22T00:00:00+00:00</published>
    <link href="/blog/2017/02/22/Debezium-at-WePay/" rel="alternate" type="text/html" />
    <author>
      <name>Chris Riccomini</name>
    </author>
    <category term="mysql"></category>
    <summary>
      
      
      
      This post originally appeared on the WePay Engineering blog.
      
      
      Change data capture has been around for a while, but some recent developments in technology have given it new life. Notably, using Kafka as a backbone to stream your database data in realtime has become increasingly common.
      
      
      If you&#8217;re wondering why you might want to stream database changes into Kafka, I highly suggest reading The Hardest Part About Microservices: Your Data. At WePay, we wanted to integrate our microservices and downstream datastores with each other, so every system could get access to the data that it needed. We use Kafka as our data...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;strong&gt;&lt;em&gt;This post originally appeared on the &lt;a href=&quot;https://wecode.wepay.com/posts/streaming-databases-in-realtime-with-mysql-debezium-kafka&quot;&gt;WePay Engineering blog&lt;/a&gt;.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Change_data_capture&quot;&gt;Change data capture&lt;/a&gt; has been around for a while, but some recent developments in technology have given it new life. Notably, using &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; as a backbone to stream your database data in realtime has become &lt;a href=&quot;https://github.com/wushujames/mysql-cdc-projects/wiki&quot;&gt;increasingly common&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’re wondering why you might want to stream database changes into Kafka, I highly suggest reading &lt;a href=&quot;http://blog.christianposta.com/microservices/the-hardest-part-about-microservices-data/&quot;&gt;The Hardest Part About Microservices: Your Data&lt;/a&gt;. At WePay, we wanted to integrate our microservices and downstream datastores with each other, so every system could get access to the data that it needed. We use Kafka as our data integration layer, so we needed a way to get our database data into it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Last year, &lt;a href=&quot;https://www.yelp.com/engineering&quot;&gt;Yelp’s engineering team&lt;/a&gt; published an excellent &lt;a href=&quot;https://engineeringblog.yelp.com/2016/11/open-sourcing-yelps-data-pipeline.html&quot;&gt;series of posts&lt;/a&gt; on their data pipeline. These included a discussion on how they &lt;a href=&quot;https://engineeringblog.yelp.com/2016/08/streaming-mysql-tables-in-real-time-to-kafka.html&quot;&gt;stream MySQL data into Kafka&lt;/a&gt;. Their architecture involves a series of homegrown pieces of software to accomplish the task, notably &lt;a href=&quot;https://github.com/Yelp/schematizer&quot;&gt;schematizer&lt;/a&gt; and &lt;a href=&quot;https://github.com/Yelp/mysql_streamer&quot;&gt;MySQL streamer&lt;/a&gt;. The write-up triggered a thoughtful post on Debezium’s blog about a proposed equivalent architecture using &lt;a href=&quot;http://docs.confluent.io/3.1.1/connect/&quot;&gt;Kafka connect&lt;/a&gt;, &lt;a href=&quot;http://debezium.io/&quot;&gt;Debezium&lt;/a&gt;, and &lt;a href=&quot;http://docs.confluent.io/3.1.1/schema-registry/docs/&quot;&gt;Confluent’s schema registry&lt;/a&gt;. This proposed architecture is what we’ve been implementing at WePay, and this post describes how we leverage Debezium and Kafka connect to stream our MySQL databases into Kafka.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_architecture&quot;&gt;Architecture&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The flow of data starts with each microservice’s MySQL database. These databases run in &lt;a href=&quot;https://cloud.google.com/&quot;&gt;Google Cloud&lt;/a&gt; as &lt;a href=&quot;https://cloud.google.com/sql/&quot;&gt;CloudSQL&lt;/a&gt; MySQL instances &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/replication-gtids.html&quot;&gt;with GTIDs enabled&lt;/a&gt;. We’ve set up a downstream MySQL cluster specifically for Debezium. Each CloudSQL instance replicates its data into the Debezium cluster, which consists of two MySQL machines: a primary (active) server and secondary (passive) server. This single Debezium cluster is an operational trick to make it easier for us to operate Debezium. Rather than having Debezium connect to dozens of microservice databases directly, we can connect to just a single database. This also isolates Debezium from impacting the production OLTP workload that the master CloudSQL instances are handling.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We run one Debezium connector (in &lt;a href=&quot;http://docs.confluent.io/2.0.0/connect/userguide.html#distributed-mode&quot;&gt;distributed mode&lt;/a&gt; on the Kafka connect framework) for each microservice database. Again, the goal here is isolation. Theoretically, we could run a single Debezium connector that produces messages for all databases (since all microservice databases are in the Debezium cluster). This approach would actually be more resource efficient since each Debezium connector has to read MySQL’s entire &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/binary-log.html&quot;&gt;binlog&lt;/a&gt; anyway. We opted not to do this because we wanted to be able to bring Debezium connectors up and down, and configure them differently for each microservice DB.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium connectors feed the MySQL messages into Kafka (and add their schemas to the Confluent schema registry), where downstream systems can consume them. We use our Kafka connect &lt;a href=&quot;https://wecode.wepay.com/posts/kafka-bigquery-connector&quot;&gt;BigQuery connector&lt;/a&gt; to load the MySQL data into BigQuery using BigQuery’s &lt;a href=&quot;https://cloud.google.com/bigquery/streaming-data-into-bigquery&quot;&gt;streaming API&lt;/a&gt;. This gives us a data warehouse in BigQuery that is usually less than 30 seconds behind the data that’s in production. Other microservices, stream processors, and data infrastructure consume the feeds as well.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;https://wecode.wepay.com/assets/2017-02-21-streaming-databases-in-realtime-with-mysql-debezium-kafka/debezium-architecture.png&quot; alt=&quot;Debezium architecture&quot;&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_debezium&quot;&gt;Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The remainder of this post will focus on Debezium (the DBZ boxes in the diagram above), and how we configure and operate it. Debezium works by connecting to MySQL and pretending to be a replica. MySQL sends its replication data to Debezium, thinking it’s actually funneling data to another downstream MySQL instance. Debezium then takes the data, converts the schemas from MySQL schemas to &lt;a href=&quot;https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Struct.html&quot;&gt;Kafka connect structures&lt;/a&gt;, and forwards them to Kafka.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;_adding_new_databases&quot;&gt;Adding new databases&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When a new microservice with a CloudSQL database comes online, we want to get that data into Kafka. The first step in the process is to load the data into the Debezium MySQL cluster. This involves several steps:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;Take a MySQL dump of the data in the microservice DB.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Pause the secondary Debezium MySQL DB.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Load the MySQL dump into the secondary Debezium MySQL DB.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Reset &lt;code&gt;GTID_PURGED&lt;/code&gt; parameter to include the GTID from the new DB dump.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Unpause the secondary Debezium MySQL DB.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Update HA Proxy to point to the secondary, which now becomes the primary.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Follow steps 2-5 for the old primary instance (now secondary).&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The actual commands that we run are:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight nowrap&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;# (1) Take a dump of the database we wish to add.
      $ mydumper  --host=123.123.123.123 --port=3306 --user=foo --password=*********  -B log --trx-consistency-only  --triggers --routines -o /mysqldata/new_db/ -c -L mydumper.log
      
      # (2) Stop all replication on the secondary Debezium cluster.
      $ mysql&amp;gt; STOP SLAVE for channel 'foo';
      $ mysql&amp;gt; STOP SLAVE for channel 'bar';
      $ mysql&amp;gt; STOP SLAVE for channel 'baz';
      
      # Get the current GTID purged values from MySQL.
      $ mysql&amp;gt; SHOW GLOBAL VARIABLES like '%gtid_purged%';
      
      # (3) Load the dump of the database into the Debezium cluster.
      $ myloader -d /mysqldata/new_db/ -s new_db
      
      # (4) Clear out existing GTID_PURGED values so that we can overwrite it to include the GTID from the new dump file.
      $ mysql&amp;gt; reset master;
      
      # Set the new GTID_PURGED value, including the GTID_PURGED value from the MySQL dump file.
      $ mysql&amp;gt; set global GTID_PURGED=&quot;f3a44d1a-11e6-44ba-bf12-040bab830af0:1-10752,c627b2bc-b36a-11e6-a886-42010af00790:1-9052,01261abc3-6ade-11e6-9647-42010af0044a:1-375342&quot;;
      
      # (5) Start replication for the new DB.
      $ mysql&amp;gt; CHANGE MASTER TO MASTER_HOST='123.123.123.123', MASTER_USER='REPLICATION_USER', MASTER_PASSWORD='REPLICATION_PASSWORD',MASTER_AUTO_POSITION=1 for CHANNEL 'new_db';
      $ mysql&amp;gt; START SLAVE for channel 'new_db';
      
      # Start replication for the DBs that we paused.
      $ mysql&amp;gt; START SLAVE for channel 'foo';
      $ mysql&amp;gt; START SLAVE for channel 'bar';
      $ mysql&amp;gt; START SLAVE for channel 'baz';
      
      # Repeat steps 2-5 on the old primary (now secondary).&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;At the end of these steps, both the primary and secondary Debezium MySQL servers have the new database. Once finished, we can then add a new Debezium connector to the Kafka connect cluster. This connector will have configuration that looks roughly like this:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight nowrap&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
         &quot;name&quot;: &quot;debezium-connector-microservice1&quot;,
         &quot;config&quot;: {
             &quot;name&quot;: &quot;debezium-connector-microservice1&quot;,
             &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
             &quot;tasks.max&quot;: &quot;1&quot;,
             &quot;database.hostname&quot;: &quot;dbz-mysql01&quot;,
             &quot;database.port&quot;: &quot;3306&quot;,
             &quot;database.user&quot;: &quot;user&quot;,
             &quot;database.password&quot;: &quot;*******&quot;,
             &quot;database.server.id&quot;: &quot;101&quot;,
             &quot;database.server.name&quot;: &quot;db.debezium.microservice1&quot;,
             &quot;gtid.source.includes&quot;: &quot;c34aeb9e-89ad-11e6-877b-42010a93af2d&quot;,
             &quot;database.whitelist&quot;: &quot;microservice1_db&quot;,
             &quot;poll.interval.ms&quot;: &quot;2&quot;,
             &quot;table.whitelist&quot;: &quot;microservice1_db.table1,microservice1_db.table2&quot;,
             &quot;column.truncate.to.1024.chars&quot; : &quot;microservice1_db.table1.text_col&quot;,
             &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka01:9093,kafka02:9093,kafka03:9093&quot;,
             &quot;database.history.kafka.topic&quot;: &quot;debezium.history.microservice1&quot;,
             &quot;database.ssl.truststore&quot;: &quot;/certs/truststore&quot;,
             &quot;database.ssl.truststore.password&quot;: &quot;*******&quot;,
             &quot;database.ssl.mode&quot;: &quot;required&quot;,
             &quot;database.history.producer.security.protocol&quot;: &quot;SSL&quot;,
             &quot;database.history.producer.ssl.truststore.location&quot;: &quot;/certs/truststore&quot;,
             &quot;database.history.producer.ssl.truststore.password&quot;: &quot;*******&quot;,
             &quot;database.history.consumer.security.protocol&quot;: &quot;SSL&quot;,
             &quot;database.history.consumer.ssl.truststore.location&quot;: &quot;/certs/truststore&quot;,
             &quot;database.history.consumer.ssl.truststore.password&quot;: &quot;*******&quot;,
         }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The details on these configuration fields are located &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/#connector-properties&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The new connector will start up and begin &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/#snapshots&quot;&gt;snapshotting&lt;/a&gt; the database, since this is the first time it’s been started. Debezium’s snapshot implementation (see &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-31&quot;&gt;DBZ-31&lt;/a&gt;) uses an approach very similar to MySQL’s mysqldump tool. Once the snapshot is complete, Debezium will switch over to using MySQL’s binlog to receive all future database updates.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka connect and Debezium work together to periodically commit Debezium’s location in the MySQL binlog described by a &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/replication-gtids-concepts.html&quot;&gt;MySQL global transaction ID&lt;/a&gt; (GTID). When Debezium restarts, Kafka connect will give it the last committed MySQL GTID, and Debezium will pick up from there.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;em&gt;Note that commits only happen periodically, so Debezium might start up from a location in the log prior to the last row that it received. In such a case, you will observe duplicate messages in Debezium Kafka topic. Debezium writes messages to Kafka with an at-least-once messaging guarantee.&lt;/em&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;_high_availability&quot;&gt;High availability&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;One of the difficulties we faced when we first began using Debezium was how to make it tolerant to machine failures (both the upstream MySQL server, and Debezium, itself). MySQL prior to version 5.6 modeled a replica’s location in its parent’s binlogs using a (binlog filename, file offset) tuple. The problem with this approach is that the binlog filenames are not the same between MySQL machines. This means that a replica reading from upstream MySQL machine 1 can’t easily fail over to MySQL machine 2. There is an entire ecosystem of tools (including &lt;a href=&quot;https://code.google.com/p/mysql-master-ha/&quot;&gt;MHA&lt;/a&gt;) to try and address this problem.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Starting with MySQL 5.6, MySQL introduced the concept of global transaction IDs. These GTIDs identify a specific location within the MySQL binlog &lt;em&gt;across machines&lt;/em&gt;. This means that a consumer reading from a binlog on one MySQL server can switch over to the other, provided that both servers have the data available. This is how we run our systems. Both the CloudSQL instances and the Debezium MySQL cluster run with GTIDs enabled. The Debezium MySQL servers also have replication binlogs enabled so that binlogs exist for Debezium to read (replicas don’t normally have binlogs enabled by default). All of this enables Debezium to consume from the primary Debezium MySQL server, but switch over to the secondary (via HA Proxy) if there’s a failure.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If the machine that Debezium, itself, is running on fails, then the Kafka connect framework fails the connector over to another machine in the cluster. When the failover occurs, Debezium receives its last committed offset (GTID) from Kafka connect, and picks up where it left off (with the same caveat as above: you might see some duplicate messages due to periodic commit frequency).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;An important configuration that needs to be called out is the &lt;code&gt;gtid.source.includes&lt;/code&gt; field that we have set above. When we first set up the topology that’s described in the architecture section, we discovered that we could not fail over from the primary Debezium DB to the secondary DB even though they both were replicating exactly the same data. This is because, in addition to the GTIDs for the various upstream DBs that both primary and secondary machines are replicating, each machine has its &lt;em&gt;own&lt;/em&gt; server UUID for its various MySQL databases (e.g. information_schema). The fact that these two servers have different UUIDs in them led MySQL to get confused when we triggered a failover, because Debezium’s GTID would include the server UUID for the primary server, which the secondary server didn’t know about. The fix was to filter out all UUIDs that we don’t care about from the GTID. Each Debezium connector filters out all server UUIDs except for the UUID for the microservice DB that it cares about. This allows the connector to fail from primary to secondary without issue. This issue is documented in detail on &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-129&quot;&gt;DBZ-129&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;_schemas&quot;&gt;Schemas&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium’s &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/#change-events-value&quot;&gt;message format&lt;/a&gt; includes both the &quot;before&quot; and &quot;after&quot; versions of a row. For inserts, the &quot;before&quot; is null. For deletes, the &quot;after&quot; is null. Updates have both the &quot;before&quot; and &quot;after&quot; fields filled out. The messages also include some server information such as the server ID that the message came from, the GTID of the message, the server timestamp, and so on.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
        &quot;before&quot;: {
          &quot;id&quot;: 1004,
          &quot;first_name&quot;: &quot;Anne&quot;,
          &quot;last_name&quot;: &quot;Kretchmar&quot;,
          &quot;email&quot;: &quot;annek@noanswer.org&quot;
        },
        &quot;after&quot;: {
          &quot;id&quot;: 1004,
          &quot;first_name&quot;: &quot;Anne Marie&quot;,
          &quot;last_name&quot;: &quot;Kretchmar&quot;,
          &quot;email&quot;: &quot;annek@noanswer.org&quot;
        },
        &quot;source&quot;: {
          &quot;name&quot;: &quot;mysql-server-1&quot;,
          &quot;server_id&quot;: 223344,
          &quot;ts_sec&quot;: 1465581,
          &quot;gtid&quot;: null,
          &quot;file&quot;: &quot;mysql-bin.000003&quot;,
          &quot;pos&quot;: 484,
          &quot;row&quot;: 0,
          &quot;snapshot&quot;: null
        },
        &quot;op&quot;: &quot;u&quot;,
        &quot;ts_ms&quot;: 1465581029523
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The serialization format that Debezium sends to Kafka is configurable. We prefer Avro at WePay for its compact size, schema DDL, performance, and rich ecosystem. We’ve configured Kafka connect to use Confluent’s &lt;a href=&quot;https://github.com/confluentinc/schema-registry/tree/master/avro-serializer/src/main/java/io/confluent/kafka/serializers&quot;&gt;Avro encoder&lt;/a&gt; codec for Kafka. This encoder serializes messages to Avro, but also registers the schemas with Confluent’s schema registry.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If a MySQL table’s schema is changed, Debezium adapts to the change by updating the structure and schema of the &quot;before&quot; and &quot;after&quot; portions of its event messages. This will appear to the Avro encoder as a new schema, which it will register with the schema registry before the message is sent to Kafka. The registry runs full compatibility checks to make sure that downstream consumers don’t break due to a schema evolution.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;em&gt;Note that it’s still possible to make an incompatible change in the MySQL schema itself, which would break downstream consumers. We have not yet added automatic compatibility checks to MySQL table alters.&lt;/em&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_future_work&quot;&gt;Future work&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;_monolithic_database&quot;&gt;Monolithic database&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In addition to our microservices, we have a legacy monolithic database that’s much larger than our microservice databases. We’re in the process of upgrading this cluster to run with GTIDs enabled. Once this is done, we plan to replicate this cluster into Kafka with Debezium as well.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;_large_table_snapshots&quot;&gt;Large table snapshots&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re lucky that all of our microservice databases are of relatively manageable size. Our monolithic database has some tables that are much larger. We have yet to test Debezium with very large tables, so it’s unclear if any tuning or patches will be required in order to snapshot these tables on the initial Debezium load. We have heard community reports that larger tables (6 billion+ rows) do work, provided that the configuration exposed in &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-152&quot;&gt;DBZ-152&lt;/a&gt; is set. This is work we’re planning to do shortly.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;_more_monitoring&quot;&gt;More monitoring&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka connect doesn’t currently make it easy to expose metrics through the Kafka metrics framework. As a result, there are very few metrics available from the Kafka connect framework. Debezium does expose metrics via JMX (see &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-134&quot;&gt;DBZ-134&lt;/a&gt;), but we aren’t exposing them to our metrics system currently. We do monitor the system, but when things go wrong, it can be difficult to determine what’s going on. &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2376&quot;&gt;KAFKA-2376&lt;/a&gt; is the open JIRA that’s meant to address the underlying Kafka connect issue.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;_more_databases&quot;&gt;More databases&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As we add more microservice databases, we’ll begin to put pressure on the two Debezium MySQL servers that we have. Eventually, we plan to split the single Debezium cluster that we have into more than one, with some microservices replicating only to one cluster, and the rest replicating to others.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;_unify_compatibility_checks&quot;&gt;Unify compatibility checks&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As I mentioned in the schema section, above, the Confluent schema registry runs schema compatibility checks out of the box right now. This makes it very easy for us to prevent backward and forward incompatible changes from making their way into Kafka. We don’t currently have an equivalent check at the MySQL layer. This is a problem because it means it’s possible for a DBA to make incompatible changes at the MySQL layer. Debezium will then fail when trying to produce the new messages into Kafka. We need to make sure this can’t happen by adding equivalent checks at the MySQL layer. &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-70&quot;&gt;DBZ-70&lt;/a&gt; discusses this more.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;_automatic_topic_configuration&quot;&gt;Automatic topic configuration&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We currently run Kafka with topic auto-create enabled with a default of 6 partitions, and time-based/size-based retention. This configuration doesn’t make much sense for Debezium topics. At the very least, they should be using log-compaction as their retention. We plan to write a script that looks for mis-configured Debezium topics, and updates them to appropriate retention settings.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_conclusion&quot;&gt;Conclusion&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve been running Debezium in production for the past 8 months. Initially, we ran it dark, and then enabled it for the realtime BigQuery pipeline shown in the architecture diagram above. Recently, we’ve begun consuming the messages in microservices and stream processing systems. We look forward to adding more data to the pipeline, and addressing some of the issues that were raised in the &lt;em&gt;Future work&lt;/em&gt; section.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A special thanks to &lt;a href=&quot;https://www.linkedin.com/in/randallhauch&quot;&gt;Randall Hauch&lt;/a&gt;, who has been invaluable in addressing a number of bug fixes and feature requests.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2017/02/08/Support-for-Postgresql/</id>
    <title>PostgreSQL support added to Debezium</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2017-02-08T00:00:00+00:00</published>
    <link href="/blog/2017/02/08/Support-for-Postgresql/" rel="alternate" type="text/html" />
    <author>
      <name>Horia Chiorean</name>
    </author>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      With the recent Debezium release, we&#8217;re happy to announce that a new PostgreSQL connector has been added alongside the already existing MySQL and MongoDB connectors.
      
      
      
      
      
      Tip
      
      
      
      Make sure you read the connector documentation for an in-depth look at the different configuration options.
      
      
      
      
      
      
      
      
      Getting started
      
      
      The fastest way to check out the new connector is using Debezium&#8217;s Postgres docker image which is based on a vanilla Postgres docker image on top of which it compiles and installs a PostgreSQL logical decoding plugin
      and sets up the necessary permissions for streaming changes locally (on localhost)
      
      
      Once you fire up the Docker machine with the database server, starting up...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With the &lt;a href=&quot;http://debezium.io/blog/2017/02/07/Debezium-0-4-0-Released&quot;&gt;recent Debezium release&lt;/a&gt;, we’re happy to announce that a new &lt;strong&gt;PostgreSQL connector&lt;/strong&gt; has been added alongside the already existing MySQL and MongoDB connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;admonitionblock tip&quot;&gt;
      &lt;table&gt;
      &lt;tr&gt;
      &lt;td class=&quot;icon&quot;&gt;
      &lt;div class=&quot;title&quot;&gt;Tip&lt;/div&gt;
      &lt;/td&gt;
      &lt;td class=&quot;content&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Make sure you &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql&quot;&gt;read the connector documentation&lt;/a&gt; for an in-depth look at the different configuration options.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/td&gt;
      &lt;/tr&gt;
      &lt;/table&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_getting_started&quot;&gt;Getting started&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The fastest way to check out the new connector is using &lt;a href=&quot;https://hub.docker.com/r/debezium/postgres&quot;&gt;Debezium’s Postgres docker image&lt;/a&gt; which is based on a vanilla Postgres docker image on top of which it compiles and installs a PostgreSQL &lt;a href=&quot;https://github.com/debezium/postgres-decoderbufs&quot;&gt;logical decoding plugin&lt;/a&gt;
      and sets up the necessary permissions for streaming changes locally (on &lt;code&gt;localhost&lt;/code&gt;)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once you fire up the Docker machine with the database server, starting up and configuring the connector to stream changes from that machine is exactly the same as described in detail by the &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;Debezium tutorial&lt;/a&gt;. The only obvious difference is that instead of the MySQL machine and MySQL connector configuration you need to use the PostgreSQL machine and the PostgreSQL connector configuration parameters.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_using_the_connector_in_your_own_environment&quot;&gt;Using the connector in your own environment&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Unlike the Mongo and MySQL connectors, getting the PostgreSQL connector up and running is a bit more complicated due to the fact that it requires a server-side logical decoding plugin running in the PostgreSQL server.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In general, there are three major steps involved in getting the connector running in your environment:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;Compiling and installing the &lt;a href=&quot;https://github.com/debezium/postgres-decoderbufs&quot;&gt;logical decoding plugin&lt;/a&gt; into your own server&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Setting up the PostgreSQL server with appropriate replication permissions&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Starting the Kafka Connect, Broker and Zookeeper machines&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For steps 1 and 2 you can check out our &lt;a href=&quot;https://github.com/debezium/docker-images/tree/master/postgres/9.6&quot;&gt;PostgreSQL Docker image&lt;/a&gt; together with the sources for the &lt;a href=&quot;https://github.com/debezium/postgres-decoderbufs&quot;&gt;logical decoding plugin&lt;/a&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For step 3 you can either use Debezium’s &lt;a href=&quot;https://github.com/debezium/docker-images&quot;&gt;Kafka Docker images&lt;/a&gt; or perform a similar setup locally. The &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;Debezium tutorial&lt;/a&gt; and the &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql&quot;&gt;the connector documentation&lt;/a&gt; are great resources for helping out with this task.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2017/02/07/Debezium-0-4-0-Released/</id>
    <title>Debezium 0.4.0 Released</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2017-02-07T00:00:00+00:00</published>
    <link href="/blog/2017/02/07/Debezium-0-4-0-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.4.0 is now available for use with Kafka Connect 0.10.1.1. This release introduces a new PostgreSQL connector, and contains over a dozen fixes combined for the MongoDB connector and MySQL connector, including preliminar support for Amazon RDS and Amazon Aurora (MySQL compatibility). See the release notes for specifics on these changes.
      
      
      We&#8217;ve also created Debezium Docker images labelled 0.4 and latest, which we use in our tutorial.
      
      
      Thanks to Horia, Chris, Akshath, Ramesh, Matthias, Anton, Sagi, barton, and others for their help with this release, issues, discussions, contributions, and questions!
      
      
      
      
      What&#8217;s next
      
      
      We&#8217;ll continue to improve the MongoDB,...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.4.0&lt;/strong&gt; is now available for use with Kafka Connect 0.10.1.1. This release introduces a new &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql&quot;&gt;PostgreSQL connector&lt;/a&gt;, and contains over a dozen fixes combined for the &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb&quot;&gt;MongoDB connector&lt;/a&gt; and &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt;, including preliminar support for &lt;a href=&quot;https://aws.amazon.com/rds/mysql/&quot;&gt;Amazon RDS&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/rds/aurora/&quot;&gt;Amazon Aurora (MySQL compatibility)&lt;/a&gt;. See the &lt;a href=&quot;http://debezium.io/docs/releases&quot;&gt;release notes&lt;/a&gt; for specifics on these changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also created &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.4&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Horia, Chris, Akshath, Ramesh, Matthias, Anton, Sagi, barton, and others for their help with this release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_what_s_next&quot;&gt;What’s next&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll continue to improve the MongoDB, MySQL, and PostgreSQL connectors and pushing out 0.4.x releases. We’re also going to work on a few new connectors, though we’ll likely increase the minor version with each new connector. Stay tuned and get involved!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/12/21/Debezium-0-3-6-Released/</id>
    <title>Debezium 0.3.6 Released</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2016-12-21T00:00:00+00:00</published>
    <link href="/blog/2016/12/21/Debezium-0-3-6-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.3.6 is now available for use with Kafka Connect 0.10.0.1. This release contains over a dozen fixes combined for the MySQL connector and MongoDB connectors. See the release notes for specifics on these changes.
      
      
      We&#8217;ve also updated the Debezium Docker images labelled 0.3 and latest, which we use in our tutorial.
      
      
      Thanks to Farid, RenZhu, Dongjun, Anton, Chris, Dennis, Sharaf, Rodrigo, Tim, and others for their help with this release, issues, discussions, contributions, and questions!
      
      
      
      
      What&#8217;s next
      
      
      We&#8217;ll continue to improve the MongoDB and MySQL connectors, and we also have a great PostgreSQL connector that is nearly ready...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.3.6&lt;/strong&gt; is now available for use with Kafka Connect 0.10.0.1. This release contains over a dozen fixes combined for the MySQL connector and MongoDB connectors. See the &lt;a href=&quot;http://debezium.io/docs/releases&quot;&gt;release notes&lt;/a&gt; for specifics on these changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Farid, RenZhu, Dongjun, Anton, Chris, Dennis, Sharaf, Rodrigo, Tim, and others for their help with this release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_what_s_next&quot;&gt;What’s next&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll continue to improve the MongoDB and MySQL connectors, and we also have a great PostgreSQL connector that is nearly ready to be released. With the new connector we’ll switch release numbers to 0.4.x and plan to stop issuing 0.3.x releases. Stay tuned for this next 0.4.0 release!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/11/14/Debezium-0-3-5-Released/</id>
    <title>Debezium 0.3.5 Released</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2016-11-14T00:00:00+00:00</published>
    <link href="/blog/2016/11/14/Debezium-0-3-5-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.3.5 is now available for use with Kafka Connect 0.10.0.1. This release contains several fixes for the MySQL connector and adds the ability to use with multi-master MySQL servers as sources. See the release notes for specifics on these changes. We&#8217;ve also updated the Debezium Docker images labelled 0.3 and latest, which we use in our tutorial.
      
      
      One of the fixes is signficant, and so we strongly urge all users to upgrade to this release from all earlier versions. In prior versions, the MySQL connector may stop without completing all updates in a transaction, and...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.3.5&lt;/strong&gt; is now available for use with Kafka Connect 0.10.0.1. This release contains several fixes for the MySQL connector and adds the ability to use with &lt;a href=&quot;http://debezium.io/docs/mysql#multi-master-mysql&quot;&gt;multi-master MySQL servers&lt;/a&gt; as sources. See the &lt;a href=&quot;http://debezium.io/docs/releases&quot;&gt;release notes&lt;/a&gt; for specifics on these changes. We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;One of the fixes is signficant, and so &lt;strong&gt;we strongly urge all users to upgrade to this release from all earlier versions.&lt;/strong&gt; In prior versions, the MySQL connector may stop without completing all updates in a transaction, and when the connector restarts it starts with the &lt;em&gt;next&lt;/em&gt; transaction and therefore might fail to capture some of the change events in the earlier transaction. This release fixes this issue so that when restarting it will always pick up where it left off, even if that point is in the middle of a transaction. Note that this fix only takes affect once a connector is upgraded and restarted. See &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-144&quot;&gt;the issue&lt;/a&gt; for more details.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Akshath, Anton, Chris, and others for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/10/25/Debezium-0-3-4-Released/</id>
    <title>Debezium 0.3.4 Released</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2016-10-25T00:00:00+00:00</published>
    <link href="/blog/2016/10/25/Debezium-0-3-4-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.3.4 is now available for use with Kafka Connect 0.10.0.1. This release contains several new features for the MySQL connector: support for MySQL&#8217;s JSON datatype, a new snapshot mode called schema_only, and JMX metrics. Also, the Debezium Docker images for Zookeeper, Kafka, and Kafka Connect have all been updated to allow optionally expose JMX metrics in these services. And, one backward-incompatible fix was made to the change event&#8217;s ts_sec field. See the release notes for specifics.
      
      
      We&#8217;ve also updated the Debezium Docker images labelled 0.3 and latest, which we use in our tutorial.
      
      
      Thanks to Akshath,...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.3.4&lt;/strong&gt; is now available for use with Kafka Connect 0.10.0.1. This release contains several new features for the MySQL connector: support for MySQL’s &lt;a href=&quot;http://debezium.io/docs/connectors/mysql#data-types&quot;&gt;&lt;code&gt;JSON&lt;/code&gt;&lt;/a&gt; datatype, a new snapshot mode called &lt;a href=&quot;http://debezium.io/docs/connectors/mysql#snapshots&quot;&gt;&lt;code&gt;schema_only&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;http://debezium.io/docs/monitoring&quot;&gt;JMX metrics&lt;/a&gt;. Also, the Debezium Docker images for Zookeeper, Kafka, and Kafka Connect have all been updated to allow optionally &lt;a href=&quot;http://debezium.io/docs/monitoring&quot;&gt;expose JMX metrics&lt;/a&gt; in these services. And, one backward-incompatible fix was made to the change event’s &lt;code&gt;ts_sec&lt;/code&gt; field. See the &lt;a href=&quot;http://debezium.io/docs/releases&quot;&gt;release notes&lt;/a&gt; for specifics.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Akshath, Chris, Vitalii, Dennis, Prannoy, and others for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/10/19/Support-for-MySQL-JSON-typpe-coming-soon/</id>
    <title>Support for MySQL&#8217;s JSON type coming soon</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2016-10-19T00:00:00+00:00</published>
    <link href="/blog/2016/10/19/Support-for-MySQL-JSON-typpe-coming-soon/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="mysql"></category>
    <category term="json"></category>
    <summary>
      
      MySQL 5.7 introduced a new data type for storing and working with JSON data. Clients can define tables with columns using the new JSON datatype, and they can store and read JSON data using SQL statements and new built-in JSON functions to construct JSON data from other relational columns, introspect the structure of JSON values, and search within and manipulate JSON data. It possible to define generated columns on tables whose values are computed from the JSON value in another column of the same table, and to then define indexes with those generated columns. Overall, this is really a very...
    </summary>
    <content type="html">
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;MySQL 5.7 introduced a new data type for &lt;a href=&quot;http://mysqlserverteam.com/whats-new-in-mysql-5-7-generally-available/&quot;&gt;storing and working with JSON data&lt;/a&gt;. Clients can define tables with columns using the new &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/json.html&quot;&gt;&lt;code&gt;JSON&lt;/code&gt; datatype&lt;/a&gt;, and they can store and read JSON data using SQL statements and new built-in JSON functions to construct JSON data from other relational columns, introspect the structure of JSON values, and search within and manipulate JSON data. It possible to define generated columns on tables whose values are computed from the JSON value in another column of the same table, and to then define indexes with those generated columns. Overall, this is really a very powerful feature in MySQL.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium’s MySQL connector will support the &lt;code&gt;JSON&lt;/code&gt; datatype starting with the upcoming 0.3.4 release. JSON document, array, and scalar values will appear in change events as strings with &lt;code&gt;io.debezium.data.json&lt;/code&gt; for the schema name. This will make it natural for consumers to work with JSON data. BTW, this is the same semantic schema type used by the MongoDB connector to represent JSON data.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This sounds straightforward, and we hope it is. But &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-126&quot;&gt;implementing this&lt;/a&gt; required a fair amount of work. That’s because although MySQL exposes JSON data as strings to client applications, &lt;em&gt;internally&lt;/em&gt; it stores all JSON data in a special binary form that allows the MySQL engine to efficiently access the JSON data in queries, JSON functions and generated columns. All JSON data appears in the binlog in this binary form as well, which meant that we had to parse the binary form ourselves if we wanted to extract the more useful string representation. Writing and testing this parser took a bit of time and effort, and ultimately we &lt;a href=&quot;https://github.com/shyiko/mysql-binlog-connector-java/issues/115&quot;&gt;donated it&lt;/a&gt; to the excellent &lt;a href=&quot;https://github.com/shyiko/mysql-binlog-connector-java&quot;&gt;MySQL binlog client library&lt;/a&gt; that the connector uses internally to read the binlog events.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’d like to thank &lt;a href=&quot;https://github.com/shyiko&quot;&gt;Stanley Shyiko&lt;/a&gt; for guiding us and helping us debug the final problems with parsing JSON in the binlog, for accepting our proposed changes into his library, for releasing his library quickly when needed, and for being so responsive on this and other issues!&lt;/p&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/10/18/Debezium-0-3-3-Released/</id>
    <title>Debezium 0.3.3 Released</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2016-10-18T00:00:00+00:00</published>
    <link href="/blog/2016/10/18/Debezium-0-3-3-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.3.3 is now available for use with Kafka Connect 0.10.0.1. This release contains a handful of bug fixes and minor improvements for the MySQL connector, including better handling of BIT(n) values, ENUM and SET values, and GTID sets, This release also improves the log messages output by the MySQL connectors to better represent the ongoing activity when consuming the changes from the source database. See the release notes for specifics.
      
      
      We&#8217;ve also updated the Debezium Docker images labelled 0.3 and latest, which we use in our tutorial. We&#8217;ve also updated the tutorial to use the...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.3.3&lt;/strong&gt; is now available for use with Kafka Connect 0.10.0.1. This release contains a handful of bug fixes and minor improvements for the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt;, including better handling of &lt;code&gt;BIT(n)&lt;/code&gt; values, &lt;code&gt;ENUM&lt;/code&gt; and &lt;code&gt;SET&lt;/code&gt; values, and GTID sets, This release also improves the log messages output by the MySQL connectors to better represent the ongoing activity when consuming the changes from the source database. See the &lt;a href=&quot;http://debezium.io/docs/releases&quot;&gt;release notes&lt;/a&gt; for specifics.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;. We’ve also updated the tutorial to use the &lt;a href=&quot;https://docs.docker.com/engine/installation/&quot;&gt;latest Docker installations&lt;/a&gt; on Linux, Windows, and OS X.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Akshath, Chris, Randy, Prannoy, Umang, Horia, and others for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/09/26/Debezium-0-3-2-Released/</id>
    <title>Debezium 0.3.2 Released</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2016-09-26T00:00:00+00:00</published>
    <link href="/blog/2016/09/26/Debezium-0-3-2-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.3.2 is now available for use with Kafka Connect 0.10.0.1. This release contains a handful of bug fixes and minor improvements for the MySQL connector and MongoDB connector. The MySQL connector better handles BIT(n) values and zero-value date and timestamp values. This release also improves the log messages output by the MySQL and MongoDB connectors to better represent the ongoing activity when consuming the changes from the source database. See the release notes for specifics.
      
      
      We&#8217;ve also updated the Debezium Docker images labelled 0.3 and latest, which we use in our tutorial. We&#8217;ve also updated...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.3.2&lt;/strong&gt; is now available for use with Kafka Connect 0.10.0.1. This release contains a handful of bug fixes and minor improvements for the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt; and &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MongoDB connector&lt;/a&gt;. The MySQL connector better handles &lt;code&gt;BIT(n)&lt;/code&gt; values and &lt;a href=&quot;http://dev.mysql.com/doc/refman/5.7/en/date-and-time-types.html&quot;&gt;zero-value&lt;/a&gt; date and timestamp values. This release also improves the log messages output by the MySQL and MongoDB connectors to better represent the ongoing activity when consuming the changes from the source database. See the &lt;a href=&quot;http://debezium.io/docs/releases&quot;&gt;release notes&lt;/a&gt; for specifics.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;. We’ve also updated the tutorial to use the &lt;a href=&quot;https://docs.docker.com/engine/installation/&quot;&gt;latest Docker installations&lt;/a&gt; on Linux, Windows, and OS X.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Akshath, Colum, Emmanuel, Konstantin, Randy, RenZhu, Umang, and others for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/09/19/Serializing-Debezium-events-with-Avro/</id>
    <title>Serializing Debezium events with Avro</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2016-09-19T00:00:00+00:00</published>
    <link href="/blog/2016/09/19/Serializing-Debezium-events-with-Avro/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="kafka"></category>
    <category term="avro"></category>
    <category term="serialization"></category>
    <summary>
      
      
      
      Although Debezium makes it easy to capture database changes and record them in Kafka, one of the more important decisions you have to make is how those change events will be serialized in Kafka. Every message in Kafka has a key and a value, and to Kafka these are opaque byte arrays. But when you set up Kafka Connect, you have to say how the Debezium event keys and values should be serialized to a binary form, and your consumers will also have to deserialize them back into a usable form.
      
      
      Debezium event keys and values are both structured, so JSON...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Although Debezium makes it easy to capture database changes and record them in Kafka, one of the more important decisions you have to make is &lt;em&gt;how&lt;/em&gt; those change events will be serialized in Kafka. Every message in Kafka has a key and a value, and to Kafka these are opaque byte arrays. But when you set up Kafka Connect, you have to say how the Debezium event keys and values should be serialized to a binary form, and your consumers will also have to deserialize them back into a usable form.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium event keys and values are both structured, so JSON is certainly a reasonable option — it’s flexible, ubiquitous, and language agnostic, but on the other hand it’s quite verbose. One alternative is Avro, which is also flexible and language agnostic, but also faster and results in smaller binary representations. Using Avro requires a bit more setup effort on your part and some additional software, but the advantages are often worth it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_kafka_serializers_and_deserializers&quot;&gt;Kafka serializers and deserializers&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Before we get too far, let’s back up and review how Kafka producers and consumers normally do this serialization and deserialization. Because the keys and values are simple opaque byte arrays, you can use anything for your keys and values. For example, consider a case where we’re using simple whole numbers for the keys and strings for the values. Here, a producer of these messages would use a &lt;em&gt;long serializer&lt;/em&gt; to convert the &lt;code&gt;long&lt;/code&gt; keys to binary form and a &lt;em&gt;string serializer&lt;/em&gt; to convert the &lt;code&gt;String&lt;/code&gt; values to binary form. Meanwhile, the consumers use a &lt;em&gt;long deserializer&lt;/em&gt; to convert the binary keys into usable &lt;code&gt;long&lt;/code&gt; values, and a &lt;em&gt;string deserializer&lt;/em&gt; to convert the binary values back into &lt;code&gt;String&lt;/code&gt; objects.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In cases where the keys and/or values need to be a bit more structured, the producers and consumers can be written to use JSON structures for keys and/or values, and the Kafka-provided &lt;em&gt;JSON serializer and deserializer&lt;/em&gt; to do the conversion to and from binary form stored within the Kafka messages. As we said earlier, using JSON for keys and/or values is very flexible and language agnostic, but it is also produces keys and values that are relatively large since the fields and structure of the JSON values need to be encoded as well.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_avro_serialization&quot;&gt;Avro serialization&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://avro.apache.org/&quot;&gt;Avro&lt;/a&gt; is a data serialization mechanism that uses a &lt;em&gt;schema&lt;/em&gt; to define the structure of data. Avro relies upon this schema when writing the data to the binary format, and the schema allows it to encode the fields within the data in a much more compact form. Avro also relies upon the schema when &lt;em&gt;reading&lt;/em&gt; the data, too. But interestingly, Avro schemas are designed to evolve, so it is actually possible to use a slightly different schema for reading than what was used for writing. This feature makes Avro a great choice for Kafka serialization and deserialization.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://confluent.io&quot;&gt;Confluent&lt;/a&gt; provides a &lt;a href=&quot;http://docs.confluent.io/3.0.1/app-development.html&quot;&gt;Kafka serializer and deserializer that uses Avro&lt;/a&gt; and a separate &lt;a href=&quot;http://docs.confluent.io/3.0.1/schema-registry/docs/intro.html&quot;&gt;Schema Registry&lt;/a&gt;, and it works like this: when a numeric or string object are to be serialized, the &lt;em&gt;Avro serializer&lt;/em&gt; will determine the corresponding Avro Schema for the given type, register with the Schema Registry this schema and the topic its used on, get back the unique identifier for the schema, and then encode in the binary form the unique identifier of the schema and the encoded value. The next message is likely to have the same type and thus schema, so the serializer can quickly encode the schema identifier and value for this message without having to talk to the Schema Registry. Only when needing to serialize a schema it hasn’t already seen does the Avro serializer talk with the Schema Registry. So not only is this fast, but it also produces very compact binary forms and allows for the producer to &lt;em&gt;evolve&lt;/em&gt; its key and/or value schemas over time. The Schema Registry can also be configured to allow new versions of schemas to be registered only when they are &lt;em&gt;compatible&lt;/em&gt; with the Avro schema evolution rules, ensuring that producers do not produce messages that consumers will not be able to read.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Consumers, meanwhile, use the &lt;em&gt;Avro deserializer&lt;/em&gt;, which works in a similar manner, albeit backwards: when it reads the binary form of a key or value, it first looks for the schema identifier and, if it hasn’t seen it before asks the Schema Registry for the schema, and then uses that schema to decode the remainder of the binary representation into its object form. Again, if the deserializer has previously seen a particular schema identifier, it already has the schema needed to decode the data and doesn’t have to consult the Schema Registry.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_kafka_connect_converters&quot;&gt;Kafka Connect converters&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka Connect is a bit different than many Kafka producers/consumers, since the keys and values will often be structured. And rather than require connectors to work with JSON objects, Kafka Connect defines its own lightweight framework for defining data structures with a schema, making it much easier to write connectors to work with structured data. Kafka Connect defines its own &lt;em&gt;converters&lt;/em&gt; that are similar to Kafka (de)serializers, except that Kafka Connect’s converters know about these structures and schemas and can serialize the keys and values to binary form. Kafka Connect provides a &lt;em&gt;JSON converter&lt;/em&gt; that converts the structures into JSON and then uses the normal Kafka JSON serializer, so downstream consumers can just use the normal Kafka JSON deserializer and get a JSON representation of the Kafka Connect structs and schema. This is exactly what the &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;Debezium tutorial&lt;/a&gt; is using, and the &lt;code&gt;watch-topic&lt;/code&gt; consumer knows to use the JSON deserializer.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;One great feature of Kafka Connect is that the connectors simply provide the structured messages, and Kafka Connect takes care of serializing them using the configured converter. This means that you can use any Kafka Connect &lt;em&gt;converters&lt;/em&gt; with any Kafka Connect connector, including all of Debezium’s connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka Connect’s schema system was designed specifically with Avro in mind, so there is a one-to-one mapping between Kafka Connect schemas and Avro schemas. Confluent provides an &lt;em&gt;Avro Converter&lt;/em&gt; for Kafka Connect that serializes the Kafka Connect structs provided by the connectors into the compact Avro binary representation, again using the Schema Registry just like the Avro serializer. The consumer just uses the normal Avro deserializer as mentioned above.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Using Avro for serialization of Debezium events brings several significant advantages:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;The encoded binary forms of the Debezium events are &lt;em&gt;significantly&lt;/em&gt; smaller than the JSON representations. Not only is the structured data encoded in a more compact form, but the &lt;em&gt;schema&lt;/em&gt; associated with that structured data is represented in the binary form as a single integer.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Encoding the Debezium events into their Avro binary forms is fast. Only when the converter sees a new schema does it have to consult with the Schema Registry; otherwise, the schema has already been seen and its encoding logic already precomputed.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The Avro Converter for Kafka Connect produces messages with Avro-encoded keys and values that can be read by any Kafka consumers using the Avro deserializer.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Debezium event structures are based upon the structure of the table from which the changes were captured. When the structure of the source table changes (e.g., because an &lt;code&gt;ALTER&lt;/code&gt; statement was applied to it), the structure and schema of the events will also change. If this is done in a manner such that the new Avro schema is &lt;em&gt;compatible with&lt;/em&gt; the older Avro schema, then consumers will be able to process the events without disruption, even though the event structures evolve over time.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Avro’s schema mechanism is far more formal and rigorous than the free-form JSON structure, and the changes in the schemas are clearly identified when comparing any two messages.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The Avro converter, Avro (de)serializers, and Schema Registry are all open source.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It is true that using the Avro converter and deserializer requires a running Schema Registry, and that the registry becomes an integral part of your streaming infrastructure. However, this is a small price to pay for the benefits listed above.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_using_the_avro_converter_with_debezium&quot;&gt;Using the Avro Converter with Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As mentioned above, in the interest of keeping the Debezium tutorial as simple as possible, we avoid using the Schema Registry or the Avro converter in the tutorial. We also don’t (yet) include the Avro converter in our Docker images, though that &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-59&quot;&gt;will change soon&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Nevertheless, it is absolutely possible to use the Avro Converter with the Debezium connectors when you are installing the connectors into either the Confluent Platform or into your own installation of Kafka Connect. Simply configure the &lt;a href=&quot;http://docs.confluent.io/3.0.1/connect/userguide.html&quot;&gt;Kafka Connect workers&lt;/a&gt; to use the Avro converter for the keys and values:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;key.converter=io.confluent.connect.avro.AvroConverter
      value.converter=io.confluent.connect.avro.AvroConverter&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;And, if you want to use the Avro Converter for Kafka Connect internal messages, then set these as well:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;internal.key.converter=io.confluent.connect.avro.AvroConverter
      internal.value.converter=io.confluent.connect.avro.AvroConverter&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once again, there is no need to configure the Debezium connectors any differently.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/08/30/Debezium-0-3-1-Released/</id>
    <title>Debezium 0.3.1 Released</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2016-08-30T00:00:00+00:00</published>
    <link href="/blog/2016/08/30/Debezium-0-3-1-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.3.1 is now available for use with Kafka Connect 0.10.0.1. This release contains an updated MySQL connector with a handful of bug fixes and two significant but backward-compatible changes. First, the MySQL connector now supports using secure connections to MySQL, adding to the existing ability to connect securely to Kafka. Second, the MySQL connector is able to capture MySQL string values using the proper character sets so that any values stored in the database can be captured correctly in events. See our release notes for details of these changes and for upgrading recommendations.
      
      
      We&#8217;ve also...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.3.1&lt;/strong&gt; is now available for use with Kafka Connect 0.10.0.1. This release contains an updated &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt; with a handful of bug fixes and two significant but backward-compatible changes. First, the MySQL connector now supports using secure connections to MySQL, adding to the existing ability to connect securely to Kafka. Second, the MySQL connector is able to capture MySQL string values using the proper character sets so that any values stored in the database can be captured correctly in events. See our &lt;a href=&quot;http://debezium.io/docs/releases#release-0-3-1&quot;&gt;release notes&lt;/a&gt; for details of these changes and for upgrading recommendations.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Chris, Akshath, barten, and and others for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/08/16/Debezium-0-3-0-Released/</id>
    <title>Debezium 0.3.0 Released</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2016-08-16T00:00:00+00:00</published>
    <link href="/blog/2016/08/16/Debezium-0-3-0-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      After a few weeks delay, Debezium 0.3.0 is now available for use with Kafka Connect 0.10.0.1. This release contains an updated MySQL connector with quite a few bug fixes, and a new MongoDB connector that captures the changes made to a MongoDB replica set or MongoDB sharded cluster. See the documentation for details about how to configure these connectors and how they work.
      
      
      We&#8217;ve also updated the Debezium Docker images (with labels 0.3 and latest) used in our tutorial.
      
      
      Thanks to Andrew, Bhupinder, Chris, David, Horia, Konstantin, Tony, and others for their help with the release, issues, discussions, contributions, and questions!
      
      
      
      
      About Debezium
      
      
      Debezium...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;After a few weeks delay, &lt;strong&gt;Debezium 0.3.0 is now available&lt;/strong&gt; for use with Kafka Connect 0.10.0.1. This release contains an updated &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt; with quite a few bug fixes, and a new &lt;strong&gt;&lt;em&gt;&lt;a href=&quot;http://debezium.io/docs/connectors/mongodb&quot;&gt;MongoDB connector&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt; that captures the changes made to a MongoDB replica set or MongoDB sharded cluster. See the &lt;a href=&quot;http://debezium.io/docs/connectors&quot;&gt;documentation&lt;/a&gt; for details about how to configure these connectors and how they work.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; (with labels &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;) used in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Andrew, Bhupinder, Chris, David, Horia, Konstantin, Tony, and others for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/08/16/Debezium-0-2-4-Released/</id>
    <title>Debezium 0.2.4 Released</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2016-08-16T00:00:00+00:00</published>
    <link href="/blog/2016/08/16/Debezium-0-2-4-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m happy to announce that Debezium 0.2.4 is now available for use with Kafka Connect 0.9.0.1. This release adds more verbose logging during MySQL snapshots, enables taking snapshots of very large MySQL databases, and correct a potential exception during graceful shutdown. See our release notes for details of these changes and for upgrading recommendations.
      
      
      We&#8217;ve also updated the Debezium Docker images (with label 0.2 and latest) used in our tutorial.
      
      
      Thanks to David and wangshao for their help with the release, issues, discussions, contributions, and questions!
      Stay tuned for our next release, which will be 0.3 and will have a new MongoDB connector...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m happy to announce that &lt;strong&gt;Debezium 0.2.4 is now available&lt;/strong&gt; for use with Kafka Connect 0.9.0.1. This release adds more verbose logging during MySQL snapshots, enables taking snapshots of very large MySQL databases, and correct a potential exception during graceful shutdown. See our &lt;a href=&quot;http://debezium.io/docs/releases#release-0-2-4&quot;&gt;release notes&lt;/a&gt; for details of these changes and for upgrading recommendations.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; (with label &lt;code&gt;0.2&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;) used in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to David and wangshao for their help with the release, issues, discussions, contributions, and questions!
      Stay tuned for our next release, which will be 0.3 and will have a new MongoDB connector and will support Kafka Connect 0.10.0.1.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/08/02/capturing-changes-from-mysql/</id>
    <title>Capturing changes from MySQL</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2016-08-02T00:00:00+00:00</published>
    <link href="/blog/2016/08/02/capturing-changes-from-mysql/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="mysql"></category>
    <summary>
      
      
      
      Change data capture is a hot topic. Debezium&#8217;s goal is to make change data capture easy for multiple DBMSes, but admittedly we&#8217;re still a young open source project and so far we&#8217;ve only released a connector for MySQL with a connector for MongoDB that&#8217;s just around the corner. So it&#8217;s great to see how others are using and implementing change data capture. In this post, we&#8217;ll review Yelp&#8217;s approach and see how it is strikingly similar to Debezium&#8217;s MySQL connector.
      
      
      
      
      Streaming data at Yelp
      
      
      The Yelp Engineering Blog recently began a series describing their real-time streaming data infrastructure. The first post provides...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Change data capture is a hot topic. Debezium’s goal is to make change data capture easy for multiple DBMSes, but admittedly we’re still a young open source project and so far we’ve only released a &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;connector for MySQL&lt;/a&gt; with a &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb&quot;&gt;connector for MongoDB&lt;/a&gt; that’s just around the corner. So it’s great to see how others are using and implementing change data capture. In this post, we’ll review Yelp’s approach and see how it is strikingly similar to Debezium’s MySQL connector.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_streaming_data_at_yelp&quot;&gt;Streaming data at Yelp&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;a href=&quot;http://engineeringblog.yelp.com/&quot;&gt;Yelp Engineering Blog&lt;/a&gt; recently began a series describing their real-time streaming data infrastructure. The &lt;a href=&quot;http://engineeringblog.yelp.com/2016/07/billions-of-messages-a-day-yelps-real-time-data-pipeline.html&quot;&gt;first post&lt;/a&gt; provides a good introduction and explains how moving from a monolith to a service-oriented architecture increased productivity, but also made it more challenging to work with data spread across the 100 services that own it. It’s totally worth your time to read it right now.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As Justin writes in the post, several reasons prompted them to create their own real time streaming data pipeline:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Ensuring data always remains consistent across services is always a difficult task, but especially so when things can and do go wrong. Transactions across services may be useful in some situations, but they’re not straightforward, are expensive, and can lead to request amplification where one service calls another, which coordinates with two others, etc.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Services that update data in multiple backend services suffer from the &lt;a href=&quot;http://www.confluent.io/blog/using-logs-to-build-a-solid-data-infrastructure-or-why-dual-writes-are-a-bad-idea/&quot;&gt;dual write problem&lt;/a&gt;, which is where a failure occurs after one backing service was updated but before the other could be updated and that always results in data inconsistencies that are difficult to track down and correct.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Combining and integrating data spread across multiple services can also be difficult and expensive, but it is even harder when that data is continously changing. One approach is to use bulk APIs, but these can beprohibitive to create, can result in inconsistencies, and pose real scalability problems when services need to continually receive the never-ending updates to data.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Yelp’s Real-Time Data Pipeline records changes to data on totally ordered distributed logs so that downstream consumers can receive and process the same changes in exactly the same order. Services can consume changes made by other services, and can therefore stay in sync without explicit interservice communication. This system uses among other things Kafka for event logs, a homegrown system named &lt;a href=&quot;http://engineeringblog.yelp.com/2016/08/streaming-mysql-tables-in-real-time-to-kafka.html&quot;&gt;MySQLStreamer&lt;/a&gt; to capture committed changes to MySQL tables, &lt;a href=&quot;http://avro.apache.org&quot;&gt;Avro&lt;/a&gt; for message format and schemas, and a custom &lt;a href=&quot;http://engineeringblog.yelp.com/2016/07/billions-of-messages-a-day-yelps-real-time-data-pipeline.html#yelps-real-time-data-pipeline&quot;&gt;Schematizer&lt;/a&gt; service that tracks consumers and enforces the Avro schemas used for messages on every Kafka topic.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_how_yelp_captures_mysql_changes&quot;&gt;How Yelp captures MySQL changes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Perhaps most interesting for Debezium is how Yelp captures the committed changes in their MySQL databases and write them to Kafka topics. Their &lt;a href=&quot;http://engineeringblog.yelp.com/2016/08/streaming-mysql-tables-in-real-time-to-kafka.html&quot;&gt;second post in the series&lt;/a&gt; goes into a lot more detail about their MySQLStreamer process that reads the MySQL binary log and continously processes the DDL statements and DML operations that appear in the log, generating the corresponding &lt;em&gt;insert&lt;/em&gt;, &lt;em&gt;update&lt;/em&gt;, &lt;em&gt;delete&lt;/em&gt;, and &lt;em&gt;refresh&lt;/em&gt; events, and writing these event messages to a separate Kafka topic for each MySQL table. We’ve &lt;a href=&quot;http://debezium.io/blog/2016-04-15-parsing-ddl&quot;&gt;mentioned before&lt;/a&gt; that MySQL’s row-level binlog events that result from the DML operation don’t include the full definition of the columns, so knowing what the columns mean in each event requires process the DDL statements that also appear in the binlog. Yelp uses a separate MySQL instance it calls the &lt;em&gt;schema tracker database&lt;/em&gt;, which behaves like a MySQL slave to which are applied only the DDL statements they read from the binlog. This technique lets Yelp’s MySQLStreamer system know the state of the database schema and the structure of its tables at the point in the binlog where they are processing events. This is pretty interesting, because it uses the MySQL engine to handle the DDL parsing.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Yelp’s MySQLStreamer process uses another MySQL database to track internal state describing its position in the binlog, what events have been successfully published to Kafka, and, because the binlog position varies on each replica, replica-independent information about each transaction. This latter information is similar to MySQL GTIDs, although Yelp is using earlier versions of MySQL that do not support GTIDs.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Of course, special consideration has to be taken for databases that have been around for a long time. The MySQL binlogs are capped and will not contain the &lt;em&gt;entire&lt;/em&gt; history of the databases, so Yelp’s MySQLStreamer process bootstraps the change data capture process of old databases by starting another clean MySQL replica, which will use the built-in MySQL replication mechanism with the &lt;a href=&quot;http://dev.mysql.com/doc/refman/5.7/en/blackhole-storage-engine.html&quot;&gt;MySQL blackhole database engine&lt;/a&gt; to obtain a consistent snapshot of the master and so that all activity is logged in the replica’s binlog while the replica actually stores no data.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Yelp’s MySQLStreamer mechanism is quite ingenious in its use of MySQL and multiple extra databases to capture changes from MySQL databases and write them to Kafka topics. The downside, of course, is that doing so does increase the operational complexity of the system.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_similar_purpose_similar_approach&quot;&gt;Similar purpose, similar approach&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source project that is building a change data capture for a variety of DBMSes. Like Yelp’s MySQLStreamer, Debezium’s &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL Connector&lt;/a&gt; can continously capture the committed changes to MySQL database rows and record these events in a separate Kafka topic for each table. When first started, Debezium’s MySQL Connector can perform an initial consistent snapshot and then begin reading the MySQL binlog. It uses both DDL and DML operations that appear in the binlog, directly &lt;a href=&quot;http://debezium.io/blog/2016-04-15-parsing-ddl&quot;&gt;parsing and using the DDL statements&lt;/a&gt; to learn the changes to each table’s structure and the mapping of each insert, update, and delete binlog event. And each resulting change event written to Kafka includes information about the originating MySQL server and its binlog position, as well as the before and/or after states of the affected row.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;However, unlike Yelp’s MySQLStreamer, the Debezium MySQL connector doesn’t need or use extra MySQL databases to parse DDL or to store the connector’s state. Instead, Debezium is built on top of Kafka Connect, which is a new Kafka library that provides much of the generic functionality of reliably pulling data from external systems, pushing it into Kafka topics, and tracking what data has already been processed. Kafka Connect stores this state inside Kafka itself, simplifying the operational footprint. Debezium’s MySQL connector can then focus on the details of performing a consistent snapshot when required, reading the binlog, and converting the binlog events into useful change events.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Yelp’s real time data pipeline makes use of a custom Avro schema registry, and uses those Avro schemas to encode each event into a compact binary representation while keeping the metadata about the structure of the event. It’s possible to do this with Debezium, too: simply run &lt;a href=&quot;http://docs.confluent.io/3.0.0/schema-registry/docs/index.html&quot;&gt;Confluent’s Schema Registry&lt;/a&gt; as a service and then configure the Kafka Connect worker to use the &lt;a href=&quot;http://debezium.io/docs/faq#avro-converter&quot;&gt;Avro Converter&lt;/a&gt;. As the converter serializes each event, it looks at the structure defined by the connector and, when that structure changes, generates an updated Avro Schema and registers it with the Schema Registry. That new Avro schema is then used to encode the event (and others with an identical structure) into a compact binary form written to Kafka. And of course, consumers then also use the same Avro converter so that as events are deserialized, the converter coordinates with the Schema Registry whenever it needs an Avro schema it doesn’t know about. As a result, the events are stored in a compact manner while the events' content and metadata remain available, while Schema Registry captures and maintains the history of the Avro schema for each table as it evolves over time.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_capturing_changes_from_mysql_with_debezium&quot;&gt;Capturing changes from MySQL with Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’re interested in change data capture with MySQL (or any other DBMSes), give Debezium a try by going through &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;our tutorial&lt;/a&gt; that walks you through starting Kafka, Kafka Connect, and Debezium’s MySQL Connector to see exactly what change data events look like and how they can be used. Best of all, it’s open source with a growing community of developers that has had the benefit of building on top of recently-created Kafka Connect framework. Our MySQL connector is ready now, but we’re working on &lt;a href=&quot;http://debezium.io/docs/connectors/&quot;&gt;connectors for other DBMSes&lt;/a&gt;. Specifically, our upcoming 0.3 release will include our &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb&quot;&gt;MongoDB Connector&lt;/a&gt;, with 0.4 including connectors for PostgreSQL and/or Oracle.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;em&gt;Correction: A previous version of this post incorrectly stated that Yelp was using a MySQL version that did support GTIDs, when in fact they are using a version that does &lt;strong&gt;not&lt;/strong&gt; support MySQL GTIDs. The post has been corrected, and the author regrets the mistake.&lt;/em&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/07/26/Debezium-0-2-3-Released/</id>
    <title>Debezium 0.2.3 Released</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2016-07-26T00:00:00+00:00</published>
    <link href="/blog/2016/07/26/Debezium-0-2-3-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m happy to announce that Debezium 0.2.3 is now available for use with Kafka Connect 0.9.0.1. This release corrects the MySQL connector behavior when working with TINYINT and SMALLINT columns or with TIME, DATE, and TIMESTAMP columns. See our release notes for details of these changes and for upgrading recommendations.
      
      
      We&#8217;ve also updated the Debezium Docker images (with label 0.2 and latest) used in our tutorial.
      
      
      Thanks to Chris, Christian, Laogang, and Tony for their help with the release, issues, discussions, contributions, and questions!
      Stay tuned for our next release, which will be 0.3 and will have a new MongoDB connector and will...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m happy to announce that &lt;strong&gt;Debezium 0.2.3 is now available&lt;/strong&gt; for use with Kafka Connect 0.9.0.1. This release corrects the MySQL connector behavior when working with &lt;code&gt;TINYINT&lt;/code&gt; and &lt;code&gt;SMALLINT&lt;/code&gt; columns or with &lt;code&gt;TIME&lt;/code&gt;, &lt;code&gt;DATE&lt;/code&gt;, and &lt;code&gt;TIMESTAMP&lt;/code&gt; columns. See our &lt;a href=&quot;http://debezium.io/docs/releases#release-0-2-3&quot;&gt;release notes&lt;/a&gt; for details of these changes and for upgrading recommendations.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; (with label &lt;code&gt;0.2&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;) used in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Chris, Christian, Laogang, and Tony for their help with the release, issues, discussions, contributions, and questions!
      Stay tuned for our next release, which will be 0.3 and will have a new MongoDB connector and will support Kafka Connect 0.10.0.0.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/06/22/Debezium-0-2-2-Released/</id>
    <title>Debezium 0.2.2 Released</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2016-06-22T00:00:00+00:00</published>
    <link href="/blog/2016/06/22/Debezium-0-2-2-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m happy to announce that Debezium 0.2.2 is now available. This release fixes several bugs in the MySQL connector that can produce change events with incorrect source metadata, and that eliminates the possibility a poorly-timed connector crash causing the connector to only process some of the rows in a multi-row MySQL event. See our release notes for details of these changes and for upgrading recommendations.
      
      
      Also, thanks to a community member for reporting that Debezium 0.2.x can only be used with Kafka Connect 0.9.0.1. Debezium 0.2.x cannot be used with Kafka Connect 0.10.0.0 because of its backward incompatible changes to the...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m happy to announce that &lt;strong&gt;Debezium 0.2.2 is now available&lt;/strong&gt;. This release fixes several bugs in the MySQL connector that can produce change events with incorrect &lt;code&gt;source&lt;/code&gt; metadata, and that eliminates the possibility a poorly-timed connector crash causing the connector to only process some of the rows in a multi-row MySQL event. See our &lt;a href=&quot;http://debezium.io/docs/releases#release-0-2-2&quot;&gt;release notes&lt;/a&gt; for details of these changes and for upgrading recommendations.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also, thanks to a community member for &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-80&quot;&gt;reporting&lt;/a&gt; that Debezium 0.2.x can only be used with Kafka Connect 0.9.0.1. Debezium 0.2.x cannot be used with Kafka Connect 0.10.0.0 because of its &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3006&quot;&gt;backward incompatible changes to the consumer API&lt;/a&gt;. Our next release of Debezium will support Kafka 0.10.x.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; (with label &lt;code&gt;0.2&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;) used in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;. And stay tuned, because we’re hoping to add a MongoDB connector in our next release.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Chris, Christian, Konstantin, James, and Bhupinder for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/06/10/Debezium-0/</id>
    <title>Debezium 0.2.1 Released</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2016-06-10T00:00:00+00:00</published>
    <link href="/blog/2016/06/10/Debezium-0/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m happy to announce that Debezium 0.2.1 is now available. The MySQL connector has been significantly improved and is now able to monitor and produce change events for HA MySQL clusters using GTIDs, perform a consistent snapshot when starting up the first time, and has a completely redesigned event message structure that provides a ton more information with every event. Our change log has all the details about bugs, enhancements, new features, and backward compatibility notices. We&#8217;ve also updated our tutorial.
      
      
      
      
      
      Note
      
      
      
      What happened to 0.2.0? Well, we released it to Maven Central before we&#8217;d noticed a few problems that we thought...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m happy to announce that &lt;strong&gt;Debezium 0.2.1 is now available&lt;/strong&gt;. The &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt; has been significantly improved and is now able to monitor and produce change events for &lt;a href=&quot;http://debezium.io/docs/connectors/mysql#ha-mysql-clusters#enabling-gtids&quot;&gt;HA MySQL clusters&lt;/a&gt; using &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;GTIDs&lt;/a&gt;, perform a &lt;a href=&quot;http://debezium.io/docs/connectors/mysql#snapshots&quot;&gt;consistent snapshot&lt;/a&gt; when starting up the first time, and has a completely redesigned &lt;a href=&quot;http://debezium.io/docs/connectors/mysql#events&quot;&gt;event message structure&lt;/a&gt; that provides a ton more information with every event. Our &lt;a href=&quot;http://debezium.io/docs/releases&quot;&gt;change log&lt;/a&gt; has all the details about bugs, enhancements, new features, and backward compatibility notices. We’ve also updated our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;admonitionblock note&quot;&gt;
      &lt;table&gt;
      &lt;tr&gt;
      &lt;td class=&quot;icon&quot;&gt;
      &lt;div class=&quot;title&quot;&gt;Note&lt;/div&gt;
      &lt;/td&gt;
      &lt;td class=&quot;content&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;What happened to 0.2.0? Well, we released it to Maven Central before we’d noticed a &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-71&quot;&gt;few&lt;/a&gt; &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-72&quot;&gt;problems&lt;/a&gt; that we thought it best to fix right away. Thus 0.2.1 was born.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/td&gt;
      &lt;/tr&gt;
      &lt;/table&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_installing_the_mysql_connector&quot;&gt;Installing the MySQL connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’ve already installed &lt;a href=&quot;https://zookeeper.apache.org&quot;&gt;Zookeeper&lt;/a&gt;, &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt;, and &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt;, then using Debezium’s MySQL connector is easy. Simply download the &lt;a href=&quot;https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/0.2.1/debezium-connector-mysql-0.2.1-plugin.tar.gz&quot;&gt;connector’s plugin archive&lt;/a&gt;, extract the JARs into your Kafka Connect environment, and add the directory with the JARs to &lt;a href=&quot;http://docs.confluent.io/3.0.0/connect/userguide.html#installing-connector-plugins&quot;&gt;Kafka Connect’s classpath&lt;/a&gt;. Restart your Kafka Connect process to pick up the new JARs.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If immutable containers are your thing, then check out &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium’s Docker images&lt;/a&gt; for Zookeeper, Kafka, and Kafka Connect with the MySQL connector already pre-installed and ready to go. Our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt; even walks you through using these images, and this is a great way to learn what Debezium is all about. You can even &lt;a href=&quot;http://debezium.io/blog/2016/05/31/Debezium-on-Kubernetes&quot;&gt;run Debezium on Kubernetes and OpenShift&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_using_the_mysql_connector&quot;&gt;Using the MySQL connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To use the connector to produce change events for a particular MySQL server or cluster, simply create a &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/#configuration&quot;&gt;configuration file for the MySQL Connector&lt;/a&gt; and use the &lt;a href=&quot;http://docs.confluent.io/3.0.0/connect/userguide.html#rest-interface&quot;&gt;Kafka Connect REST API&lt;/a&gt; to add that connector to your Kafka Connect cluster. When the connector starts, it will grab a consistent snapshot of the databases in your MySQL server and start reading the MySQL binlog, producing events for every inserted, updated, and deleted row. The connector can optionally produce events with the DDL statements that were applied, and you can even choose to produce events for a subset of the databases and tables. Optionally ignore, mask, or truncate columns that are sensitive, too large, or not needed. See the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL connector’s documentation&lt;/a&gt; for all the details.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_using_the_libraries&quot;&gt;Using the libraries&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Although Debezium is really intended to be used as turnkey services, all of Debezium’s JARs and other artifacts are available in &lt;a href=&quot;http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22io.debezium%22&quot;&gt;Maven Central&lt;/a&gt;. You might want to use our &lt;a href=&quot;http://debezium.io/blog/2016/04/15/parsing-ddl/&quot;&gt;MySQL DDL parser&lt;/a&gt; from our MySQL connector library to parse those DDL statments in your consumers.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We do provide a small library so applications can &lt;a href=&quot;http://debezium.io/docs/embedded&quot;&gt;embed any Kafka Connect connector&lt;/a&gt; and consume data change events read directly from the source system. This provides a much lighter weight system (since Zookeeper, Kafka, and Kafka Connect services are not needed), but as a consequence is not as fault tolerant or reliable since the application must manage and maintain all state normally kept inside Kafka’s distributed and replicated logs. It’s perfect for use in tests, and with careful consideration it may be useful in some applications.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_about_debezium&quot;&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_get_involved&quot;&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;. And stay tuned, because we’re hoping to add a MongoDB connector in our next release.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Emmanuel, Chris, Christian, Konstantin, David, Akshath, James, Ewen, Cheng, and Paul for their help with the release, discussions, design assistance, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/05/31/Debezium-on-Kubernetes/</id>
    <title>Debezium on Kubernetes</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2016-05-31T00:00:00+00:00</published>
    <link href="/blog/2016/05/31/Debezium-on-Kubernetes/" rel="alternate" type="text/html" />
    <author>
      <name>Christian Posta</name>
    </author>
    <category term="mysql"></category>
    <category term="sql"></category>
    <category term="kubernetes"></category>
    <category term="docker"></category>
    <category term="kafka"></category>
    <summary>
      
      
      
      Our Debezium Tutorial walks you step by step through using Debezium by installing, starting, and linking together all of the Docker containers running on a single host machine. Of course, you can use things like Docker Compose or your own scripts to make this easier, although that would just automating running all the containers on a single machine. What you really want is to run the containers on a cluster of machines. In this blog, we&#8217;ll run Debezium using a container cluster manager from Red Hat and Google called Kubernetes.
      
      
      Kubernetes is a container (Docker/Rocket/Hyper.sh) cluster management tool. Like many other...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;Debezium Tutorial&lt;/a&gt; walks you step by step through using Debezium by installing, starting, and linking together all of the Docker containers running on a single host machine. Of course, you can use things like Docker Compose or your own scripts to make this easier, although that would just automating running all the containers on a single machine. What you really want is to run the containers on a cluster of machines. In this blog, we’ll run Debezium using a container cluster manager from Red Hat and Google called &lt;a href=&quot;http://kubernetes.io&quot;&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://kubernetes.io&quot;&gt;Kubernetes&lt;/a&gt; is a container (Docker/Rocket/Hyper.sh) cluster management tool. Like many other popular cluster management and compute resource scheduling platforms, Kubernetes' roots are in Google, who is no stranger to running containers at scale. They start, stop, and cluster &lt;a href=&quot;https://cloudplatform.googleblog.com/2015/01/in-coming-weeks-we-will-be-publishing.html&quot;&gt;2 billion containers per week&lt;/a&gt; and they contributed a lot of the Linux kernel underpinnings that make containers possible. &lt;a href=&quot;http://research.google.com/pubs/pub43438.html&quot;&gt;One of their famous papers&lt;/a&gt; talks about an internal cluster manager named Borg. With Kubernetes, Google got tired of everyone implementing their papers in Java so they decided to implement this one themselves :)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kubernetes is written in Go-lang and is quickly becoming the de-facto API for scheduling, managing, and clustering containers at scale. This blog isn’t intended to be a primer on Kubernetes, so we recommend heading over to the &lt;a href=&quot;http://kubernetes.io/docs/getting-started-guides/&quot;&gt;Getting Started&lt;/a&gt; docs to learn more about Kubernetes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_getting_started&quot;&gt;Getting started&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To get started, we need to have access to a Kubernetes cluster. Getting one started is pretty easy: just follow the &lt;a href=&quot;http://kubernetes.io/docs/getting-started-guides/&quot;&gt;getting started&lt;/a&gt; guides. A favorite of ours is &lt;a href=&quot;https://blog.openshift.com/one-vagrant-image-openshift-origin-v3/&quot;&gt;OpenShift’s all in one VM&lt;/a&gt; or the &lt;a href=&quot;http://developers.redhat.com/products/cdk/overview/&quot;&gt;Red Hat Container Development Kit&lt;/a&gt; which provide a hardened, production-ready distribution of Kubernetes. Once you’ve installed it and logged in, you should be able to run &lt;code&gt;kubectl get pod&lt;/code&gt; to get a list of Kubernetes pods you may have running. You don’t need anything running else inside Kubernetes to get started.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To get and build the Kubernetes manifest files (yaml descriptors), go clone the &lt;a href=&quot;https://github.com/debezium/debezium-kubernetes&quot;&gt;Debezium Kubernetes&lt;/a&gt; repo and run the following command:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ mvn clean
      $ mvn install&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This project uses the awesome &lt;a href=&quot;http://fabric8.io/guide/mavenPlugin.html&quot;&gt;Fabric8 Maven plugin&lt;/a&gt; to automatically generate the Kubernetes manifest files. Here’s an example of what gets generated in &lt;code&gt;$PROJECT_ROOT/zk-standalone/target/classes/kubernetes.yml&lt;/code&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;nowrap&quot;&gt;apiVersion: &quot;v1&quot;
      items:
      - apiVersion: &quot;v1&quot;
        kind: &quot;Service&quot;
        metadata:
          annotations: {}
          labels:
            project: &quot;zookeeper&quot;
            provider: &quot;debezium&quot;
            version: &quot;0.1-SNAPSHOT&quot;
            group: &quot;io.debezium&quot;
          name: &quot;zookeeper&quot;
        spec:
          deprecatedPublicIPs: []
          externalIPs: []
          ports:
          - port: 2181
            protocol: &quot;TCP&quot;
            targetPort: 2181
          selector:
            project: &quot;zookeeper&quot;
            provider: &quot;debezium&quot;
            group: &quot;io.debezium&quot;
      - apiVersion: &quot;v1&quot;
        kind: &quot;ReplicationController&quot;
        metadata:
          annotations:
            fabric8.io/git-branch: &quot;master&quot;
            fabric8.io/git-commit: &quot;004e222462749fbaf12c3ee33edca9b077ee9003&quot;
          labels:
            project: &quot;zookeeper&quot;
            provider: &quot;debezium&quot;
            version: &quot;0.1-SNAPSHOT&quot;
            group: &quot;io.debezium&quot;
          name: &quot;zk-standalone&quot;
        spec:
          replicas: 1
          selector:
            project: &quot;zookeeper&quot;
            provider: &quot;debezium&quot;
            version: &quot;0.1-SNAPSHOT&quot;
            group: &quot;io.debezium&quot;
          template:
            metadata:
              annotations: {}
              labels:
                project: &quot;zookeeper&quot;
                provider: &quot;debezium&quot;
                version: &quot;0.1-SNAPSHOT&quot;
                group: &quot;io.debezium&quot;
            spec:
              containers:
              - args: []
                command: []
                env:
                - name: &quot;KUBERNETES_NAMESPACE&quot;
                  valueFrom:
                    fieldRef:
                      fieldPath: &quot;metadata.namespace&quot;
                image: &quot;docker.io/debezium/zookeeper:0.1&quot;
                imagePullPolicy: &quot;IfNotPresent&quot;
                name: &quot;zk-standalone&quot;
                ports:
                - containerPort: 3888
                  name: &quot;election&quot;
                - containerPort: 2888
                  name: &quot;peer&quot;
                - containerPort: 2181
                  name: &quot;client&quot;
                securityContext: {}
                volumeMounts: []
              imagePullSecrets: []
              nodeSelector: {}
              volumes: []
      kind: &quot;List&quot;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_starting_zookeeper_and_kafka_on_kubernetes&quot;&gt;Starting Zookeeper and Kafka on Kubernetes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To start &lt;a href=&quot;http://zookeeper.apache.org&quot;&gt;Apache Zookeeper&lt;/a&gt; or &lt;a href=&quot;http://kafka.apache.org&quot;&gt;Apache Kafka&lt;/a&gt; inside Kubernetes you have two options. If you have the &lt;code&gt;kubectl&lt;/code&gt; command line (or the &lt;code&gt;oc&lt;/code&gt; tool from the OpenShift client distros) on your machine you can apply any of the newly generated Kubernetes manifest files like this:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl create -f &amp;lt;path_to_file&amp;gt;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Or you can use the Fabric8 Maven plugin and its &lt;code&gt;fabric8:apply&lt;/code&gt; goal to apply the manifest files. Note for either of these two options to work, you must be currently logged into your Kubernetes cluster. (Also, OpenShift’s &lt;code&gt;oc login &amp;lt;url&amp;gt;&lt;/code&gt; makes this super easy, or see &lt;a href=&quot;http://blog.christianposta.com/kubernetes/logging-into-a-kubernetes-cluster-with-kubectl/&quot;&gt;Logging into a Kubernetes Cluster with kubectl&lt;/a&gt; for more information.)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;First, let’s deploy Zookeeper to our Kubernetes cluster. We need to be in &lt;code&gt;$PROJECT_ROOT/zk-standalone&lt;/code&gt; directory, and then we’ll apply our Kubernetes configuration.  First, let’s see how to do this with the &lt;code&gt;kubectl&lt;/code&gt; command:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd zk-standalone
      $ kubectl create -f target/classes/kubernetes.yml
      
      service &quot;zookeeper&quot; created
      replicationcontroller &quot;zk-standalone&quot; created&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You can do the same thing with Maven and the fabric8 maven plugin:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd zk-standalone
      $ mvn fabric8:apply
      
      Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=1512m; support was removed in 8.0
      [INFO] Scanning for projects...
      [INFO]
      [INFO] ------------------------------------------------------------------------
      [INFO] Building zk-standalone 0.1-SNAPSHOT
      [INFO] ------------------------------------------------------------------------
      [INFO]
      [INFO] --- fabric8-maven-plugin:2.2.115:apply (default-cli) @ zk-standalone ---
      [INFO] Using kubernetes at: https://172.28.128.4:8443/ in namespace ticket
      [INFO] Kubernetes JSON: /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/zk-standalone/target/classes/kubernetes.json
      [INFO] OpenShift platform detected
      [INFO] Using namespace: ticket
      [INFO] Looking at repo with directory /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/.git
      [INFO] Creating a Service from kubernetes.json namespace ticket name zookeeper
      [INFO] Created Service: zk-standalone/target/fabric8/applyJson/ticket/service-zookeeper.json
      [INFO] Creating a ReplicationController from kubernetes.json namespace ticket name zk-standalone
      [INFO] Created ReplicationController: zk-standalone/target/fabric8/applyJson/ticket/replicationcontroller-zk-standalone.json
      [INFO] ------------------------------------------------------------------------
      [INFO] BUILD SUCCESS
      [INFO] ------------------------------------------------------------------------
      [INFO] Total time: 2.661 s
      [INFO] Finished at: 2016-05-19T15:59:26-07:00
      [INFO] Final Memory: 26M/260M
      [INFO] ------------------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Zookeeper is deployed, so let’s continue with deploying Kafka. Navigate to &lt;code&gt;$PROJECT_ROOT/kafka&lt;/code&gt;, and then apply the Kafka deployment configuration:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd ../kafka
      $ kubectl create -f target/classes/kubernetes.yml
      
      service &quot;kafka&quot; created
      replicationcontroller &quot;kafka&quot; created&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Or with fabric8 maven plugin:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd ../kafka
      $ mvn fabric8:apply
      
      Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=1512m; support was removed in 8.0
      [INFO] Scanning for projects...
      [INFO]
      [INFO] ------------------------------------------------------------------------
      [INFO] Building kafka 0.1-SNAPSHOT
      [INFO] ------------------------------------------------------------------------
      [INFO]
      [INFO] --- fabric8-maven-plugin:2.2.115:apply (default-cli) @ kafka ---
      [INFO] Using kubernetes at: https://172.28.128.4:8443/ in namespace ticket
      [INFO] Kubernetes JSON: /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/kafka/target/classes/kubernetes.json
      [INFO] OpenShift platform detected
      [INFO] Using namespace: ticket
      [INFO] Looking at repo with directory /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/.git
      [INFO] Creating a Service from kubernetes.json namespace ticket name kafka
      [INFO] Created Service: kafka/target/fabric8/applyJson/ticket/service-kafka.json
      [INFO] Creating a ReplicationController from kubernetes.json namespace ticket name kafka
      [INFO] Created ReplicationController: kafka/target/fabric8/applyJson/ticket/replicationcontroller-kafka.json
      [INFO] ------------------------------------------------------------------------
      [INFO] BUILD SUCCESS
      [INFO] ------------------------------------------------------------------------
      [INFO] Total time: 2.563 s
      [INFO] Finished at: 2016-05-19T16:03:25-07:00
      [INFO] Final Memory: 26M/259M
      [INFO] ------------------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Use the &lt;code&gt;kubectl get pod&lt;/code&gt; command to see what is running:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl get pod
      
      NAME                  READY     STATUS    RESTARTS   AGE
      kafka-mqmxt           1/1       Running   0          46s
      zk-standalone-4mo02   1/1       Running   0          4m&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Did you notice that we didn’t manually &quot;link&quot; the containers as we started them? Kubernetes has a cluster service discovery feature called &lt;a href=&quot;http://kubernetes.io/docs/user-guide/services/&quot;&gt;Kubernetes Services&lt;/a&gt; that load-balances against and lets us use internal DNS (or cluster IPs) to discover pods. For example, in the &lt;code&gt;kubernetes.yml&lt;/code&gt; deployment configuration for Kafka, you’ll see the following:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;nowrap&quot;&gt;    ...
          containers:
          - args: []
            command: []
            env:
            - name: &quot;KAFKA_ADVERTISED_PORT&quot;
              value: &quot;9092&quot;
            - name: &quot;KAFKA_ADVERTISED_HOST_NAME&quot;
              value: &quot;kafka&quot;
            - name: &quot;KAFKA_ZOOKEEPER_CONNECT&quot;
              value: &quot;zookeeper:2181&quot;
            - name: &quot;KAFKA_PORT&quot;
              value: &quot;9092&quot;
            - name: &quot;KUBERNETES_NAMESPACE&quot;
              valueFrom:
                fieldRef:
                  fieldPath: &quot;metadata.namespace&quot;
            image: &quot;docker.io/debezium/kafka:0.1&quot;
            imagePullPolicy: &quot;IfNotPresent&quot;
            name: &quot;kafka&quot;
          ...&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re specifying values for the &lt;code&gt;KAFKA_ZOOKEEPER_CONNECT&lt;/code&gt; environment variable used by the Docker image, and thus enabling Kafka to discover Zookeeper pods wherever they are running. Although we could have used any hostname, to keep things simple we use just &lt;code&gt;zookeeper&lt;/code&gt; for the DNS name. So, if you were to log in to one of the pods and try to reach the host named &lt;code&gt;zookeeper&lt;/code&gt;, Kubernetes would transparently resolve that request to one of the Zookeeper pods (if there are multiple). Slick! This discovery mechanism is used for the rest of the components, too. (Note, this cluster IP that the DNS resolves to &lt;strong&gt;never&lt;/strong&gt; changes for the life of the Kubernetes Service regardless of how many Pods exist for a given service. This means you can rely on this service discovery without all of the DNS caching issues you may otherwise run into.)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The next step is to create a &lt;code&gt;schema-changes&lt;/code&gt; topic that Debezium’s MySQL connector will use. Let’s use the Kafka tools to create this:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ KAFKA_POD_NAME=$(kubectl get pod | grep -i running | grep kafka | awk '{ print $1 }')
      
      $ kubectl exec $KAFKA_POD_NAME --  /kafka/bin/kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic schema-changes.inventory&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_start_up_a_mysql_database_on_kubernetes&quot;&gt;Start up a MySQL Database on Kubernetes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Starting the MySQL database follows the same instructions as installing Zookeeper or Kafka. We will navigate to the &lt;code&gt;$PROJECT_ROOT/mysql56&lt;/code&gt; directory, and we’ll use the &lt;a href=&quot;https://github.com/openshift/mysql&quot;&gt;MySQL 5.6 OpenShift Docker image&lt;/a&gt; so that it runs on both vanilla Kubernetes and OpenShift v3.x. Here’s the &lt;code&gt;kubectl&lt;/code&gt; command to start up our MySQL instance:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd ../mysql56
      $ kubectl create -f target/classes/kubernetes.yml
      
      service &quot;mysql&quot; created
      replicationcontroller &quot;mysql56&quot; created&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Or the equivalent Maven command:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd mysql56
      $ mvn fabric8:apply&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now, when we run &lt;code&gt;kubectl get pod&lt;/code&gt; we should see our MySQL database running, too:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;nowrap&quot;&gt;NAME                  READY     STATUS    RESTARTS   AGE
      kafka-mqmxt           1/1       Running   0          17m
      mysql56-b4f36         1/1       Running   0          9m
      zk-standalone-4mo02   1/1       Running   0          21m&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s run a command to get client access to the database. First, set a few environment variables to the pod’s name and IP address:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ MYSQL_POD_NAME=$(kubectl get pod | grep Running | grep ^mysql | awk '{ print $1 }')
      $ MYSQL_POD_IP=$(kubectl describe pod $MYSQL_POD_NAME | grep IP | awk '{ print $2 }')&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Then, log in to the Kubernetes pod that’s running the MySQL database, and start the MySQL command client:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl exec -it $MYSQL_POD_NAME   -- /opt/rh/rh-mysql56/root/usr/bin/mysql -h$MYSQL_POD_IP -P3306 -uroot -padmin
      Warning: Using a password on the command line interface can be insecure.
      Welcome to the MySQL monitor.  Commands end with ; or \g.
      Your MySQL connection id is 1
      Server version: 5.6.26-log MySQL Community Server (GPL)
      
      Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.
      
      Oracle is a registered trademark of Oracle Corporation and/or its
      affiliates. Other names may be trademarks of their respective
      owners.
      
      Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
      
      mysql&amp;gt;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This shows that the &lt;code&gt;kubectl&lt;/code&gt; command line lets us easily get access to a pod or Docker container regardless of where it’s running in the cluster.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Next, exit out of the mysql shell (type &lt;code&gt;exit&lt;/code&gt;) and run the following command to download a &lt;a href=&quot;https://gist.github.com/christian-posta/e20ddb5c945845b4b9f6eba94a98af09/raw&quot;&gt;SQL script&lt;/a&gt; that populates an &lt;code&gt;inventory&lt;/code&gt; sample database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl exec  -it $MYSQL_POD_NAME -- bash -c &quot;curl -s -L https://gist.github.com/christian-posta/e20ddb5c945845b4b9f6eba94a98af09/raw | /opt/rh/rh-mysql56/root/usr/bin/mysql -h$MYSQL_POD_IP -P3306 -uroot -padmin&quot;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now, if we log back into the MySQL pod we can show the databases and tables:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl exec -it $MYSQL_POD_NAME   -- /opt/rh/rh-mysql56/root/usr/bin/mysql -h$MYSQL_POD_IP -P3306 -uroot -padmin -e 'use inventory; show tables;'
      
      +---------------------+
      | Tables_in_inventory |
      +---------------------+
      | customers           |
      | orders              |
      | products            |
      | products_on_hand    |
      +---------------------+
      4 rows in set (0.00 sec)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_start_kafka_connect_and_debezium&quot;&gt;Start Kafka Connect and Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Navigate into the directory &lt;code&gt;$PROJECT_ROOT/connect-mysql&lt;/code&gt; directory. Here, we’ll start a Kubernetes pod that runs Kafka Connect with the Debezium MySQL connector already installed. The Debezium MySQL connector connects to a MySQL database, reads the binlog, and writes those row events to Kafka. Start up Kafka Connect with Debezium on Kubernetes similarly to the previous components:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd ../connect-mysql
      $ kubectl create -f target/classes/kubernetes.yml
      
      service &quot;connect-mysql&quot; created
      replicationcontroller &quot;connect-mysql&quot; created&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Or with the fabric8 maven plugin:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd ../connect-mysql
      $ mvn fabric8:apply
      Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=1512m; support was removed in 8.0
      [INFO] Scanning for projects...
      [INFO]
      [INFO] ------------------------------------------------------------------------
      [INFO] Building connect-mysql 0.1-SNAPSHOT
      [INFO] ------------------------------------------------------------------------
      [INFO]
      [INFO] --- fabric8-maven-plugin:2.2.115:apply (default-cli) @ connect-mysql ---
      [INFO] Using kubernetes at: https://172.28.128.4:8443/ in namespace ticket
      [INFO] Kubernetes JSON: /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/connect-mysql/target/classes/kubernetes.json
      [INFO] OpenShift platform detected
      [INFO] Using namespace: ticket
      [INFO] Looking at repo with directory /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/.git
      [INFO] Creating a Service from kubernetes.json namespace ticket name connect-mysql
      [INFO] Created Service: connect-mysql/target/fabric8/applyJson/ticket/service-connect-mysql.json
      [INFO] Creating a ReplicationController from kubernetes.json namespace ticket name connect-mysql
      [INFO] Created ReplicationController: connect-mysql/target/fabric8/applyJson/ticket/replicationcontroller-connect-mysql.json
      [INFO] ------------------------------------------------------------------------
      [INFO] BUILD SUCCESS
      [INFO] ------------------------------------------------------------------------
      [INFO] Total time: 2.255 s
      [INFO] Finished at: 2016-05-25T09:21:04-07:00
      [INFO] Final Memory: 27M/313M
      [INFO] ------------------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Just like in the Docker tutorial for Debezium, we now want to send a JSON object to the Kafka Connect API to start up our Debezium connector. First, we need to expose the API for the Kafka Connect cluster. You can do this however you want: on Kubernetes (&lt;a href=&quot;http://kubernetes.io/docs/user-guide/ingress/&quot;&gt;Ingress definitions&lt;/a&gt;, &lt;a href=&quot;http://kubernetes.io/docs/user-guide/services/&quot;&gt;NodePort services&lt;/a&gt;, etc) or on OpenShift you can use &lt;a href=&quot;https://docs.openshift.com/enterprise/3.2/architecture/core_concepts/routes.html&quot;&gt;OpenShift Routes&lt;/a&gt;. For this simple example, we’ll use simple Pod port-forwarding to forward the &lt;code&gt;connect-mysql&lt;/code&gt; pod’s &lt;code&gt;8083&lt;/code&gt; port to our local machine (again, regardless of where the Pod is actually running the cluster. (This is such an incredible feature of Kubernetes that makes it so easy to develop distributed services!)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s determine the pod name and then use port forwarding to our local machine:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ CONNECT_POD_NAME=$(kubectl get pod | grep -i running | grep ^connect | awk '{ print $1 }')
      $ kubectl port-forward $CONNECT_POD_NAME 8083:8083
      
      I0525 09:30:08.390491    6651 portforward.go:213] Forwarding from 127.0.0.1:8083 -&amp;gt; 8083
      I0525 09:30:08.390631    6651 portforward.go:213] Forwarding from [::1]:8083 -&amp;gt; 8083&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We are forwarding the pod’s port &lt;code&gt;8083&lt;/code&gt; to our local machine’s &lt;code&gt;8083&lt;/code&gt;. Now if we hit &lt;code&gt;&lt;a href=&quot;http://localhost:8083&quot; class=&quot;bare&quot;&gt;http://localhost:8083&lt;/a&gt;&lt;/code&gt; it will be directed to the pod which runs our Kafka Connect and Debezium services.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Since it may be useful to see the output from the pod to see whether or not there are any exceptions, start another terminal and type the following to follow the Kafka Connect output:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ CONNECT_POD_NAME=$(kubectl get pod | grep -i running | grep ^connect | awk '{ print $1 }')
      $ kubectl logs -f $CONNECT_POD_NAME&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now, let’s use an HTTP client to post the Debezium Connector/Task to the endpoint we’ve just exposed locally:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; -H &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ -d '{ &quot;name&quot;: &quot;inventory-connector&quot;, &quot;config&quot;: { &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;, &quot;tasks.max&quot;: &quot;1&quot;, &quot;database.hostname&quot;: &quot;mysql&quot;, &quot;database.port&quot;: &quot;3306&quot;, &quot;database.user&quot;: &quot;replicator&quot;, &quot;database.password&quot;: &quot;replpass&quot;, &quot;database.server.id&quot;: &quot;184054&quot;, &quot;database.server.name&quot;: &quot;mysql-server-1&quot;, &quot;database.binlog&quot;: &quot;mysql-bin.000001&quot;, &quot;database.whitelist&quot;: &quot;inventory&quot;, &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;, &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot; } }'&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If we’re watching the log output for the &lt;code&gt;connect-mysql&lt;/code&gt; pod, we’ll see it eventually end up looking something like this:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;nowrap&quot;&gt;2016-05-27 18:50:14,580 - WARN  [kafka-producer-network-thread | producer-1:NetworkClient$DefaultMetadataUpdater@582] - Error while fetching metadata with correlation id 2 : {mysql-server-1.inventory.products=LEADER_NOT_AVAILABLE}
      2016-05-27 18:50:14,690 - WARN  [kafka-producer-network-thread | producer-1:NetworkClient$DefaultMetadataUpdater@582] - Error while fetching metadata with correlation id 3 : {mysql-server-1.inventory.products=LEADER_NOT_AVAILABLE}
      2016-05-27 18:50:14,911 - WARN  [kafka-producer-network-thread | producer-1:NetworkClient$DefaultMetadataUpdater@582] - Error while fetching metadata with correlation id 7 : {mysql-server-1.inventory.products_on_hand=LEADER_NOT_AVAILABLE}
      2016-05-27 18:50:15,136 - WARN  [kafka-producer-network-thread | producer-1:NetworkClient$DefaultMetadataUpdater@582] - Error while fetching metadata with correlation id 10 : {mysql-server-1.inventory.customers=LEADER_NOT_AVAILABLE}
      2016-05-27 18:50:15,362 - WARN  [kafka-producer-network-thread | producer-1:NetworkClient$DefaultMetadataUpdater@582] - Error while fetching metadata with correlation id 13 : {mysql-server-1.inventory.orders=LEADER_NOT_AVAILABLE}&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;These error are just Kafka’s way of telling us the topics didn’t exist but were created.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If we now do a listing of our topics inside Kafka, we should see a Kafka topic for each table in the mysql &lt;code&gt;inventory&lt;/code&gt; database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl exec  $KAFKA_POD_NAME --  /kafka/bin/kafka-topics.sh --list --zookeeper zookeeper:2181
      __consumer_offsets
      my-connect-configs
      my-connect-offsets
      mysql-server-1.inventory.customers
      mysql-server-1.inventory.orders
      mysql-server-1.inventory.products
      mysql-server-1.inventory.products_on_hand
      schema-changes.inventory&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s take a look at what’s in one of these topics:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl exec  $KAFKA_POD_NAME --  /kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --new-consumer --topic mysql-server-1.inventory.customers --from-beginning --property print.key=true
      {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers/pk&quot;},&quot;payload&quot;:{&quot;id&quot;:1001}}   {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;first_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;last_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;email&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers&quot;},&quot;payload&quot;:{&quot;id&quot;:1001,&quot;first_name&quot;:&quot;Sally&quot;,&quot;last_name&quot;:&quot;Thomas&quot;,&quot;email&quot;:&quot;sally.thomas@acme.com&quot;}}
      {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers/pk&quot;},&quot;payload&quot;:{&quot;id&quot;:1002}}   {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;first_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;last_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;email&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers&quot;},&quot;payload&quot;:{&quot;id&quot;:1002,&quot;first_name&quot;:&quot;George&quot;,&quot;last_name&quot;:&quot;Bailey&quot;,&quot;email&quot;:&quot;gbailey@foobar.com&quot;}}
      {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers/pk&quot;},&quot;payload&quot;:{&quot;id&quot;:1003}}   {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;first_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;last_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;email&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers&quot;},&quot;payload&quot;:{&quot;id&quot;:1003,&quot;first_name&quot;:&quot;Edward&quot;,&quot;last_name&quot;:&quot;Walker&quot;,&quot;email&quot;:&quot;ed@walker.com&quot;}}
      {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers/pk&quot;},&quot;payload&quot;:{&quot;id&quot;:1004}}   {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;first_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;last_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;email&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers&quot;},&quot;payload&quot;:{&quot;id&quot;:1004,&quot;first_name&quot;:&quot;Anne&quot;,&quot;last_name&quot;:&quot;Kretchmar&quot;,&quot;email&quot;:&quot;annek@noanswer.org&quot;}}&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;What happened? When we started Debezium’s MySQL connector, it started reading the binary replication log from the MySQL server, and it replayed all of the history and generated an event for each INSERT, UPDATE, and DELETE operation (though in our sample &lt;code&gt;inventory&lt;/code&gt; database we only had INSERTs). If we or some client apps were to commit other changes to the database, Debezium would see those immediately and write those to the correct topic. In other words, Debezium records all of the changes to our MySQL database as events in Kafka topics! And from there, any tool, connector, or service can independnetly consume those event streams from Kafka and process them or put them into a different database, into Hadoop, elasticsearch, data grid, etc.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_cleanup&quot;&gt;Cleanup&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you want to delete the connector, simply issue a REST request to remove it:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X DELETE -H &quot;Accept:application/json&quot; http://localhost:8083/connectors/inventory-connector&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/04/15/parsing-ddl/</id>
    <title>Parsing DDL</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2016-04-15T00:00:00+00:00</published>
    <link href="/blog/2016/04/15/parsing-ddl/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="mysql"></category>
    <category term="sql"></category>
    <summary>
      
      
      
      When our MySQL connector is reading the binlog of a MySQL server or cluster, it parses the DDL statements in the log and builds an in-memory model of each table&#8217;s schema as it evolves over time. This process is important because the connector generates events for each table using the definition of the table at the time of each event. We can&#8217;t use the database&#8217;s current schema, since it may have changed since the point in time (or position in the log) where the connector is reading.
      
      
      Parsing DDL of MySQL or any other major relational database can seem to be...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When our &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt; is reading the binlog of a MySQL server or cluster, it parses the DDL statements in the log and builds an in-memory model of each table’s schema as it evolves over time. This process is important because the connector generates events for each table using the definition of the table at the time of each event. We can’t use the database’s &lt;em&gt;current&lt;/em&gt; schema, since it may have changed since the point in time (or position in the log) where the connector is reading.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Parsing DDL of MySQL or any other major relational database can seem to be a daunting task. Usually each DBMS has a highly-customized SQL grammar, and although the &lt;em&gt;data manipulation language&lt;/em&gt; (DML) statements are often fairly close the standards, the &lt;em&gt;data definition language&lt;/em&gt; (DDL) statements are usually less so and involve more DBMS-specific features.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So given this, why did we write our own DDL parser for MySQL? Let’s first look at what Debezium needs a DDL parser to do.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_parsing_ddl_in_the_debezium_mysql_connector&quot;&gt;Parsing DDL in the Debezium MySQL connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The MySQL binlog contains various kinds of events. For example, when a row is inserted into a table, the binlog event contains an indirect reference to the table and the values for each column in the table, but there is no information about the columns that make up the table. The only thing in the binlog referencing table structures are SQL DDL statements that were generated by MySQL when it processed user-supplied DDL statements.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The connector also produces messages using Kafka Connect Schemas, which are simple data structures that define the various names and types of each field, and the way the fields are organized. So, when we generate an event message for the table insert, we first have to have a Kafka Connect &lt;code&gt;Schema&lt;/code&gt; object with all the appropriate fields, and then we have to convert the ordered array of column values into a Kafka Connect &lt;code&gt;Struct&lt;/code&gt; object using the fields and the individual column values in the table insert event.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Luckily, when we come across a DDL statement we can update our in-memory model and then use this to generate a &lt;code&gt;Schema&lt;/code&gt; object. At the same time, we can create a component that will use this &lt;code&gt;Schema&lt;/code&gt; object to create a &lt;code&gt;Struct&lt;/code&gt; object from the ordered array of column values that appear in the events. All of this can be done once and used for all row events on that table, until we come across another DDL statement that changes the table’s schema at which point we updated our model again.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So all of this requires parsing all of the DDL statements, though for our purposes we only have to &lt;em&gt;understand&lt;/em&gt; a small subset of the DDL grammer. We then have to use that subset of statements to update our in-memory model of our tables. And since our in-memory table model is not specific to MySQL, the rest of the functionality to generate &lt;code&gt;Schema&lt;/code&gt; objects and components that convert an array of values into &lt;code&gt;Struct&lt;/code&gt; objects used in messages is all generic.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_existing_ddl_libraries&quot;&gt;Existing DDL libraries&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Unfortunately, there aren’t really that many 3rd party open source libraries for parsing DDL statements for MySQL, PostgreSQL, or other popular RDBMSes. &lt;a href=&quot;http://jsqlparser.sourceforge.net&quot;&gt;JSqlParser&lt;/a&gt; is often cited, but it has a &lt;em&gt;single grammar&lt;/em&gt; that is a combination of multiple DBMS grammars and therefore is not a strict parser for any specific DBMS. Adding support for other DBMSes by updating the composite grammar would likely be difficult.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Other libraries, such as &lt;a href=&quot;https://github.com/prestodb/presto/tree/master/presto-parser&quot;&gt;PrestoDB&lt;/a&gt;, define their own SQL grammar and are unable to handle the intracacies and nuances of the MySQL DDL grammar. The Antlr parser generator project has a &lt;a href=&quot;https://github.com/antlr/grammars-v4/tree/master/mysql&quot;&gt;grammar for MySQL 5.6&lt;/a&gt;, but this is limited to a small subset of DML and has no support for DDL or newer 5.7 features. There are &lt;a href=&quot;http://www.antlr3.org/grammar/list.html&quot;&gt;older SQL-related grammars for Antlr 3&lt;/a&gt;, but these are often massive, suffer from bugs, and limited to specific DBMSes. The &lt;a href=&quot;http://teiid.jboss.org&quot;&gt;Teiid project&lt;/a&gt; is a data virtualization engine that sits atop a wide variety of DBMSes and data sources, and it’s tooling has a series of &lt;a href=&quot;https://github.com/Teiid-Designer/teiid-modeshape&quot;&gt;DDL parsers&lt;/a&gt; that construct ASTs in a special repository (the author actually helped develop these). There are also Ruby libraries, like &lt;a href=&quot;https://github.com/square/mysql-parser&quot;&gt;Square’s MySQL Parser library&lt;/a&gt;. There is also a &lt;a href=&quot;http://www.sqlparser.com/sql-parser-java.php&quot;&gt;proprietary commercial product&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_our_ddl_parser_framework&quot;&gt;Our DDL parser framework&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Since we couldn’t find a useful 3rd party open source library, we chose to create our own DDL parser framework limited to our needs:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Parse DDL statements and update our in-memory model.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Focus on consuming those essential statements (e.g., create, alter, and drop tables and views), while completely ignoring other statements without having to parse them.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Structure the parser code similarly to the &lt;a href=&quot;http://dev.mysql.com/doc/refman/5.7/en/sql-syntax-data-definition.html&quot;&gt;MySQL DDL grammar documentation&lt;/a&gt; and use method names that mirror the rules in the grammar. This will make it easier to maintain over time.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Allow creation of parsers for PostgreSQL, Oracle, SQLServer, and other DBMSes as needed.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Support customization through subclassing: be able to easily override narrow portions of the logic without having to copy lots of code.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Make it easy to develop, debug, and test parsers.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The resulting framework includes a tokenizer that converts one or more DDL statements in a string into a rewindable sequence of tokens, where each token represents punctuation, quoted strings, case-insentivie words and symbols, numbers, keywords, comments, and terminating characters  (such as &lt;code&gt;;&lt;/code&gt; for MySQL). The DDL parser, then, walks the token stream looking for patterns using a simple and easy to read fluent API, calling methods on itself to process the various sets of tokens. The parser also uses an internal &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/relational/ddl/DataTypeParser.java&quot;&gt;data type parser&lt;/a&gt; for processing SQL data type expressions, such as &lt;code&gt;INT&lt;/code&gt;, &lt;code&gt;VARCHAR(64)&lt;/code&gt;, &lt;code&gt;NUMERIC(32,3)&lt;/code&gt;, &lt;code&gt;TIMESTAMP(8) WITH TIME ZONE&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/debezium-connector-mysql/src/main/java/io/debezium/connector/mysql/MySqlDdlParser.java&quot;&gt;MySqlDdlParser&lt;/a&gt; class extends a &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/relational/ddl/DdlParser.java&quot;&gt;base class&lt;/a&gt; and provides all of the MySQL-specific parsing logic. For example, the DDL statements:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;# Create and populate our products using a single insert with many rows
      CREATE TABLE products (
        id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
        name VARCHAR(255) NOT NULL,
        description VARCHAR(512),
        weight FLOAT
      );
      ALTER TABLE products AUTO_INCREMENT = 101;
      
      # Create and populate the products on hand using multiple inserts
      CREATE TABLE products_on_hand (
        product_id INTEGER NOT NULL PRIMARY KEY,
        quantity INTEGER NOT NULL,
        FOREIGN KEY (product_id) REFERENCES products(id)
      );&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;can be easily parsed with:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;String ddlStatements = ...
      DdlParser parser = new MySqlDdlParser();
      Tables tables = new Tables();
      parser.parse(ddl, tables);&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Here, the &lt;code&gt;Tables&lt;/code&gt; object is our in-memory representation of our named table definitions. The parser processes the DDL statements, applying each to the appropriate table definition inside the &lt;code&gt;Tables&lt;/code&gt; object.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_how_it_works&quot;&gt;How it works&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Each &lt;code&gt;DdlParser&lt;/code&gt; implementation has the following public method that will parse the statements in the supplied String:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;    public final void parse(String ddlContent, Tables databaseTables) {
              Tokenizer tokenizer = new DdlTokenizer(!skipComments(), this::determineTokenType);
              TokenStream stream = new TokenStream(ddlContent, tokenizer, false);
              stream.start();
              parse(stream, databaseTables);
          }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Here, the method creates a new &lt;code&gt;TokenStream&lt;/code&gt; from the content using a &lt;code&gt;DdlTokenizer&lt;/code&gt; that knows how to separate the characters in the string into the various typed token objects. It then calls another &lt;code&gt;parse&lt;/code&gt; method that does the bulk of the work:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;    public final void parse(TokenStream ddlContent, Tables databaseTables)
                                 throws ParsingException, IllegalStateException {
              this.tokens = ddlContent;
              this.databaseTables = databaseTables;
              Marker marker = ddlContent.mark();
              try {
                  while (ddlContent.hasNext()) {
                      parseNextStatement(ddlContent.mark());
                      // Consume the statement terminator if it is still there ...
                      tokens.canConsume(DdlTokenizer.STATEMENT_TERMINATOR);
                  }
              } catch (ParsingException e) {
                  ddlContent.rewind(marker);
                  throw e;
              } catch (Throwable t) {
                  parsingFailed(ddlContent.nextPosition(),
                                &quot;Unexpected exception (&quot; + t.getMessage() + &quot;) parsing&quot;, t);
              }
          }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This sets up some local state, marks the current starting point, and tries to parse DDL statements until no more can be found. If the parsing logic fails to find a match, it generates a &lt;code&gt;ParsingException&lt;/code&gt; with the offending line and column plus a message signaling what was found and what was expected. In such cases, this method rewinds the token stream (in case the caller wishes to try an alternative different parser).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Each time the &lt;code&gt;parseNextStatement&lt;/code&gt; method is called, the starting position of that statement is passed into the method, giving it the starting position of the statement. Our &lt;code&gt;MySqlDdlParser&lt;/code&gt; subclass overrides the &lt;code&gt;parseNextStatement&lt;/code&gt; method to use the first token in the statement to determine the kinds of statement allowed in the MySQL DDL grammar:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;    @Override
          protected void parseNextStatement(Marker marker) {
              if (tokens.matches(DdlTokenizer.COMMENT)) {
                  parseComment(marker);
              } else if (tokens.matches(&quot;CREATE&quot;)) {
                  parseCreate(marker);
              } else if (tokens.matches(&quot;ALTER&quot;)) {
                  parseAlter(marker);
              } else if (tokens.matches(&quot;DROP&quot;)) {
                  parseDrop(marker);
              } else if (tokens.matches(&quot;RENAME&quot;)) {
                  parseRename(marker);
              } else {
                  parseUnknownStatement(marker);
              }
          }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When a matching token is found, the method calls the appropriate method. For example, if the statement begins with &lt;code&gt;CREATE TABLE …​&lt;/code&gt;, then the &lt;code&gt;parseCreate&lt;/code&gt; method is called with the same marker that identifies the starting position of the statement:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;    @Override
          protected void parseCreate(Marker marker) {
              tokens.consume(&quot;CREATE&quot;);
              if (tokens.matches(&quot;TABLE&quot;) || tokens.matches(&quot;TEMPORARY&quot;, &quot;TABLE&quot;)) {
                  parseCreateTable(marker);
              } else if (tokens.matches(&quot;VIEW&quot;)) {
                  parseCreateView(marker);
              } else if (tokens.matchesAnyOf(&quot;DATABASE&quot;, &quot;SCHEMA&quot;)) {
                  parseCreateUnknown(marker);
              } else if (tokens.matchesAnyOf(&quot;EVENT&quot;)) {
                  parseCreateUnknown(marker);
              } else if (tokens.matchesAnyOf(&quot;FUNCTION&quot;, &quot;PROCEDURE&quot;)) {
                  parseCreateUnknown(marker);
              } else if (tokens.matchesAnyOf(&quot;UNIQUE&quot;, &quot;FULLTEXT&quot;, &quot;SPATIAL&quot;, &quot;INDEX&quot;)) {
                  parseCreateIndex(marker);
              } else if (tokens.matchesAnyOf(&quot;SERVER&quot;)) {
                  parseCreateUnknown(marker);
              } else if (tokens.matchesAnyOf(&quot;TABLESPACE&quot;)) {
                  parseCreateUnknown(marker);
              } else if (tokens.matchesAnyOf(&quot;TRIGGER&quot;)) {
                  parseCreateUnknown(marker);
              } else {
                  // It could be several possible things (including more
                  // elaborate forms of those matches tried above),
                  sequentially(this::parseCreateView,
                               this::parseCreateUnknown);
              }
          }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Here, the method first consumes the token with the &lt;code&gt;CREATE&lt;/code&gt; literal, and then tries to match the tokens with various patterns of token literals. If a match is found, this method delegates to other more specific parsing methods. Note how the fluent API of the framework makes it quite easy to understand the match patterns.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s go one step further. Assuming our DDL statement starts with &lt;code&gt;CREATE TABLE products (&lt;/code&gt;, then the parser will then invoke the &lt;code&gt;parseCreateTable&lt;/code&gt; method, again with the same marker denoting the start of the statement:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;    protected void parseCreateTable(Marker start) {
              tokens.canConsume(&quot;TEMPORARY&quot;);
              tokens.consume(&quot;TABLE&quot;);
              boolean onlyIfNotExists = tokens.canConsume(&quot;IF&quot;, &quot;NOT&quot;, &quot;EXISTS&quot;);
              TableId tableId = parseQualifiedTableName(start);
              if ( tokens.canConsume(&quot;LIKE&quot;)) {
                  TableId originalId = parseQualifiedTableName(start);
                  Table original = databaseTables.forTable(originalId);
                  if ( original != null ) {
                      databaseTables.overwriteTable(tableId, original.columns(),
                                                    original.primaryKeyColumnNames());
                  }
                  consumeRemainingStatement(start);
                  debugParsed(start);
                  return;
              }
              if (onlyIfNotExists &amp;amp;&amp;amp; databaseTables.forTable(tableId) != null) {
                  // The table does exist, so we should do nothing ...
                  consumeRemainingStatement(start);
                  debugParsed(start);
                  return;
              }
              TableEditor table = databaseTables.editOrCreateTable(tableId);
      
              // create_definition ...
              if (tokens.matches('(')) parseCreateDefinitionList(start, table);
              // table_options ...
              parseTableOptions(start, table);
              // partition_options ...
              if (tokens.matches(&quot;PARTITION&quot;)) {
                  parsePartitionOptions(start, table);
              }
              // select_statement
              if (tokens.canConsume(&quot;AS&quot;) || tokens.canConsume(&quot;IGNORE&quot;, &quot;AS&quot;)
                  || tokens.canConsume(&quot;REPLACE&quot;, &quot;AS&quot;)) {
                  parseAsSelectStatement(start, table);
              }
      
              // Update the table definition ...
              databaseTables.overwriteTable(table.create());
              debugParsed(start);
          }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This method tries to mirror the &lt;a href=&quot;http://dev.mysql.com/doc/refman/5.7/en/create-table.html&quot;&gt;MySQL &lt;code&gt;CREATE TABLE&lt;/code&gt; grammar rules&lt;/a&gt;, which start with:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name
          (create_definition,...)
          [table_options]
          [partition_options]
      
      CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name
          [(create_definition,...)]
          [table_options]
          [partition_options]
          select_statement
      
      CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name
          { LIKE old_tbl_name | (LIKE old_tbl_name) }
      
      create_definition:
          ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;code&gt;CREATE&lt;/code&gt; literal was already consumed before our &lt;code&gt;parseCreateTable&lt;/code&gt; begins, so it first tries to consume the &lt;code&gt;TEMPORARY&lt;/code&gt; literal if available, the &lt;code&gt;TABLE&lt;/code&gt; literal, the &lt;code&gt;IF NOT EXISTS&lt;/code&gt; fragment if avaialble, and then consumes and parses the qualified name of the table. If the statement includes &lt;code&gt;LIKE otherTable&lt;/code&gt;, it uses the &lt;code&gt;databaseTables&lt;/code&gt; (which is the reference to our &lt;code&gt;Tables&lt;/code&gt; object) to overwrite the definition of the named table with that of the referenced table. Otherwise, it obtains an editor for the new table, and then (like the grammar rules) parses a list of &lt;em&gt;create_definition&lt;/em&gt; fragments, followed by &lt;em&gt;table_options&lt;/em&gt;, &lt;em&gt;partition_options&lt;/em&gt;, and possibly a &lt;em&gt;select_statement&lt;/em&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Take a look at the full &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/debezium-connector-mysql/src/main/java/io/debezium/connector/mysql/MySqlDdlParser.java&quot;&gt;MySqlDdlParser&lt;/a&gt; class to see far more details.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;_wrap_up&quot;&gt;Wrap up&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This post goes into some detail about why the MySQL connector uses the DDL statements in the binlog, though we only scratched the surface about &lt;em&gt;how&lt;/em&gt; the connector does the DDL parsing with its framework, and how that can be reused in future parsers for other DBMS dialects.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Try our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt; to see the MySQL connector in action, and stay tuned for more connectors, releases, and news.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/04/14/Debezium-website/</id>
    <title>Debezium Website</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2016-04-14T00:00:00+00:00</published>
    <link href="/blog/2016/04/14/Debezium-website/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="website"></category>
    <summary>
      
      As you may have noticed, we have a new website with documentation, a blog, and information about the Debezium community and how you can contribute. Let us know what you think, and contribute improvements.
      ...
    </summary>
    <content type="html">
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As you may have noticed, we have a &lt;a href=&quot;http://debezium.io&quot;&gt;new website&lt;/a&gt; with &lt;a href=&quot;http://debezium.io/docs&quot;&gt;documentation&lt;/a&gt;, a &lt;a href=&quot;http://debezium.io/blog&quot;&gt;blog&lt;/a&gt;, and information about the &lt;a href=&quot;http://debezium.io/community&quot;&gt;Debezium community&lt;/a&gt; and how you can &lt;a href=&quot;http://debezium.io/docs/contribute&quot;&gt;contribute&lt;/a&gt;. Let us know what you think, and &lt;a href=&quot;http://debezium.io/docs/contribute&quot;&gt;contribute improvements&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>/blog/2016/03/18/Debezium-0-1-Released/</id>
    <title>Debezium 0.1 Released</title>
    <updated>2017-09-21T07:35:59+00:00</updated>
    <published>2016-03-18T00:00:00+00:00</published>
    <link href="/blog/2016/03/18/Debezium-0-1-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      Debezium is a distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of Kafka and provides Kafka Connect compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is open source under the Apache...
    </summary>
    <content type="html">
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is a distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now the good news — &lt;strong&gt;&lt;em&gt;Debezium 0.1 is now available&lt;/em&gt;&lt;/strong&gt; and includes several significant features:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;A &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;connector for MySQL&lt;/a&gt; to monitor MySQL databases. It’s a Kafka Connect source connector, so simply install it into a Kafka Connect service (see below) and use the service’s REST API to configure and manage connectors to each DBMS server. The connector reads the MySQL binlog and generates data change events for every committed row-level modification in the monitored databases. The MySQL connector generates events based upon the tables' structure at the time the row is changed, and it automatically handles changes to the table structures.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;A small library so applications can &lt;a href=&quot;http://debezium.io/docs/embedded&quot;&gt;embed any Kafka Connect connector&lt;/a&gt; and consume data change events read directly from the source system. This provides a much lighter weight system (since Zookeeper, Kafka, and Kafka Connect services are not needed), but as a consequence is not as fault tolerant or reliable since the application must maintain state normally kept inside Kafka’s distributed and replicated logs. Thus the application becomes completely responsible for managing all state.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Although Debezium is really intended to be used as turnkey services, all of Debezium’s JARs and other artifacts are available in &lt;a href=&quot;http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22io.debezium%22&quot;&gt;Maven Central&lt;/a&gt;. Detailed information about the features, tasks, and bugs are outlined in our release notes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To make it easier to use a Debezium’s connector inside your own Kafka Connect service, we created a plugin archive (in both zip and tar.gz formats) that includes all JARs used by the connector not already included in Kafka Connect 0.9.0.1. Simply download, extract to your Kafka Connect 0.9.0.1 installation, and add all of the JARs to the service’s classpath. Once the service is restarted, you can then use the REST API to configure and manage connector instances that monitor the databases of your choice. &lt;a href=&quot;http://search.maven.org/#artifactdetails%7Cio.debezium%7Cdebezium-connector-mysql%7C0.1.0%7Cjar&quot;&gt;MySQL connector plugin archive&lt;/a&gt; is located in Maven Central, so it’s even possible to use Maven to build a customized Kafka Connect service. We’ll generate these plugins for future connectors, too.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium platform has a lot of moving parts in Zookeeper, Kafka, and Kafka Connect. To make it much easier for you to try it out and play with it, we created &lt;a href=&quot;https://hub.docker.com/u/debezium/&quot;&gt;Docker images&lt;/a&gt; and a &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt; that walks you through using Debezium. First, it has you use Docker to start a container for each of these services and a MySQL server with an example &quot;inventory&quot; database. It shows you how to use the RESTful API to register a connector to monitor the inventory database, how to watch the streams of data changes for various tables, and how changing the database produces new change events with very low latency. It also walks you through shutting down the Kafka Connect service, changing data while the service is not monitoring the database, and then restarting the Kafka Connect service to see how all of the data changes that occurred while the service was not running are still captured correctly in the streams. This tutorial really is a great way to interactively learn the basics of Debezium and change data capture.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;. We plan to release 0.2 very soon with at least one additional connector.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Emmanuel, Chris, Akshath, James, and Paul for their help with the release, questions, and discussions!&lt;/p&gt;
      &lt;/div&gt;
    </content>
  </entry>
</feed>
