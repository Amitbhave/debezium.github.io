<?xml version="1.0" encoding="utf-8" ?>
<feed xml:lang="en-US" xmlns="http://www.w3.org/2005/Atom">
  <id>http://debezium.io/</id>
  <title>Debezium Blog</title>
  <updated>2018-06-11T08:15:21+00:00</updated>
  <link href="http://debezium.io/blog.atom" rel="self" type="application/atom+xml" />
  <link href="http://debezium.io/" rel="alternate" type="text/html" />
  <entry>
    <id>http://debezium.io/blog/2018/05/24/querying-debezium-change-data-eEvents-with-ksql/</id>
    <title>Querying Debezium Change Data Events With KSQL</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2018-05-24T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/05/24/querying-debezium-change-data-eEvents-with-ksql/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="mysql"></category>
    <category term="ksql"></category>
    <category term="example"></category>
    <summary>
      
      
      
      Last year we have seen the inception of a new open-source project in the Apache Kafka universe, KSQL,
      which is a streaming SQL engine build on top of Kafka Streams.
      In this post, we are going to try out KSQL querying with data change events generated by Debezium from a MySQL database.
      
      
      As a source of data we will use the database and setup from our tutorial.
      The result of this exercise should be similar to the recent post about aggregation of events into domain driven aggregates.
      
      
      
      
      Entity diagram
      
      
      First let&#8217;s look at the entities in the database and the relations between them.
      
      
      
      
      
      Figure 1: Entity diagram...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Last year we have seen the inception of a new open-source project in the &lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Apache Kafka&lt;/a&gt; universe, &lt;a href=&quot;https://github.com/confluentinc/ksql&quot;&gt;KSQL&lt;/a&gt;,
      which is a streaming SQL engine build on top of &lt;a href=&quot;https://kafka.apache.org/documentation/streams/&quot;&gt;Kafka Streams&lt;/a&gt;.
      In this post, we are going to try out KSQL querying with data change events generated by Debezium from a MySQL database.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As a source of data we will use the database and setup from our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.
      The result of this exercise should be similar to the recent &lt;a href=&quot;http://debezium.io/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/&quot;&gt;post&lt;/a&gt; about aggregation of events into &lt;a href=&quot;https://martinfowler.com/bliki/DDD_Aggregate.html&quot;&gt;domain driven aggregates&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;entity_diagram&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#entity_diagram&quot;&gt;&lt;/a&gt;Entity diagram&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;First let’s look at the entities in the database and the relations between them.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;http://debezium.io/images/tutorial-erd.svg&quot; alt=&quot;Entity diagram&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;title&quot;&gt;Figure 1: Entity diagram of the example entities&lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt; &lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The picture above shows the full ER diagram for the inventory database in the example MySQL instance.
      We are going to focus on two entities:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;customers&lt;/code&gt; - the list of customers in the system&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;orders&lt;/code&gt; - the list of orders in the system&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;There is a &lt;code&gt;1:n&lt;/code&gt; relation between &lt;code&gt;customers&lt;/code&gt; and &lt;code&gt;orders&lt;/code&gt;, modelled by the &lt;code&gt;purchaser&lt;/code&gt; column in the &lt;code&gt;orders&lt;/code&gt; table, which is a foreign key to the &lt;code&gt;customers&lt;/code&gt; table.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;configuration&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#configuration&quot;&gt;&lt;/a&gt;Configuration&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We are going to use a &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/tutorial/docker-compose-mysql.yaml&quot;&gt;Docker Compose file&lt;/a&gt; for the deployment of the environment.
      The deployment consists of the following Docker images:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://hub.docker.com/r/debezium/zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://hub.docker.com/r/debezium/kafka/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Kafka Connect including the Debezium connectors &lt;a href=&quot;https://hub.docker.com/r/debezium/connect/&quot;&gt;image&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;A pre-populated MySQL database as used in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We also need the KSQL client.
      To make things simple we are going to use a pre-built &lt;a href=&quot;https://hub.docker.com/r/confluentinc/ksql-cli/&quot;&gt;Docker image&lt;/a&gt; but you can download and directly use the client from the KSQL &lt;a href=&quot;https://github.com/confluentinc/ksql/releases&quot;&gt;download&lt;/a&gt; page.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;example&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#example&quot;&gt;&lt;/a&gt;Example&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;First we need to start the Debezium and Kafka infrastructure.
      To do so, clone the &lt;a href=&quot;https://github.com/debezium/debezium-examples/&quot;&gt;debezium-examples&lt;/a&gt; GitHub repository and start the required components using the provided Compose file:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;export DEBEZIUM_VERSION=0.7
      git clone https://github.com/debezium/debezium-examples.git
      cd debezium-examples/tutorial/
      docker-compose -f docker-compose-mysql.yaml up&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Next we must register an instance of the Debezium MySQL connector to listen to changes in the database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ -d @- &amp;lt;&amp;lt;-EOF
      {
          &quot;name&quot;: &quot;inventory-connector&quot;,
          &quot;config&quot;: {
              &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
              &quot;tasks.max&quot;: &quot;1&quot;,
              &quot;database.hostname&quot;: &quot;mysql&quot;,
              &quot;database.port&quot;: &quot;3306&quot;,
              &quot;database.user&quot;: &quot;debezium&quot;,
              &quot;database.password&quot;: &quot;dbz&quot;,
              &quot;database.server.id&quot;: &quot;184055&quot;,
              &quot;database.server.name&quot;: &quot;dbserver&quot;,
              &quot;database.whitelist&quot;: &quot;inventory&quot;,
              &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
              &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot;,
              &quot;transforms&quot;: &quot;unwrap&quot;,
              &quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.UnwrapFromEnvelope&quot;,
              &quot;key.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;,
              &quot;key.converter.schemas.enable&quot;: &quot;false&quot;,
              &quot;value.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;,
              &quot;value.converter.schemas.enable&quot;: &quot;false&quot;
          }
      }
      EOF&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now we should have all components up and running and initial data change events are already streamed into Kafka topics.
      There are multiple properties that are especially important for our use case:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;The &lt;a href=&quot;http://debezium.io/docs/configuration/event-flattening/&quot;&gt;UnwrapFromEnvelope SMT&lt;/a&gt; is used.
      This allows us to directly map fields from the &lt;code&gt;after&lt;/code&gt; part of change records into KSQL statements.
      Without it, we would need to use &lt;code&gt;EXTRACTJSONFIELD&lt;/code&gt; for each field to be extracted from the &lt;code&gt;after&lt;/code&gt; part of messages.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Schemas are disabled for the JSON converter.
      The reason is the same as above.
      With schemas enabled, for JSON the record is encapsulated in a JSON structure that contains the fields &lt;code&gt;schema&lt;/code&gt; (with schema information) and &lt;code&gt;payload&lt;/code&gt; (with the actual data itself).
      We would again need to use &lt;code&gt;EXTRACTJSONFIELD&lt;/code&gt; to get to the relevant fields.
      There is no such issue with Avro converter so this option does not need to be set when Avro is used.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Next we are going to start the KSQL command shell.
      We will run a local engine in the CLI.
      Also please note &lt;code&gt;--net&lt;/code&gt; parameter. This guarantees that KSQL container runs in the same network as Debezium containers and allows proper DNS resolution.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker run -it --net tutorial_default confluentinc/ksql-cli ksql-cli local --bootstrap-server kafka:9092&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;First we will list all Kafka topics that exist in the broker:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; LIST TOPICS;
      
       Kafka Topic                         | Registered | Partitions | Partition Replicas
      ------------------------------------------------------------------------------------
       connect-status                      | false      | 5          | 1
       dbserver                            | false      | 1          | 1
       dbserver.inventory.addresses        | false      | 1          | 1
       dbserver.inventory.customers        | false      | 1          | 1
       dbserver.inventory.orders           | false      | 1          | 1
       dbserver.inventory.products         | false      | 1          | 1
       dbserver.inventory.products_on_hand | false      | 1          | 1
       ksql__commands                      | true       | 1          | 1
       my_connect_configs                  | false      | 1          | 1
       my_connect_offsets                  | false      | 25         | 1
       schema-changes.inventory            | false      | 1          | 1&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The topics we are interested in are &lt;code&gt;dbserver.inventory.orders&lt;/code&gt; and &lt;code&gt;dbserver.inventory.customers&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;KSQL processing by default starts with &lt;code&gt;latest&lt;/code&gt; offsets.
      We want to process the events already in the topics so we switch processing from &lt;code&gt;earliest&lt;/code&gt; offsets.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; SET 'auto.offset.reset' = 'earliest';
      Successfully changed local property 'auto.offset.reset' from 'null' to 'earliest'&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;First we need to create streams from the topics containing the Debezium data change events.
      A &lt;em&gt;stream&lt;/em&gt; in KSQL and Kafka Streams terminology is an unbounded incoming data set with no state.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; CREATE STREAM orders_from_debezium (order_number integer, order_date string, purchaser integer, quantity integer, product_id integer) WITH (KAFKA_TOPIC='dbserver.inventory.orders',VALUE_FORMAT='json');
      
       Message
      ----------------
       Stream created
      ksql&amp;gt;
      ksql&amp;gt; CREATE STREAM customers_from_debezium (id integer, first_name string, last_name string, email string) WITH (KAFKA_TOPIC='dbserver.inventory.customers',VALUE_FORMAT='json');
      
       Message
      ----------------
       Stream created&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;partitioning&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#partitioning&quot;&gt;&lt;/a&gt;Partitioning&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Our deployment uses only one partition per topic.
      In a production system there will likely be multiple partitions per topic and we need to ensure that all events belonging to our aggregated object end up in the same partition.
      The natural partioning in our case is per customer id.
      We are going to repartition the &lt;code&gt;orders_from_debezium&lt;/code&gt; stream according to the &lt;code&gt;purchaser&lt;/code&gt; field that contains the customer id.
      The repartitioned data are written into a new topic &lt;code&gt;ORDERS_REPART&lt;/code&gt;:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; CREATE STREAM orders WITH (KAFKA_TOPIC='ORDERS_REPART',VALUE_FORMAT='json',PARTITIONS=1) as SELECT * FROM orders_from_debezium PARTITION BY PURCHASER;
      
       Message
      ----------------------------
       Stream created and running
      ksql&amp;gt; LIST TOPICS;
      
       Kafka Topic                         | Registered | Partitions | Partition Replicas
      ------------------------------------------------------------------------------------
      ...
       ORDERS_REPART                       | true       | 1          | 1
      ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We are going to execute the same operation for customers too.
      It is necessary for two reasons:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;The current key is a struct that contains a field named &lt;code&gt;id&lt;/code&gt; with the customer id.
      This is different from the repartitioned order topic which contains only the &lt;code&gt;id&lt;/code&gt; value as the key, so the partitions would not match.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;When we will create a JOIN later, there is a limitation that requires the key to have the same value as a key field in the table.
      The table field contains a plain value but the key contains a struct so they would not match.
      See &lt;a href=&quot;https://github.com/confluentinc/ksql/issues/749&quot;&gt;this KSQL issue&lt;/a&gt; for more details.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; CREATE STREAM customers_stream WITH (KAFKA_TOPIC='CUSTOMERS_REPART',VALUE_FORMAT='json',PARTITIONS=1) as SELECT * FROM customers_from_debezium PARTITION BY ID;
      
       Message
      ----------------------------
       Stream created and running
      ksql&amp;gt; LIST TOPICS;
      
       Kafka Topic                         | Registered | Partitions | Partition Replicas
      ------------------------------------------------------------------------------------
      ...
       CUSTOMERS_REPART                    | true       | 1          | 1
      ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To verify that records have a new key and are thus repartioned we can issue few statements to compare the results:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; SELECT * FROM orders_from_debezium LIMIT 1;
      1524034842810 | {&quot;order_number&quot;:10001} | 10001 | 16816 | 1001 | 1 | 102
      LIMIT reached for the partition.
      Query terminated
      ksql&amp;gt; SELECT * FROM orders LIMIT 1;
      1524034842810 | 1001 | 10001 | 16816 | 1001 | 1 | 102
      LIMIT reached for the partition.
      Query terminated&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The second column contains &lt;code&gt;ROWKEY&lt;/code&gt; which is the key of the message.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect3&quot;&gt;
      &lt;h4 id=&quot;customer_order_join&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#customer_order_join&quot;&gt;&lt;/a&gt;Customer/order join&lt;/h4&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So far we were only declaring streams as an unbounded stateless data set.
      In our use case the &lt;code&gt;order&lt;/code&gt; is really an event that comes and goes.
      But &lt;code&gt;customer&lt;/code&gt; is an entity that can be updated and generally is a part of a state fo the system.
      Such quality is represented in KSQL or Kafka Streams as table.
      We are going to create a table of customers from the topic containing repartitioned customers.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; CREATE TABLE customers (id integer, first_name string, last_name string, email string) WITH (KAFKA_TOPIC='CUSTOMERS_REPART',VALUE_FORMAT='json',KEY='id');
      
       Message
      ---------------
       Table created&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now we have everything in place to make a join between customer and its orders and create a query that will monitor incoming orders and list them with associated customer fields.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; SELECT order_number,quantity,customers.first_name,customers.last_name FROM orders left join customers on orders.purchaser=customers.id;
      10001 | 1 | Sally | Thomas
      10002 | 2 | George | Bailey
      10003 | 2 | George | Bailey
      10004 | 1 | Edward | Walker&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s apply a few changes to the database, which will result in corresponding CDC events being emitted by Debezium:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose -f docker-compose-mysql.yaml exec mysql bash -c 'mysql -u $MYSQL_USER -p$MYSQL_PASSWORD inventory'
      
      mysql&amp;gt; INSERT INTO orders VALUES(default,NOW(), 1003,5,101);
      Query OK, 1 row affected, 1 warning (0.02 sec)
      
      mysql&amp;gt; UPDATE customers SET first_name='Annie' WHERE id=1004;
      Query OK, 1 row affected (0.02 sec)
      Rows matched: 1  Changed: 1  Warnings: 0
      
      mysql&amp;gt; UPDATE orders SET quantity=20 WHERE order_number=10004;
      Query OK, 1 row affected (0.02 sec)
      Rows matched: 1  Changed: 1  Warnings: 0&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You may notice that only changes in the &lt;code&gt;orders&lt;/code&gt; table have triggered changes in the joined stream.
      This is a product of the stream/table join.
      We would need a stream/stream join to trigger changes if any of input streams is modified.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So the final result of the select after the database is modified is&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;10001 | 1 | Sally | Thomas
      10002 | 2 | George | Bailey
      10003 | 2 | George | Bailey
      10004 | 1 | Edward | Walker
      10005 | 5 | Edward | Walker
      10004 | 20 | Edward | Walker&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot;&gt;&lt;/a&gt;Summary&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We have successfully started a KSQL instance. We have mapped KSQL streams to Debezium topics filled by Debezium and made a join between them.
      We have also discussed the problem of repartioning in streaming applications.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to try out this example with Avro encoding and schema registry then you can use our &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/tutorial/docker-compose-mysql-avro.yaml&quot;&gt;Avro example&lt;/a&gt;.
      Also for further details and more advanced usages just refer to the KSQL &lt;a href=&quot;https://github.com/confluentinc/ksql/blob/master/docs/syntax-reference.md&quot;&gt;syntax reference&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In case you need help, have feature requests or would like to share your experiences with this example, please let us know in the comments below.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/03/20/debezium-0-7-5-released/</id>
    <title>Debezium 0.7.5 Is Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2018-03-20T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/03/20/debezium-0-7-5-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="mongodb"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s my pleasure to announce the release of Debezium 0.7.5!
      
      
      This is a bugfix release to the 0.7 release line, which we decided to do while working towards Debezium 0.8.
      Most notably it fixes an unfortunate bug introduced in 0.7.3 (DBZ-663),
      where the internal database history topic of the Debezium MySQL connector could be partly deleted under some specific conditions.
      Please see the dedicated blog post on this issue to find out whether this affects you and what you should do to prevent this issue.
      
      
      Together with this, we released a couple of other fixes and improvements.
      Thanks to Maciej Brynski, the performance of the logical...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.7.5&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This is a bugfix release to the 0.7 release line, which we decided to do while working towards Debezium 0.8.
      Most notably it fixes an unfortunate bug introduced in 0.7.3 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-663&quot;&gt;DBZ-663&lt;/a&gt;),
      where the internal database history topic of the Debezium MySQL connector could be partly deleted under some specific conditions.
      Please see the &lt;a href=&quot;http://debezium.io/2018/03/16/note-on-database-history-topic-configuration/&quot;&gt;dedicated blog post&lt;/a&gt; on this issue to find out whether this affects you and what you should do to prevent this issue.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Together with this, we released a couple of other fixes and improvements.
      Thanks to &lt;a href=&quot;https://github.com/maver1ck&quot;&gt;Maciej Brynski&lt;/a&gt;, the performance of the &lt;a href=&quot;http://debezium.io/docs/configuration/topic-routing&quot;&gt;logical table routing SMT&lt;/a&gt; has been improved significantly (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-655&quot;&gt;DBZ-655&lt;/a&gt;).
      Another fix contributed by Maciej is for &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-646&quot;&gt;DBZ-646&lt;/a&gt; which lets the MySQL connector handle &lt;code&gt;CREATE TABLE&lt;/code&gt; statements for the TokuDB storage engine now.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;And we got some more bugfixes by our fantastic community:
      Long-term community member &lt;a href=&quot;https://github.com/pgoranss&quot;&gt;Peter Goransson&lt;/a&gt; fixed an issue about the snapshot JMX metrics of the MySQL connector,
      which are now also accessible after the snapshot has been completed (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-640&quot;&gt;DBZ-640&lt;/a&gt;).
      &lt;a href=&quot;https://github.com/atongen&quot;&gt;Andrew Tongen&lt;/a&gt; spotted and fixed an issue for the Debezium embedded engine (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-665&quot;&gt;DBZ-665&lt;/a&gt;) which caused offsets to be committed more often than needed.
      And &lt;a href=&quot;https://github.com/matzew&quot;&gt;Matthias Wessendorf&lt;/a&gt; upgraded the Debezium dependencies and Docker images to Apache Kafka 1.0.1 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-647&quot;&gt;DBZ-647&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thank you all for your help!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please refer to the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-4&quot;&gt;change log&lt;/a&gt; for the complete list of changes in Debezium 0.7.5.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/blog/2018/03/07/debezium-0-7-4-released/&quot;&gt;previous release announcement&lt;/a&gt; for the next planned features.
      Due to the unplanned 0.7.5 release, though, the schedule of the next one will likely be extended a little bit.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/03/16/note-on-database-history-topic-configuration/</id>
    <title>A Note On Database History Topic Configuration</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2018-03-16T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/03/16/note-on-database-history-topic-configuration/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="mysql"></category>
    <summary>
      
      
      
      A user of the Debezium connector for MySQL informed us about a potential issue with the configuration of the connector&#8217;s internal database history topic,
      which may cause the deletion of parts of that topic (DBZ-663).
      Please continue reading if you&#8217;re using the Debezium MySQL connector in versions 0.7.3 or 0.7.4.
      
      
      
      
      What is the issue about?
      
      
      In Debezium 0.7.3 we rolled out a feature for creating the database history automatically if it doesn&#8217;t exist yet (DBZ-278).
      While this feature sets the retention time for the topic to an "infinite" period, it doesn&#8217;t specify the "retention.bytes" option for the history topic.
      This may cause parts of the history...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A user of the Debezium connector for MySQL informed us about a potential issue with the configuration of the connector’s internal database history topic,
      which may cause the deletion of parts of that topic (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-663&quot;&gt;DBZ-663&lt;/a&gt;).
      Please continue reading if you’re using the Debezium MySQL connector in versions 0.7.3 or 0.7.4.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_is_the_issue_about&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_is_the_issue_about&quot;&gt;&lt;/a&gt;What is the issue about?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In Debezium 0.7.3 we rolled out a feature for creating the database history automatically if it doesn’t exist yet (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-278&quot;&gt;DBZ-278&lt;/a&gt;).
      While this feature sets the retention time for the topic to an &quot;infinite&quot; period, it doesn’t specify the &quot;retention.bytes&quot; option for the history topic.
      This may cause parts of the history topic to be deleted in case all of the following conditions are met:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;You are using versions 0.7.3 or 0.7.4 of the Debezium connector for MySQL&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The database history topic has been created by the connector (i.e. you haven’t created it yourself)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The broker level option &quot;log.retention.bytes&quot; is set to another value than -1
      (note that the default &lt;strong&gt;is&lt;/strong&gt; -1, in which case things work as intended)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The database history topic grows beyond the threshold configured via &quot;log.retention.bytes&quot;&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If the history topic is incomplete, the connector will fail to recover the database history after a restart of the connector and will not continue with reading the MySQL binlog.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;how_to_prevent_the_issue&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#how_to_prevent_the_issue&quot;&gt;&lt;/a&gt;How to prevent the issue?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You should either create the database history topic yourself with an infinite retention
      or alternatively override the &quot;retention.bytes&quot; configuration for the history topic created by the connector:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&amp;lt;KAFKA_DIR&amp;gt;/bin/kafka-configs.sh \
        --zookeeper zookeeper:2181 \
        --entity-type topics \
        --entity-name &amp;lt;DB_HISTORY_TOPIC&amp;gt; \
        --alter \
        --add-config retention.bytes=-1&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In case parts of the history topic were removed already,
      you can use the snapshot mode &lt;code&gt;schema_only_recovery&lt;/code&gt; for re-creating the history topic in case no schema changes have happened since the last committed offset of the connector.
      Alternatively, a complete new snapshot should be taken, e.g. by setting up a new connector instance.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;next_steps&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#next_steps&quot;&gt;&lt;/a&gt;Next steps&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll release Debezium 0.7.5 with a fix for this issue early next week.
      Note that previously created database history topics should be re-configured as described above.
      Please don’t hesitate to get in touch in the comments below, the chat room or the mailing list in case you have any further questions on this issue.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/</id>
    <title>Creating DDD aggregates with Debezium and Kafka Streams</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2018-03-08T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/" rel="alternate" type="text/html" />
    <author>
      <name>Hans-Peter Grahsl, Gunnar Morling</name>
    </author>
    <category term="discussion"></category>
    <category term="examples"></category>
    <summary>
      
      
      
      Microservice-based architectures can be considered an industry trend and are thus
      often found in enterprise applications lately. One possible way to keep data
      synchronized across multiple services and their backing data stores is to make us of an approach
      called change data capture, or CDC for short.
      
      
      Essentially CDC allows to listen to any modifications which are occurring at one end of a data flow (i.e. the data source)
      and communicate them as change events to other interested parties or storing them into a data sink.
      Instead of doing this in a point-to-point fashion, it&#8217;s advisable to decouple this flow of events
      between data sources and data...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Microservice-based architectures can be considered an industry trend and are thus
      often found in enterprise applications lately. One possible way to keep data
      synchronized across multiple services and their backing data stores is to make us of an approach
      called &lt;a href=&quot;https://vladmihalcea.com/a-beginners-guide-to-cdc-change-data-capture/&quot;&gt;change data capture&lt;/a&gt;, or CDC for short.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Essentially CDC allows to listen to any modifications which are occurring at one end of a data flow (i.e. the data source)
      and communicate them as change events to other interested parties or storing them into a data sink.
      Instead of doing this in a point-to-point fashion, it’s advisable to decouple this flow of events
      between data sources and data sinks. Such a scenario can be implemented based on &lt;a href=&quot;http://debezium.io/&quot;&gt;Debezium&lt;/a&gt;
      and &lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Apache Kafka&lt;/a&gt; with relative ease and effectively no coding.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As an example, consider the following microservice-based architecture of an order management system:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;http://debezium.io/images/msa_streaming.png&quot; alt=&quot;Microservice-based architecture of an order management system&quot; /&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This system comprises three services, &lt;em&gt;Order&lt;/em&gt;, &lt;em&gt;Item&lt;/em&gt; and &lt;em&gt;Stock&lt;/em&gt;.
      If the &lt;em&gt;Order&lt;/em&gt; service receives an order request, it will need information from the other two,
      such as item definitions or the stock count for specific items.
      Instead of making synchronous calls to these services to obtain this information,
      CDC can be used to set up change event streams for the data managed by the &lt;em&gt;Item&lt;/em&gt; and &lt;em&gt;Stock&lt;/em&gt; services.
      The &lt;em&gt;Order&lt;/em&gt; service can subscribe to these event streams and keep a local copy of the relevant item and stock data in its own database.
      This approach helps to decouple the services
      (e.g. no direct impact by service outages)
      and can also be beneficial for overall performance,
      as each service can hold optimized views just of those data items owned by other services which it is interested in.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;how_to_handle_aggregate_objects&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#how_to_handle_aggregate_objects&quot;&gt;&lt;/a&gt;How to Handle Aggregate Objects?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;There are use cases however, where things are a bit more tricky. It is sometimes
      useful to share information across services and data stores by means of so-called
      aggregates, which are a concept/pattern defined by domain-driven design (DDD).
      In general, a &lt;a href=&quot;https://martinfowler.com/bliki/DDD_Aggregate.html&quot;&gt;DDD aggregate&lt;/a&gt; is used
      to transfer state which can be comprised of multiple different domain objects that are
      together treated as a single unit of information.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Concrete examples are:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;strong&gt;customers and their addresses&lt;/strong&gt; which are represented as a customer record &lt;em&gt;aggregate&lt;/em&gt;
      storing a customer and a list of addresses&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;strong&gt;orders and corresponding line items&lt;/strong&gt; which are represented as an order record
      &lt;em&gt;aggregate&lt;/em&gt; storing an order and all its line items&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Chances are that the data of the involved domain objects backing these DDD aggregates are stored in
      separate relations of an RDBMS. When making use of the CDC capabilities currently found
      in Debezium, all changes to domain objects will be independently captured and by default eventually
      reflected in separate Kafka topics, one per RDBMS relation. While this behaviour
      is tremendously helpful for a lot of use cases it can be pretty limiting to others,
      like the DDD aggregate scenario described above.
      Therefore, this blog post explores how DDD aggregates can be built based on Debezium CDC events,
      using the &lt;a href=&quot;https://kafka.apache.org/documentation/streams/&quot;&gt;Kafka Streams API&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;capturing_change_events_from_a_data_source&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#capturing_change_events_from_a_data_source&quot;&gt;&lt;/a&gt;Capturing Change Events from a Data Source&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The complete source code for this blog post is provided in the Debezium &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/kstreams&quot;&gt;examples repository&lt;/a&gt; on GitHub.
      Begin by cloning this repository and changing into the &lt;em&gt;kstreams&lt;/em&gt; directory:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;git clone https://github.com/debezium/debezium-examples.git
      cd kstreams&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The project provides a Docker Compose file with services for all the components you may already know from the &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;Debezium tutorial&lt;/a&gt;:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://zookeeper.apache.org/&quot;&gt;Apache ZooKeeper&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;A &lt;a href=&quot;https://kafka.apache.org/documentation/#connect&quot;&gt;Kafka Connect&lt;/a&gt; instance with the Debezium CDC connectors&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.mysql.com/&quot;&gt;MySQL&lt;/a&gt; (populated with some test data)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In addition it declares the following services:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.mongodb.com/&quot;&gt;MongoDB&lt;/a&gt; which will be used as a data sink&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Another Kafka Connect instance which will host the MongoDB sink connector&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;A service for running the DDD aggregation process we’re going to build in the following&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll get to those three in a bit, for now let’s prepare the source side of our pipeline:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;export DEBEZIUM_VERSION=0.7
      docker-compose up mysql zookeeper kafka connect_source&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once all services have been started, register an instance of the Debezium MySQL connector by submitting the following JSON document:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
          &quot;name&quot;: &quot;mysql-source&quot;,
          &quot;config&quot;: {
              &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
              &quot;tasks.max&quot;: &quot;1&quot;,
              &quot;database.hostname&quot;: &quot;mysql&quot;,
              &quot;database.port&quot;: &quot;3306&quot;,
              &quot;database.user&quot;: &quot;debezium&quot;,
              &quot;database.password&quot;: &quot;dbz&quot;,
              &quot;database.server.id&quot;: &quot;184054&quot;,
              &quot;database.server.name&quot;: &quot;dbserver1&quot;,
              &quot;table.whitelist&quot;: &quot;inventory.customers,inventory.addresses&quot;,
              &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
              &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot;,
              &quot;transforms&quot;: &quot;unwrap&quot;,
              &quot;transforms.unwrap.type&quot;:&quot;io.debezium.transforms.UnwrapFromEnvelope&quot;,
              &quot;transforms.unwrap.drop.tombstones&quot;:&quot;false&quot;
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To do so, run the following curl command:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ -d @mysql-source.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This sets up the connector for the specified database, using the given credentials.
      For our purposes we’re only interested in changes to the &lt;code&gt;customers&lt;/code&gt; and &lt;code&gt;addresses&lt;/code&gt; tables,
      hence the &lt;code&gt;table.whitelist&lt;/code&gt; property is given to just select these two tables.
      Another noteworthy thing is the &quot;unwrap&quot; transform that is applied.
      By default, Debezium’s CDC events would contain the old and new state of changed rows and some additional metadata on the source of the change.
      By applying the &lt;a href=&quot;http://debezium.io/docs/configuration/event-flattening/&quot;&gt;UnwrapFromEnvelope&lt;/a&gt; SMT (single message transformation),
      only the new state will be propagated into the corresponding Kafka topics.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We can take a look at them once the connector has been deployed and finished its initial snapshot of the two captured tables:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker-compose exec kafka /kafka/bin/kafka-console-consumer.sh \
          --bootstrap-server kafka:9092 \
          --from-beginning \
          --property print.key=true \
          --topic dbserver1.inventory.customers # or dbserver1.inventory.addresses&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;E.g. you should see the following output&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;(formatted and omitting the schema information for the sake of readability) for the topic with customer changes:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;{
          &quot;schema&quot;: { ... },
          &quot;payload&quot;: {
              &quot;id&quot;: 1001
          }
      }
      {
          &quot;schema&quot;: { ... },
          &quot;payload&quot;: {
              &quot;id&quot;: 1001,
              &quot;first_name&quot;: &quot;Sally&quot;,
              &quot;last_name&quot;: &quot;Thomas&quot;,
              &quot;email&quot;: &quot;sally.thomas@acme.com&quot;
          }
      }
      ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;building_ddd_aggregates&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#building_ddd_aggregates&quot;&gt;&lt;/a&gt;Building DDD Aggregates&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The KStreams application is going to process data from the two Kafka topics. These topics
      receive CDC events based on the customers and addresses relations found in MySQL, each of which has its
      corresponding Jackson-annotated POJO (Customer and Address), enriched by a field holding the CDC event type (i.e. UPSERT/DELETE).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Since the Kafka topic records are in Debezium JSON format with unwrapped envelopes, a special &lt;strong&gt;SerDe&lt;/strong&gt;
      has been written in order to be able to read/write these records using their POJO or Debezium event representation respectively.
      While the serializer simply converts the POJOs into JSON using Jackson, the deserializer is a &quot;hybrid&quot;
      one, being able to deserialize from either Debezium CDC events or jsonified POJOs.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With that in place, the KStreams topology to create and maintain DDD aggregates on-the-fly can be built as follows:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;customers_topic_parent&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#customers_topic_parent&quot;&gt;&lt;/a&gt;Customers Topic (&quot;parent&quot;)&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;All the customer records are simply read from the customer topic into a &lt;strong&gt;KTable&lt;/strong&gt; which will automatically maintain
      the latest state per customer according to the record key (i.e. the customer’s PK)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;KTable&amp;lt;DefaultId, Customer&amp;gt; customerTable =
              builder.table(parentTopic, Consumed.with(defaultIdSerde,customerSerde));&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;addresses_topic_children&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#addresses_topic_children&quot;&gt;&lt;/a&gt;Addresses Topic (&quot;children&quot;)&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For the address records the processing is a bit more involved and needs several steps. First, all the address
      records are read into a &lt;strong&gt;KStream&lt;/strong&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;KStream&amp;lt;DefaultId, Address&amp;gt; addressStream = builder.stream(childrenTopic,
              Consumed.with(defaultIdSerde, addressSerde));&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Second, a 'pseudo' grouping of these address records is done based on their keys (the original primary key in the relation),
      During this step the relationships towards the corresponding customer records are maintained. This effectively allows to keep
      track which address record belongs to which customer record, even in the light of address record deletions.
      To achieve this an additional &lt;em&gt;LatestAddress&lt;/em&gt; POJO is introduced which allows to store the latest known PK &amp;lt;→ FK
      relation in addition to the &lt;em&gt;Address&lt;/em&gt; record itself.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;KTable&amp;lt;DefaultId,LatestAddress&amp;gt; tempTable = addressStream
              .groupByKey(Serialized.with(defaultIdSerde, addressSerde))
              .aggregate(
                      () -&amp;gt; new LatestAddress(),
                      (DefaultId addressId, Address address, LatestAddress latest) -&amp;gt; {
                          latest.update(
                              address, addressId, new DefaultId(address.getCustomer_id()));
                          return latest;
                      },
                      Materialized.&amp;lt;DefaultId,LatestAddress,KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;
                              as(childrenTopic+&quot;_table_temp&quot;)
                                  .withKeySerde(defaultIdSerde)
                                      .withValueSerde(latestAddressSerde)
              );&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Third, the intermediate &lt;strong&gt;KTable&lt;/strong&gt; is again converted to a &lt;strong&gt;KStream&lt;/strong&gt;. The &lt;em&gt;LatestAddress&lt;/em&gt; records are transformed
      to have the customer id (FK relationship) as their new key in order to group them per customer.
      During the grouping step, customer specific addresses are updated which can result in an address
      record being added or deleted. For this purpose, another POJO called &lt;em&gt;Addresses&lt;/em&gt; is introduced, which
      holds a map of address records that gets updated accordingly. The result is a &lt;strong&gt;KTable&lt;/strong&gt; holding the
      most recent &lt;em&gt;Addresses&lt;/em&gt; per customer id.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;KTable&amp;lt;DefaultId, Addresses&amp;gt; addressTable = tempTable.toStream()
              .map((addressId, latestAddress) -&amp;gt;
                  new KeyValue&amp;lt;&amp;gt;(latestAddress.getCustomerId(),latestAddress))
              .groupByKey(Serialized.with(defaultIdSerde,latestAddressSerde))
              .aggregate(
                      () -&amp;gt; new Addresses(),
                      (customerId, latestAddress, addresses) -&amp;gt; {
                          addresses.update(latestAddress);
                          return addresses;
                      },
                      Materialized.&amp;lt;DefaultId,Addresses,KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;
                              as(childrenTopic+&quot;_table_aggregate&quot;)
                                  .withKeySerde(defaultIdSerde)
                                      .withValueSerde(addressesSerde)
              );&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;combining_customers_with_addresses&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#combining_customers_with_addresses&quot;&gt;&lt;/a&gt;Combining Customers With Addresses&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, it’s easy to bring customers and addresses together by &lt;strong&gt;joining the customers KTable with
      the addresses KTable&lt;/strong&gt; and thereby building the DDD aggregates which are represented by the &lt;em&gt;CustomerAddressAggregate&lt;/em&gt; POJO.
      At the end, the KTable changes are written to a KStream, which in turn gets saved into a kafka topic.
      This allows to make use of the resulting DDD aggregates in manifold ways.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;KTable&amp;lt;DefaultId,CustomerAddressAggregate&amp;gt; dddAggregate =
                customerTable.join(addressTable, (customer, addresses) -&amp;gt;
                    customer.get_eventType() == EventType.DELETE ?
                            null :
                            new CustomerAddressAggregate(customer,addresses.getEntries())
                );
      
        dddAggregate.toStream().to(&quot;final_ddd_aggregates&quot;,
                                    Produced.with(defaultIdSerde,(Serde)aggregateSerde));&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;admonitionblock note&quot;&gt;
      &lt;table&gt;
      &lt;tr&gt;
      &lt;td class=&quot;icon&quot;&gt;
      &lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot;&gt;&lt;/i&gt;
      &lt;/td&gt;
      &lt;td class=&quot;content&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Records in the customers KTable might receive a CDC delete event. If so, this can be detected by
      checking the event type field of the customer POJO and e.g. return 'null' instead of a DDD aggregate.
      Such a convention can be helpful whenever consuming parties also need to act to deletions accordingly._&lt;/p&gt;
      &lt;/div&gt;
      &lt;/td&gt;
      &lt;/tr&gt;
      &lt;/table&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;running_the_aggregation_pipeline&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#running_the_aggregation_pipeline&quot;&gt;&lt;/a&gt;Running the Aggregation Pipeline&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Having implemented the aggregation pipeline, it’s time to give it a test run.
      To do so, build the &lt;em&gt;poc-ddd-aggregates&lt;/em&gt; Maven project which contains the complete implementation:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;mvn clean package -f poc-ddd-aggregates/pom.xml&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Then run the &lt;code&gt;aggregator&lt;/code&gt; service from the Compose file which takes the JAR built by this project
      and launches it using the &lt;a href=&quot;https://hub.docker.com/r/fabric8/java-jboss-openjdk8-jdk/&quot;&gt;java-jboss-openjdk8-jdk&lt;/a&gt; base image:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker-compose up -d aggregator&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once the aggregation pipeline is running, we can take a look at the aggregated events using the console consumer:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker-compose exec kafka /kafka/bin/kafka-console-consumer.sh \
          --bootstrap-server kafka:9092 \
          --from-beginning \
          --property print.key=true \
          --topic final_ddd_aggregates&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;transferring_ddd_aggregates_to_data_sinks&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#transferring_ddd_aggregates_to_data_sinks&quot;&gt;&lt;/a&gt;Transferring DDD Aggregates to Data Sinks&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We originally set out to build these DDD aggregates in order to transfer data and synchronize changes between
      a data source (MySQL tables in this case) and a convenient data sink. By definition,
      DDD aggregates are typically complex data structures and therefore it makes perfect sense to write them
      to data stores which offer flexible ways and means to query and/or index them. Talking about NoSQL databases, a
      document store seems the most natural choice with &lt;a href=&quot;https://www.mongodb.com/&quot;&gt;MongoDB&lt;/a&gt; being the leading database
      for such use cases.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to &lt;a href=&quot;https://kafka.apache.org/documentation/#connect&quot;&gt;Kafka Connect&lt;/a&gt; and numerous turn-key ready
      &lt;a href=&quot;https://www.confluent.io/product/connectors/&quot;&gt;connectors&lt;/a&gt; it is almost effortless to get this done.
      Using a &lt;a href=&quot;https://github.com/hpgrahsl/kafka-connect-mongodb&quot;&gt;MongoDB sink connector&lt;/a&gt; from the open-source community,
      it is easy to have the DDD aggregates written into MongoDB. All it needs is a proper configuration which can be posted
      to the &lt;a href=&quot;https://docs.confluent.io/current/connect/restapi.html&quot;&gt;REST API&lt;/a&gt; of Kafka Connect in order to run the connector.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So let’s start MongoDb and another Kafka Connect instance for hosting the sink connector:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker-compose up -d mongodb connect_sink&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In case the DDD aggregates should get written unmodified into MongoDB, a configuration may look as simple as follows:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
          &quot;name&quot;: &quot;mongodb-sink&quot;,
          &quot;config&quot;: {
              &quot;connector.class&quot;: &quot;at.grahsl.kafka.connect.mongodb.MongoDbSinkConnector&quot;,
              &quot;tasks.max&quot;: &quot;1&quot;,
              &quot;topics&quot;: &quot;final_ddd_aggregates&quot;,
              &quot;mongodb.connection.uri&quot;: &quot;mongodb://mongodb:27017/inventory?w=1&amp;amp;journal=true&quot;,
              &quot;mongodb.collection&quot;: &quot;customers_with_addresses&quot;,
              &quot;mongodb.document.id.strategy&quot;: &quot;at.grahsl.kafka.connect.mongodb.processor.id.strategy.FullKeyStrategy&quot;,
              &quot;mongodb.delete.on.null.values&quot;: true
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As with the source connector, deploy the connector using curl:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; -H  &quot;Content-Type:application/json&quot; http://localhost:8084/connectors/ -d @mongodb-sink.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This connector will consume messages from the &quot;final_ddd_aggregates&quot; Kafka topic and
      write them as &lt;strong&gt;MongoDB documents&lt;/strong&gt; into the &quot;customers_with_addresses&quot; collection.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You can take a look by firing up a Mongo shell and querying the collection’s contents:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker-compose exec mongodb bash -c 'mongo inventory'
      
      &amp;gt; db.customers_with_addresses.find().pretty()&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
          &quot;_id&quot;: {
              &quot;id&quot;: &quot;1001&quot;
          },
          &quot;addresses&quot;: [
              {
                  &quot;zip&quot;: &quot;76036&quot;,
                  &quot;_eventType&quot;: &quot;UPSERT&quot;,
                  &quot;city&quot;: &quot;Euless&quot;,
                  &quot;street&quot;: &quot;3183 Moore Avenue&quot;,
                  &quot;id&quot;: &quot;10&quot;,
                  &quot;state&quot;: &quot;Texas&quot;,
                  &quot;customer_id&quot;: &quot;1001&quot;,
                  &quot;type&quot;: &quot;SHIPPING&quot;
              },
              {
                  &quot;zip&quot;: &quot;17116&quot;,
                  &quot;_eventType&quot;: &quot;UPSERT&quot;,
                  &quot;city&quot;: &quot;Harrisburg&quot;,
                  &quot;street&quot;: &quot;2389 Hidden Valley Road&quot;,
                  &quot;id&quot;: &quot;11&quot;,
                  &quot;state&quot;: &quot;Pennsylvania&quot;,
                  &quot;customer_id&quot;: &quot;1001&quot;,
                  &quot;type&quot;: &quot;BILLING&quot;
              }
          ],
          &quot;customer&quot;: {
              &quot;_eventType&quot;: &quot;UPSERT&quot;,
              &quot;last_name&quot;: &quot;Thomas&quot;,
              &quot;id&quot;: &quot;1001&quot;,
              &quot;first_name&quot;: &quot;Sally&quot;,
              &quot;email&quot;: &quot;sally.thomas@acme.com&quot;
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Due to the combination of the data in a single document some parts aren’t needed or redundant. To get rid of any
      unwanted data (e.g. _eventType, customer_id of each address sub-document) it would also be possible
      to adapt the configuration in order to blacklist said fields.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, you update some customer or address data in the MySQL source database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER -p$MYSQL_PASSWORD inventory'
      
      mysql&amp;gt; update customers set first_name= &quot;Sarah&quot; where id = 1001;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Shortly thereafter, you should see that the corresponding aggregate document in MongoDB has been updated accordingly.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;drawbacks_and_limitations&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#drawbacks_and_limitations&quot;&gt;&lt;/a&gt;Drawbacks and Limitations&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While this first version for creating DDD aggregates from table-based CDC events basically works, it is very important to understand its current limitations:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;not generically applicable thus needs custom code for POJOs and intermediate types&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;cannot be scaled across multiple instances as is due to missing but necessary data repartitioning prior to processing&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;limited to building aggregates based on a single JOIN between 1:N relationships&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;resulting DDD aggregates are eventually consistent, meaning that it is possible for them to temporarily exhibit intermediate state before converging&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The first few can be addressed with a reasonable amount of work on the KStreams application. The last one,
      dealing with the eventually consistent nature of resulting DDD aggregates is much harder to correct
      and will require some efforts at Debezium’s own CDC mechanism.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;outlook&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#outlook&quot;&gt;&lt;/a&gt;Outlook&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In this post we described an approach for creating aggregated events from Debezium’s CDC events.
      In a follow-up blog post we may dive a bit more into the topic of how to be able to horizontally scale
      the DDD creation by running multiple KStreams aggregator instances. For that purpose, the data needs proper
      re-partitioning before running the topology. In addition, it could be interesting to look into
      a somewhat more generic version which only needs custom classes to the describe the two main POJOs involved.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We also thought about providing a ready-to-use component which would work in a generic way
      (based on Connect records, i.e. not tied to a specific serialization format such as JSON) and
      could be set up as a configurable stand-alone process running given aggregations.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also on the topic of dealing with eventual consistency we got some ideas,
      but those will need some more exploration and investigation for sure.
      Stay tuned!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’d love to hear about your feedback on the topic of event aggreation.
      If you got any ideas or thoughts on the subject,
      please get in touch by posting a comment below or sending a message to our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/03/07/debezium-0-7-4-released/</id>
    <title>Debezium 0.7.4 Is Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2018-03-07T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/03/07/debezium-0-7-4-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s my pleasure to announce the release of Debezium 0.7.4!
      
      
      Continuing the 0.7 release line, this new version brings several bug fixes and a handful of new features.
      We recommend this upgrade to all users.
      When upgrading from earlier versions,
      please check out the release notes of all versions between the one you&#8217;re currently on and 0.7.4 in order to learn about any steps potentially required for upgrading.
      
      
      
      
      New features
      
      
      In terms of new features, there&#8217;s a new mode for handling decimal columns in Postgres and MySQL (DBZ-611).
      By setting the decimal.handling.mode connector option to string, Debezium will emit decimal and numeric columns as Strings.
      That oftentimes is...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.7.4&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Continuing the 0.7 release line, this new version brings several bug fixes and a handful of new features.
      We recommend this upgrade to all users.
      When upgrading from earlier versions,
      please check out the &lt;a href=&quot;http://debezium.io/docs/releases/&quot;&gt;release notes&lt;/a&gt; of all versions between the one you’re currently on and 0.7.4 in order to learn about any steps potentially required for upgrading.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;new_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_features&quot;&gt;&lt;/a&gt;New features&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In terms of new features, there’s a new mode for handling decimal columns in Postgres and MySQL (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-611&quot;&gt;DBZ-611&lt;/a&gt;).
      By setting the &lt;code&gt;decimal.handling.mode&lt;/code&gt; connector option to &lt;code&gt;string&lt;/code&gt;, Debezium will emit decimal and numeric columns as Strings.
      That oftentimes is easier to handle for consumers than the byte-array based representation used by default, while keeping the full precision.
      As a bonus, &lt;code&gt;string&lt;/code&gt; also allows to convey the special numeric values &lt;code&gt;NaN&lt;/code&gt; and &lt;code&gt;Infinity&lt;/code&gt; as supported by Postgres.
      Note that this functionality required an update to Debezium’s logical decoding plug-in which runs within the Postgres database server.
      This plug-in must be upgraded to the new version &lt;em&gt;before&lt;/em&gt; upgrading the Debezium Postgres connector.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Speaking of byte arrays, the &lt;code&gt;BYTEA&lt;/code&gt; column type in Postgres is now also supported (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-605&quot;&gt;DBZ-605&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For the MySQL connector, there’s a new option to the snapshotting routine: &lt;code&gt;snapshot.locking.mode&lt;/code&gt; (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-602&quot;&gt;DBZ-602&lt;/a&gt;).
      By setting this to &lt;code&gt;NONE&lt;/code&gt;, this option allows to skip any table locks during snapshotting.
      This should be used if and only if you’re absolutely sure that the tables don’t undergo structural changes (columns added, removed etc.)
      while the snapshot is taken.
      But if that’s guaranteed, the new mode can be a useful tool for increasing overall system performance, as writes by concurrent processes won’t be blocked.
      That’s especially useful on environments such as Amazon RDS, where the connector otherwise would be required to keep a lock for the entirety of the snapshot.
      The new option supersedes the existing &lt;code&gt;snapshot.minimal.locks&lt;/code&gt; option.
      Please see the connector documentation for &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/#connector-properties&quot;&gt;the details&lt;/a&gt;.
      This feature was contributed by our community member &lt;a href=&quot;https://github.com/Crim&quot;&gt;Stephen Powis&lt;/a&gt;; many thanks to you!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;bug_fixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bug_fixes&quot;&gt;&lt;/a&gt;Bug Fixes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;0.7.4 brings multiple fixes related to how numeric columns are handled.
      E.g. columns without scale couldn’t correctly be processed by the MySQL connector during binlog reading (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-615&quot;&gt;DBZ-615&lt;/a&gt;).
      That’s fixed now.
      And when using the Postgres connector, arbitrary precision column values are correctly converted into change data message fields now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-351&quot;&gt;DBZ-351&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We also noticed a regression introduced in Debezium 0.6:
      the field schema for &lt;code&gt;NUMERIC&lt;/code&gt; columns was always marked as optional, also if that column was actually declared as &lt;code&gt;NOT NULL&lt;/code&gt;.
      The same affected geo-spatial array types on Postgres as supported as of Debezium 0.7.
      This has been fixed with &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-635&quot;&gt;DBZ-635&lt;/a&gt;.
      We don’t expect any impact on consumers by this change
      (just as before, they’ll always get a value for such field, only its schema won’t be incorrectly marked as optional any more).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-4&quot;&gt;full change log&lt;/a&gt; for more details and the complete list of issues fixed in Debezium 0.7.4.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Following our three weeks release cadence, the next Debezium release is planned for March 28th.
      We got some exciting changes in the works for that:
      if things go as planned, we’ll release the first version of our Oracle connector (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-20&quot;&gt;DBZ-20&lt;/a&gt;).
      This will be based on the Oracle XStream API in the first iteration and not support snapshots yet.
      But we felt it’d make sense to roll out this connector incrementally, so to get out the new feature early on and collect feedback on it.
      We’ve also planned to explore alternatives to using the XStream API in future releases.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Another great new feature will be &lt;a href=&quot;http://www.reactive-streams.org/&quot;&gt;Reactive Streams&lt;/a&gt; support (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-566&quot;&gt;DBZ-566&lt;/a&gt;).
      Based on top of the existing &lt;a href=&quot;http://debezium.io/docs/embedded/&quot;&gt;embedded mode&lt;/a&gt;,
      this will make it very easy to consume change data events using Reactive Streams implementations such as RxJava 2, the Java 9 Flow API and many more.
      It’ll also be very useful to consume change events in reactive frameworks such as Vert.x.
      We’re really looking forward to shipping this feature and already have a pending &lt;a href=&quot;https://github.com/debezium/debezium/pull/458&quot;&gt;pull request&lt;/a&gt; for it.
      If you like, take a look and let us know about your feedback!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please also check out our &lt;a href=&quot;http://debezium.io/docs/roadmap&quot;&gt;roadmap&lt;/a&gt; for the coming months of Debezium’s development.
      This is our current plan for the things we’ll work on,
      but it’s not cast in stone, so please tell us about your feature requests by sending a message to our Google group.
      We’re looking forward to your feedback!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/02/15/debezium-0-7-3-released/</id>
    <title>Debezium 0.7.3 Is Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2018-02-15T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/02/15/debezium-0-7-3-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m very happy to announce the release of Debezium 0.7.3!
      
      
      This is primarily a bugfix release, but we&#8217;ve also added a handful of smaller new features.
      It&#8217;s a recommended upgrade for all users.
      When upgrading from earlier versions,
      please check out the release notes of all versions between the one your&#8217;re currently on and 0.7.3 in order to learn about any steps potentially required for upgrading.
      
      
      Let&#8217;s take a closer look at some of the new features.
      
      
      
      
      All Connectors
      
      
      Using the new connector option tombstones.on.delete you can now control whether upon record deletions a tombstone event should be emitted or not
      (DBZ-582).
      Doing so is usually the right thing...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m very happy to announce the release of Debezium &lt;strong&gt;0.7.3&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This is primarily a bugfix release, but we’ve also added a handful of smaller new features.
      It’s a recommended upgrade for all users.
      When upgrading from earlier versions,
      please check out the &lt;a href=&quot;http://debezium.io/docs/releases/&quot;&gt;release notes&lt;/a&gt; of all versions between the one your’re currently on and 0.7.3 in order to learn about any steps potentially required for upgrading.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s take a closer look at some of the new features.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;all_connectors&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#all_connectors&quot;&gt;&lt;/a&gt;All Connectors&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Using the new connector option &lt;code&gt;tombstones.on.delete&lt;/code&gt; you can now control whether upon record deletions a tombstone event should be emitted or not
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-582&quot;&gt;DBZ-582&lt;/a&gt;).
      Doing so is usually the right thing and thus remains the default behaviour.
      But disabling tombstones may be desirable in certain situations,
      and this gets a bit easier now using that option
      (before you’d have to use an SMT - single message transform -, which for instance isn’t supported when using Debezium’s embedded mode).
      This feature was contributed by our community member &lt;a href=&quot;https://github.com/rliwoch&quot;&gt;Raf Liwoch&lt;/a&gt;. Thanks!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also spent some time on a few operational aspects:
      The &lt;code&gt;sourceInfo&lt;/code&gt; element of Debezium’s change data messages contains a new field representing the version of the connector that created the message
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-593&quot;&gt;DBZ-593&lt;/a&gt;).
      This lets message consumers take specific action based on the version.
      For instance this can be helpful where a new Debezium release fixes a bug, which consumers could work around so far.
      Now, after the update to that new Debezium version, that workaround should not be applied anymore.
      The version field will allow consumers to decide whether to apply the workaround or not.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The names of all the threads managed by Debezium are now structured in the form of &quot;debezium-&amp;lt;connector&amp;gt;-…​&quot;
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-587&quot;&gt;DBZ-587&lt;/a&gt;).
      This helps with identifying Debezium’s threads when analyzing thread dumps for instance.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;postgres_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#postgres_connector&quot;&gt;&lt;/a&gt;Postgres Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Here we’ve focused on improving the support for array types:
      besides fixing a bug related to numeric arrays (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-577&quot;&gt;DBZ-577&lt;/a&gt;)
      we’ve also completed the support for the PostGIS types (which was introduced in 0.7.2),
      allowing you to capture array columns of types &lt;code&gt;GEOMETRY&lt;/code&gt; and &lt;code&gt;GEOGRAPHY&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Snapshots are now correctly interruptable (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-586&quot;&gt;DBZ-586&lt;/a&gt;)
      and the connector will correctly handle the case where after a restart it should continue from a WAL position which isn’t available any more:
      it’ll stop, requiring you to do a new snapshot (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-590&quot;&gt;DBZ-590&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mysql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mysql_connector&quot;&gt;&lt;/a&gt;MySQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The MySQL connector can create the DB history topic automatically, if needed
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-278&quot;&gt;DBZ-278&lt;/a&gt;).
      This means you don’t have to create that topic yourself and you also don’t need to rely on Kafka’s automatic topic creation any longer
      (any change data topics will automatically be created by Kafka Connect).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also the connector can optionally emit messages to a dedicated heartbeat topic in a configurable interval
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-220&quot;&gt;DBZ-220&lt;/a&gt;).
      This comes in handy in situations where you only want to capture tables with low traffic,
      while other tables in the database are changed more frequently.
      In that case, no messages would have been emitted to Kafka Connect for a long time,
      and thus no offset would have been committed either.
      This could have caused trouble when restarting the connector: it wanted to resume from the last comitted offset,
      which may not be available in the binlogs any longer.
      But as the captured tables didn’t change, it actually wouldn’t be necessary to resume from such old binlog position.
      This all is avoided by emitting messages to the heartbeat topic regularly, which causes the last offset the connector has seen to be committed.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll roll out this change to the other connectors, too, in future releases.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-3&quot;&gt;full change log&lt;/a&gt; for more details and the complete list of issues fixed in Debezium 0.7.3.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The next release is scheduled for March 7th.
      We’ll still have to decide whether that will be 0.7.4 or 0.8.0, depending on how far we are by then with our work on the Oracle connector
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-137&quot;&gt;DBZ-137&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please also our &lt;a href=&quot;http://debezium.io/docs/roadmap&quot;&gt;roadmap&lt;/a&gt; describing our ideas for future development of Debezium.
      This is our current thinking of the things we’d like to tackle in the coming months,
      but it’s not cast in stone, so please let us know about your feature requests by sending a message to our Google group.
      We’re looking forward to your feedback!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/01/25/debezium-0-7-2-released/</id>
    <title>Debezium 0.7.2 Is Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2018-01-25T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/01/25/debezium-0-7-2-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s my pleasure to announce the release of Debezium 0.7.2!
      
      
      Amongst the new features there&#8217;s support for geo-spatial types,
      a new snapshotting mode for recovering a lost DB history topic for the MySQL connector,
      and a message transformation for converting MongoDB change events into a structure which can be consumed by many more sink connectors.
      And of course we fixed a whole lot of bugs, too.
      
      
      Debezium 0.7.2 is a drop-in replacement for previous 0.7.x versions.
      When upgrading from versions earlier than 0.7.0,
      please check out the release notes of all 0.7.x releases to learn about any steps potentially required for upgrading.
      
      
      A big thank you goes out...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.7.2&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Amongst the new features there’s support for geo-spatial types,
      a new snapshotting mode for recovering a lost DB history topic for the MySQL connector,
      and a message transformation for converting MongoDB change events into a structure which can be consumed by many more sink connectors.
      And of course we fixed a whole lot of bugs, too.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium 0.7.2 is a drop-in replacement for previous 0.7.x versions.
      When upgrading from versions earlier than 0.7.0,
      please check out the &lt;a href=&quot;http://debezium.io/docs/releases/&quot;&gt;release notes&lt;/a&gt; of all 0.7.x releases to learn about any steps potentially required for upgrading.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A big thank you goes out to our fantastic community members for their hard work on this release:
      &lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/notxcain&quot;&gt;Denis Mikhaylov&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/pgoranss&quot;&gt;Peter Goransson&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/rcoup&quot;&gt;Robert Coup&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/sairam881990&quot;&gt;Sairam Polavarapu&lt;/a&gt; and
      &lt;a href=&quot;https://github.com/tombentley&quot;&gt;Tom Bentley&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now let’s take a closer look at some of new features.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mysql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mysql_connector&quot;&gt;&lt;/a&gt;MySQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The biggest change of the MySQL connector is &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-507&quot;&gt;support for geo-spatial column types&lt;/a&gt; such as &lt;code&gt;GEOMETRY&lt;/code&gt;, &lt;code&gt;POLYGON&lt;/code&gt;, &lt;code&gt;MULTIPOINT&lt;/code&gt; etc.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;There are two new logical field types — &lt;code&gt;io.debezium.data.geometry.Geometry&lt;/code&gt; and &lt;code&gt;io.debezium.data.geometry.Geography&lt;/code&gt; — for representing geo-spatial columns in change data messages.
      These types represent geo-spatial data via WKB (&quot;well-known binary&quot;) and SRID (coordinate reference system identifier),
      allowing downstream consumers to interpret the change events using any existing library with support for parsing WKB.
      A blog post with more details on this will follow soon.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-443&quot;&gt;new snapshotting mode&lt;/a&gt; &lt;code&gt;schema_only_recovery&lt;/code&gt; comes in handy
      when for some reason you lost (parts of) the DB history topic used by the MySQL connector.
      It’s also useful if you’d like to compact that topic by re-creating it.
      Please refer to the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;connector documentation&lt;/a&gt; for the details of this mode,
      esp. when it’s safe (and when not) to make use of it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Another new feature related to managing the size of the DB history topic is &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-541&quot;&gt;the option&lt;/a&gt; to control
      whether to include all DDL events or only those pertaining to tables captured as per the whitelist/blacklist configuration.
      Again, check out the connector docs to learn more about the specifics of that setting.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, we fixed a few shortcomings of the MySQL DDL parser (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-524&quot;&gt;DBZ-524&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-530&quot;&gt;DBZ-530&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;postgresql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#postgresql_connector&quot;&gt;&lt;/a&gt;PostgreSQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Similar to the MySQL connector, there’s largely improved support for geo-spatial columns in Postgres now.
      More specifically, PostGIS column types can be represented in change data events now.
      Thanks a lot for Robert Coup who contributed this feature!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also the support for Postgres &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-506&quot;&gt;array columns&lt;/a&gt; has been expanded,
      e.g. we now support to track changes to &lt;code&gt;VARCHAR&lt;/code&gt; and &lt;code&gt;DATE&lt;/code&gt; array columns.
      Note that the connector doesn’t yet work with  geo-spatial array columns (should you ever have those),
      but this &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-571&quot;&gt;should be added&lt;/a&gt; soon, too.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to include just a subset of the rows of a captured table in snapshots, you may like the ability to &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-506&quot;&gt;specify
      dedicated SELECT statements&lt;/a&gt; to do so.
      For instance this can be used to exclude any logically deleted records — which you can recognize based on some flag in that table — from the snapshot.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A few bugs in this connector where reported and fixed by community members, too,
      e.g. the connector can be &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-519&quot;&gt;correctly paused&lt;/a&gt; now (thanks, Andrey Pustovetov),
      and we fixed an issue which could potentially have committed an &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-521&quot;&gt;incorrect offset&lt;/a&gt; to Kafka Connect (thanks, Thon Mekathikom).&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mongodb_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mongodb_connector&quot;&gt;&lt;/a&gt;MongoDB Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’ve ever compared the structures of change events emitted by the Debezium RDBMS connectors (MySQL, Postgres) and the MongoDB connector,
      you’ll know that the message structure of the latter is a bit different than the others.
      Due to the schemaless nature of MongoDB, the change events essentially contain a String with a JSON representation of the applied insert or patch.
      This structure cannot be consumed by existing sink connectors, such as the Confluent connectors for JDBC or Elasticsearch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This gets &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-409&quot;&gt;possible now&lt;/a&gt; by means of a newly added single message transformation (SMT),
      which parses these JSON strings and creates a structured Kafka Connect record from it (thanks, Sairam Polavarapu!).
      When applying this SMT to the JDBC sink connector, you can now stream data changes from MongoDB to any supported relational database.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note that this SMT is work-in-progress, details of its emitted message structure may still change.
      Also there are some inherent limitations to what can be achieved with it, if you e.g. have arrays in your MongoDB documents,
      the record created by this SMT will be structured accordingly, but many sink connectors cannot process such structure.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We have some ideas for further development here, e.g. there could be &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-561&quot;&gt;an option&lt;/a&gt; for flattening out (non-array) nested structures,
      so that e.g. &lt;code&gt;{ &quot;address&quot; { &quot;street&quot; : &quot;...&quot; } }&lt;/code&gt; would be represented as &lt;code&gt;address_street&lt;/code&gt;,
      which then could be consumed by sink connectors expecting a flat structure.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The new SMT is described in detail in &lt;a href=&quot;http://debezium.io/docs/configuration/mongodb-event-flattening&quot;&gt;our docs&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-2&quot;&gt;full change log&lt;/a&gt; for more details and the complete list of issues fixed in Debezium 0.7.2.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The 0.7.3 release is scheduled for February 14th.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll focus on some more bug fixes, also we’re working on having Debezium regulary emit &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-409&quot;&gt;heartbeat messages&lt;/a&gt; to a dedicated topic.
      This will be practical for diagnostic purposes but also help to regularly trigger commits of the offset in Kafka Connect.
      That’s beneficial in certain situations when capturing tables which only very infrequently change.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also worked out &lt;a href=&quot;http://debezium.io/docs/roadmap&quot;&gt;a roadmap&lt;/a&gt; describing our ideas for future work on Debezium, going beyond the next bugfix releases.
      While nothing is cast in stone, this is our idea of the features to add in the coming months.
      If you miss anything important on this roadmap, please tell us either in the comments below or send a message to our Google group.
      Looking forward to your feedback!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/01/17/streaming-to-elasticsearch/</id>
    <title>Streaming Data Changes from Your Database to Elasticsearch</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2018-01-17T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/01/17/streaming-to-elasticsearch/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="elasticsearch"></category>
    <category term="smt"></category>
    <category term="example"></category>
    <summary>
      
      
      
      We wish all the best to the Debezium community for 2018!
      
      
      While we&#8217;re working on the 0.7.2 release, we thought we&#8217;d publish another post describing an end-to-end data streaming use case based on Debezium.
      We have seen how to set up a change data stream to a downstream database a few weeks ago.
      In this blog post we will follow the same approach to stream the data to an Elasticsearch server to leverage its excellent capabilities for full-text search on our data.
      But to make the matter a little bit more interesting, we will stream the data to both, a PostgreSQL database and Elasticsearch,...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We wish all the best to the Debezium community for 2018!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While we’re working on the 0.7.2 release, we thought we’d publish another post describing an end-to-end data streaming use case based on Debezium.
      We have seen how to set up a change data stream to a downstream database &lt;a href=&quot;http://debezium.io/blog/2017/09/25/streaming-to-another-database&quot;&gt;a few weeks ago&lt;/a&gt;.
      In this blog post we will follow the same approach to stream the data to an &lt;a href=&quot;https://www.elastic.co/&quot;&gt;Elasticsearch&lt;/a&gt; server to leverage its excellent capabilities for full-text search on our data.
      But to make the matter a little bit more interesting, we will stream the data to both, a PostgreSQL database and Elasticsearch, so we will optimize access to the data via the SQL query language as well as via full-text search.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;topology&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#topology&quot;&gt;&lt;/a&gt;Topology&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Here’s a diagram that shows how the data is flowing through our distributed system.
      First, the Debezium MySQL connector is continuously capturing the changes from the MySQL database, and sending the changes for each table to separate Kafka topics.
      Then, the Confluent &lt;a href=&quot;https://docs.confluent.io/current/connect/connect-jdbc/docs/sink_connector.html&quot;&gt;JDBC sink connector&lt;/a&gt; is continuously reading those topics and writing the events into the PostgreSQL database.
      And, at the same time, the Confluent &lt;a href=&quot;https://github.com/confluentinc/kafka-connect-elasticsearch&quot;&gt;Elasticsearch connector&lt;/a&gt; is continuously reading those same topics and writing the events into Elasticsearch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt; &lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div id=&quot;img-general&quot; class=&quot;imageblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;http://debezium.io/images/dbz-to-multiple.svg&quot; alt=&quot;Scenario topology&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;title&quot;&gt;Figure 1: A general topology&lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt; &lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We are going to deploy these components into several different processes.
      In this example, we’ll deploy all three connectors to a single Kafka Connect instance that will write to and read from Kafka on behalf of all of the connectors
      (in production you might need to keep the connectors separated to achieve better performance).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt; &lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div id=&quot;img-general&quot; class=&quot;imageblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;http://debezium.io/images/dbz-to-multiple-simplified.svg&quot; alt=&quot;Scenario topology&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;title&quot;&gt;Figure 2: A simplified topology&lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;configuration&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#configuration&quot;&gt;&lt;/a&gt;Configuration&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We will use this &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/unwrap-smt&quot;&gt;Docker Compose file&lt;/a&gt; for a fast deployment of the demo.
      The deployment consists of the following Docker images:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://hub.docker.com/r/debezium/zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://hub.docker.com/r/debezium/kafka/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;An &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/unwrap-smt/debezium-jdbc&quot;&gt;enriched&lt;/a&gt; Kafka Connect / Debezium &lt;a href=&quot;https://hub.docker.com/r/debezium/connect/&quot;&gt;image&lt;/a&gt; with a few changes:&lt;/p&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;PostgreSQL JDBC driver placed into &lt;em&gt;/kafka/libs&lt;/em&gt; directory&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The Confluent JDBC connector placed into &lt;em&gt;/kafka/connect/kafka-connect-jdbc&lt;/em&gt; directory&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Pre-populated MySQL as used in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Empty PostgreSQL&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Empty Elasticsearch&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The message format is not the same for the Debezium source connector and the JDBC and Elasticsearch connectors as they are developed separately and each focuses on slightly different objectives.
      Debezium emits a more complex event structure so that it captures all of the information available.
      In particular, the change events contain the old and the new state of a changed record.
      Both sink connectors on the other hand expect a simple message that just represents the record state to be written.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium’s &lt;a href=&quot;http://debezium.io/docs/configuration/event-flattening/&quot;&gt;UnwrapFromEnvelope&lt;/a&gt; single message transformation (SMT) collapses the complex change event structure into the same row-based format expected by the two sink connectors and effectively acts as a &lt;a href=&quot;http://www.enterpriseintegrationpatterns.com/patterns/messaging/MessageTranslator.html&quot;&gt;message translator&lt;/a&gt; between the two aforementioned formats.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;example&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#example&quot;&gt;&lt;/a&gt;Example&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s move directly to our example as that’s where the changes are visible.
      First of all we need to deploy all components:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;export DEBEZIUM_VERSION=0.7
      docker-compose up&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When all components are started we are going to register the Elasticsearch Sink connector writing into the Elasticsearch instance.
      We want to use the same key (primary id) in the source and both PostgreSQL and Elasticsearch:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; \
          -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ \
          -d @es-sink.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re using this registration request:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
        {
          &quot;name&quot;: &quot;elastic-sink&quot;,
          &quot;config&quot;: {
            &quot;connector.class&quot;:
                &quot;io.confluent.connect.elasticsearch.ElasticsearchSinkConnector&quot;,
            &quot;tasks.max&quot;: &quot;1&quot;,
            &quot;topics&quot;: &quot;customers&quot;,
            &quot;connection.url&quot;: &quot;http://elastic:9200&quot;,
            &quot;transforms&quot;: &quot;unwrap,key&quot;,
            &quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.UnwrapFromEnvelope&quot;,        (1)
            &quot;transforms.key.type&quot;: &quot;org.apache.kafka.connect.transforms.ExtractField$Key&quot;,(2)
            &quot;transforms.key.field&quot;: &quot;id&quot;,                                                 (2)
            &quot;key.ignore&quot;: &quot;false&quot;,                                                        (3)
            &quot;type.name&quot;: &quot;customer&quot;                                                       (4)
          }
        }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The request configures these options:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;extracting only the new row’s state from Debezium’s change data message&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;extracting the &lt;code&gt;id&lt;/code&gt; field from the key &lt;code&gt;struct&lt;/code&gt;, then the same key is used for the source and both destinations.
      This is to address the fact that the Elasticsearch connector only supports numeric types and &lt;code&gt;string&lt;/code&gt; as keys. If we do not extract the &lt;code&gt;id&lt;/code&gt; the messages will be filtered out by the connector because of unknown key type.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;use key from the event instead of generating a synthetic one&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;type under which the events will be registered in Elasticsearch&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Next we are going to register the JDBC Sink connector writing into PostgreSQL database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; \
          -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ \
          -d @jdbc-sink.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, the source connector must be set up:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; \
          -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ \
          -d @source.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s check if the databases and the search server are synchronized.
      All the rows of the &lt;code&gt;customers&lt;/code&gt; table should be found in the source database (MySQL) as well as the target database (Postgres) and Elasticsearch:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER  -p$MYSQL_PASSWORD inventory -e &quot;select * from customers&quot;'
      +------+------------+-----------+-----------------------+
      | id   | first_name | last_name | email                 |
      +------+------------+-----------+-----------------------+
      | 1001 | Sally      | Thomas    | sally.thomas@acme.com |
      | 1002 | George     | Bailey    | gbailey@foobar.com    |
      | 1003 | Edward     | Walker    | ed@walker.com         |
      | 1004 | Anne       | Kretchmar | annek@noanswer.org    |
      +------+------------+-----------+-----------------------+&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec postgres bash -c 'psql -U $POSTGRES_USER $POSTGRES_DB -c &quot;select * from customers&quot;'
       last_name |  id  | first_name |         email
      -----------+------+------------+-----------------------
       Thomas    | 1001 | Sally      | sally.thomas@acme.com
       Bailey    | 1002 | George     | gbailey@foobar.com
       Walker    | 1003 | Edward     | ed@walker.com
       Kretchmar | 1004 | Anne       | annek@noanswer.org&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl 'http://localhost:9200/customers/_search?pretty'
      {
        &quot;took&quot; : 42,
        &quot;timed_out&quot; : false,
        &quot;_shards&quot; : {
          &quot;total&quot; : 5,
          &quot;successful&quot; : 5,
          &quot;failed&quot; : 0
        },
        &quot;hits&quot; : {
          &quot;total&quot; : 4,
          &quot;max_score&quot; : 1.0,
          &quot;hits&quot; : [
            {
              &quot;_index&quot; : &quot;customers&quot;,
              &quot;_type&quot; : &quot;customer&quot;,
              &quot;_id&quot; : &quot;1001&quot;,
              &quot;_score&quot; : 1.0,
              &quot;_source&quot; : {
                &quot;id&quot; : 1001,
                &quot;first_name&quot; : &quot;Sally&quot;,
                &quot;last_name&quot; : &quot;Thomas&quot;,
                &quot;email&quot; : &quot;sally.thomas@acme.com&quot;
              }
            },
            {
              &quot;_index&quot; : &quot;customers&quot;,
              &quot;_type&quot; : &quot;customer&quot;,
              &quot;_id&quot; : &quot;1004&quot;,
              &quot;_score&quot; : 1.0,
              &quot;_source&quot; : {
                &quot;id&quot; : 1004,
                &quot;first_name&quot; : &quot;Anne&quot;,
                &quot;last_name&quot; : &quot;Kretchmar&quot;,
                &quot;email&quot; : &quot;annek@noanswer.org&quot;
              }
            },
            {
              &quot;_index&quot; : &quot;customers&quot;,
              &quot;_type&quot; : &quot;customer&quot;,
              &quot;_id&quot; : &quot;1002&quot;,
              &quot;_score&quot; : 1.0,
              &quot;_source&quot; : {
                &quot;id&quot; : 1002,
                &quot;first_name&quot; : &quot;George&quot;,
                &quot;last_name&quot; : &quot;Bailey&quot;,
                &quot;email&quot; : &quot;gbailey@foobar.com&quot;
              }
            },
            {
              &quot;_index&quot; : &quot;customers&quot;,
              &quot;_type&quot; : &quot;customer&quot;,
              &quot;_id&quot; : &quot;1003&quot;,
              &quot;_score&quot; : 1.0,
              &quot;_source&quot; : {
                &quot;id&quot; : 1003,
                &quot;first_name&quot; : &quot;Edward&quot;,
                &quot;last_name&quot; : &quot;Walker&quot;,
                &quot;email&quot; : &quot;ed@walker.com&quot;
              }
            }
          ]
        }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With the connectors still running, we can add a new row to the MySQL database and then check that it was replicated into both the PostgreSQL database and Elasticsearch:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER  -p$MYSQL_PASSWORD inventory'
      
      mysql&amp;gt; insert into customers values(default, 'John', 'Doe', 'john.doe@example.com');
      Query OK, 1 row affected (0.02 sec)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec -postgres bash -c 'psql -U $POSTGRES_USER $POSTGRES_DB -c &quot;select * from customers&quot;'
       last_name |  id  | first_name |         email
      -----------+------+------------+-----------------------
      ...
      Doe        | 1005 | John       | john.doe@example.com
      (5 rows)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl 'http://localhost:9200/customers/_search?pretty'
      ...
      {
        &quot;_index&quot; : &quot;customers&quot;,
        &quot;_type&quot; : &quot;customer&quot;,
        &quot;_id&quot; : &quot;1005&quot;,
        &quot;_score&quot; : 1.0,
        &quot;_source&quot; : {
          &quot;id&quot; : 1005,
          &quot;first_name&quot; : &quot;John&quot;,
          &quot;last_name&quot; : &quot;Doe&quot;,
          &quot;email&quot; : &quot;john.doe@example.com&quot;
        }
      }
      ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot;&gt;&lt;/a&gt;Summary&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We set up a complex streaming data pipeline to synchronize a MySQL database with another database and also with an Elasticsearch instance.
      We managed to keep the same identifier across all systems which allows us to correlate records across the system as a whole.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Propagating data changes from a primary database in near realtime to a search engine such as Elasticsearch enables many interesting use cases.
      Besides different applications of fulltext search one could for instance also think about creating dashboards and all kinds of visualizations using &lt;a href=&quot;https://www.elastic.co/de/products/kibana&quot;&gt;Kibana&lt;/a&gt;, to gain further insight into the data.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to try out this set-up yourself, just clone the project from our &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/unwrap-smt&quot;&gt;examples repo&lt;/a&gt;.
      In case you need help, have feature requests or would like to share your experiences with this pipeline, please let us know in the comments below.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/12/20/debezium-0-7-1-released/</id>
    <title>Debezium 0.7.1 Is Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2017-12-20T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/12/20/debezium-0-7-1-released/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      Just last few days before Christmas we are releasing Debezium  0.7.1!
      This is a bugfix release that fixes few annoying issues that were found during first rounds of use of Debezium 0.7 by our community.
      All issues relate to either newly provided wal2json support or reduced risk of internal race condition improvement.
      
      
      Robert Coup has found a performance regression in situations when 0.7.0 was used with old version of Protobuf decoder.
      
      
      Suraj Savita (and others) has found an issue when our code failed to correctly detect it runs with Amazon RDS wal2json plug-in.
      We are outsmarted by the JDBC driver internals and included a...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Just last few days before Christmas we are releasing Debezium  &lt;strong&gt;0.7.1&lt;/strong&gt;!
      This is a bugfix release that fixes few annoying issues that were found during first rounds of use of Debezium 0.7 by our community.
      All issues relate to either newly provided wal2json support or reduced risk of internal race condition improvement.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/rcoup&quot;&gt;Robert Coup&lt;/a&gt; has found a &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-512&quot;&gt;performance regression&lt;/a&gt; in situations when 0.7.0 was used with old version of Protobuf decoder.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Suraj Savita (and others) has found an issue when our code failed to &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-513&quot;&gt;correctly detect&lt;/a&gt; it runs with Amazon RDS wal2json plug-in.
      We are outsmarted by the JDBC driver internals and included a distinct plugin decoder name &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-517&quot;&gt;wal2json_rds&lt;/a&gt; that bypasses detection routine and by default expects it runs against Amazon RDS instance. This mode should be used only with RDS instances.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We have also gathered feedback from first tries to run with Amazon RDS and included &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql/#amazon-rds&quot;&gt;a short section&lt;/a&gt; in our documentation on this topic.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/12/15/debezium-0-7-0-released/</id>
    <title>Debezium 0.7.0 Is Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2017-12-15T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/12/15/debezium-0-7-0-released/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s not Christmas yet, but we already got a present for you: Debezium  0.7.0 is here, full of new features as well as many bug fixes!
      A big thank you goes out to all the community members who contributed to this release.
      It is very encouraging for us to see not only more and more issues and feature requests being reported, but also pull requests coming in.
      
      
      Note that this release comes with a small number of changes to the default mappings for some data types.
      We try to avoid this sort of changes as far as possible, but in some cases it...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s not Christmas yet, but we already got a present for you: Debezium  &lt;strong&gt;0.7.0&lt;/strong&gt; is here, full of new features as well as many bug fixes!
      A big thank you goes out to all the community members who contributed to this release.
      It is very encouraging for us to see not only more and more issues and feature requests being reported, but also pull requests coming in.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note that this release comes with a small number of changes to the default mappings for some data types.
      We try to avoid this sort of changes as far as possible, but in some cases it is required,
      e.g. if the previous mapping could have caused potential value losses.
      Please see below for the details and also make sure to check out the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-0&quot;&gt;full change log&lt;/a&gt; which describes these changes in detail.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now let’s take a closer look at some of new features.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;based_on_apache_kafka_1_0&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#based_on_apache_kafka_1_0&quot;&gt;&lt;/a&gt;Based on Apache Kafka 1.0&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A few weeks ago the Apache Kafka team has &lt;a href=&quot;https://www.confluent.io/blog/apache-kafka-goes-1-0/&quot;&gt;released version 1.0.0&lt;/a&gt;.
      This was an important milestone for the Kafka community,
      and we now can happily declare that Debezium is &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-432&quot;&gt;built&lt;/a&gt; against and runs on that Apache Kafka version.
      Our Docker images were also &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-433&quot;&gt;promoted&lt;/a&gt; to contain Apache Kafka and Kafka Connect 1.0.0.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;postgresql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#postgresql_connector&quot;&gt;&lt;/a&gt;PostgreSQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The big news for the PostgreSQL connector is that it now &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-258&quot;&gt;supports&lt;/a&gt; the &lt;a href=&quot;https://github.com/eulerto/wal2json&quot;&gt;wal2json&lt;/a&gt; logical decoding plugin as an alternative to the existing &lt;a href=&quot;https://github.com/debezium/postgres-decoderbufs&quot;&gt;DecoderBufs plug-in&lt;/a&gt;.
      This means that you now can use Debezium to stream changes out of PostgreSQL on &lt;a href=&quot;https://aws.amazon.com/rds/postgresql/&quot;&gt;Amazon RDS&lt;/a&gt;, as wal2json is the logical decoding plugin used in this environment.
      Many thanks to &lt;a href=&quot;https://github.com/rcoup&quot;&gt;Robert Coup&lt;/a&gt; who significantly contributed to this feature.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Working on this plug-in, we noticed that there was a &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-379&quot;&gt;potential race condition&lt;/a&gt; when it comes to applying changes to the schema of captured tables.
      In that case it could have happened that a number of messages pertaining to data changes done before the schema change were emitted using the new schema.
      With the exception of a few corner cases (which are described &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-0&quot;&gt;here&lt;/a&gt;), this has been addressed when using Debezium’s own DecoderBufs plug-in.
      So it’s highly recommended to upgrade the DecoderBufs plug-in to the new version before upgrading the Debezium connector.
      We’ve also worked closely with the author of the wal2json plug-in (big thanks for the quick help!) to prevent the issue when using the wal2json plug-in.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While the Debezium Docker images for Postgres already come with the latest version of DecoderBufs and wal2json,
      RDS for now is still using an older version of wal2json.
      Until this has been updated, special attention must be paid when applying schema changes to captured tables.
      Please see &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-0&quot;&gt;the changelog&lt;/a&gt; for a in-depth description of this issue and ways to mitigate it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;There are new daily running &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-495&quot;&gt;CI jobs&lt;/a&gt; that verify that the wal2json plugin passes our test suite.
      For the foreseeable future we’ll support both, wal2json as well as the existing DecoderBufs plug-in.
      The latter should be more efficient due to the usage of the Protocol Buffers binary format,
      whereas the former comes in handy for RDS or other cloud environments where you don’t have control over the installed logical decoding plug-ins, but wal2json is available.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In other news on the Postgres connector, &lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt; discovered and proposed a fix for a &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-501&quot;&gt;multi-threading bug&lt;/a&gt; that could have put the connector into an undefined state if a rebalance in the Connect cluster was triggered during snapshotting.
      Thanks, Andrey!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mysql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mysql_connector&quot;&gt;&lt;/a&gt;MySQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In the MySQL connector we’ve fixed two issues which affect the default mapping of certain column types.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Following up to the new &lt;code&gt;BIGINT UNSIGNED&lt;/code&gt; mapping introduced in &lt;a href=&quot;http://debezium.io/blog/2017/10/26/debezium-0-6-1-released/&quot;&gt;Debezium 0.6.1&lt;/a&gt;, this type is now encoded as &lt;code&gt;int64&lt;/code&gt; in Debezium messages &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-461&quot;&gt;by default&lt;/a&gt; as it is easier for (polyglot) clients to work with.
      This is a reasonable mapping for the vast majority of cases.
      Only when using values &amp;gt; 2^63, you should switch it back to the &lt;code&gt;Decimal&lt;/code&gt; logical type
      which is a bit more cumbersome to handle, though.
      This should be a rare situation, as MySQL &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/numeric-type-overview.html&quot;&gt;advices against&lt;/a&gt; using unsigned values &amp;gt; 2^63 due to potential value losses when performing DB-side calculations.
      Please see the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;connector documentation&lt;/a&gt; for the details.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/rk3rn3r&quot;&gt;Rene Kerner&lt;/a&gt; has improved the support for the MySQL &lt;code&gt;TIME&lt;/code&gt; type.
      MySQL allows to store values larger than &lt;code&gt;23:59:59&lt;/code&gt; in such columns, and the type &lt;code&gt;int32&lt;/code&gt; which was previously used for &lt;code&gt;TIME(0-3)&lt;/code&gt; columns isn’t enough to convey the entire possible value range.
      Therefore all &lt;code&gt;TIME&lt;/code&gt; columns in MySQL are by default represented as &lt;code&gt;int64&lt;/code&gt; now,
      using the &lt;code&gt;io.debezium.time.MicroTime&lt;/code&gt; logical type, i.e. the value represents micro-seconds.
      If needed, you can switch to the previous mapping by setting &lt;code&gt;time.precision.mode&lt;/code&gt; to &lt;code&gt;adaptive&lt;/code&gt;,
      but you should only do so if you’re sure that you only ever will have values that fit into &lt;code&gt;int32&lt;/code&gt;.
      This option is only kept for a transitioning period and will be removed in a future release.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Recently we got a &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-390&quot;&gt;report&lt;/a&gt; that MySQL’s binlog can contain &lt;code&gt;ROLLBACK&lt;/code&gt; statements and thus transactions that are actually not committed.
      Of course no data change messages should be emitted in this situation.
      This e.g. can be the case when temporary tables are dropped.
      So we introduced a &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-406&quot;&gt;look-ahead buffer&lt;/a&gt; functionality that reads the binlog by transaction and excludes those that were rolled back.
      This feature should be considered incubating and is disabled by default for the time being.
      We’d like to gather your feedback on this, so if you’d benefit from this feature, please give it a try and let us know if you run into any issues.
      For further details please refer to the &lt;code&gt;binlog.buffer.size&lt;/code&gt; setting in the MySQL connector docs.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/ainagy&quot;&gt;Andras Istvan Nagy&lt;/a&gt; came with the idea and &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-349&quot;&gt;implemented&lt;/a&gt; a way for explicitly selecting the rows from each table that will be part of the snapshotting process.
      This can for instance be very useful if you work with soft deletes and would like to exclude all logically deleted records from snapshotting.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-0&quot;&gt;full change log&lt;/a&gt; for more details and the complete list of fixed issues.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium 0.7.1 release is planned to be out roughly two weeks after Christmas.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It will contain a new SMT that will unwind MongoDB change events into a regular JSON consumable by sink connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A big overhaul of &lt;code&gt;GEOMETRY&lt;/code&gt; types is in progress.
      When completed, all &lt;code&gt;GEOMETRY&lt;/code&gt; types will be supported by both MySQL and PostgreSQL connectors and they will be available in standard &lt;code&gt;WKB&lt;/code&gt; format for easy consumption by polyglot clients.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;There is ongoing work for the MySQL connector to allow dynamic update of &lt;code&gt;table.whitelist&lt;/code&gt; option.
      This will allow the user to re-configure the set of tables captured without need to re-create connector.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to contribute, please let us know.
      We’re happy about any help and will work with you to get you started quickly.
      Check out the details below on how to get in touch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/11/15/debezium-0-6-2-released/</id>
    <title>Debezium 0.6.2 Is Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2017-11-15T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/11/15/debezium-0-6-2-released/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We are accelerating! Three weeks after the 0.6.1 release, the Debezium team is bringing Debezium 0.6.2 to you!
      
      
      This release revolves mostly around bug fixes, but there are a few new features, too.
      Let&#8217;s take a closer look at some of the changes.
      
      
      
      
      PostgreSQL Connector
      
      
      The big news for the Postgres connector is that Debezium now runs against PostgreSQL 10 thanks to a contribution from Scofield Xu.
      As a part of this change we are providing a Docker Image with PostgreSQL 10, too, and we have set up a daily run of our integration tests against it.
      
      
      If you are building Postgres yourself using the Debezium...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We are accelerating! Three weeks after the 0.6.1 release, the Debezium team is bringing &lt;strong&gt;Debezium 0.6.2&lt;/strong&gt; to you!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release revolves mostly around bug fixes, but there are a few new features, too.
      Let’s take a closer look at some of the changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;postgresql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#postgresql_connector&quot;&gt;&lt;/a&gt;PostgreSQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The big news for the Postgres connector is that Debezium now runs against &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-424&quot;&gt;PostgreSQL 10&lt;/a&gt; thanks to a contribution from &lt;a href=&quot;https://github.com/ScofieldXu&quot;&gt;Scofield Xu&lt;/a&gt;.
      As a part of this change we are providing a &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-426&quot;&gt;Docker Image&lt;/a&gt; with PostgreSQL 10, too, and we have set up a &lt;a href=&quot;http://ci.hibernate.org/view/Debezium/job/debezium-postgresql-10-test/&quot;&gt;daily run&lt;/a&gt; of our integration tests against it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you are building Postgres yourself using the Debezium &lt;a href=&quot;https://github.com/debezium/postgres-decoderbufs&quot;&gt;logical decoding plug-in&lt;/a&gt;,
      you can save quite some megabytes if you don’t need the PostGIS geometric extension:
      thanks to the work by &lt;a href=&quot;https://github.com/QazerLab&quot;&gt;Danila Kiver&lt;/a&gt;, it’s now possible to omit that extension.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mysql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mysql_connector&quot;&gt;&lt;/a&gt;MySQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve received multiple reports related to parsing MySQL DDL statements, e.g. there were a few specific invocations of the &lt;code&gt;ALTER TABLE&lt;/code&gt; statement which weren’t handled correctly.
      Those as well as a few other parser bugs have been fixed.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you work with the &lt;code&gt;TIMESTAMP&lt;/code&gt; column type and your Kafka Connect server isn’t using UTC as timezone, then the fix for &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-260&quot;&gt;DBZ-260&lt;/a&gt; is applying to you.
      In that case, the ISO 8601 formatted String emitted by Debezium would have, incorrectly, contained the UTC date and time plus the zone offset (as per the time zone the Kafka Connect server is located in) before.
      Whereas now it will contain the date and time adjusted to the zone offset.
      This may require adjustments to to downstream consumers if they were relying on the previous, incorrect behavior.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-217&quot;&gt;DBZ-217&lt;/a&gt; gives you more flexibility for handling corrupt events encountered in the MySQL binlog.
      By default, the connector will stop at the problematic event in such case.
      But you now also have the option to just log the event and its position and continue the processing after it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Another nice improvement for the MySQL connector is a much reduced CPU load after the snapshot has been completed, when using the &quot;snapshot only&quot; mode (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-396&quot;&gt;DBZ-396&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mongodb_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mongodb_connector&quot;&gt;&lt;/a&gt;MongoDB Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This connector received an important fix applying when more than one thread is used to performing the initial snapshot (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-438&quot;&gt;DBZ-438&lt;/a&gt;).
      Before, it could happen that single messages got lost during snapshotting which is fixed now.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;examples_and_docker_images&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#examples_and_docker_images&quot;&gt;&lt;/a&gt;Examples and Docker Images&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We have expanded our examples repository with &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/tutorial#using-mysql-and-the-avro-message-format&quot;&gt;an Avro example&lt;/a&gt;,
      which may be interesting to you if you’d like to not work with JSON messages but rather the compact Avro binary format and the Confluent schema registry.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As a part of our release process we are now creating &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-418&quot;&gt;micro tags&lt;/a&gt; for our Docker images for every released version.
      While tags in the format &lt;code&gt;x.y.z&lt;/code&gt; are fixed in time, tags in the format &lt;code&gt;x.y&lt;/code&gt; are rolling updates and always point to the latest micro release of that image.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-6-2&quot;&gt;full change log&lt;/a&gt; for more details and the complete list of fixed issues.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium 0.7 release is planned to be out in two to three weeks from now.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It will contain the move to Apache Kafka 1.0.0 and bring support for the wal2json logical decoding plug-in for Postgres.
      This will eventually allow to use the Debezium Postgres connector on Amazon RDS (once the correct wal2json version is available there).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In parallel, the work around handling updates to the whitelist configuration of the MySQL connector continues (it may be ready for 0.7.0),
      and so does the work on the Oracle connector (which will be shipping in a future release).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to contribute, please let us know.
      We’re happy about any help and will work with you to get you started quickly.
      Check out the details below on how to get in touch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/11/11/debezium-at-devoxx-belgium/</id>
    <title>Debezium at Devoxx Belgium</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2017-11-11T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/11/11/debezium-at-devoxx-belgium/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="introduction"></category>
    <category term="presentation"></category>
    <summary>
      
      
      
      Debezium&#8217;s project lead Gunnar Morling gave a few talks during recent Devoxx Belgium 2017.
      One of his talks was dedicated to Debezium and change data capture in general.
      
      
      If you are interested in those topics and you want to obtain a fast and simple introduction to it, do not hesitate and watch the talk.
      Batteries and demo included!
      
      
      
      
      
      The slide deck is available, too:
      
      
      
      
      
      
      
      About Debezium
      
      
      Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of Kafka and provides Kafka Connect...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium’s project lead &lt;a href=&quot;https://twitter.com/gunnarmorling&quot;&gt;Gunnar Morling&lt;/a&gt; gave a few talks during recent &lt;a href=&quot;https://cfp.devoxx.be/2017/index.html&quot;&gt;Devoxx Belgium 2017&lt;/a&gt;.
      One of his talks was dedicated to Debezium and change data capture in general.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you are interested in those topics and you want to obtain a fast and simple introduction to it, do not hesitate and watch the talk.
      Batteries and demo included!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;responsive-video&quot;&gt;
      &lt;iframe width=&quot;1600&quot; height=&quot;900&quot; src=&quot;https://www.youtube.com/embed/IOZ2Um6e430?rel=0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The slide deck is &lt;a href=&quot;https://speakerdeck.com/gunnarmorling/streaming-database-changes-with-debezium&quot;&gt;available&lt;/a&gt;, too:&lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div style=&quot;text-align-center&quot;&gt;
      &lt;script async=&quot;&quot; class=&quot;speakerdeck-embed&quot; data-id=&quot;4fb7aa5af1c54d7ea807c9d46fb5b1fa&quot; data-ratio=&quot;1.77777777777778&quot; src=&quot;//speakerdeck.com/assets/embed.js&quot;&gt;&lt;/script&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/10/26/debezium-0-6-1-released/</id>
    <title>Debezium 0.6.1 Is Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2017-10-26T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/10/26/debezium-0-6-1-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      Just shy of a month after the 0.6.0 release, I&#8217;m happy to announce the release of Debezium 0.6.1!
      
      
      This release contains several bugfixes, dependency upgrades and a new option for controlling how BIGINT UNSIGNED columns are conveyed.
      We also expanded the set of Docker images and Docker Compose files accompanying our tutorial, so you can run it now with all the databases we support.
      
      
      Let&#8217;s take a closer look at some of the changes.
      
      
      
      
      New connector option for controlling BIGINT UNSIGNED representation
      
      
      BIGINT UNSIGNED columns from MySQL databases have been represented using Kafka Connect&#8217;s Decimal type until now.
      This type allows to represent all possible values...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Just shy of a month after the 0.6.0 release, I’m happy to announce the release of &lt;strong&gt;Debezium 0.6.1&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release contains several bugfixes, dependency upgrades and a new option for controlling how &lt;code&gt;BIGINT UNSIGNED&lt;/code&gt; columns are conveyed.
      We also expanded the set of Docker images and Docker Compose files accompanying &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;our tutorial&lt;/a&gt;, so you can run it now with all the databases we support.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s take a closer look at some of the changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;new_connector_option_for_controlling_bigint_unsigned_representation&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_connector_option_for_controlling_bigint_unsigned_representation&quot;&gt;&lt;/a&gt;New connector option for controlling BIGINT UNSIGNED representation&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;code&gt;BIGINT UNSIGNED&lt;/code&gt; columns from MySQL databases have been represented using Kafka Connect’s &lt;code&gt;Decimal&lt;/code&gt; type until now.
      This type allows to represent all possible values of such columns, but its based on a byte array, so it can be a bit cumbersome to handle for consumers.
      Therefore we added a new option named &lt;code&gt;bigint.unsigned.handling.mode&lt;/code&gt; to the MySQL connector that allows to represent such columns using &lt;code&gt;long&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For the very most cases that’s the preferable option, only if your column contains values larger than 2^63
      (which &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/numeric-type-overview.html&quot;&gt;MySQL doesn’t recommend&lt;/a&gt; due to potential value losses when performing calculations),
      you should stick to the &lt;code&gt;Decimal&lt;/code&gt; representation.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Using &lt;code&gt;long&lt;/code&gt; will be the default as of Debezium 0.7, for the 0.6.x timeline we decided to go with the previous behavior (i.e. using &lt;code&gt;Decimal&lt;/code&gt;) for the sake of backwards compatibility.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks a lot to &lt;a href=&quot;https://github.com/vultron81&quot;&gt;Ben Williams&lt;/a&gt; who contributed this feature!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;new_example_docker_images_and_docker_compose_files&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_example_docker_images_and_docker_compose_files&quot;&gt;&lt;/a&gt;New example Docker images and Docker Compose files&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In the &lt;a href=&quot;https://github.com/debezium/debezium-examples/&quot;&gt;Debezium examples repository&lt;/a&gt; we now provide &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/tutorial&quot;&gt;Docker Compose files&lt;/a&gt; which let you run the tutorial with all the three databases we currently support, MySQL, Postgres and MongoDB.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Just choose the Compose file for your preferred database and get a all the required components (ZooKeeper, Apache Kafka, Kafka Connect and the database) running within a few seconds.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also deployed Docker images for Postgres and MongoDB to the &lt;a href=&quot;https://hub.docker.com/u/debezium/&quot;&gt;Debezium organization&lt;/a&gt; on Docker Hub, so you got some data to play with.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;version_upgrades&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#version_upgrades&quot;&gt;&lt;/a&gt;Version upgrades&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve upgraded our images from Kafka 0.11.0.0 to &lt;a href=&quot;https://issues.apache.org/jira/projects/KAFKA/versions/12340632&quot;&gt;0.11.0.1&lt;/a&gt;.
      Also the &lt;a href=&quot;https://github.com/shyiko/mysql-binlog-connector-java&quot;&gt;binlog client library&lt;/a&gt; used by the MySQL connector was upgraded from 0.9.0 to 0.13.0.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;bugfixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bugfixes&quot;&gt;&lt;/a&gt;Bugfixes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, several bugs were fixed in 0.6.1.
      E.g. you can now name a column &lt;code&gt;column&lt;/code&gt; in MySQL (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-408&quot;&gt;DBZ-408&lt;/a&gt;),
      generated &lt;code&gt;DROP TEMP TABLE&lt;/code&gt; statements won’t flood the DB history topic (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-395&quot;&gt;DBZ-295&lt;/a&gt;)
      and we’ve fixed a case where the Postgres connector would stop working due to an internal error but fail to report though via the task/connector status (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-380&quot;&gt;DBZ-380&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-6-1&quot;&gt;full change log&lt;/a&gt; for more details and the complete list of fixed issues.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The work on Debezium 0.7 has already begun and we’ve merged the first set of changes.
      You can expect to see support for using the &lt;a href=&quot;https://github.com/eulerto/wal2json&quot;&gt;wal2json&lt;/a&gt; logical decoding plug-in with the Postgres connector, which will finally allow it to use Debezium with Postgres on Amazon RDS!
      We’ve also started our explorations of providing a connector for Oracle (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-20&quot;&gt;DBZ-20&lt;/a&gt;) and hope to report some progress here soon.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While the work on Debezium 0.7 continues, you will likely continue to see one or more 0.6.x bugfix releases.
      We’ve automated the release process as much as possible, making it a breeze to ship a new release and getting fixes into your hands quickly.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to contribute, please let us know.
      We’re happy about any help and will work with you to get you started quickly.
      Check out the details below on how to get in touch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/09/25/streaming-to-another-database/</id>
    <title>Streaming data to a downstream database</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2017-09-25T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/09/25/streaming-to-another-database/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="smt"></category>
    <category term="example"></category>
    <summary>
      
      
      
      In this blog post we will create a simple streaming data pipeline to continuously capture the changes in a MySQL database and replicate them in near real-time into a PostgreSQL database.
      We&#8217;ll show how to do this without writing any code, but instead by using and configuring Kafka Connect, the Debezium MySQL source connector, the Confluent JDBC sink connector, and a few single message transforms (SMTs).
      
      
      This approach of replicating data through Kafka is really useful on its own, but it becomes even more advantageous when we can combine our near real-time streams of data changes with other streams, connectors, and stream...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In this blog post we will create a simple streaming data pipeline to continuously capture the changes in a MySQL database and replicate them in near real-time into a PostgreSQL database.
      We’ll show how to do this without writing any code, but instead by using and configuring Kafka Connect, the Debezium MySQL source connector, the Confluent JDBC sink connector, and a few single message transforms (SMTs).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This approach of replicating data through Kafka is really useful on its own, but it becomes even more advantageous when we can combine our near real-time streams of data changes with other streams, connectors, and stream processing applications.
      A recent &lt;a href=&quot;https://www.confluent.io/blog/simplest-useful-kafka-connect-data-pipeline-world-thereabouts-part-1/&quot;&gt;Confluent blog post series&lt;/a&gt; shows a similar streaming data pipeline but using different connectors and SMTs.
      What’s great about Kafka Connect is that you can mix and match connectors to move data between multiple systems.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We will also demonstrate a new functionality that was released with &lt;a href=&quot;2017/09/21/debezium-0-6-0-released&quot;&gt;Debezium 0.6.0&lt;/a&gt;: a single message transform for &lt;a href=&quot;http://debezium.io/docs/configuration/event-flattening&quot;&gt;CDC Event Flattening&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;topology&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#topology&quot;&gt;&lt;/a&gt;Topology&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The general topology for this scenario is displayed on the following picture:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div id=&quot;img-general&quot; class=&quot;imageblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;http://debezium.io/images/dbz-to-jdbc.svg&quot; alt=&quot;Scenario topology&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;title&quot;&gt;Figure 1: A General topology&lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt; &lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To simplify the setup a little bit, we will use only one Kafka Connect instance that will contain all connectors.
      I.e. this instance will serve as an event producer and an event consumer:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt; &lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div id=&quot;img-general&quot; class=&quot;imageblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;http://debezium.io/images/dbz-to-jdbc-simplified.svg&quot; alt=&quot;Scenario topology&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;title&quot;&gt;Figure 2: A Simplified topology&lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;configuration&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#configuration&quot;&gt;&lt;/a&gt;Configuration&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We will use this &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/unwrap-smt&quot;&gt;compose&lt;/a&gt; for a fast deployment of the demo.
      The deployment consists of following Docker images:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://hub.docker.com/r/debezium/zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://hub.docker.com/r/debezium/kafka/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;An &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/unwrap-smt/debezium-jdbc&quot;&gt;enriched&lt;/a&gt; Kafka Connect / Debezium &lt;a href=&quot;https://hub.docker.com/r/debezium/connect/&quot;&gt;image&lt;/a&gt; with changes&lt;/p&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;PostgreSQL JDBC driver placed into &lt;code&gt;/kafka/libs&lt;/code&gt; directory&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://docs.confluent.io/current/connect/connect-jdbc/docs/index.html&quot;&gt;Kafka Connect JDBC Connector&lt;/a&gt; (developed by &lt;a href=&quot;https://www.confluent.io/&quot;&gt;Confluent&lt;/a&gt;) placed into &lt;code&gt;/kafka/connect/kafka-connect-jdbc&lt;/code&gt; directory&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Pre-populated MySQL used in our &lt;a href=&quot;docs/tutorial&quot;&gt;tutorial&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Empty PostgreSQL&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium MySQL Connector was designed to specifically capture database changes and provide as much information as possible about those events beyond just the new state of each row.
      Meanwhile, the Confluent JDBC Sink Connector was designed to simply convert each message into a database insert/upsert based upon the structure of the message.
      So, the two connectors have different structures for the messages, but they also use different topic naming conventions and behavior of representing deleted records.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;These mismatches in structure and behavior will be common when using connectors that were not designed to work together. But this is something that we can easily deal with, and we discuss how in the next few sections.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;event_format&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#event_format&quot;&gt;&lt;/a&gt;Event format&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium emits events in a complex format that contains all of the information about the captured data change:
      the type of operation, source metadata, the timestamp the event was processed by the connector, and state of the row before and after the change was made.
      Debezium calls this structure an &lt;em&gt;&quot;envelope&quot;&lt;/em&gt;:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
      	&quot;op&quot;: &quot;u&quot;,
      	&quot;source&quot;: {
      		...
      	},
      	&quot;ts_ms&quot; : &quot;...&quot;,
      	&quot;before&quot; : {
      		&quot;field1&quot; : &quot;oldvalue1&quot;,
      		&quot;field2&quot; : &quot;oldvalue2&quot;
      	},
      	&quot;after&quot; : {
      		&quot;field1&quot; : &quot;newvalue1&quot;,
      		&quot;field2&quot; : &quot;newvalue2&quot;
      	}
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Many other Kafka Connect source connectors don’t have the luxury of knowing this much about the changes, and instead use a simpler model where each message directly represents the after state of the row.
      This is also what many sink connectors expect, and the Confluent JDBC Sink Connector is not different:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
      	&quot;field1&quot; : &quot;newvalue1&quot;,
      	&quot;field2&quot; : &quot;newvalue2&quot;
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While we think it’s actually a great thing that Debezium CDC connectors provide as much detail as possible, we also make it easy for you to transform Debezium’s &lt;em&gt;&quot;envelope&quot;&lt;/em&gt; format into the &lt;em&gt;&quot;row&quot;&lt;/em&gt; format that is expected by many other connectors.
      Debezium provides a bridge between those two formats in a form of a &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-66%3A+Single+Message+Transforms+for+Kafka+Connect&quot;&gt;single message transform&lt;/a&gt;.
      The &lt;code&gt;UnwrapFromEnvelope&lt;/code&gt; transformation automatically extracts a new row record and thus effectively &lt;em&gt;flattens&lt;/em&gt; the complex record into a simple one consumable by other connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You can use this SMT on the source connector to transform the message &lt;em&gt;before&lt;/em&gt; it is written to Kafka, or you can instead store the source connector’s richer &lt;em&gt;&quot;envelope&quot;&lt;/em&gt; form of the message in Kafka and use this SMT on the sink connector to transform the message &lt;em&gt;after&lt;/em&gt; it is read from Kafka and before it is passed to the sink connector.
      Both options work, and it just depends on whether you find the envelope form of the message useful for other purposes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In our example we apply the SMT at the sink connector using these configuration properties:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;transforms&quot;: &quot;unwrap&quot;,
      &quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.UnwrapFromEnvelope&quot;,&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;delete_records&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#delete_records&quot;&gt;&lt;/a&gt;Delete records&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When the Debezium connector detects a row is deleted, it creates two event messages: a &lt;em&gt;delete&lt;/em&gt; event and a &lt;em&gt;tombstone&lt;/em&gt; message.
      The &lt;em&gt;delete&lt;/em&gt; message has an envelope with the state of the deleted row in the &lt;code&gt;before&lt;/code&gt; field, and an &lt;code&gt;after&lt;/code&gt; field that is &lt;code&gt;null&lt;/code&gt;.
      The &lt;em&gt;tombstone&lt;/em&gt; message contains same key as the &lt;em&gt;delete&lt;/em&gt; message, but the entire message value is &lt;code&gt;null&lt;/code&gt;, and Kafka’s log compaction utilizes this to know that it can remove any earlier messages with the same key.
      A number of sink connectors, including the Confluent’s JDBC Sink Connector, are not expecting these messages and will instead fail if they see either kind of message.
      The &lt;code&gt;UnwrapFromEnvelope&lt;/code&gt; SMT will by default filter out both &lt;em&gt;delete&lt;/em&gt; and &lt;em&gt;tombstone&lt;/em&gt; records, though you can change this if you’re using the SMT and want to keep one or both of these kinds of messages.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;topic_naming&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#topic_naming&quot;&gt;&lt;/a&gt;Topic naming&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Last but not least there is a difference in naming of topics.
      Debezium uses fully qualified naming for target topics representing each table it manages.
      The naming follows the pattern &lt;code&gt;&amp;lt;logical-name&amp;gt;.&amp;lt;database-name&amp;gt;.&amp;lt;table-name&amp;gt;&lt;/code&gt;.
      Kafka Connect JDBC Connector works with simple names &lt;code&gt;&amp;lt;table-name&amp;gt;&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In more complex scenarios the user may deploy the &lt;a href=&quot;https://kafka.apache.org/documentation/streams/&quot;&gt;Kafka Streams&lt;/a&gt; framework to establish elaborated routing between source and target routes.
      In our example we will use a stock &lt;code&gt;RegexRouter&lt;/code&gt; SMT that would route records created by Debezium into topics named according to JDBC Connector schema.
      Again, we could use this SMT in either the source or sink connectors, but for this example we’re going to use it in the source connector so we can choose the names of the Kafka topics where the records will be written.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;transforms&quot;: &quot;route&quot;,
      &quot;transforms.route.type&quot;: &quot;org.apache.kafka.connect.transforms.RegexRouter&quot;,
      &quot;transforms.route.regex&quot;: &quot;([^.]+)\\.([^.]+)\\.([^.]+)&quot;,
      &quot;transforms.route.replacement&quot;: &quot;$3&quot;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;example&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#example&quot;&gt;&lt;/a&gt;Example&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kick the tires and let’s try our example!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;First of all we need to deploy all components.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;export DEBEZIUM_VERSION=0.6
      docker-compose up&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When all components are started we are going to register the JDBC Sink connector writing into PostgreSQL database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ -d @jdbc-sink.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Using this registration request:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
          &quot;name&quot;: &quot;jdbc-sink&quot;,
          &quot;config&quot;: {
              &quot;connector.class&quot;: &quot;io.confluent.connect.jdbc.JdbcSinkConnector&quot;,
              &quot;tasks.max&quot;: &quot;1&quot;,
              &quot;topics&quot;: &quot;customers&quot;,
              &quot;connection.url&quot;: &quot;jdbc:postgresql://postgres:5432/inventory?user=postgresuser&amp;amp;password=postgrespw&quot;,
              &quot;transforms&quot;: &quot;unwrap&quot;,                                                  (1)
              &quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.UnwrapFromEnvelope&quot;,   (1)
              &quot;auto.create&quot;: &quot;true&quot;,                                                   (2)
              &quot;insert.mode&quot;: &quot;upsert&quot;,                                                 (3)
              &quot;pk.fields&quot;: &quot;id&quot;,                                                       (4)
              &quot;pk.mode&quot;: &quot;record_value&quot;                                                (4)
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The request configures these options:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;unwrapping Debezium’s complex format into a simple one&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;automatically create target tables&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;insert a row if it does not exist or update an existing one&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;identify the primary key stored in Kafka’s record value field&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Then the source connector must be set up:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ -d @source.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Using this registration request:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
          &quot;name&quot;: &quot;inventory-connector&quot;,
          &quot;config&quot;: {
              &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
              &quot;tasks.max&quot;: &quot;1&quot;,
              &quot;database.hostname&quot;: &quot;mysql&quot;,
              &quot;database.port&quot;: &quot;3306&quot;,
              &quot;database.user&quot;: &quot;debezium&quot;,
              &quot;database.password&quot;: &quot;dbz&quot;,
              &quot;database.server.id&quot;: &quot;184054&quot;,
              &quot;database.server.name&quot;: &quot;dbserver1&quot;,                                         (1)
              &quot;database.whitelist&quot;: &quot;inventory&quot;,                                           (2)
              &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
              &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot;,
              &quot;transforms&quot;: &quot;route&quot;,                                                       (3)
              &quot;transforms.route.type&quot;: &quot;org.apache.kafka.connect.transforms.RegexRouter&quot;,  (3)
              &quot;transforms.route.regex&quot;: &quot;([^.]+)\\.([^.]+)\\.([^.]+)&quot;,                     (3)
              &quot;transforms.route.replacement&quot;: &quot;$3&quot;                                         (3)
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The request configures these options:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;logical name of the database&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;the database we want to monitor&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;an SMT which defines a regular expression matching the topic name &lt;code&gt;&amp;lt;logical-name&amp;gt;.&amp;lt;database-name&amp;gt;.&amp;lt;table-name&amp;gt;&lt;/code&gt; and extracts the third part of it as the final topic name&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s check if the databases are synchronized.
      All the rows of the &lt;code&gt;customers&lt;/code&gt; table should be found in the source database (MySQL) as well as the target database (Postgres):&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER  -p$MYSQL_PASSWORD inventory -e &quot;select * from customers&quot;'
      +------+------------+-----------+-----------------------+
      | id   | first_name | last_name | email                 |
      +------+------------+-----------+-----------------------+
      | 1001 | Sally      | Thomas    | sally.thomas@acme.com |
      | 1002 | George     | Bailey    | gbailey@foobar.com    |
      | 1003 | Edward     | Walker    | ed@walker.com         |
      | 1004 | Anne       | Kretchmar | annek@noanswer.org    |
      +------+------------+-----------+-----------------------+
      
      docker-compose exec postgres bash -c 'psql -U $POSTGRES_USER $POSTGRES_DB -c &quot;select * from customers&quot;'
       last_name |  id  | first_name |         email
      -----------+------+------------+-----------------------
       Thomas    | 1001 | Sally      | sally.thomas@acme.com
       Bailey    | 1002 | George     | gbailey@foobar.com
       Walker    | 1003 | Edward     | ed@walker.com
       Kretchmar | 1004 | Anne       | annek@noanswer.org&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With the connectors still running, we can add a new row to the MySQL database and then check that it was replicated into the PostgreSQL database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER  -p$MYSQL_PASSWORD inventory'
      mysql&amp;gt; insert into customers values(default, 'John', 'Doe', 'john.doe@example.com');
      Query OK, 1 row affected (0.02 sec)
      
      docker-compose exec -postgres bash -c 'psql -U $POSTGRES_USER $POSTGRES_DB -c &quot;select * from customers&quot;'
       last_name |  id  | first_name |         email
      -----------+------+------------+-----------------------
      ...
      Doe        | 1005 | John       | john.doe@example.com
      (5 rows)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot;&gt;&lt;/a&gt;Summary&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We set up a simple streaming data pipeline to replicate data in near real-time from a MySQL database to a PostgreSQL database. We accomplished this using Kafka Connect, the Debezium MySQL source connector, the Confluent JDBC sink connector, and a few SMTs — all without having to write any code.
      And since it is a streaming system, it will continue to capture all changes made to the MySQL database and replicating them in near real time.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In a future blog post we will reproduce the same scenario with Elasticsearch as a target for events.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/09/21/debezium-0-6-0-released/</id>
    <title>Debezium 0.6 Is Out</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2017-09-21T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/09/21/debezium-0-6-0-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      What&#8217;s better than getting Java 9?
      Getting Java 9 and a new version of Debezium at the same time!
      So it&#8217;s with great happiness that I&#8217;m announcing the release of Debezium 0.6 today.
      
      
      
      
      What&#8217;s in it?
      
      
      Debezium is now built against and tested with Apache Kafka 0.11.0.
      Also the Debezium Docker images have been updated do that version (DBZ-305).
      You should make sure to read the Kafka update guide when upgrading from an earlier version.
      
      
      To improve integration with existing Kafka sink connectors such as the JDBC sink connector or the Elasticsearch connector,
      Debezium provides a new single message transformation (DBZ-226).
      This SMT converts Debezium&#8217;s CDC event structure into...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;What’s better than getting &lt;a href=&quot;http://openjdk.java.net/projects/jdk9/&quot;&gt;Java 9&lt;/a&gt;?
      Getting Java 9 and a new version of Debezium at the same time!
      So it’s with great happiness that I’m announcing the release of &lt;strong&gt;Debezium 0.6&lt;/strong&gt; today.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_in_it&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_in_it&quot;&gt;&lt;/a&gt;What’s in it?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is now built against and tested with Apache Kafka 0.11.0.
      Also the Debezium Docker images have been updated do that version (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-305&quot;&gt;DBZ-305&lt;/a&gt;).
      You should make sure to read the Kafka &lt;a href=&quot;https://kafka.apache.org/documentation/#upgrade&quot;&gt;update guide&lt;/a&gt; when upgrading from an earlier version.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To improve integration with existing Kafka sink connectors such as the &lt;a href=&quot;https://docs.confluent.io/current/connect/connect-jdbc/docs/sink_connector.html&quot;&gt;JDBC sink connector&lt;/a&gt; or the &lt;a href=&quot;https://docs.confluent.io/current/connect/connect-elasticsearch/docs/elasticsearch_connector.html&quot;&gt;Elasticsearch&lt;/a&gt; connector,
      Debezium provides a new &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/transforms/UnwrapFromEnvelope.java&quot;&gt;single message transformation&lt;/a&gt; (&lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-226&quot;&gt;DBZ-226&lt;/a&gt;).
      This SMT converts Debezium’s CDC event structure into a more conventional structure commonly used in other sink and non-CDC source connectors where the message represents the state of the inserted or updated row, or null in the case of a deleted row.
      This lets your for instance capture the changes from a table in MySQL and update a corresponding table in a Postgres database accordingly.
      We’ll provide a complete example showing the usage of that new SMT in the next few days.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you are doing the Debezium &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;, you will like the new &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/tutorial&quot;&gt;Docker Compose set-up&lt;/a&gt; provided in the examples repo (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-127&quot;&gt;DBZ-127&lt;/a&gt;).
      This lets you start all the required Docker containers with a single command.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;new_connector_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_connector_features&quot;&gt;&lt;/a&gt;New connector features&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now let’s take a look at some of the changes around the specific Debezium connectors.
      The &lt;strong&gt;MySQL connector&lt;/strong&gt; has seen multiple improvements, e.g.:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Snapshot consistency wasn’t guaranteed before in some corner cases (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-210&quot;&gt;DBZ-210&lt;/a&gt;); that’s fixed now&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;DEC and FIXED types supported in the DDL parser (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-359&quot;&gt;DBZ-359&lt;/a&gt;; thanks to &lt;a href=&quot;https://github.com/ooooorz&quot;&gt;Liu Hanlin&lt;/a&gt;!)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;UNION clause supported for ALTER TABLE (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-346&quot;&gt;DBZ-346&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For the &lt;strong&gt;MongoDB connector&lt;/strong&gt;, the way of serializing ids into the key payload of CDC events has changed (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-306&quot;&gt;DBZ-306&lt;/a&gt;).
      The new format allows to read back ids into the correct type.
      We also took the opportunity and made the id field name consistent with the other connectors, i.e. it’s &quot;id&quot; now.
      &lt;strong&gt;Note:&lt;/strong&gt; that change may break existing consumers, so some work on your end may be required, depending on the implementation of your consumer.
      The details are discussed in the &lt;a href=&quot;http://debezium.io/docs/releases/#_breaking_changes&quot;&gt;release notes&lt;/a&gt; and the format of message keys is described in depth in the &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/#change-events-key&quot;&gt;connector documentation&lt;/a&gt;.
      Kudos to &lt;a href=&quot;https://github.com/hpgrahsl&quot;&gt;Hans-Peter Grahsl&lt;/a&gt; who contributed on this feature!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Another nice improvement for this connector is support for SSL connections (&lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-343&quot;&gt;DBZ-343&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, the &lt;strong&gt;Postgres connector&lt;/strong&gt; learned some new tricks, too:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Support for variable-width numeric columns (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-318&quot;&gt;DBZ-318&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Views won’t stop the connector any more (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-319&quot;&gt;DBZ-319&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Warnings and notifications emitted by the server are correctly forwarded to the log (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-279&quot;&gt;DBZ-279&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please refer to the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-6-0&quot;&gt;changelog&lt;/a&gt; for an overview of all the 20 issues fixed in Debezium 0.6.0.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;High on our agenda is exploring support for Oracle (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-20&quot;&gt;DBZ-20&lt;/a&gt;).
      We are also looking into using another logical decoding plug-in (wal2json) for the Postgres connector, which would enable to use Debezium with Postgres instances running on Amazon RDS.
      Another feature being worked on by community member &lt;a href=&quot;https://github.com/mtagle&quot;&gt;Moira Tagle&lt;/a&gt; is support for updates to the &lt;code&gt;table.whitelist&lt;/code&gt; for existing connector instances.
      Finally, we’ve planned to test and adapt the existing MySQL connector for providing CDC functionality to MariaDB.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium 0.7 with one or more out of those features as well as hopefully some others will be released later this year.
      We’ll likely also do further 0.6.x releases with bug fixes as required.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You’d like to contribute?
      That’s great - let us know and we’ll get you started.
      Check out the details below on how to get in touch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/08/17/debezium-0-5-2-is-out/</id>
    <title>Debezium 0.5.2 Is Out</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2017-08-17T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/08/17/debezium-0-5-2-is-out/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m very happy to announce the release of Debezium 0.5.2!
      
      
      As the previous release, the 0.5.2 release fixes several bugs in the MySQL, Postgres and MongoDB connectors.
      But there are also several new features and options:
      
      
      
      
      The decimal.handling.mode option already known from the MySQL connector is now also supported for PostgreSQL (DBZ-337).
      It lets you control how NUMERIC and DECIMAL columns are represented in change events (either using Kafka&#8217;s Decimal type or as double).
      
      
      The MongoDB connector supports the options database.whitelist and database.blacklist now (DBZ-302)
      
      
      The PostgreSQL connector can deal with array-typed columns as well as with quoted identifiers for tables, schemas etc. (DBZ-297, DBZ-298)
      
      
      The Debezium...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m very happy to announce the release of &lt;strong&gt;Debezium 0.5.2&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As the previous release, the 0.5.2 release fixes several bugs in the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL&lt;/a&gt;, &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql/&quot;&gt;Postgres&lt;/a&gt; and &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/&quot;&gt;MongoDB&lt;/a&gt; connectors.
      But there are also several new features and options:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;The &lt;code&gt;decimal.handling.mode&lt;/code&gt; option already known from the MySQL connector is now also supported for PostgreSQL (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-337&quot;&gt;DBZ-337&lt;/a&gt;).
      It lets you control how &lt;code&gt;NUMERIC&lt;/code&gt; and &lt;code&gt;DECIMAL&lt;/code&gt; columns are represented in change events (either using Kafka’s &lt;code&gt;Decimal&lt;/code&gt; type or as &lt;code&gt;double&lt;/code&gt;).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The MongoDB connector supports the options &lt;code&gt;database.whitelist&lt;/code&gt; and &lt;code&gt;database.blacklist&lt;/code&gt; now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-302&quot;&gt;DBZ-302&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The PostgreSQL connector can deal with array-typed columns as well as with quoted identifiers for tables, schemas etc. (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-297&quot;&gt;DBZ-297&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-298&quot;&gt;DBZ-298&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The Debezium Docker images run on Red Hat’s &lt;a href=&quot;https://www.openshift.com/&quot;&gt;OpenShift&lt;/a&gt; cloud environment (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-267&quot;&gt;DBZ-267&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Speaking about the Docker images, we’ve set up &lt;em&gt;nightly&lt;/em&gt; tags for the &lt;a href=&quot;https://hub.docker.com/u/debezium/&quot;&gt;Debezium images on Docker Hub&lt;/a&gt;,
      allowing you to grab the latest improvements even before an official release has been cut.
      The connector archives are also deployed to the &lt;a href=&quot;https://oss.sonatype.org/content/repositories/snapshots/io/debezium/&quot;&gt;Sonatype OSS Maven repository&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, we’ve spent some time to extend the documentation on some things not covered before:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;http://debezium.io/docs/configuration/avro/&quot;&gt;Avro Serialization&lt;/a&gt; describes how to use the use the Avro converter and the Confluent Schema Registry instead of the JSON converter instead of the default JSON converter for serializing change events, resulting in much smaller message sizes;
      The Avro converter itself has also been added to the Debezium Docker image for Kafka Connect, so you can use it right away&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;http://debezium.io/docs/configuration/topic-routing/&quot;&gt;Topic Routing&lt;/a&gt; describes how to use Debezium’s &lt;code&gt;ByLogicalTableRouter&lt;/code&gt; single message transformation (SMT) for routing the change events from multiple tables into a single topic, which for instance is very useful when working with sharded tables&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please refer to the &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/CHANGELOG.md#052&quot;&gt;changelog&lt;/a&gt; for an overview of all the 19 issues fixed in Debezium 0.5.2.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The following people from the community have sent in pull requests for this release:
      &lt;a href=&quot;https://github.com/emrul&quot;&gt;Emrul Islam&lt;/a&gt;, &lt;a href=&quot;https://github.com/ekreiser&quot;&gt;Eric S. Kreiser&lt;/a&gt;, &lt;a href=&quot;https://github.com/xenji&quot;&gt;Mario Mueller&lt;/a&gt;, &lt;a href=&quot;https://github.com/mcapitanio&quot;&gt;Matteo Capitanio&lt;/a&gt;, &lt;a href=&quot;https://github.com/omarsmak&quot;&gt;Omar Al-Safi&lt;/a&gt; and &lt;a href=&quot;https://github.com/Satyajitv&quot;&gt;Satyajit Vegesna&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks a lot to you and everyone else in the community for contributing to Debezium via feature requests, bug reports, discussions and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The next version of Debezium will be 0.6 (planned for September).
      This release is planned to bring the upgrade to Kafka 0.11.
      We’ll also look into an SMT for transforming the change events emitted by Debezium into a flat representation, which for instance will be very useful in conjunction with the JDBC sink connector.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While 0.6 is planned to be more of a &quot;stabilization release&quot;, 0.7 should bring a long-awaited major feature:
      we’ve planned to explore support for Oracle and hopefully will do an initial release of a Debezium connector for that database.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In other words, exciting times are ahead!
      If you’d like to get involved, let us know.
      Check out the details below on how to get in touch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/06/12/debezium-0-5-1-released/</id>
    <title>Debezium 0.5.1 Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2017-06-12T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/06/12/debezium-0-5-1-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s my pleasure to announce the release of Debezium 0.5.1!
      
      
      This release fixes several bugs in the MySQL, Postgres and MongoDB connectors.
      There&#8217;s also support for some new datatypes: POINT on MySQL (DBZ-222) and TSTZRANGE on Postgres (DBZ-280).
      This release is a drop-in replacement for 0.5.0, upgrading is recommended to all users.
      
      
      Note that in the&#8201;&#8212;&#8201;rather unlikely&#8201;&#8212;&#8201;case that you happened to enable Debezium for all the system tables of MySQL,
      any configured table filters will be applied to these system tables now, too (DBZ-242).
      This may require an adjustment of your filters if you indeed wanted to capture all system tables but only selected non-system tables.
      
      
      Please...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s my pleasure to announce the release of &lt;strong&gt;Debezium 0.5.1&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release fixes several bugs in the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL&lt;/a&gt;, &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql/&quot;&gt;Postgres&lt;/a&gt; and &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/&quot;&gt;MongoDB&lt;/a&gt; connectors.
      There’s also support for some new datatypes: &lt;code&gt;POINT&lt;/code&gt; on MySQL (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-222&quot;&gt;DBZ-222&lt;/a&gt;) and &lt;code&gt;TSTZRANGE&lt;/code&gt; on Postgres (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-280&quot;&gt;DBZ-280&lt;/a&gt;).
      This release is a drop-in replacement for 0.5.0, upgrading is recommended to all users.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note that in the — rather unlikely — case that you happened to enable Debezium for all the system tables of MySQL,
      any configured table filters will be applied to these system tables now, too (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-242&quot;&gt;DBZ-242&lt;/a&gt;).
      This may require an adjustment of your filters if you indeed wanted to capture all system tables but only selected non-system tables.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please refer to the &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/CHANGELOG.md#051&quot;&gt;changelog&lt;/a&gt; for an overview of all the 29 issues fixed in Debezium 0.5.1.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Docker image containing &lt;a href=&quot;https://hub.docker.com/r/debezium/connect/&quot;&gt;Kafka Connect and all the Debezium 0.5.x connectors&lt;/a&gt;
      as well as the image containing &lt;a href=&quot;https://hub.docker.com/r/debezium/postgres/&quot;&gt;Postgres and the Debezium logical decoding plug-in&lt;/a&gt; have been updated to 0.5.1, too.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As Debezium continues to evolve and grow, the number of people contributing to the project is also going up.
      The following people have sent in pull requests for this release:
      &lt;a href=&quot;https://github.com/arosenber&quot;&gt;Aaron Rosenberg&lt;/a&gt;, &lt;a href=&quot;https://github.com/CyberDem0n&quot;&gt;Alexander Kukushkin&lt;/a&gt;, &lt;a href=&quot;https://github.com/brendanmaguire&quot;&gt;Brendan Maguire&lt;/a&gt;, &lt;a href=&quot;https://github.com/DuncanSands&quot;&gt;Duncan Sands&lt;/a&gt;, &lt;a href=&quot;https://github.com/dasl-&quot;&gt;David Leibovic&lt;/a&gt;, &lt;a href=&quot;https://github.com/jpechane&quot;&gt;Jiri Pechanec&lt;/a&gt;, &lt;a href=&quot;https://github.com/nacivida&quot;&gt;nacivida&lt;/a&gt;, &lt;a href=&quot;https://github.com/omarsmak&quot;&gt;Omar Al-Safi&lt;/a&gt;, &lt;a href=&quot;https://github.com/rhauch&quot;&gt;Randall Hauch&lt;/a&gt; and &lt;a href=&quot;https://github.com/tombentley&quot;&gt;Tom Bentley&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks a lot to you and everyone else in the community contributing via feature requests, bug reports, discussions and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve planned to do further bug fix releases for the 0.5.x line.
      Specifically, we’ll release a fix for &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-217&quot;&gt;DBZ-217&lt;/a&gt; shortly,
      which is about the MySQL connector stumbling when getting across a corrupt event in the binlog.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In parallel we’re looking into Debezium connectors for &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-40&quot;&gt;SQL Server&lt;/a&gt; and &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-137&quot;&gt;Oracle&lt;/a&gt;.
      While we cannot promise anything yet in terms of when these will be ready to be published, we hope to have at least one of them ready some time soon.
      Stay tuned and get involved!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/04/27/hello-debezium/</id>
    <title>Hello Debezium!</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2017-04-27T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/04/27/hello-debezium/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="community"></category>
    <category term="news"></category>
    <summary>
      
      When I first learned about the Debezium project last year, I was very excited about it right away.
      
      
      I could see how this project would be very useful for many people out there and I was very impressed by the professional way it was set up:
      a solid architecture for change data capture based on Apache Kafka, a strong focus on robustness and correctness also in the case of failures, the overall idea of creating a diverse eco-system of CDC connectors.
      All that based on the principles of open source, combined with extensive documentation from day one, a friendly and welcoming web site...
    </summary>
    <content type="html">
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When I first learned about the Debezium project last year, I was very excited about it right away.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I could see how this project would be very useful for many people out there and I was very impressed by the professional way it was set up:
      a solid architecture for change data capture based on Apache Kafka, a strong focus on robustness and correctness also in the case of failures, the overall idea of creating a diverse eco-system of CDC connectors.
      All that based on the principles of open source, combined with extensive documentation from day one, a friendly and welcoming web site and a great getting-started experience.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So you can imagine that I was more than enthusiastic about the opportunity to take over the role of Debezium’s project lead.
      Debezium and CDC have close links to some data-centric projects I’ve been previously working on and also tie in with ideas I’ve been pursuing around CQRS, even sourcing and denormalization.
      As core member of the &lt;a href=&quot;http://hibernate.org/&quot;&gt;Hibernate team&lt;/a&gt; at Red Hat, I’ve implemented the initial Elasticsearch support for &lt;a href=&quot;http://hibernate.org/search/&quot;&gt;Hibernate Search&lt;/a&gt;
      (which deals with full-text index updates via JPA/Hibernate).
      I’ve also contributed to &lt;a href=&quot;http://hibernate.org/ogm/&quot;&gt;Hibernate OGM&lt;/a&gt; - a project which connects JPA and the world of NoSQL.
      One of the plans for OGM is to create a declarative denormalization engine for creating read models optimized for specific use cases.
      It will be very interesting to see how this plays together with the capabilities provided by Debezium.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Currently I am serving as the lead of the &lt;a href=&quot;http://beanvalidation.org/&quot;&gt;Bean Validation 2.0&lt;/a&gt; specification (JSR 380) as well as its reference implementation &lt;a href=&quot;http://hibernate.org/validator/&quot;&gt;Hibernate Validator&lt;/a&gt;.
      Two other projects close to my heart are &lt;a href=&quot;http://mapstruct.org/&quot;&gt;MapStruct&lt;/a&gt; - a code generator for bean-to-bean mappings - and &lt;a href=&quot;https://github.com/moditect/moditect&quot;&gt;ModiTect&lt;/a&gt;, which is tooling for Java 9 modules and their descriptors.
      In general, I’m a strong believer into the idea of open source and I just love it to work with folks from all over the world to create useful tools and libraries.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Joining the Debezium community and working on change data capture is a great next step.
      There are so many things to do: connectors for Oracle, SQL Server and Cassandra,
      but also things like an entity join processor which would allow to step from row-level events to more aggregated business-level events (e.g. for updating a combined search index for an order and its order lines) or tooling for managing and visualizing histories of event schema changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;One thing I’d like to emphasize is that the project’s direction generally isn’t going to change very much.
      Red Hat is fully committed to maintaining and evolving the project together with you, the Debezium community.
      The ride really has just begun!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, let me say a huge thank you to Randall for his excellent work!
      You’ve been a true role model for going from an idea over pitching it - within Red Hat as well as within the wider community - to building a steadily growing and evolving project.
      It’s stating the obvious, but it wouldn’t be for Debezium without you.
      Thanks for everything and looking forward very much to working with you and the community on this great project!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Onwards,&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;--Gunnar&lt;/p&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/04/26/Debezium-evolving/</id>
    <title>Debezium Evolving</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2017-04-26T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/04/26/Debezium-evolving/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="community"></category>
    <category term="news"></category>
    <summary>
      
      Just before I started the Debezium project in early 2016, Martin Kleppmann gave several presentations about turning the database inside out and how his Bottled Water project demonstrated the importantance that change data capture can play in using Kafka for stream processing. Then Kafka Connect was announced, and at that point it seemed obvious to me that Kafka Connect was the foundation upon which practical and reusable change data capture can be built. As these techniques and technologies were becoming more important to Red Hat, I was given the opportunity to start a new open source project and community around...
    </summary>
    <content type="html">
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Just before I started the Debezium project in early 2016, &lt;a href=&quot;https://martin.kleppmann.com&quot;&gt;Martin Kleppmann&lt;/a&gt; gave several presentations about &lt;a href=&quot;https://martin.kleppmann.com/2015/03/04/turning-the-database-inside-out.html&quot;&gt;turning the database inside out&lt;/a&gt; and how his &lt;a href=&quot;https://martin.kleppmann.com/2015/04/23/bottled-water-real-time-postgresql-kafka.html&quot;&gt;Bottled Water&lt;/a&gt; project demonstrated the importantance that change data capture can play in using Kafka for stream processing. Then Kafka Connect was &lt;a href=&quot;https://www.confluent.io/blog/announcing-kafka-connect-building-large-scale-low-latency-data-pipelines/&quot;&gt;announced&lt;/a&gt;, and at that point it seemed obvious to me that Kafka Connect was the foundation upon which practical and reusable change data capture can be built. As these techniques and technologies were becoming more important to &lt;a href=&quot;https://www.redhat.com/&quot;&gt;Red Hat&lt;/a&gt;, I was given the opportunity to start a new open source project and community around building great CDC connectors for a variety of databases management systems.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Over the past few years, we have created Kafka Connect connectors for &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL&lt;/a&gt;, then &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/&quot;&gt;MongoDB&lt;/a&gt;, and most recently &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql/&quot;&gt;PostgreSQL&lt;/a&gt;. Each were initially limited and had a number of problems and issues, but over time more and more people have tried the connectors, asked questions, answered questions, mentioned &lt;a href=&quot;https://twitter.com/search?vertical=default&amp;amp;q=debezium&amp;amp;src=typd&quot;&gt;Debezium on Twitter&lt;/a&gt;, tested connectors in their own environments, reported problems, fixed bugs, discussed limitations and potential new features, implemented enhancements and new features, improved the documentation, and wrote blog posts. Simply put, people with similar needs and interests have worked together and have formed a community. Additional connectors for &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-137&quot;&gt;Oracle&lt;/a&gt; and &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-40&quot;&gt;SQL Server&lt;/a&gt; are in the works, but could use some help to move things along more quickly.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s really exciting to see how far we’ve come and how the Debezium community continues to evolve and grow. And it’s perhaps as good a time as any to hand the reigns over to someone else. In fact, after nearly 10 wonderful years at Red Hat, I’m making a bigger change and as of today am part of &lt;a href=&quot;https://www.confluent.io&quot;&gt;Confluent’s&lt;/a&gt; engineering team, where I expect to play a more active role in the broader &lt;a href=&quot;https://kafka.apache.org&quot;&gt;Kafka&lt;/a&gt; community and more directly with Kafka Connect and Kafka Streams. I &lt;strong&gt;definitely&lt;/strong&gt; plan to stay involved in the Debezium community, but will no longer be leading the project. That role will instead be filled by &lt;a href=&quot;https://github.com/gunnarmorling/&quot;&gt;Gunnar Morling&lt;/a&gt;, who’s recently joined the Debezium community but has extensive experience in open source, the &lt;a href=&quot;http://in.relation.to/gunnar-morling/&quot;&gt;Hibernate community&lt;/a&gt;, and the &lt;a href=&quot;http://beanvalidation.org&quot;&gt;Bean Validation&lt;/a&gt; specification effort. Gunnar is a great guy and an excellent developer, and will be an excellent lead for the Debezium community.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Will the Debezium project change? To some degree it will always continue to evolve just as it has from the very beginning, and that’s a healthy thing. But a lot is staying the same. Red Hat remains committed to the Debezium project, and will continue its sponsorship and community-oriented governance that has worked so well from the beginning. And just as importantly, we the community are still here and will continue building the best open source CDC connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So keep up the great work, and look for and take advantage of opportunities to &lt;a href=&quot;http://debezium.io/community/&quot;&gt;become more involved&lt;/a&gt; in Debezium. Please give a warm welcome to Gunnar by introducing yourself in the &lt;a href=&quot;https://gitter.im/debezium/dev&quot;&gt;developer&lt;/a&gt; and / or &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;user&lt;/a&gt; chat rooms and mention how you’re using Debezium and what the Debezium community means to you.&lt;/p&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/03/27/Debezium-0-5-0-Released/</id>
    <title>Debezium 0.5.0 Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2017-03-27T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/03/27/Debezium-0-5-0-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.5.0 is now available for use with Kafka Connect 0.10.2.0. This release also includes a few fixes for the MySQL connector. See the release notes for specifics on these changes, and be sure to check out the Kafka documentation for compatibility with the version of the Kafka broker that you are using.
      
      
      Kafka Connect 0.10.2.0 comes with a significant new feature called Single Message Transforms, and you can now use them with Debezium connectors. SMTs allow you to modify the messages produced by Debezium connectors and any oher Kafka Connect source connectors, before those messages...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.5.0&lt;/strong&gt; is now available for use with &lt;strong&gt;Kafka Connect 0.10.2.0&lt;/strong&gt;. This release also includes a few fixes for the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt;. See the &lt;a href=&quot;http://debezium.io/docs/releases&quot;&gt;release notes&lt;/a&gt; for specifics on these changes, and be sure to check out the &lt;a href=&quot;https://kafka.apache.org/documentation/#upgrade&quot;&gt;Kafka documentation&lt;/a&gt; for compatibility with the version of the Kafka broker that you are using.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka Connect 0.10.2.0 comes with a significant new feature called &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-66%3A+Single+Message+Transforms+for+Kafka+Connect&quot;&gt;Single Message Transforms&lt;/a&gt;, and you can now use them with Debezium connectors. SMTs allow you to modify the messages produced by Debezium connectors and any oher Kafka Connect source connectors, before those messages are written to Kafka. SMTs can also be used with Kafka Connect sink connectors to modify the messages &lt;em&gt;before&lt;/em&gt; the sink connectors processes them. You can use SMTs to filter out or mask specific fields, add new fields, modify existing fields, change the topic and/or topic partition to which the messages are written, and even more. And you can even chain multiple SMTs together.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka Connect comes with a number of built-in SMTs that you can simply configure and use, but you can also create your own SMT implementations to do more complex and interesting things. For example, although Debezium connectors normally map all of the changes in each table (or collection) to separate topics, you can write a custom SMT that uses a completely different mapping between tables and topics and even add fields to message keys and/or values. Using your new SMT is also very easy - simply put it on the Kafka Connect classpath and update the connector configuration to use it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also added &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.5&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Sanjay and everyone in the community for their help with this release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll continue to improve the MongoDB, MySQL, and PostgreSQL connectors and pushing out 0.5.x releases with fixes. And we’re still working on connectors for &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-40&quot;&gt;SQL Server&lt;/a&gt; and &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-137&quot;&gt;Oracle&lt;/a&gt;. Stay tuned and get involved!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/03/17/Debezium-0-4-1-Released/</id>
    <title>Debezium 0.4.1 Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2017-03-17T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/03/17/Debezium-0-4-1-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="rds"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.4.1 is now available for use with Kafka Connect 0.10.1.1. This release includes several fixes for the MongoDB connector and MySQL connector, including improved support for Amazon RDS and Amazon Aurora (MySQL compatibility). See the release notes for specifics on these changes.
      
      
      We&#8217;ve also updated the Debezium Docker images labelled 0.4 and latest, which we use in our tutorial.
      
      
      Thanks to Jan, Horia, David, Josh, Johan, Sanjay, Saulius, and everyone in the community for their help with this release, issues, discussions, contributions, and questions!
      
      
      
      
      What&#8217;s next
      
      
      Kafka 0.10.2.0 is out, so we plan to release 0.5.0 next week...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.4.1&lt;/strong&gt; is now available for use with Kafka Connect 0.10.1.1. This release includes several fixes for the &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb&quot;&gt;MongoDB connector&lt;/a&gt; and &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt;, including improved support for &lt;a href=&quot;https://aws.amazon.com/rds/mysql/&quot;&gt;Amazon RDS&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/rds/aurora/&quot;&gt;Amazon Aurora (MySQL compatibility)&lt;/a&gt;. See the &lt;a href=&quot;http://debezium.io/docs/releases&quot;&gt;release notes&lt;/a&gt; for specifics on these changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.4&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Jan, Horia, David, Josh, Johan, Sanjay, Saulius, and everyone in the community for their help with this release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka 0.10.2.0 is out, so we plan to release 0.5.0 next week with all of the changes/fixes in 0.4.1 but with support for Kafka 0.10.2.0. We’ll then continue to improve the MongoDB, MySQL, and PostgreSQL connectors and pushing out 0.5.x releases. Stay tuned and get involved!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/02/22/Debezium-at-WePay/</id>
    <title>Streaming databases in realtime with MySQL, Debezium, and Kafka</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2017-02-22T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/02/22/Debezium-at-WePay/" rel="alternate" type="text/html" />
    <author>
      <name>Chris Riccomini</name>
    </author>
    <category term="mysql"></category>
    <summary>
      
      
      
      This post originally appeared on the WePay Engineering blog.
      
      
      Change data capture has been around for a while, but some recent developments in technology have given it new life. Notably, using Kafka as a backbone to stream your database data in realtime has become increasingly common.
      
      
      If you&#8217;re wondering why you might want to stream database changes into Kafka, I highly suggest reading The Hardest Part About Microservices: Your Data. At WePay, we wanted to integrate our microservices and downstream datastores with each other, so every system could get access to the data that it needed. We use Kafka as our data...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;strong&gt;&lt;em&gt;This post originally appeared on the &lt;a href=&quot;https://wecode.wepay.com/posts/streaming-databases-in-realtime-with-mysql-debezium-kafka&quot;&gt;WePay Engineering blog&lt;/a&gt;.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Change_data_capture&quot;&gt;Change data capture&lt;/a&gt; has been around for a while, but some recent developments in technology have given it new life. Notably, using &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; as a backbone to stream your database data in realtime has become &lt;a href=&quot;https://github.com/wushujames/mysql-cdc-projects/wiki&quot;&gt;increasingly common&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’re wondering why you might want to stream database changes into Kafka, I highly suggest reading &lt;a href=&quot;http://blog.christianposta.com/microservices/the-hardest-part-about-microservices-data/&quot;&gt;The Hardest Part About Microservices: Your Data&lt;/a&gt;. At WePay, we wanted to integrate our microservices and downstream datastores with each other, so every system could get access to the data that it needed. We use Kafka as our data integration layer, so we needed a way to get our database data into it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Last year, &lt;a href=&quot;https://www.yelp.com/engineering&quot;&gt;Yelp’s engineering team&lt;/a&gt; published an excellent &lt;a href=&quot;https://engineeringblog.yelp.com/2016/11/open-sourcing-yelps-data-pipeline.html&quot;&gt;series of posts&lt;/a&gt; on their data pipeline. These included a discussion on how they &lt;a href=&quot;https://engineeringblog.yelp.com/2016/08/streaming-mysql-tables-in-real-time-to-kafka.html&quot;&gt;stream MySQL data into Kafka&lt;/a&gt;. Their architecture involves a series of homegrown pieces of software to accomplish the task, notably &lt;a href=&quot;https://github.com/Yelp/schematizer&quot;&gt;schematizer&lt;/a&gt; and &lt;a href=&quot;https://github.com/Yelp/mysql_streamer&quot;&gt;MySQL streamer&lt;/a&gt;. The write-up triggered a thoughtful post on Debezium’s blog about a proposed equivalent architecture using &lt;a href=&quot;http://docs.confluent.io/3.1.1/connect/&quot;&gt;Kafka connect&lt;/a&gt;, &lt;a href=&quot;http://debezium.io/&quot;&gt;Debezium&lt;/a&gt;, and &lt;a href=&quot;http://docs.confluent.io/3.1.1/schema-registry/docs/&quot;&gt;Confluent’s schema registry&lt;/a&gt;. This proposed architecture is what we’ve been implementing at WePay, and this post describes how we leverage Debezium and Kafka connect to stream our MySQL databases into Kafka.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;architecture&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#architecture&quot;&gt;&lt;/a&gt;Architecture&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The flow of data starts with each microservice’s MySQL database. These databases run in &lt;a href=&quot;https://cloud.google.com/&quot;&gt;Google Cloud&lt;/a&gt; as &lt;a href=&quot;https://cloud.google.com/sql/&quot;&gt;CloudSQL&lt;/a&gt; MySQL instances &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/replication-gtids.html&quot;&gt;with GTIDs enabled&lt;/a&gt;. We’ve set up a downstream MySQL cluster specifically for Debezium. Each CloudSQL instance replicates its data into the Debezium cluster, which consists of two MySQL machines: a primary (active) server and secondary (passive) server. This single Debezium cluster is an operational trick to make it easier for us to operate Debezium. Rather than having Debezium connect to dozens of microservice databases directly, we can connect to just a single database. This also isolates Debezium from impacting the production OLTP workload that the master CloudSQL instances are handling.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We run one Debezium connector (in &lt;a href=&quot;http://docs.confluent.io/2.0.0/connect/userguide.html#distributed-mode&quot;&gt;distributed mode&lt;/a&gt; on the Kafka connect framework) for each microservice database. Again, the goal here is isolation. Theoretically, we could run a single Debezium connector that produces messages for all databases (since all microservice databases are in the Debezium cluster). This approach would actually be more resource efficient since each Debezium connector has to read MySQL’s entire &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/binary-log.html&quot;&gt;binlog&lt;/a&gt; anyway. We opted not to do this because we wanted to be able to bring Debezium connectors up and down, and configure them differently for each microservice DB.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium connectors feed the MySQL messages into Kafka (and add their schemas to the Confluent schema registry), where downstream systems can consume them. We use our Kafka connect &lt;a href=&quot;https://wecode.wepay.com/posts/kafka-bigquery-connector&quot;&gt;BigQuery connector&lt;/a&gt; to load the MySQL data into BigQuery using BigQuery’s &lt;a href=&quot;https://cloud.google.com/bigquery/streaming-data-into-bigquery&quot;&gt;streaming API&lt;/a&gt;. This gives us a data warehouse in BigQuery that is usually less than 30 seconds behind the data that’s in production. Other microservices, stream processors, and data infrastructure consume the feeds as well.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;https://wecode.wepay.com/assets/2017-02-21-streaming-databases-in-realtime-with-mysql-debezium-kafka/debezium-architecture.png&quot; alt=&quot;Debezium architecture&quot; /&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#debezium&quot;&gt;&lt;/a&gt;Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The remainder of this post will focus on Debezium (the DBZ boxes in the diagram above), and how we configure and operate it. Debezium works by connecting to MySQL and pretending to be a replica. MySQL sends its replication data to Debezium, thinking it’s actually funneling data to another downstream MySQL instance. Debezium then takes the data, converts the schemas from MySQL schemas to &lt;a href=&quot;https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Struct.html&quot;&gt;Kafka connect structures&lt;/a&gt;, and forwards them to Kafka.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;adding_new_databases&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#adding_new_databases&quot;&gt;&lt;/a&gt;Adding new databases&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When a new microservice with a CloudSQL database comes online, we want to get that data into Kafka. The first step in the process is to load the data into the Debezium MySQL cluster. This involves several steps:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;Take a MySQL dump of the data in the microservice DB.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Pause the secondary Debezium MySQL DB.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Load the MySQL dump into the secondary Debezium MySQL DB.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Reset &lt;code&gt;GTID_PURGED&lt;/code&gt; parameter to include the GTID from the new DB dump.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Unpause the secondary Debezium MySQL DB.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Update HA Proxy to point to the secondary, which now becomes the primary.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Follow steps 2-5 for the old primary instance (now secondary).&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The actual commands that we run are:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight nowrap&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;# (1) Take a dump of the database we wish to add.
      $ mydumper  --host=123.123.123.123 --port=3306 --user=foo --password=*********  -B log --trx-consistency-only  --triggers --routines -o /mysqldata/new_db/ -c -L mydumper.log
      
      # (2) Stop all replication on the secondary Debezium cluster.
      $ mysql&amp;gt; STOP SLAVE for channel 'foo';
      $ mysql&amp;gt; STOP SLAVE for channel 'bar';
      $ mysql&amp;gt; STOP SLAVE for channel 'baz';
      
      # Get the current GTID purged values from MySQL.
      $ mysql&amp;gt; SHOW GLOBAL VARIABLES like '%gtid_purged%';
      
      # (3) Load the dump of the database into the Debezium cluster.
      $ myloader -d /mysqldata/new_db/ -s new_db
      
      # (4) Clear out existing GTID_PURGED values so that we can overwrite it to include the GTID from the new dump file.
      $ mysql&amp;gt; reset master;
      
      # Set the new GTID_PURGED value, including the GTID_PURGED value from the MySQL dump file.
      $ mysql&amp;gt; set global GTID_PURGED=&quot;f3a44d1a-11e6-44ba-bf12-040bab830af0:1-10752,c627b2bc-b36a-11e6-a886-42010af00790:1-9052,01261abc3-6ade-11e6-9647-42010af0044a:1-375342&quot;;
      
      # (5) Start replication for the new DB.
      $ mysql&amp;gt; CHANGE MASTER TO MASTER_HOST='123.123.123.123', MASTER_USER='REPLICATION_USER', MASTER_PASSWORD='REPLICATION_PASSWORD',MASTER_AUTO_POSITION=1 for CHANNEL 'new_db';
      $ mysql&amp;gt; START SLAVE for channel 'new_db';
      
      # Start replication for the DBs that we paused.
      $ mysql&amp;gt; START SLAVE for channel 'foo';
      $ mysql&amp;gt; START SLAVE for channel 'bar';
      $ mysql&amp;gt; START SLAVE for channel 'baz';
      
      # Repeat steps 2-5 on the old primary (now secondary).&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;At the end of these steps, both the primary and secondary Debezium MySQL servers have the new database. Once finished, we can then add a new Debezium connector to the Kafka connect cluster. This connector will have configuration that looks roughly like this:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight nowrap&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
         &quot;name&quot;: &quot;debezium-connector-microservice1&quot;,
         &quot;config&quot;: {
             &quot;name&quot;: &quot;debezium-connector-microservice1&quot;,
             &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
             &quot;tasks.max&quot;: &quot;1&quot;,
             &quot;database.hostname&quot;: &quot;dbz-mysql01&quot;,
             &quot;database.port&quot;: &quot;3306&quot;,
             &quot;database.user&quot;: &quot;user&quot;,
             &quot;database.password&quot;: &quot;*******&quot;,
             &quot;database.server.id&quot;: &quot;101&quot;,
             &quot;database.server.name&quot;: &quot;db.debezium.microservice1&quot;,
             &quot;gtid.source.includes&quot;: &quot;c34aeb9e-89ad-11e6-877b-42010a93af2d&quot;,
             &quot;database.whitelist&quot;: &quot;microservice1_db&quot;,
             &quot;poll.interval.ms&quot;: &quot;2&quot;,
             &quot;table.whitelist&quot;: &quot;microservice1_db.table1,microservice1_db.table2&quot;,
             &quot;column.truncate.to.1024.chars&quot; : &quot;microservice1_db.table1.text_col&quot;,
             &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka01:9093,kafka02:9093,kafka03:9093&quot;,
             &quot;database.history.kafka.topic&quot;: &quot;debezium.history.microservice1&quot;,
             &quot;database.ssl.truststore&quot;: &quot;/certs/truststore&quot;,
             &quot;database.ssl.truststore.password&quot;: &quot;*******&quot;,
             &quot;database.ssl.mode&quot;: &quot;required&quot;,
             &quot;database.history.producer.security.protocol&quot;: &quot;SSL&quot;,
             &quot;database.history.producer.ssl.truststore.location&quot;: &quot;/certs/truststore&quot;,
             &quot;database.history.producer.ssl.truststore.password&quot;: &quot;*******&quot;,
             &quot;database.history.consumer.security.protocol&quot;: &quot;SSL&quot;,
             &quot;database.history.consumer.ssl.truststore.location&quot;: &quot;/certs/truststore&quot;,
             &quot;database.history.consumer.ssl.truststore.password&quot;: &quot;*******&quot;,
         }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The details on these configuration fields are located &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/#connector-properties&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The new connector will start up and begin &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/#snapshots&quot;&gt;snapshotting&lt;/a&gt; the database, since this is the first time it’s been started. Debezium’s snapshot implementation (see &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-31&quot;&gt;DBZ-31&lt;/a&gt;) uses an approach very similar to MySQL’s mysqldump tool. Once the snapshot is complete, Debezium will switch over to using MySQL’s binlog to receive all future database updates.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka connect and Debezium work together to periodically commit Debezium’s location in the MySQL binlog described by a &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/replication-gtids-concepts.html&quot;&gt;MySQL global transaction ID&lt;/a&gt; (GTID). When Debezium restarts, Kafka connect will give it the last committed MySQL GTID, and Debezium will pick up from there.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;em&gt;Note that commits only happen periodically, so Debezium might start up from a location in the log prior to the last row that it received. In such a case, you will observe duplicate messages in Debezium Kafka topic. Debezium writes messages to Kafka with an at-least-once messaging guarantee.&lt;/em&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;high_availability&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#high_availability&quot;&gt;&lt;/a&gt;High availability&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;One of the difficulties we faced when we first began using Debezium was how to make it tolerant to machine failures (both the upstream MySQL server, and Debezium, itself). MySQL prior to version 5.6 modeled a replica’s location in its parent’s binlogs using a (binlog filename, file offset) tuple. The problem with this approach is that the binlog filenames are not the same between MySQL machines. This means that a replica reading from upstream MySQL machine 1 can’t easily fail over to MySQL machine 2. There is an entire ecosystem of tools (including &lt;a href=&quot;https://code.google.com/p/mysql-master-ha/&quot;&gt;MHA&lt;/a&gt;) to try and address this problem.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Starting with MySQL 5.6, MySQL introduced the concept of global transaction IDs. These GTIDs identify a specific location within the MySQL binlog &lt;em&gt;across machines&lt;/em&gt;. This means that a consumer reading from a binlog on one MySQL server can switch over to the other, provided that both servers have the data available. This is how we run our systems. Both the CloudSQL instances and the Debezium MySQL cluster run with GTIDs enabled. The Debezium MySQL servers also have replication binlogs enabled so that binlogs exist for Debezium to read (replicas don’t normally have binlogs enabled by default). All of this enables Debezium to consume from the primary Debezium MySQL server, but switch over to the secondary (via HA Proxy) if there’s a failure.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If the machine that Debezium, itself, is running on fails, then the Kafka connect framework fails the connector over to another machine in the cluster. When the failover occurs, Debezium receives its last committed offset (GTID) from Kafka connect, and picks up where it left off (with the same caveat as above: you might see some duplicate messages due to periodic commit frequency).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;An important configuration that needs to be called out is the &lt;code&gt;gtid.source.includes&lt;/code&gt; field that we have set above. When we first set up the topology that’s described in the architecture section, we discovered that we could not fail over from the primary Debezium DB to the secondary DB even though they both were replicating exactly the same data. This is because, in addition to the GTIDs for the various upstream DBs that both primary and secondary machines are replicating, each machine has its &lt;em&gt;own&lt;/em&gt; server UUID for its various MySQL databases (e.g. information_schema). The fact that these two servers have different UUIDs in them led MySQL to get confused when we triggered a failover, because Debezium’s GTID would include the server UUID for the primary server, which the secondary server didn’t know about. The fix was to filter out all UUIDs that we don’t care about from the GTID. Each Debezium connector filters out all server UUIDs except for the UUID for the microservice DB that it cares about. This allows the connector to fail from primary to secondary without issue. This issue is documented in detail on &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-129&quot;&gt;DBZ-129&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;schemas&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#schemas&quot;&gt;&lt;/a&gt;Schemas&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium’s &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/#change-events-value&quot;&gt;message format&lt;/a&gt; includes both the &quot;before&quot; and &quot;after&quot; versions of a row. For inserts, the &quot;before&quot; is null. For deletes, the &quot;after&quot; is null. Updates have both the &quot;before&quot; and &quot;after&quot; fields filled out. The messages also include some server information such as the server ID that the message came from, the GTID of the message, the server timestamp, and so on.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
        &quot;before&quot;: {
          &quot;id&quot;: 1004,
          &quot;first_name&quot;: &quot;Anne&quot;,
          &quot;last_name&quot;: &quot;Kretchmar&quot;,
          &quot;email&quot;: &quot;annek@noanswer.org&quot;
        },
        &quot;after&quot;: {
          &quot;id&quot;: 1004,
          &quot;first_name&quot;: &quot;Anne Marie&quot;,
          &quot;last_name&quot;: &quot;Kretchmar&quot;,
          &quot;email&quot;: &quot;annek@noanswer.org&quot;
        },
        &quot;source&quot;: {
          &quot;name&quot;: &quot;mysql-server-1&quot;,
          &quot;server_id&quot;: 223344,
          &quot;ts_sec&quot;: 1465581,
          &quot;gtid&quot;: null,
          &quot;file&quot;: &quot;mysql-bin.000003&quot;,
          &quot;pos&quot;: 484,
          &quot;row&quot;: 0,
          &quot;snapshot&quot;: null
        },
        &quot;op&quot;: &quot;u&quot;,
        &quot;ts_ms&quot;: 1465581029523
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The serialization format that Debezium sends to Kafka is configurable. We prefer Avro at WePay for its compact size, schema DDL, performance, and rich ecosystem. We’ve configured Kafka connect to use Confluent’s &lt;a href=&quot;https://github.com/confluentinc/schema-registry/tree/master/avro-serializer/src/main/java/io/confluent/kafka/serializers&quot;&gt;Avro encoder&lt;/a&gt; codec for Kafka. This encoder serializes messages to Avro, but also registers the schemas with Confluent’s schema registry.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If a MySQL table’s schema is changed, Debezium adapts to the change by updating the structure and schema of the &quot;before&quot; and &quot;after&quot; portions of its event messages. This will appear to the Avro encoder as a new schema, which it will register with the schema registry before the message is sent to Kafka. The registry runs full compatibility checks to make sure that downstream consumers don’t break due to a schema evolution.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;em&gt;Note that it’s still possible to make an incompatible change in the MySQL schema itself, which would break downstream consumers. We have not yet added automatic compatibility checks to MySQL table alters.&lt;/em&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;future_work&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#future_work&quot;&gt;&lt;/a&gt;Future work&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;monolithic_database&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#monolithic_database&quot;&gt;&lt;/a&gt;Monolithic database&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In addition to our microservices, we have a legacy monolithic database that’s much larger than our microservice databases. We’re in the process of upgrading this cluster to run with GTIDs enabled. Once this is done, we plan to replicate this cluster into Kafka with Debezium as well.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;large_table_snapshots&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#large_table_snapshots&quot;&gt;&lt;/a&gt;Large table snapshots&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re lucky that all of our microservice databases are of relatively manageable size. Our monolithic database has some tables that are much larger. We have yet to test Debezium with very large tables, so it’s unclear if any tuning or patches will be required in order to snapshot these tables on the initial Debezium load. We have heard community reports that larger tables (6 billion+ rows) do work, provided that the configuration exposed in &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-152&quot;&gt;DBZ-152&lt;/a&gt; is set. This is work we’re planning to do shortly.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;more_monitoring&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#more_monitoring&quot;&gt;&lt;/a&gt;More monitoring&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka connect doesn’t currently make it easy to expose metrics through the Kafka metrics framework. As a result, there are very few metrics available from the Kafka connect framework. Debezium does expose metrics via JMX (see &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-134&quot;&gt;DBZ-134&lt;/a&gt;), but we aren’t exposing them to our metrics system currently. We do monitor the system, but when things go wrong, it can be difficult to determine what’s going on. &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2376&quot;&gt;KAFKA-2376&lt;/a&gt; is the open JIRA that’s meant to address the underlying Kafka connect issue.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;more_databases&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#more_databases&quot;&gt;&lt;/a&gt;More databases&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As we add more microservice databases, we’ll begin to put pressure on the two Debezium MySQL servers that we have. Eventually, we plan to split the single Debezium cluster that we have into more than one, with some microservices replicating only to one cluster, and the rest replicating to others.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;unify_compatibility_checks&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#unify_compatibility_checks&quot;&gt;&lt;/a&gt;Unify compatibility checks&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As I mentioned in the schema section, above, the Confluent schema registry runs schema compatibility checks out of the box right now. This makes it very easy for us to prevent backward and forward incompatible changes from making their way into Kafka. We don’t currently have an equivalent check at the MySQL layer. This is a problem because it means it’s possible for a DBA to make incompatible changes at the MySQL layer. Debezium will then fail when trying to produce the new messages into Kafka. We need to make sure this can’t happen by adding equivalent checks at the MySQL layer. &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-70&quot;&gt;DBZ-70&lt;/a&gt; discusses this more.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;automatic_topic_configuration&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#automatic_topic_configuration&quot;&gt;&lt;/a&gt;Automatic topic configuration&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We currently run Kafka with topic auto-create enabled with a default of 6 partitions, and time-based/size-based retention. This configuration doesn’t make much sense for Debezium topics. At the very least, they should be using log-compaction as their retention. We plan to write a script that looks for mis-configured Debezium topics, and updates them to appropriate retention settings.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;conclusion&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#conclusion&quot;&gt;&lt;/a&gt;Conclusion&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve been running Debezium in production for the past 8 months. Initially, we ran it dark, and then enabled it for the realtime BigQuery pipeline shown in the architecture diagram above. Recently, we’ve begun consuming the messages in microservices and stream processing systems. We look forward to adding more data to the pipeline, and addressing some of the issues that were raised in the &lt;em&gt;Future work&lt;/em&gt; section.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A special thanks to &lt;a href=&quot;https://www.linkedin.com/in/randallhauch&quot;&gt;Randall Hauch&lt;/a&gt;, who has been invaluable in addressing a number of bug fixes and feature requests.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/02/08/Support-for-Postgresql/</id>
    <title>PostgreSQL support added to Debezium</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2017-02-08T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/02/08/Support-for-Postgresql/" rel="alternate" type="text/html" />
    <author>
      <name>Horia Chiorean</name>
    </author>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      With the recent Debezium release, we&#8217;re happy to announce that a new PostgreSQL connector has been added alongside the already existing MySQL and MongoDB connectors.
      
      
      
      
      
      
      
      
      
      Make sure you read the connector documentation for an in-depth look at the different configuration options.
      
      
      
      
      
      
      
      
      Getting started
      
      
      The fastest way to check out the new connector is using Debezium&#8217;s Postgres docker image which is based on a vanilla Postgres docker image on top of which it compiles and installs a PostgreSQL logical decoding plugin
      and sets up the necessary permissions for streaming changes locally (on localhost)
      
      
      Once you fire up the Docker machine with the database server, starting up...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With the &lt;a href=&quot;http://debezium.io/blog/2017/02/07/Debezium-0-4-0-Released&quot;&gt;recent Debezium release&lt;/a&gt;, we’re happy to announce that a new &lt;strong&gt;PostgreSQL connector&lt;/strong&gt; has been added alongside the already existing MySQL and MongoDB connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;admonitionblock tip&quot;&gt;
      &lt;table&gt;
      &lt;tr&gt;
      &lt;td class=&quot;icon&quot;&gt;
      &lt;i class=&quot;fa icon-tip&quot; title=&quot;Tip&quot;&gt;&lt;/i&gt;
      &lt;/td&gt;
      &lt;td class=&quot;content&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Make sure you &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql&quot;&gt;read the connector documentation&lt;/a&gt; for an in-depth look at the different configuration options.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/td&gt;
      &lt;/tr&gt;
      &lt;/table&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;getting_started&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#getting_started&quot;&gt;&lt;/a&gt;Getting started&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The fastest way to check out the new connector is using &lt;a href=&quot;https://hub.docker.com/r/debezium/postgres&quot;&gt;Debezium’s Postgres docker image&lt;/a&gt; which is based on a vanilla Postgres docker image on top of which it compiles and installs a PostgreSQL &lt;a href=&quot;https://github.com/debezium/postgres-decoderbufs&quot;&gt;logical decoding plugin&lt;/a&gt;
      and sets up the necessary permissions for streaming changes locally (on &lt;code&gt;localhost&lt;/code&gt;)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once you fire up the Docker machine with the database server, starting up and configuring the connector to stream changes from that machine is exactly the same as described in detail by the &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;Debezium tutorial&lt;/a&gt;. The only obvious difference is that instead of the MySQL machine and MySQL connector configuration you need to use the PostgreSQL machine and the PostgreSQL connector configuration parameters.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;using_the_connector_in_your_own_environment&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#using_the_connector_in_your_own_environment&quot;&gt;&lt;/a&gt;Using the connector in your own environment&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Unlike the Mongo and MySQL connectors, getting the PostgreSQL connector up and running is a bit more complicated due to the fact that it requires a server-side logical decoding plugin running in the PostgreSQL server.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In general, there are three major steps involved in getting the connector running in your environment:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;Compiling and installing the &lt;a href=&quot;https://github.com/debezium/postgres-decoderbufs&quot;&gt;logical decoding plugin&lt;/a&gt; into your own server&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Setting up the PostgreSQL server with appropriate replication permissions&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Starting the Kafka Connect, Broker and Zookeeper machines&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For steps 1 and 2 you can check out our &lt;a href=&quot;https://github.com/debezium/docker-images/tree/master/postgres/9.6&quot;&gt;PostgreSQL Docker image&lt;/a&gt; together with the sources for the &lt;a href=&quot;https://github.com/debezium/postgres-decoderbufs&quot;&gt;logical decoding plugin&lt;/a&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For step 3 you can either use Debezium’s &lt;a href=&quot;https://github.com/debezium/docker-images&quot;&gt;Kafka Docker images&lt;/a&gt; or perform a similar setup locally. The &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;Debezium tutorial&lt;/a&gt; and the &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql&quot;&gt;the connector documentation&lt;/a&gt; are great resources for helping out with this task.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/02/07/Debezium-0-4-0-Released/</id>
    <title>Debezium 0.4.0 Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2017-02-07T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/02/07/Debezium-0-4-0-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.4.0 is now available for use with Kafka Connect 0.10.1.1. This release introduces a new PostgreSQL connector, and contains over a dozen fixes combined for the MongoDB connector and MySQL connector, including preliminar support for Amazon RDS and Amazon Aurora (MySQL compatibility). See the release notes for specifics on these changes.
      
      
      We&#8217;ve also created Debezium Docker images labelled 0.4 and latest, which we use in our tutorial.
      
      
      Thanks to Horia, Chris, Akshath, Ramesh, Matthias, Anton, Sagi, barton, and others for their help with this release, issues, discussions, contributions, and questions!
      
      
      
      
      What&#8217;s next
      
      
      We&#8217;ll continue to improve the MongoDB,...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.4.0&lt;/strong&gt; is now available for use with Kafka Connect 0.10.1.1. This release introduces a new &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql&quot;&gt;PostgreSQL connector&lt;/a&gt;, and contains over a dozen fixes combined for the &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb&quot;&gt;MongoDB connector&lt;/a&gt; and &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt;, including preliminar support for &lt;a href=&quot;https://aws.amazon.com/rds/mysql/&quot;&gt;Amazon RDS&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/rds/aurora/&quot;&gt;Amazon Aurora (MySQL compatibility)&lt;/a&gt;. See the &lt;a href=&quot;http://debezium.io/docs/releases&quot;&gt;release notes&lt;/a&gt; for specifics on these changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also created &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.4&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Horia, Chris, Akshath, Ramesh, Matthias, Anton, Sagi, barton, and others for their help with this release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll continue to improve the MongoDB, MySQL, and PostgreSQL connectors and pushing out 0.4.x releases. We’re also going to work on a few new connectors, though we’ll likely increase the minor version with each new connector. Stay tuned and get involved!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/12/21/Debezium-0-3-6-Released/</id>
    <title>Debezium 0.3.6 Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2016-12-21T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/12/21/Debezium-0-3-6-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.3.6 is now available for use with Kafka Connect 0.10.0.1. This release contains over a dozen fixes combined for the MySQL connector and MongoDB connectors. See the release notes for specifics on these changes.
      
      
      We&#8217;ve also updated the Debezium Docker images labelled 0.3 and latest, which we use in our tutorial.
      
      
      Thanks to Farid, RenZhu, Dongjun, Anton, Chris, Dennis, Sharaf, Rodrigo, Tim, and others for their help with this release, issues, discussions, contributions, and questions!
      
      
      
      
      What&#8217;s next
      
      
      We&#8217;ll continue to improve the MongoDB and MySQL connectors, and we also have a great PostgreSQL connector that is nearly ready...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.3.6&lt;/strong&gt; is now available for use with Kafka Connect 0.10.0.1. This release contains over a dozen fixes combined for the MySQL connector and MongoDB connectors. See the &lt;a href=&quot;http://debezium.io/docs/releases&quot;&gt;release notes&lt;/a&gt; for specifics on these changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Farid, RenZhu, Dongjun, Anton, Chris, Dennis, Sharaf, Rodrigo, Tim, and others for their help with this release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll continue to improve the MongoDB and MySQL connectors, and we also have a great PostgreSQL connector that is nearly ready to be released. With the new connector we’ll switch release numbers to 0.4.x and plan to stop issuing 0.3.x releases. Stay tuned for this next 0.4.0 release!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/11/14/Debezium-0-3-5-Released/</id>
    <title>Debezium 0.3.5 Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2016-11-14T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/11/14/Debezium-0-3-5-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.3.5 is now available for use with Kafka Connect 0.10.0.1. This release contains several fixes for the MySQL connector and adds the ability to use with multi-master MySQL servers as sources. See the release notes for specifics on these changes. We&#8217;ve also updated the Debezium Docker images labelled 0.3 and latest, which we use in our tutorial.
      
      
      One of the fixes is signficant, and so we strongly urge all users to upgrade to this release from all earlier versions. In prior versions, the MySQL connector may stop without completing all updates in a transaction, and...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.3.5&lt;/strong&gt; is now available for use with Kafka Connect 0.10.0.1. This release contains several fixes for the MySQL connector and adds the ability to use with &lt;a href=&quot;http://debezium.io/docs/mysql#multi-master-mysql&quot;&gt;multi-master MySQL servers&lt;/a&gt; as sources. See the &lt;a href=&quot;http://debezium.io/docs/releases&quot;&gt;release notes&lt;/a&gt; for specifics on these changes. We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;One of the fixes is signficant, and so &lt;strong&gt;we strongly urge all users to upgrade to this release from all earlier versions.&lt;/strong&gt; In prior versions, the MySQL connector may stop without completing all updates in a transaction, and when the connector restarts it starts with the &lt;em&gt;next&lt;/em&gt; transaction and therefore might fail to capture some of the change events in the earlier transaction. This release fixes this issue so that when restarting it will always pick up where it left off, even if that point is in the middle of a transaction. Note that this fix only takes affect once a connector is upgraded and restarted. See &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-144&quot;&gt;the issue&lt;/a&gt; for more details.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Akshath, Anton, Chris, and others for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/10/25/Debezium-0-3-4-Released/</id>
    <title>Debezium 0.3.4 Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2016-10-25T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/10/25/Debezium-0-3-4-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.3.4 is now available for use with Kafka Connect 0.10.0.1. This release contains several new features for the MySQL connector: support for MySQL&#8217;s JSON datatype, a new snapshot mode called schema_only, and JMX metrics. Also, the Debezium Docker images for Zookeeper, Kafka, and Kafka Connect have all been updated to allow optionally expose JMX metrics in these services. And, one backward-incompatible fix was made to the change event&#8217;s ts_sec field. See the release notes for specifics.
      
      
      We&#8217;ve also updated the Debezium Docker images labelled 0.3 and latest, which we use in our tutorial.
      
      
      Thanks to Akshath,...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.3.4&lt;/strong&gt; is now available for use with Kafka Connect 0.10.0.1. This release contains several new features for the MySQL connector: support for MySQL’s &lt;a href=&quot;http://debezium.io/docs/connectors/mysql#data-types&quot;&gt;&lt;code&gt;JSON&lt;/code&gt;&lt;/a&gt; datatype, a new snapshot mode called &lt;a href=&quot;http://debezium.io/docs/connectors/mysql#snapshots&quot;&gt;&lt;code&gt;schema_only&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;http://debezium.io/docs/monitoring&quot;&gt;JMX metrics&lt;/a&gt;. Also, the Debezium Docker images for Zookeeper, Kafka, and Kafka Connect have all been updated to allow optionally &lt;a href=&quot;http://debezium.io/docs/monitoring&quot;&gt;expose JMX metrics&lt;/a&gt; in these services. And, one backward-incompatible fix was made to the change event’s &lt;code&gt;ts_sec&lt;/code&gt; field. See the &lt;a href=&quot;http://debezium.io/docs/releases&quot;&gt;release notes&lt;/a&gt; for specifics.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Akshath, Chris, Vitalii, Dennis, Prannoy, and others for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/10/19/Support-for-MySQL-JSON-typpe-coming-soon/</id>
    <title>Support for MySQL&#8217;s JSON type coming soon</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2016-10-19T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/10/19/Support-for-MySQL-JSON-typpe-coming-soon/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="mysql"></category>
    <category term="json"></category>
    <summary>
      
      MySQL 5.7 introduced a new data type for storing and working with JSON data. Clients can define tables with columns using the new JSON datatype, and they can store and read JSON data using SQL statements and new built-in JSON functions to construct JSON data from other relational columns, introspect the structure of JSON values, and search within and manipulate JSON data. It possible to define generated columns on tables whose values are computed from the JSON value in another column of the same table, and to then define indexes with those generated columns. Overall, this is really a very...
    </summary>
    <content type="html">
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;MySQL 5.7 introduced a new data type for &lt;a href=&quot;http://mysqlserverteam.com/whats-new-in-mysql-5-7-generally-available/&quot;&gt;storing and working with JSON data&lt;/a&gt;. Clients can define tables with columns using the new &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/json.html&quot;&gt;&lt;code&gt;JSON&lt;/code&gt; datatype&lt;/a&gt;, and they can store and read JSON data using SQL statements and new built-in JSON functions to construct JSON data from other relational columns, introspect the structure of JSON values, and search within and manipulate JSON data. It possible to define generated columns on tables whose values are computed from the JSON value in another column of the same table, and to then define indexes with those generated columns. Overall, this is really a very powerful feature in MySQL.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium’s MySQL connector will support the &lt;code&gt;JSON&lt;/code&gt; datatype starting with the upcoming 0.3.4 release. JSON document, array, and scalar values will appear in change events as strings with &lt;code&gt;io.debezium.data.json&lt;/code&gt; for the schema name. This will make it natural for consumers to work with JSON data. BTW, this is the same semantic schema type used by the MongoDB connector to represent JSON data.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This sounds straightforward, and we hope it is. But &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-126&quot;&gt;implementing this&lt;/a&gt; required a fair amount of work. That’s because although MySQL exposes JSON data as strings to client applications, &lt;em&gt;internally&lt;/em&gt; it stores all JSON data in a special binary form that allows the MySQL engine to efficiently access the JSON data in queries, JSON functions and generated columns. All JSON data appears in the binlog in this binary form as well, which meant that we had to parse the binary form ourselves if we wanted to extract the more useful string representation. Writing and testing this parser took a bit of time and effort, and ultimately we &lt;a href=&quot;https://github.com/shyiko/mysql-binlog-connector-java/issues/115&quot;&gt;donated it&lt;/a&gt; to the excellent &lt;a href=&quot;https://github.com/shyiko/mysql-binlog-connector-java&quot;&gt;MySQL binlog client library&lt;/a&gt; that the connector uses internally to read the binlog events.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’d like to thank &lt;a href=&quot;https://github.com/shyiko&quot;&gt;Stanley Shyiko&lt;/a&gt; for guiding us and helping us debug the final problems with parsing JSON in the binlog, for accepting our proposed changes into his library, for releasing his library quickly when needed, and for being so responsive on this and other issues!&lt;/p&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/10/18/Debezium-0-3-3-Released/</id>
    <title>Debezium 0.3.3 Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2016-10-18T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/10/18/Debezium-0-3-3-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.3.3 is now available for use with Kafka Connect 0.10.0.1. This release contains a handful of bug fixes and minor improvements for the MySQL connector, including better handling of BIT(n) values, ENUM and SET values, and GTID sets, This release also improves the log messages output by the MySQL connectors to better represent the ongoing activity when consuming the changes from the source database. See the release notes for specifics.
      
      
      We&#8217;ve also updated the Debezium Docker images labelled 0.3 and latest, which we use in our tutorial. We&#8217;ve also updated the tutorial to use the...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.3.3&lt;/strong&gt; is now available for use with Kafka Connect 0.10.0.1. This release contains a handful of bug fixes and minor improvements for the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt;, including better handling of &lt;code&gt;BIT(n)&lt;/code&gt; values, &lt;code&gt;ENUM&lt;/code&gt; and &lt;code&gt;SET&lt;/code&gt; values, and GTID sets, This release also improves the log messages output by the MySQL connectors to better represent the ongoing activity when consuming the changes from the source database. See the &lt;a href=&quot;http://debezium.io/docs/releases&quot;&gt;release notes&lt;/a&gt; for specifics.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;. We’ve also updated the tutorial to use the &lt;a href=&quot;https://docs.docker.com/engine/installation/&quot;&gt;latest Docker installations&lt;/a&gt; on Linux, Windows, and OS X.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Akshath, Chris, Randy, Prannoy, Umang, Horia, and others for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/09/26/Debezium-0-3-2-Released/</id>
    <title>Debezium 0.3.2 Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2016-09-26T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/09/26/Debezium-0-3-2-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.3.2 is now available for use with Kafka Connect 0.10.0.1. This release contains a handful of bug fixes and minor improvements for the MySQL connector and MongoDB connector. The MySQL connector better handles BIT(n) values and zero-value date and timestamp values. This release also improves the log messages output by the MySQL and MongoDB connectors to better represent the ongoing activity when consuming the changes from the source database. See the release notes for specifics.
      
      
      We&#8217;ve also updated the Debezium Docker images labelled 0.3 and latest, which we use in our tutorial. We&#8217;ve also updated...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.3.2&lt;/strong&gt; is now available for use with Kafka Connect 0.10.0.1. This release contains a handful of bug fixes and minor improvements for the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt; and &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MongoDB connector&lt;/a&gt;. The MySQL connector better handles &lt;code&gt;BIT(n)&lt;/code&gt; values and &lt;a href=&quot;http://dev.mysql.com/doc/refman/5.7/en/date-and-time-types.html&quot;&gt;zero-value&lt;/a&gt; date and timestamp values. This release also improves the log messages output by the MySQL and MongoDB connectors to better represent the ongoing activity when consuming the changes from the source database. See the &lt;a href=&quot;http://debezium.io/docs/releases&quot;&gt;release notes&lt;/a&gt; for specifics.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;. We’ve also updated the tutorial to use the &lt;a href=&quot;https://docs.docker.com/engine/installation/&quot;&gt;latest Docker installations&lt;/a&gt; on Linux, Windows, and OS X.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Akshath, Colum, Emmanuel, Konstantin, Randy, RenZhu, Umang, and others for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/09/19/Serializing-Debezium-events-with-Avro/</id>
    <title>Serializing Debezium events with Avro</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2016-09-19T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/09/19/Serializing-Debezium-events-with-Avro/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="kafka"></category>
    <category term="avro"></category>
    <category term="serialization"></category>
    <summary>
      
      
      
      Although Debezium makes it easy to capture database changes and record them in Kafka, one of the more important decisions you have to make is how those change events will be serialized in Kafka. Every message in Kafka has a key and a value, and to Kafka these are opaque byte arrays. But when you set up Kafka Connect, you have to say how the Debezium event keys and values should be serialized to a binary form, and your consumers will also have to deserialize them back into a usable form.
      
      
      Debezium event keys and values are both structured, so JSON...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Although Debezium makes it easy to capture database changes and record them in Kafka, one of the more important decisions you have to make is &lt;em&gt;how&lt;/em&gt; those change events will be serialized in Kafka. Every message in Kafka has a key and a value, and to Kafka these are opaque byte arrays. But when you set up Kafka Connect, you have to say how the Debezium event keys and values should be serialized to a binary form, and your consumers will also have to deserialize them back into a usable form.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium event keys and values are both structured, so JSON is certainly a reasonable option — it’s flexible, ubiquitous, and language agnostic, but on the other hand it’s quite verbose. One alternative is Avro, which is also flexible and language agnostic, but also faster and results in smaller binary representations. Using Avro requires a bit more setup effort on your part and some additional software, but the advantages are often worth it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;kafka_serializers_and_deserializers&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#kafka_serializers_and_deserializers&quot;&gt;&lt;/a&gt;Kafka serializers and deserializers&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Before we get too far, let’s back up and review how Kafka producers and consumers normally do this serialization and deserialization. Because the keys and values are simple opaque byte arrays, you can use anything for your keys and values. For example, consider a case where we’re using simple whole numbers for the keys and strings for the values. Here, a producer of these messages would use a &lt;em&gt;long serializer&lt;/em&gt; to convert the &lt;code&gt;long&lt;/code&gt; keys to binary form and a &lt;em&gt;string serializer&lt;/em&gt; to convert the &lt;code&gt;String&lt;/code&gt; values to binary form. Meanwhile, the consumers use a &lt;em&gt;long deserializer&lt;/em&gt; to convert the binary keys into usable &lt;code&gt;long&lt;/code&gt; values, and a &lt;em&gt;string deserializer&lt;/em&gt; to convert the binary values back into &lt;code&gt;String&lt;/code&gt; objects.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In cases where the keys and/or values need to be a bit more structured, the producers and consumers can be written to use JSON structures for keys and/or values, and the Kafka-provided &lt;em&gt;JSON serializer and deserializer&lt;/em&gt; to do the conversion to and from binary form stored within the Kafka messages. As we said earlier, using JSON for keys and/or values is very flexible and language agnostic, but it is also produces keys and values that are relatively large since the fields and structure of the JSON values need to be encoded as well.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;avro_serialization&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#avro_serialization&quot;&gt;&lt;/a&gt;Avro serialization&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://avro.apache.org/&quot;&gt;Avro&lt;/a&gt; is a data serialization mechanism that uses a &lt;em&gt;schema&lt;/em&gt; to define the structure of data. Avro relies upon this schema when writing the data to the binary format, and the schema allows it to encode the fields within the data in a much more compact form. Avro also relies upon the schema when &lt;em&gt;reading&lt;/em&gt; the data, too. But interestingly, Avro schemas are designed to evolve, so it is actually possible to use a slightly different schema for reading than what was used for writing. This feature makes Avro a great choice for Kafka serialization and deserialization.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://confluent.io&quot;&gt;Confluent&lt;/a&gt; provides a &lt;a href=&quot;http://docs.confluent.io/3.0.1/app-development.html&quot;&gt;Kafka serializer and deserializer that uses Avro&lt;/a&gt; and a separate &lt;a href=&quot;http://docs.confluent.io/3.0.1/schema-registry/docs/intro.html&quot;&gt;Schema Registry&lt;/a&gt;, and it works like this: when a numeric or string object are to be serialized, the &lt;em&gt;Avro serializer&lt;/em&gt; will determine the corresponding Avro Schema for the given type, register with the Schema Registry this schema and the topic its used on, get back the unique identifier for the schema, and then encode in the binary form the unique identifier of the schema and the encoded value. The next message is likely to have the same type and thus schema, so the serializer can quickly encode the schema identifier and value for this message without having to talk to the Schema Registry. Only when needing to serialize a schema it hasn’t already seen does the Avro serializer talk with the Schema Registry. So not only is this fast, but it also produces very compact binary forms and allows for the producer to &lt;em&gt;evolve&lt;/em&gt; its key and/or value schemas over time. The Schema Registry can also be configured to allow new versions of schemas to be registered only when they are &lt;em&gt;compatible&lt;/em&gt; with the Avro schema evolution rules, ensuring that producers do not produce messages that consumers will not be able to read.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Consumers, meanwhile, use the &lt;em&gt;Avro deserializer&lt;/em&gt;, which works in a similar manner, albeit backwards: when it reads the binary form of a key or value, it first looks for the schema identifier and, if it hasn’t seen it before asks the Schema Registry for the schema, and then uses that schema to decode the remainder of the binary representation into its object form. Again, if the deserializer has previously seen a particular schema identifier, it already has the schema needed to decode the data and doesn’t have to consult the Schema Registry.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;kafka_connect_converters&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#kafka_connect_converters&quot;&gt;&lt;/a&gt;Kafka Connect converters&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka Connect is a bit different than many Kafka producers/consumers, since the keys and values will often be structured. And rather than require connectors to work with JSON objects, Kafka Connect defines its own lightweight framework for defining data structures with a schema, making it much easier to write connectors to work with structured data. Kafka Connect defines its own &lt;em&gt;converters&lt;/em&gt; that are similar to Kafka (de)serializers, except that Kafka Connect’s converters know about these structures and schemas and can serialize the keys and values to binary form. Kafka Connect provides a &lt;em&gt;JSON converter&lt;/em&gt; that converts the structures into JSON and then uses the normal Kafka JSON serializer, so downstream consumers can just use the normal Kafka JSON deserializer and get a JSON representation of the Kafka Connect structs and schema. This is exactly what the &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;Debezium tutorial&lt;/a&gt; is using, and the &lt;code&gt;watch-topic&lt;/code&gt; consumer knows to use the JSON deserializer.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;One great feature of Kafka Connect is that the connectors simply provide the structured messages, and Kafka Connect takes care of serializing them using the configured converter. This means that you can use any Kafka Connect &lt;em&gt;converters&lt;/em&gt; with any Kafka Connect connector, including all of Debezium’s connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka Connect’s schema system was designed specifically with Avro in mind, so there is a one-to-one mapping between Kafka Connect schemas and Avro schemas. Confluent provides an &lt;em&gt;Avro Converter&lt;/em&gt; for Kafka Connect that serializes the Kafka Connect structs provided by the connectors into the compact Avro binary representation, again using the Schema Registry just like the Avro serializer. The consumer just uses the normal Avro deserializer as mentioned above.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Using Avro for serialization of Debezium events brings several significant advantages:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;The encoded binary forms of the Debezium events are &lt;em&gt;significantly&lt;/em&gt; smaller than the JSON representations. Not only is the structured data encoded in a more compact form, but the &lt;em&gt;schema&lt;/em&gt; associated with that structured data is represented in the binary form as a single integer.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Encoding the Debezium events into their Avro binary forms is fast. Only when the converter sees a new schema does it have to consult with the Schema Registry; otherwise, the schema has already been seen and its encoding logic already precomputed.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The Avro Converter for Kafka Connect produces messages with Avro-encoded keys and values that can be read by any Kafka consumers using the Avro deserializer.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Debezium event structures are based upon the structure of the table from which the changes were captured. When the structure of the source table changes (e.g., because an &lt;code&gt;ALTER&lt;/code&gt; statement was applied to it), the structure and schema of the events will also change. If this is done in a manner such that the new Avro schema is &lt;em&gt;compatible with&lt;/em&gt; the older Avro schema, then consumers will be able to process the events without disruption, even though the event structures evolve over time.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Avro’s schema mechanism is far more formal and rigorous than the free-form JSON structure, and the changes in the schemas are clearly identified when comparing any two messages.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The Avro converter, Avro (de)serializers, and Schema Registry are all open source.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It is true that using the Avro converter and deserializer requires a running Schema Registry, and that the registry becomes an integral part of your streaming infrastructure. However, this is a small price to pay for the benefits listed above.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;using_the_avro_converter_with_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#using_the_avro_converter_with_debezium&quot;&gt;&lt;/a&gt;Using the Avro Converter with Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As mentioned above, in the interest of keeping the Debezium tutorial as simple as possible, we avoid using the Schema Registry or the Avro converter in the tutorial. We also don’t (yet) include the Avro converter in our Docker images, though that &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-59&quot;&gt;will change soon&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Nevertheless, it is absolutely possible to use the Avro Converter with the Debezium connectors when you are installing the connectors into either the Confluent Platform or into your own installation of Kafka Connect. Simply configure the &lt;a href=&quot;http://docs.confluent.io/3.0.1/connect/userguide.html&quot;&gt;Kafka Connect workers&lt;/a&gt; to use the Avro converter for the keys and values:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;key.converter=io.confluent.connect.avro.AvroConverter
      value.converter=io.confluent.connect.avro.AvroConverter&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;And, if you want to use the Avro Converter for Kafka Connect internal messages, then set these as well:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;internal.key.converter=io.confluent.connect.avro.AvroConverter
      internal.value.converter=io.confluent.connect.avro.AvroConverter&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once again, there is no need to configure the Debezium connectors any differently.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/08/30/Debezium-0-3-1-Released/</id>
    <title>Debezium 0.3.1 Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2016-08-30T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/08/30/Debezium-0-3-1-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.3.1 is now available for use with Kafka Connect 0.10.0.1. This release contains an updated MySQL connector with a handful of bug fixes and two significant but backward-compatible changes. First, the MySQL connector now supports using secure connections to MySQL, adding to the existing ability to connect securely to Kafka. Second, the MySQL connector is able to capture MySQL string values using the proper character sets so that any values stored in the database can be captured correctly in events. See our release notes for details of these changes and for upgrading recommendations.
      
      
      We&#8217;ve also...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.3.1&lt;/strong&gt; is now available for use with Kafka Connect 0.10.0.1. This release contains an updated &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt; with a handful of bug fixes and two significant but backward-compatible changes. First, the MySQL connector now supports using secure connections to MySQL, adding to the existing ability to connect securely to Kafka. Second, the MySQL connector is able to capture MySQL string values using the proper character sets so that any values stored in the database can be captured correctly in events. See our &lt;a href=&quot;http://debezium.io/docs/releases#release-0-3-1&quot;&gt;release notes&lt;/a&gt; for details of these changes and for upgrading recommendations.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Chris, Akshath, barten, and and others for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/08/16/Debezium-0-3-0-Released/</id>
    <title>Debezium 0.3.0 Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2016-08-16T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/08/16/Debezium-0-3-0-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      After a few weeks delay, Debezium 0.3.0 is now available for use with Kafka Connect 0.10.0.1. This release contains an updated MySQL connector with quite a few bug fixes, and a new MongoDB connector that captures the changes made to a MongoDB replica set or MongoDB sharded cluster. See the documentation for details about how to configure these connectors and how they work.
      
      
      We&#8217;ve also updated the Debezium Docker images (with labels 0.3 and latest) used in our tutorial.
      
      
      Thanks to Andrew, Bhupinder, Chris, David, Horia, Konstantin, Tony, and others for their help with the release, issues, discussions, contributions, and questions!
      
      
      
      
      About Debezium
      
      
      Debezium...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;After a few weeks delay, &lt;strong&gt;Debezium 0.3.0 is now available&lt;/strong&gt; for use with Kafka Connect 0.10.0.1. This release contains an updated &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt; with quite a few bug fixes, and a new &lt;strong&gt;&lt;em&gt;&lt;a href=&quot;http://debezium.io/docs/connectors/mongodb&quot;&gt;MongoDB connector&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt; that captures the changes made to a MongoDB replica set or MongoDB sharded cluster. See the &lt;a href=&quot;http://debezium.io/docs/connectors&quot;&gt;documentation&lt;/a&gt; for details about how to configure these connectors and how they work.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; (with labels &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;) used in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Andrew, Bhupinder, Chris, David, Horia, Konstantin, Tony, and others for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/08/16/Debezium-0-2-4-Released/</id>
    <title>Debezium 0.2.4 Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2016-08-16T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/08/16/Debezium-0-2-4-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m happy to announce that Debezium 0.2.4 is now available for use with Kafka Connect 0.9.0.1. This release adds more verbose logging during MySQL snapshots, enables taking snapshots of very large MySQL databases, and correct a potential exception during graceful shutdown. See our release notes for details of these changes and for upgrading recommendations.
      
      
      We&#8217;ve also updated the Debezium Docker images (with label 0.2 and latest) used in our tutorial.
      
      
      Thanks to David and wangshao for their help with the release, issues, discussions, contributions, and questions!
      Stay tuned for our next release, which will be 0.3 and will have a new MongoDB connector...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m happy to announce that &lt;strong&gt;Debezium 0.2.4 is now available&lt;/strong&gt; for use with Kafka Connect 0.9.0.1. This release adds more verbose logging during MySQL snapshots, enables taking snapshots of very large MySQL databases, and correct a potential exception during graceful shutdown. See our &lt;a href=&quot;http://debezium.io/docs/releases#release-0-2-4&quot;&gt;release notes&lt;/a&gt; for details of these changes and for upgrading recommendations.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; (with label &lt;code&gt;0.2&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;) used in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to David and wangshao for their help with the release, issues, discussions, contributions, and questions!
      Stay tuned for our next release, which will be 0.3 and will have a new MongoDB connector and will support Kafka Connect 0.10.0.1.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/08/02/capturing-changes-from-mysql/</id>
    <title>Capturing changes from MySQL</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2016-08-02T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/08/02/capturing-changes-from-mysql/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="mysql"></category>
    <summary>
      
      
      
      Change data capture is a hot topic. Debezium&#8217;s goal is to make change data capture easy for multiple DBMSes, but admittedly we&#8217;re still a young open source project and so far we&#8217;ve only released a connector for MySQL with a connector for MongoDB that&#8217;s just around the corner. So it&#8217;s great to see how others are using and implementing change data capture. In this post, we&#8217;ll review Yelp&#8217;s approach and see how it is strikingly similar to Debezium&#8217;s MySQL connector.
      
      
      
      
      Streaming data at Yelp
      
      
      The Yelp Engineering Blog recently began a series describing their real-time streaming data infrastructure. The first post provides...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Change data capture is a hot topic. Debezium’s goal is to make change data capture easy for multiple DBMSes, but admittedly we’re still a young open source project and so far we’ve only released a &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;connector for MySQL&lt;/a&gt; with a &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb&quot;&gt;connector for MongoDB&lt;/a&gt; that’s just around the corner. So it’s great to see how others are using and implementing change data capture. In this post, we’ll review Yelp’s approach and see how it is strikingly similar to Debezium’s MySQL connector.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;streaming_data_at_yelp&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#streaming_data_at_yelp&quot;&gt;&lt;/a&gt;Streaming data at Yelp&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;a href=&quot;http://engineeringblog.yelp.com/&quot;&gt;Yelp Engineering Blog&lt;/a&gt; recently began a series describing their real-time streaming data infrastructure. The &lt;a href=&quot;http://engineeringblog.yelp.com/2016/07/billions-of-messages-a-day-yelps-real-time-data-pipeline.html&quot;&gt;first post&lt;/a&gt; provides a good introduction and explains how moving from a monolith to a service-oriented architecture increased productivity, but also made it more challenging to work with data spread across the 100 services that own it. It’s totally worth your time to read it right now.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As Justin writes in the post, several reasons prompted them to create their own real time streaming data pipeline:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Ensuring data always remains consistent across services is always a difficult task, but especially so when things can and do go wrong. Transactions across services may be useful in some situations, but they’re not straightforward, are expensive, and can lead to request amplification where one service calls another, which coordinates with two others, etc.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Services that update data in multiple backend services suffer from the &lt;a href=&quot;http://www.confluent.io/blog/using-logs-to-build-a-solid-data-infrastructure-or-why-dual-writes-are-a-bad-idea/&quot;&gt;dual write problem&lt;/a&gt;, which is where a failure occurs after one backing service was updated but before the other could be updated and that always results in data inconsistencies that are difficult to track down and correct.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Combining and integrating data spread across multiple services can also be difficult and expensive, but it is even harder when that data is continously changing. One approach is to use bulk APIs, but these can beprohibitive to create, can result in inconsistencies, and pose real scalability problems when services need to continually receive the never-ending updates to data.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Yelp’s Real-Time Data Pipeline records changes to data on totally ordered distributed logs so that downstream consumers can receive and process the same changes in exactly the same order. Services can consume changes made by other services, and can therefore stay in sync without explicit interservice communication. This system uses among other things Kafka for event logs, a homegrown system named &lt;a href=&quot;http://engineeringblog.yelp.com/2016/08/streaming-mysql-tables-in-real-time-to-kafka.html&quot;&gt;MySQLStreamer&lt;/a&gt; to capture committed changes to MySQL tables, &lt;a href=&quot;http://avro.apache.org&quot;&gt;Avro&lt;/a&gt; for message format and schemas, and a custom &lt;a href=&quot;http://engineeringblog.yelp.com/2016/07/billions-of-messages-a-day-yelps-real-time-data-pipeline.html#yelps-real-time-data-pipeline&quot;&gt;Schematizer&lt;/a&gt; service that tracks consumers and enforces the Avro schemas used for messages on every Kafka topic.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;how_yelp_captures_mysql_changes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#how_yelp_captures_mysql_changes&quot;&gt;&lt;/a&gt;How Yelp captures MySQL changes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Perhaps most interesting for Debezium is how Yelp captures the committed changes in their MySQL databases and write them to Kafka topics. Their &lt;a href=&quot;http://engineeringblog.yelp.com/2016/08/streaming-mysql-tables-in-real-time-to-kafka.html&quot;&gt;second post in the series&lt;/a&gt; goes into a lot more detail about their MySQLStreamer process that reads the MySQL binary log and continously processes the DDL statements and DML operations that appear in the log, generating the corresponding &lt;em&gt;insert&lt;/em&gt;, &lt;em&gt;update&lt;/em&gt;, &lt;em&gt;delete&lt;/em&gt;, and &lt;em&gt;refresh&lt;/em&gt; events, and writing these event messages to a separate Kafka topic for each MySQL table. We’ve &lt;a href=&quot;http://debezium.io/blog/2016-04-15-parsing-ddl&quot;&gt;mentioned before&lt;/a&gt; that MySQL’s row-level binlog events that result from the DML operation don’t include the full definition of the columns, so knowing what the columns mean in each event requires process the DDL statements that also appear in the binlog. Yelp uses a separate MySQL instance it calls the &lt;em&gt;schema tracker database&lt;/em&gt;, which behaves like a MySQL slave to which are applied only the DDL statements they read from the binlog. This technique lets Yelp’s MySQLStreamer system know the state of the database schema and the structure of its tables at the point in the binlog where they are processing events. This is pretty interesting, because it uses the MySQL engine to handle the DDL parsing.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Yelp’s MySQLStreamer process uses another MySQL database to track internal state describing its position in the binlog, what events have been successfully published to Kafka, and, because the binlog position varies on each replica, replica-independent information about each transaction. This latter information is similar to MySQL GTIDs, although Yelp is using earlier versions of MySQL that do not support GTIDs.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Of course, special consideration has to be taken for databases that have been around for a long time. The MySQL binlogs are capped and will not contain the &lt;em&gt;entire&lt;/em&gt; history of the databases, so Yelp’s MySQLStreamer process bootstraps the change data capture process of old databases by starting another clean MySQL replica, which will use the built-in MySQL replication mechanism with the &lt;a href=&quot;http://dev.mysql.com/doc/refman/5.7/en/blackhole-storage-engine.html&quot;&gt;MySQL blackhole database engine&lt;/a&gt; to obtain a consistent snapshot of the master and so that all activity is logged in the replica’s binlog while the replica actually stores no data.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Yelp’s MySQLStreamer mechanism is quite ingenious in its use of MySQL and multiple extra databases to capture changes from MySQL databases and write them to Kafka topics. The downside, of course, is that doing so does increase the operational complexity of the system.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;similar_purpose_similar_approach&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#similar_purpose_similar_approach&quot;&gt;&lt;/a&gt;Similar purpose, similar approach&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source project that is building a change data capture for a variety of DBMSes. Like Yelp’s MySQLStreamer, Debezium’s &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL Connector&lt;/a&gt; can continously capture the committed changes to MySQL database rows and record these events in a separate Kafka topic for each table. When first started, Debezium’s MySQL Connector can perform an initial consistent snapshot and then begin reading the MySQL binlog. It uses both DDL and DML operations that appear in the binlog, directly &lt;a href=&quot;http://debezium.io/blog/2016-04-15-parsing-ddl&quot;&gt;parsing and using the DDL statements&lt;/a&gt; to learn the changes to each table’s structure and the mapping of each insert, update, and delete binlog event. And each resulting change event written to Kafka includes information about the originating MySQL server and its binlog position, as well as the before and/or after states of the affected row.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;However, unlike Yelp’s MySQLStreamer, the Debezium MySQL connector doesn’t need or use extra MySQL databases to parse DDL or to store the connector’s state. Instead, Debezium is built on top of Kafka Connect, which is a new Kafka library that provides much of the generic functionality of reliably pulling data from external systems, pushing it into Kafka topics, and tracking what data has already been processed. Kafka Connect stores this state inside Kafka itself, simplifying the operational footprint. Debezium’s MySQL connector can then focus on the details of performing a consistent snapshot when required, reading the binlog, and converting the binlog events into useful change events.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Yelp’s real time data pipeline makes use of a custom Avro schema registry, and uses those Avro schemas to encode each event into a compact binary representation while keeping the metadata about the structure of the event. It’s possible to do this with Debezium, too: simply run &lt;a href=&quot;http://docs.confluent.io/3.0.0/schema-registry/docs/index.html&quot;&gt;Confluent’s Schema Registry&lt;/a&gt; as a service and then configure the Kafka Connect worker to use the &lt;a href=&quot;http://debezium.io/docs/faq#avro-converter&quot;&gt;Avro Converter&lt;/a&gt;. As the converter serializes each event, it looks at the structure defined by the connector and, when that structure changes, generates an updated Avro Schema and registers it with the Schema Registry. That new Avro schema is then used to encode the event (and others with an identical structure) into a compact binary form written to Kafka. And of course, consumers then also use the same Avro converter so that as events are deserialized, the converter coordinates with the Schema Registry whenever it needs an Avro schema it doesn’t know about. As a result, the events are stored in a compact manner while the events' content and metadata remain available, while Schema Registry captures and maintains the history of the Avro schema for each table as it evolves over time.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;capturing_changes_from_mysql_with_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#capturing_changes_from_mysql_with_debezium&quot;&gt;&lt;/a&gt;Capturing changes from MySQL with Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’re interested in change data capture with MySQL (or any other DBMSes), give Debezium a try by going through &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;our tutorial&lt;/a&gt; that walks you through starting Kafka, Kafka Connect, and Debezium’s MySQL Connector to see exactly what change data events look like and how they can be used. Best of all, it’s open source with a growing community of developers that has had the benefit of building on top of recently-created Kafka Connect framework. Our MySQL connector is ready now, but we’re working on &lt;a href=&quot;http://debezium.io/docs/connectors/&quot;&gt;connectors for other DBMSes&lt;/a&gt;. Specifically, our upcoming 0.3 release will include our &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb&quot;&gt;MongoDB Connector&lt;/a&gt;, with 0.4 including connectors for PostgreSQL and/or Oracle.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;em&gt;Correction: A previous version of this post incorrectly stated that Yelp was using a MySQL version that did support GTIDs, when in fact they are using a version that does &lt;strong&gt;not&lt;/strong&gt; support MySQL GTIDs. The post has been corrected, and the author regrets the mistake.&lt;/em&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/07/26/Debezium-0-2-3-Released/</id>
    <title>Debezium 0.2.3 Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2016-07-26T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/07/26/Debezium-0-2-3-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m happy to announce that Debezium 0.2.3 is now available for use with Kafka Connect 0.9.0.1. This release corrects the MySQL connector behavior when working with TINYINT and SMALLINT columns or with TIME, DATE, and TIMESTAMP columns. See our release notes for details of these changes and for upgrading recommendations.
      
      
      We&#8217;ve also updated the Debezium Docker images (with label 0.2 and latest) used in our tutorial.
      
      
      Thanks to Chris, Christian, Laogang, and Tony for their help with the release, issues, discussions, contributions, and questions!
      Stay tuned for our next release, which will be 0.3 and will have a new MongoDB connector and will...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m happy to announce that &lt;strong&gt;Debezium 0.2.3 is now available&lt;/strong&gt; for use with Kafka Connect 0.9.0.1. This release corrects the MySQL connector behavior when working with &lt;code&gt;TINYINT&lt;/code&gt; and &lt;code&gt;SMALLINT&lt;/code&gt; columns or with &lt;code&gt;TIME&lt;/code&gt;, &lt;code&gt;DATE&lt;/code&gt;, and &lt;code&gt;TIMESTAMP&lt;/code&gt; columns. See our &lt;a href=&quot;http://debezium.io/docs/releases#release-0-2-3&quot;&gt;release notes&lt;/a&gt; for details of these changes and for upgrading recommendations.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; (with label &lt;code&gt;0.2&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;) used in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Chris, Christian, Laogang, and Tony for their help with the release, issues, discussions, contributions, and questions!
      Stay tuned for our next release, which will be 0.3 and will have a new MongoDB connector and will support Kafka Connect 0.10.0.0.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/06/22/Debezium-0-2-2-Released/</id>
    <title>Debezium 0.2.2 Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2016-06-22T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/06/22/Debezium-0-2-2-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m happy to announce that Debezium 0.2.2 is now available. This release fixes several bugs in the MySQL connector that can produce change events with incorrect source metadata, and that eliminates the possibility a poorly-timed connector crash causing the connector to only process some of the rows in a multi-row MySQL event. See our release notes for details of these changes and for upgrading recommendations.
      
      
      Also, thanks to a community member for reporting that Debezium 0.2.x can only be used with Kafka Connect 0.9.0.1. Debezium 0.2.x cannot be used with Kafka Connect 0.10.0.0 because of its backward incompatible changes to the...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m happy to announce that &lt;strong&gt;Debezium 0.2.2 is now available&lt;/strong&gt;. This release fixes several bugs in the MySQL connector that can produce change events with incorrect &lt;code&gt;source&lt;/code&gt; metadata, and that eliminates the possibility a poorly-timed connector crash causing the connector to only process some of the rows in a multi-row MySQL event. See our &lt;a href=&quot;http://debezium.io/docs/releases#release-0-2-2&quot;&gt;release notes&lt;/a&gt; for details of these changes and for upgrading recommendations.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also, thanks to a community member for &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-80&quot;&gt;reporting&lt;/a&gt; that Debezium 0.2.x can only be used with Kafka Connect 0.9.0.1. Debezium 0.2.x cannot be used with Kafka Connect 0.10.0.0 because of its &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3006&quot;&gt;backward incompatible changes to the consumer API&lt;/a&gt;. Our next release of Debezium will support Kafka 0.10.x.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; (with label &lt;code&gt;0.2&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;) used in our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;. And stay tuned, because we’re hoping to add a MongoDB connector in our next release.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Chris, Christian, Konstantin, James, and Bhupinder for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/06/10/Debezium-0/</id>
    <title>Debezium 0.2.1 Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2016-06-10T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/06/10/Debezium-0/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m happy to announce that Debezium 0.2.1 is now available. The MySQL connector has been significantly improved and is now able to monitor and produce change events for HA MySQL clusters using GTIDs, perform a consistent snapshot when starting up the first time, and has a completely redesigned event message structure that provides a ton more information with every event. Our change log has all the details about bugs, enhancements, new features, and backward compatibility notices. We&#8217;ve also updated our tutorial.
      
      
      
      
      
      
      
      
      
      What happened to 0.2.0? Well, we released it to Maven Central before we&#8217;d noticed a few problems that we thought...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m happy to announce that &lt;strong&gt;Debezium 0.2.1 is now available&lt;/strong&gt;. The &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt; has been significantly improved and is now able to monitor and produce change events for &lt;a href=&quot;http://debezium.io/docs/connectors/mysql#ha-mysql-clusters#enabling-gtids&quot;&gt;HA MySQL clusters&lt;/a&gt; using &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;GTIDs&lt;/a&gt;, perform a &lt;a href=&quot;http://debezium.io/docs/connectors/mysql#snapshots&quot;&gt;consistent snapshot&lt;/a&gt; when starting up the first time, and has a completely redesigned &lt;a href=&quot;http://debezium.io/docs/connectors/mysql#events&quot;&gt;event message structure&lt;/a&gt; that provides a ton more information with every event. Our &lt;a href=&quot;http://debezium.io/docs/releases&quot;&gt;change log&lt;/a&gt; has all the details about bugs, enhancements, new features, and backward compatibility notices. We’ve also updated our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;admonitionblock note&quot;&gt;
      &lt;table&gt;
      &lt;tr&gt;
      &lt;td class=&quot;icon&quot;&gt;
      &lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot;&gt;&lt;/i&gt;
      &lt;/td&gt;
      &lt;td class=&quot;content&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;What happened to 0.2.0? Well, we released it to Maven Central before we’d noticed a &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-71&quot;&gt;few&lt;/a&gt; &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-72&quot;&gt;problems&lt;/a&gt; that we thought it best to fix right away. Thus 0.2.1 was born.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/td&gt;
      &lt;/tr&gt;
      &lt;/table&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;installing_the_mysql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#installing_the_mysql_connector&quot;&gt;&lt;/a&gt;Installing the MySQL connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’ve already installed &lt;a href=&quot;https://zookeeper.apache.org&quot;&gt;Zookeeper&lt;/a&gt;, &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt;, and &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt;, then using Debezium’s MySQL connector is easy. Simply download the &lt;a href=&quot;https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/0.2.1/debezium-connector-mysql-0.2.1-plugin.tar.gz&quot;&gt;connector’s plugin archive&lt;/a&gt;, extract the JARs into your Kafka Connect environment, and add the directory with the JARs to &lt;a href=&quot;http://docs.confluent.io/3.0.0/connect/userguide.html#installing-connector-plugins&quot;&gt;Kafka Connect’s classpath&lt;/a&gt;. Restart your Kafka Connect process to pick up the new JARs.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If immutable containers are your thing, then check out &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium’s Docker images&lt;/a&gt; for Zookeeper, Kafka, and Kafka Connect with the MySQL connector already pre-installed and ready to go. Our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt; even walks you through using these images, and this is a great way to learn what Debezium is all about. You can even &lt;a href=&quot;http://debezium.io/blog/2016/05/31/Debezium-on-Kubernetes&quot;&gt;run Debezium on Kubernetes and OpenShift&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;using_the_mysql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#using_the_mysql_connector&quot;&gt;&lt;/a&gt;Using the MySQL connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To use the connector to produce change events for a particular MySQL server or cluster, simply create a &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/#configuration&quot;&gt;configuration file for the MySQL Connector&lt;/a&gt; and use the &lt;a href=&quot;http://docs.confluent.io/3.0.0/connect/userguide.html#rest-interface&quot;&gt;Kafka Connect REST API&lt;/a&gt; to add that connector to your Kafka Connect cluster. When the connector starts, it will grab a consistent snapshot of the databases in your MySQL server and start reading the MySQL binlog, producing events for every inserted, updated, and deleted row. The connector can optionally produce events with the DDL statements that were applied, and you can even choose to produce events for a subset of the databases and tables. Optionally ignore, mask, or truncate columns that are sensitive, too large, or not needed. See the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL connector’s documentation&lt;/a&gt; for all the details.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;using_the_libraries&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#using_the_libraries&quot;&gt;&lt;/a&gt;Using the libraries&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Although Debezium is really intended to be used as turnkey services, all of Debezium’s JARs and other artifacts are available in &lt;a href=&quot;http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22io.debezium%22&quot;&gt;Maven Central&lt;/a&gt;. You might want to use our &lt;a href=&quot;http://debezium.io/blog/2016/04/15/parsing-ddl/&quot;&gt;MySQL DDL parser&lt;/a&gt; from our MySQL connector library to parse those DDL statments in your consumers.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We do provide a small library so applications can &lt;a href=&quot;http://debezium.io/docs/embedded&quot;&gt;embed any Kafka Connect connector&lt;/a&gt; and consume data change events read directly from the source system. This provides a much lighter weight system (since Zookeeper, Kafka, and Kafka Connect services are not needed), but as a consequence is not as fault tolerant or reliable since the application must manage and maintain all state normally kept inside Kafka’s distributed and replicated logs. It’s perfect for use in tests, and with careful consideration it may be useful in some applications.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;. And stay tuned, because we’re hoping to add a MongoDB connector in our next release.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Emmanuel, Chris, Christian, Konstantin, David, Akshath, James, Ewen, Cheng, and Paul for their help with the release, discussions, design assistance, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/05/31/Debezium-on-Kubernetes/</id>
    <title>Debezium on Kubernetes</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2016-05-31T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/05/31/Debezium-on-Kubernetes/" rel="alternate" type="text/html" />
    <author>
      <name>Christian Posta</name>
    </author>
    <category term="mysql"></category>
    <category term="sql"></category>
    <category term="kubernetes"></category>
    <category term="docker"></category>
    <category term="kafka"></category>
    <summary>
      
      
      
      Our Debezium Tutorial walks you step by step through using Debezium by installing, starting, and linking together all of the Docker containers running on a single host machine. Of course, you can use things like Docker Compose or your own scripts to make this easier, although that would just automating running all the containers on a single machine. What you really want is to run the containers on a cluster of machines. In this blog, we&#8217;ll run Debezium using a container cluster manager from Red Hat and Google called Kubernetes.
      
      
      Kubernetes is a container (Docker/Rocket/Hyper.sh) cluster management tool. Like many other...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;Debezium Tutorial&lt;/a&gt; walks you step by step through using Debezium by installing, starting, and linking together all of the Docker containers running on a single host machine. Of course, you can use things like Docker Compose or your own scripts to make this easier, although that would just automating running all the containers on a single machine. What you really want is to run the containers on a cluster of machines. In this blog, we’ll run Debezium using a container cluster manager from Red Hat and Google called &lt;a href=&quot;http://kubernetes.io&quot;&gt;Kubernetes&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://kubernetes.io&quot;&gt;Kubernetes&lt;/a&gt; is a container (Docker/Rocket/Hyper.sh) cluster management tool. Like many other popular cluster management and compute resource scheduling platforms, Kubernetes' roots are in Google, who is no stranger to running containers at scale. They start, stop, and cluster &lt;a href=&quot;https://cloudplatform.googleblog.com/2015/01/in-coming-weeks-we-will-be-publishing.html&quot;&gt;2 billion containers per week&lt;/a&gt; and they contributed a lot of the Linux kernel underpinnings that make containers possible. &lt;a href=&quot;http://research.google.com/pubs/pub43438.html&quot;&gt;One of their famous papers&lt;/a&gt; talks about an internal cluster manager named Borg. With Kubernetes, Google got tired of everyone implementing their papers in Java so they decided to implement this one themselves :)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kubernetes is written in Go-lang and is quickly becoming the de-facto API for scheduling, managing, and clustering containers at scale. This blog isn’t intended to be a primer on Kubernetes, so we recommend heading over to the &lt;a href=&quot;http://kubernetes.io/docs/getting-started-guides/&quot;&gt;Getting Started&lt;/a&gt; docs to learn more about Kubernetes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;getting_started&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#getting_started&quot;&gt;&lt;/a&gt;Getting started&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To get started, we need to have access to a Kubernetes cluster. Getting one started is pretty easy: just follow the &lt;a href=&quot;http://kubernetes.io/docs/getting-started-guides/&quot;&gt;getting started&lt;/a&gt; guides. A favorite of ours is &lt;a href=&quot;https://blog.openshift.com/one-vagrant-image-openshift-origin-v3/&quot;&gt;OpenShift’s all in one VM&lt;/a&gt; or the &lt;a href=&quot;http://developers.redhat.com/products/cdk/overview/&quot;&gt;Red Hat Container Development Kit&lt;/a&gt; which provide a hardened, production-ready distribution of Kubernetes. Once you’ve installed it and logged in, you should be able to run &lt;code&gt;kubectl get pod&lt;/code&gt; to get a list of Kubernetes pods you may have running. You don’t need anything running else inside Kubernetes to get started.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To get and build the Kubernetes manifest files (yaml descriptors), go clone the &lt;a href=&quot;https://github.com/debezium/debezium-kubernetes&quot;&gt;Debezium Kubernetes&lt;/a&gt; repo and run the following command:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ mvn clean
      $ mvn install&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This project uses the awesome &lt;a href=&quot;http://fabric8.io/guide/mavenPlugin.html&quot;&gt;Fabric8 Maven plugin&lt;/a&gt; to automatically generate the Kubernetes manifest files. Here’s an example of what gets generated in &lt;code&gt;$PROJECT_ROOT/zk-standalone/target/classes/kubernetes.yml&lt;/code&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;nowrap&quot;&gt;apiVersion: &quot;v1&quot;
      items:
      - apiVersion: &quot;v1&quot;
        kind: &quot;Service&quot;
        metadata:
          annotations: {}
          labels:
            project: &quot;zookeeper&quot;
            provider: &quot;debezium&quot;
            version: &quot;0.1-SNAPSHOT&quot;
            group: &quot;io.debezium&quot;
          name: &quot;zookeeper&quot;
        spec:
          deprecatedPublicIPs: []
          externalIPs: []
          ports:
          - port: 2181
            protocol: &quot;TCP&quot;
            targetPort: 2181
          selector:
            project: &quot;zookeeper&quot;
            provider: &quot;debezium&quot;
            group: &quot;io.debezium&quot;
      - apiVersion: &quot;v1&quot;
        kind: &quot;ReplicationController&quot;
        metadata:
          annotations:
            fabric8.io/git-branch: &quot;master&quot;
            fabric8.io/git-commit: &quot;004e222462749fbaf12c3ee33edca9b077ee9003&quot;
          labels:
            project: &quot;zookeeper&quot;
            provider: &quot;debezium&quot;
            version: &quot;0.1-SNAPSHOT&quot;
            group: &quot;io.debezium&quot;
          name: &quot;zk-standalone&quot;
        spec:
          replicas: 1
          selector:
            project: &quot;zookeeper&quot;
            provider: &quot;debezium&quot;
            version: &quot;0.1-SNAPSHOT&quot;
            group: &quot;io.debezium&quot;
          template:
            metadata:
              annotations: {}
              labels:
                project: &quot;zookeeper&quot;
                provider: &quot;debezium&quot;
                version: &quot;0.1-SNAPSHOT&quot;
                group: &quot;io.debezium&quot;
            spec:
              containers:
              - args: []
                command: []
                env:
                - name: &quot;KUBERNETES_NAMESPACE&quot;
                  valueFrom:
                    fieldRef:
                      fieldPath: &quot;metadata.namespace&quot;
                image: &quot;docker.io/debezium/zookeeper:0.1&quot;
                imagePullPolicy: &quot;IfNotPresent&quot;
                name: &quot;zk-standalone&quot;
                ports:
                - containerPort: 3888
                  name: &quot;election&quot;
                - containerPort: 2888
                  name: &quot;peer&quot;
                - containerPort: 2181
                  name: &quot;client&quot;
                securityContext: {}
                volumeMounts: []
              imagePullSecrets: []
              nodeSelector: {}
              volumes: []
      kind: &quot;List&quot;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;starting_zookeeper_and_kafka_on_kubernetes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#starting_zookeeper_and_kafka_on_kubernetes&quot;&gt;&lt;/a&gt;Starting Zookeeper and Kafka on Kubernetes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To start &lt;a href=&quot;http://zookeeper.apache.org&quot;&gt;Apache Zookeeper&lt;/a&gt; or &lt;a href=&quot;http://kafka.apache.org&quot;&gt;Apache Kafka&lt;/a&gt; inside Kubernetes you have two options. If you have the &lt;code&gt;kubectl&lt;/code&gt; command line (or the &lt;code&gt;oc&lt;/code&gt; tool from the OpenShift client distros) on your machine you can apply any of the newly generated Kubernetes manifest files like this:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl create -f &amp;lt;path_to_file&amp;gt;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Or you can use the Fabric8 Maven plugin and its &lt;code&gt;fabric8:apply&lt;/code&gt; goal to apply the manifest files. Note for either of these two options to work, you must be currently logged into your Kubernetes cluster. (Also, OpenShift’s &lt;code&gt;oc login &amp;lt;url&amp;gt;&lt;/code&gt; makes this super easy, or see &lt;a href=&quot;http://blog.christianposta.com/kubernetes/logging-into-a-kubernetes-cluster-with-kubectl/&quot;&gt;Logging into a Kubernetes Cluster with kubectl&lt;/a&gt; for more information.)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;First, let’s deploy Zookeeper to our Kubernetes cluster. We need to be in &lt;code&gt;$PROJECT_ROOT/zk-standalone&lt;/code&gt; directory, and then we’ll apply our Kubernetes configuration.  First, let’s see how to do this with the &lt;code&gt;kubectl&lt;/code&gt; command:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd zk-standalone
      $ kubectl create -f target/classes/kubernetes.yml
      
      service &quot;zookeeper&quot; created
      replicationcontroller &quot;zk-standalone&quot; created&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You can do the same thing with Maven and the fabric8 maven plugin:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd zk-standalone
      $ mvn fabric8:apply
      
      Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=1512m; support was removed in 8.0
      [INFO] Scanning for projects...
      [INFO]
      [INFO] ------------------------------------------------------------------------
      [INFO] Building zk-standalone 0.1-SNAPSHOT
      [INFO] ------------------------------------------------------------------------
      [INFO]
      [INFO] --- fabric8-maven-plugin:2.2.115:apply (default-cli) @ zk-standalone ---
      [INFO] Using kubernetes at: https://172.28.128.4:8443/ in namespace ticket
      [INFO] Kubernetes JSON: /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/zk-standalone/target/classes/kubernetes.json
      [INFO] OpenShift platform detected
      [INFO] Using namespace: ticket
      [INFO] Looking at repo with directory /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/.git
      [INFO] Creating a Service from kubernetes.json namespace ticket name zookeeper
      [INFO] Created Service: zk-standalone/target/fabric8/applyJson/ticket/service-zookeeper.json
      [INFO] Creating a ReplicationController from kubernetes.json namespace ticket name zk-standalone
      [INFO] Created ReplicationController: zk-standalone/target/fabric8/applyJson/ticket/replicationcontroller-zk-standalone.json
      [INFO] ------------------------------------------------------------------------
      [INFO] BUILD SUCCESS
      [INFO] ------------------------------------------------------------------------
      [INFO] Total time: 2.661 s
      [INFO] Finished at: 2016-05-19T15:59:26-07:00
      [INFO] Final Memory: 26M/260M
      [INFO] ------------------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Zookeeper is deployed, so let’s continue with deploying Kafka. Navigate to &lt;code&gt;$PROJECT_ROOT/kafka&lt;/code&gt;, and then apply the Kafka deployment configuration:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd ../kafka
      $ kubectl create -f target/classes/kubernetes.yml
      
      service &quot;kafka&quot; created
      replicationcontroller &quot;kafka&quot; created&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Or with fabric8 maven plugin:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd ../kafka
      $ mvn fabric8:apply
      
      Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=1512m; support was removed in 8.0
      [INFO] Scanning for projects...
      [INFO]
      [INFO] ------------------------------------------------------------------------
      [INFO] Building kafka 0.1-SNAPSHOT
      [INFO] ------------------------------------------------------------------------
      [INFO]
      [INFO] --- fabric8-maven-plugin:2.2.115:apply (default-cli) @ kafka ---
      [INFO] Using kubernetes at: https://172.28.128.4:8443/ in namespace ticket
      [INFO] Kubernetes JSON: /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/kafka/target/classes/kubernetes.json
      [INFO] OpenShift platform detected
      [INFO] Using namespace: ticket
      [INFO] Looking at repo with directory /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/.git
      [INFO] Creating a Service from kubernetes.json namespace ticket name kafka
      [INFO] Created Service: kafka/target/fabric8/applyJson/ticket/service-kafka.json
      [INFO] Creating a ReplicationController from kubernetes.json namespace ticket name kafka
      [INFO] Created ReplicationController: kafka/target/fabric8/applyJson/ticket/replicationcontroller-kafka.json
      [INFO] ------------------------------------------------------------------------
      [INFO] BUILD SUCCESS
      [INFO] ------------------------------------------------------------------------
      [INFO] Total time: 2.563 s
      [INFO] Finished at: 2016-05-19T16:03:25-07:00
      [INFO] Final Memory: 26M/259M
      [INFO] ------------------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Use the &lt;code&gt;kubectl get pod&lt;/code&gt; command to see what is running:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl get pod
      
      NAME                  READY     STATUS    RESTARTS   AGE
      kafka-mqmxt           1/1       Running   0          46s
      zk-standalone-4mo02   1/1       Running   0          4m&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Did you notice that we didn’t manually &quot;link&quot; the containers as we started them? Kubernetes has a cluster service discovery feature called &lt;a href=&quot;http://kubernetes.io/docs/user-guide/services/&quot;&gt;Kubernetes Services&lt;/a&gt; that load-balances against and lets us use internal DNS (or cluster IPs) to discover pods. For example, in the &lt;code&gt;kubernetes.yml&lt;/code&gt; deployment configuration for Kafka, you’ll see the following:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;nowrap&quot;&gt;    ...
          containers:
          - args: []
            command: []
            env:
            - name: &quot;KAFKA_ADVERTISED_PORT&quot;
              value: &quot;9092&quot;
            - name: &quot;KAFKA_ADVERTISED_HOST_NAME&quot;
              value: &quot;kafka&quot;
            - name: &quot;KAFKA_ZOOKEEPER_CONNECT&quot;
              value: &quot;zookeeper:2181&quot;
            - name: &quot;KAFKA_PORT&quot;
              value: &quot;9092&quot;
            - name: &quot;KUBERNETES_NAMESPACE&quot;
              valueFrom:
                fieldRef:
                  fieldPath: &quot;metadata.namespace&quot;
            image: &quot;docker.io/debezium/kafka:0.1&quot;
            imagePullPolicy: &quot;IfNotPresent&quot;
            name: &quot;kafka&quot;
          ...&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re specifying values for the &lt;code&gt;KAFKA_ZOOKEEPER_CONNECT&lt;/code&gt; environment variable used by the Docker image, and thus enabling Kafka to discover Zookeeper pods wherever they are running. Although we could have used any hostname, to keep things simple we use just &lt;code&gt;zookeeper&lt;/code&gt; for the DNS name. So, if you were to log in to one of the pods and try to reach the host named &lt;code&gt;zookeeper&lt;/code&gt;, Kubernetes would transparently resolve that request to one of the Zookeeper pods (if there are multiple). Slick! This discovery mechanism is used for the rest of the components, too. (Note, this cluster IP that the DNS resolves to &lt;strong&gt;never&lt;/strong&gt; changes for the life of the Kubernetes Service regardless of how many Pods exist for a given service. This means you can rely on this service discovery without all of the DNS caching issues you may otherwise run into.)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The next step is to create a &lt;code&gt;schema-changes&lt;/code&gt; topic that Debezium’s MySQL connector will use. Let’s use the Kafka tools to create this:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ KAFKA_POD_NAME=$(kubectl get pod | grep -i running | grep kafka | awk '{ print $1 }')
      
      $ kubectl exec $KAFKA_POD_NAME --  /kafka/bin/kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic schema-changes.inventory&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;start_up_a_mysql_database_on_kubernetes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#start_up_a_mysql_database_on_kubernetes&quot;&gt;&lt;/a&gt;Start up a MySQL Database on Kubernetes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Starting the MySQL database follows the same instructions as installing Zookeeper or Kafka. We will navigate to the &lt;code&gt;$PROJECT_ROOT/mysql56&lt;/code&gt; directory, and we’ll use the &lt;a href=&quot;https://github.com/openshift/mysql&quot;&gt;MySQL 5.6 OpenShift Docker image&lt;/a&gt; so that it runs on both vanilla Kubernetes and OpenShift v3.x. Here’s the &lt;code&gt;kubectl&lt;/code&gt; command to start up our MySQL instance:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd ../mysql56
      $ kubectl create -f target/classes/kubernetes.yml
      
      service &quot;mysql&quot; created
      replicationcontroller &quot;mysql56&quot; created&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Or the equivalent Maven command:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd mysql56
      $ mvn fabric8:apply&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now, when we run &lt;code&gt;kubectl get pod&lt;/code&gt; we should see our MySQL database running, too:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;nowrap&quot;&gt;NAME                  READY     STATUS    RESTARTS   AGE
      kafka-mqmxt           1/1       Running   0          17m
      mysql56-b4f36         1/1       Running   0          9m
      zk-standalone-4mo02   1/1       Running   0          21m&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s run a command to get client access to the database. First, set a few environment variables to the pod’s name and IP address:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ MYSQL_POD_NAME=$(kubectl get pod | grep Running | grep ^mysql | awk '{ print $1 }')
      $ MYSQL_POD_IP=$(kubectl describe pod $MYSQL_POD_NAME | grep IP | awk '{ print $2 }')&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Then, log in to the Kubernetes pod that’s running the MySQL database, and start the MySQL command client:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl exec -it $MYSQL_POD_NAME   -- /opt/rh/rh-mysql56/root/usr/bin/mysql -h$MYSQL_POD_IP -P3306 -uroot -padmin
      Warning: Using a password on the command line interface can be insecure.
      Welcome to the MySQL monitor.  Commands end with ; or \g.
      Your MySQL connection id is 1
      Server version: 5.6.26-log MySQL Community Server (GPL)
      
      Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.
      
      Oracle is a registered trademark of Oracle Corporation and/or its
      affiliates. Other names may be trademarks of their respective
      owners.
      
      Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
      
      mysql&amp;gt;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This shows that the &lt;code&gt;kubectl&lt;/code&gt; command line lets us easily get access to a pod or Docker container regardless of where it’s running in the cluster.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Next, exit out of the mysql shell (type &lt;code&gt;exit&lt;/code&gt;) and run the following command to download a &lt;a href=&quot;https://gist.github.com/christian-posta/e20ddb5c945845b4b9f6eba94a98af09/raw&quot;&gt;SQL script&lt;/a&gt; that populates an &lt;code&gt;inventory&lt;/code&gt; sample database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl exec  -it $MYSQL_POD_NAME -- bash -c &quot;curl -s -L https://gist.github.com/christian-posta/e20ddb5c945845b4b9f6eba94a98af09/raw | /opt/rh/rh-mysql56/root/usr/bin/mysql -h$MYSQL_POD_IP -P3306 -uroot -padmin&quot;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now, if we log back into the MySQL pod we can show the databases and tables:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl exec -it $MYSQL_POD_NAME   -- /opt/rh/rh-mysql56/root/usr/bin/mysql -h$MYSQL_POD_IP -P3306 -uroot -padmin -e 'use inventory; show tables;'
      
      +---------------------+
      | Tables_in_inventory |
      +---------------------+
      | customers           |
      | orders              |
      | products            |
      | products_on_hand    |
      +---------------------+
      4 rows in set (0.00 sec)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;start_kafka_connect_and_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#start_kafka_connect_and_debezium&quot;&gt;&lt;/a&gt;Start Kafka Connect and Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Navigate into the directory &lt;code&gt;$PROJECT_ROOT/connect-mysql&lt;/code&gt; directory. Here, we’ll start a Kubernetes pod that runs Kafka Connect with the Debezium MySQL connector already installed. The Debezium MySQL connector connects to a MySQL database, reads the binlog, and writes those row events to Kafka. Start up Kafka Connect with Debezium on Kubernetes similarly to the previous components:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd ../connect-mysql
      $ kubectl create -f target/classes/kubernetes.yml
      
      service &quot;connect-mysql&quot; created
      replicationcontroller &quot;connect-mysql&quot; created&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Or with the fabric8 maven plugin:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ cd ../connect-mysql
      $ mvn fabric8:apply
      Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=1512m; support was removed in 8.0
      [INFO] Scanning for projects...
      [INFO]
      [INFO] ------------------------------------------------------------------------
      [INFO] Building connect-mysql 0.1-SNAPSHOT
      [INFO] ------------------------------------------------------------------------
      [INFO]
      [INFO] --- fabric8-maven-plugin:2.2.115:apply (default-cli) @ connect-mysql ---
      [INFO] Using kubernetes at: https://172.28.128.4:8443/ in namespace ticket
      [INFO] Kubernetes JSON: /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/connect-mysql/target/classes/kubernetes.json
      [INFO] OpenShift platform detected
      [INFO] Using namespace: ticket
      [INFO] Looking at repo with directory /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/.git
      [INFO] Creating a Service from kubernetes.json namespace ticket name connect-mysql
      [INFO] Created Service: connect-mysql/target/fabric8/applyJson/ticket/service-connect-mysql.json
      [INFO] Creating a ReplicationController from kubernetes.json namespace ticket name connect-mysql
      [INFO] Created ReplicationController: connect-mysql/target/fabric8/applyJson/ticket/replicationcontroller-connect-mysql.json
      [INFO] ------------------------------------------------------------------------
      [INFO] BUILD SUCCESS
      [INFO] ------------------------------------------------------------------------
      [INFO] Total time: 2.255 s
      [INFO] Finished at: 2016-05-25T09:21:04-07:00
      [INFO] Final Memory: 27M/313M
      [INFO] ------------------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Just like in the Docker tutorial for Debezium, we now want to send a JSON object to the Kafka Connect API to start up our Debezium connector. First, we need to expose the API for the Kafka Connect cluster. You can do this however you want: on Kubernetes (&lt;a href=&quot;http://kubernetes.io/docs/user-guide/ingress/&quot;&gt;Ingress definitions&lt;/a&gt;, &lt;a href=&quot;http://kubernetes.io/docs/user-guide/services/&quot;&gt;NodePort services&lt;/a&gt;, etc) or on OpenShift you can use &lt;a href=&quot;https://docs.openshift.com/enterprise/3.2/architecture/core_concepts/routes.html&quot;&gt;OpenShift Routes&lt;/a&gt;. For this simple example, we’ll use simple Pod port-forwarding to forward the &lt;code&gt;connect-mysql&lt;/code&gt; pod’s &lt;code&gt;8083&lt;/code&gt; port to our local machine (again, regardless of where the Pod is actually running the cluster. (This is such an incredible feature of Kubernetes that makes it so easy to develop distributed services!)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s determine the pod name and then use port forwarding to our local machine:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ CONNECT_POD_NAME=$(kubectl get pod | grep -i running | grep ^connect | awk '{ print $1 }')
      $ kubectl port-forward $CONNECT_POD_NAME 8083:8083
      
      I0525 09:30:08.390491    6651 portforward.go:213] Forwarding from 127.0.0.1:8083 -&amp;gt; 8083
      I0525 09:30:08.390631    6651 portforward.go:213] Forwarding from [::1]:8083 -&amp;gt; 8083&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We are forwarding the pod’s port &lt;code&gt;8083&lt;/code&gt; to our local machine’s &lt;code&gt;8083&lt;/code&gt;. Now if we hit &lt;code&gt;&lt;a href=&quot;http://localhost:8083&quot; class=&quot;bare&quot;&gt;http://localhost:8083&lt;/a&gt;&lt;/code&gt; it will be directed to the pod which runs our Kafka Connect and Debezium services.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Since it may be useful to see the output from the pod to see whether or not there are any exceptions, start another terminal and type the following to follow the Kafka Connect output:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ CONNECT_POD_NAME=$(kubectl get pod | grep -i running | grep ^connect | awk '{ print $1 }')
      $ kubectl logs -f $CONNECT_POD_NAME&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now, let’s use an HTTP client to post the Debezium Connector/Task to the endpoint we’ve just exposed locally:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; -H &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ -d '{ &quot;name&quot;: &quot;inventory-connector&quot;, &quot;config&quot;: { &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;, &quot;tasks.max&quot;: &quot;1&quot;, &quot;database.hostname&quot;: &quot;mysql&quot;, &quot;database.port&quot;: &quot;3306&quot;, &quot;database.user&quot;: &quot;replicator&quot;, &quot;database.password&quot;: &quot;replpass&quot;, &quot;database.server.id&quot;: &quot;184054&quot;, &quot;database.server.name&quot;: &quot;mysql-server-1&quot;, &quot;database.binlog&quot;: &quot;mysql-bin.000001&quot;, &quot;database.whitelist&quot;: &quot;inventory&quot;, &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;, &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot; } }'&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If we’re watching the log output for the &lt;code&gt;connect-mysql&lt;/code&gt; pod, we’ll see it eventually end up looking something like this:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;nowrap&quot;&gt;2016-05-27 18:50:14,580 - WARN  [kafka-producer-network-thread | producer-1:NetworkClient$DefaultMetadataUpdater@582] - Error while fetching metadata with correlation id 2 : {mysql-server-1.inventory.products=LEADER_NOT_AVAILABLE}
      2016-05-27 18:50:14,690 - WARN  [kafka-producer-network-thread | producer-1:NetworkClient$DefaultMetadataUpdater@582] - Error while fetching metadata with correlation id 3 : {mysql-server-1.inventory.products=LEADER_NOT_AVAILABLE}
      2016-05-27 18:50:14,911 - WARN  [kafka-producer-network-thread | producer-1:NetworkClient$DefaultMetadataUpdater@582] - Error while fetching metadata with correlation id 7 : {mysql-server-1.inventory.products_on_hand=LEADER_NOT_AVAILABLE}
      2016-05-27 18:50:15,136 - WARN  [kafka-producer-network-thread | producer-1:NetworkClient$DefaultMetadataUpdater@582] - Error while fetching metadata with correlation id 10 : {mysql-server-1.inventory.customers=LEADER_NOT_AVAILABLE}
      2016-05-27 18:50:15,362 - WARN  [kafka-producer-network-thread | producer-1:NetworkClient$DefaultMetadataUpdater@582] - Error while fetching metadata with correlation id 13 : {mysql-server-1.inventory.orders=LEADER_NOT_AVAILABLE}&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;These error are just Kafka’s way of telling us the topics didn’t exist but were created.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If we now do a listing of our topics inside Kafka, we should see a Kafka topic for each table in the mysql &lt;code&gt;inventory&lt;/code&gt; database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl exec  $KAFKA_POD_NAME --  /kafka/bin/kafka-topics.sh --list --zookeeper zookeeper:2181
      __consumer_offsets
      my-connect-configs
      my-connect-offsets
      mysql-server-1.inventory.customers
      mysql-server-1.inventory.orders
      mysql-server-1.inventory.products
      mysql-server-1.inventory.products_on_hand
      schema-changes.inventory&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s take a look at what’s in one of these topics:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ kubectl exec  $KAFKA_POD_NAME --  /kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --new-consumer --topic mysql-server-1.inventory.customers --from-beginning --property print.key=true
      {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers/pk&quot;},&quot;payload&quot;:{&quot;id&quot;:1001}}   {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;first_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;last_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;email&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers&quot;},&quot;payload&quot;:{&quot;id&quot;:1001,&quot;first_name&quot;:&quot;Sally&quot;,&quot;last_name&quot;:&quot;Thomas&quot;,&quot;email&quot;:&quot;sally.thomas@acme.com&quot;}}
      {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers/pk&quot;},&quot;payload&quot;:{&quot;id&quot;:1002}}   {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;first_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;last_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;email&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers&quot;},&quot;payload&quot;:{&quot;id&quot;:1002,&quot;first_name&quot;:&quot;George&quot;,&quot;last_name&quot;:&quot;Bailey&quot;,&quot;email&quot;:&quot;gbailey@foobar.com&quot;}}
      {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers/pk&quot;},&quot;payload&quot;:{&quot;id&quot;:1003}}   {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;first_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;last_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;email&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers&quot;},&quot;payload&quot;:{&quot;id&quot;:1003,&quot;first_name&quot;:&quot;Edward&quot;,&quot;last_name&quot;:&quot;Walker&quot;,&quot;email&quot;:&quot;ed@walker.com&quot;}}
      {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers/pk&quot;},&quot;payload&quot;:{&quot;id&quot;:1004}}   {&quot;schema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;type&quot;:&quot;int32&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;id&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;first_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;last_name&quot;},{&quot;type&quot;:&quot;string&quot;,&quot;optional&quot;:false,&quot;field&quot;:&quot;email&quot;}],&quot;optional&quot;:false,&quot;name&quot;:&quot;inventory.customers&quot;},&quot;payload&quot;:{&quot;id&quot;:1004,&quot;first_name&quot;:&quot;Anne&quot;,&quot;last_name&quot;:&quot;Kretchmar&quot;,&quot;email&quot;:&quot;annek@noanswer.org&quot;}}&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;What happened? When we started Debezium’s MySQL connector, it started reading the binary replication log from the MySQL server, and it replayed all of the history and generated an event for each INSERT, UPDATE, and DELETE operation (though in our sample &lt;code&gt;inventory&lt;/code&gt; database we only had INSERTs). If we or some client apps were to commit other changes to the database, Debezium would see those immediately and write those to the correct topic. In other words, Debezium records all of the changes to our MySQL database as events in Kafka topics! And from there, any tool, connector, or service can independnetly consume those event streams from Kafka and process them or put them into a different database, into Hadoop, elasticsearch, data grid, etc.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;cleanup&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#cleanup&quot;&gt;&lt;/a&gt;Cleanup&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you want to delete the connector, simply issue a REST request to remove it:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X DELETE -H &quot;Accept:application/json&quot; http://localhost:8083/connectors/inventory-connector&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/04/15/parsing-ddl/</id>
    <title>Parsing DDL</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2016-04-15T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/04/15/parsing-ddl/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="mysql"></category>
    <category term="sql"></category>
    <summary>
      
      
      
      When our MySQL connector is reading the binlog of a MySQL server or cluster, it parses the DDL statements in the log and builds an in-memory model of each table&#8217;s schema as it evolves over time. This process is important because the connector generates events for each table using the definition of the table at the time of each event. We can&#8217;t use the database&#8217;s current schema, since it may have changed since the point in time (or position in the log) where the connector is reading.
      
      
      Parsing DDL of MySQL or any other major relational database can seem to be...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When our &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;MySQL connector&lt;/a&gt; is reading the binlog of a MySQL server or cluster, it parses the DDL statements in the log and builds an in-memory model of each table’s schema as it evolves over time. This process is important because the connector generates events for each table using the definition of the table at the time of each event. We can’t use the database’s &lt;em&gt;current&lt;/em&gt; schema, since it may have changed since the point in time (or position in the log) where the connector is reading.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Parsing DDL of MySQL or any other major relational database can seem to be a daunting task. Usually each DBMS has a highly-customized SQL grammar, and although the &lt;em&gt;data manipulation language&lt;/em&gt; (DML) statements are often fairly close the standards, the &lt;em&gt;data definition language&lt;/em&gt; (DDL) statements are usually less so and involve more DBMS-specific features.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So given this, why did we write our own DDL parser for MySQL? Let’s first look at what Debezium needs a DDL parser to do.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;parsing_ddl_in_the_debezium_mysql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#parsing_ddl_in_the_debezium_mysql_connector&quot;&gt;&lt;/a&gt;Parsing DDL in the Debezium MySQL connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The MySQL binlog contains various kinds of events. For example, when a row is inserted into a table, the binlog event contains an indirect reference to the table and the values for each column in the table, but there is no information about the columns that make up the table. The only thing in the binlog referencing table structures are SQL DDL statements that were generated by MySQL when it processed user-supplied DDL statements.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The connector also produces messages using Kafka Connect Schemas, which are simple data structures that define the various names and types of each field, and the way the fields are organized. So, when we generate an event message for the table insert, we first have to have a Kafka Connect &lt;code&gt;Schema&lt;/code&gt; object with all the appropriate fields, and then we have to convert the ordered array of column values into a Kafka Connect &lt;code&gt;Struct&lt;/code&gt; object using the fields and the individual column values in the table insert event.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Luckily, when we come across a DDL statement we can update our in-memory model and then use this to generate a &lt;code&gt;Schema&lt;/code&gt; object. At the same time, we can create a component that will use this &lt;code&gt;Schema&lt;/code&gt; object to create a &lt;code&gt;Struct&lt;/code&gt; object from the ordered array of column values that appear in the events. All of this can be done once and used for all row events on that table, until we come across another DDL statement that changes the table’s schema at which point we updated our model again.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So all of this requires parsing all of the DDL statements, though for our purposes we only have to &lt;em&gt;understand&lt;/em&gt; a small subset of the DDL grammer. We then have to use that subset of statements to update our in-memory model of our tables. And since our in-memory table model is not specific to MySQL, the rest of the functionality to generate &lt;code&gt;Schema&lt;/code&gt; objects and components that convert an array of values into &lt;code&gt;Struct&lt;/code&gt; objects used in messages is all generic.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;existing_ddl_libraries&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#existing_ddl_libraries&quot;&gt;&lt;/a&gt;Existing DDL libraries&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Unfortunately, there aren’t really that many 3rd party open source libraries for parsing DDL statements for MySQL, PostgreSQL, or other popular RDBMSes. &lt;a href=&quot;http://jsqlparser.sourceforge.net&quot;&gt;JSqlParser&lt;/a&gt; is often cited, but it has a &lt;em&gt;single grammar&lt;/em&gt; that is a combination of multiple DBMS grammars and therefore is not a strict parser for any specific DBMS. Adding support for other DBMSes by updating the composite grammar would likely be difficult.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Other libraries, such as &lt;a href=&quot;https://github.com/prestodb/presto/tree/master/presto-parser&quot;&gt;PrestoDB&lt;/a&gt;, define their own SQL grammar and are unable to handle the intracacies and nuances of the MySQL DDL grammar. The Antlr parser generator project has a &lt;a href=&quot;https://github.com/antlr/grammars-v4/tree/master/mysql&quot;&gt;grammar for MySQL 5.6&lt;/a&gt;, but this is limited to a small subset of DML and has no support for DDL or newer 5.7 features. There are &lt;a href=&quot;http://www.antlr3.org/grammar/list.html&quot;&gt;older SQL-related grammars for Antlr 3&lt;/a&gt;, but these are often massive, suffer from bugs, and limited to specific DBMSes. The &lt;a href=&quot;http://teiid.jboss.org&quot;&gt;Teiid project&lt;/a&gt; is a data virtualization engine that sits atop a wide variety of DBMSes and data sources, and it’s tooling has a series of &lt;a href=&quot;https://github.com/Teiid-Designer/teiid-modeshape&quot;&gt;DDL parsers&lt;/a&gt; that construct ASTs in a special repository (the author actually helped develop these). There are also Ruby libraries, like &lt;a href=&quot;https://github.com/square/mysql-parser&quot;&gt;Square’s MySQL Parser library&lt;/a&gt;. There is also a &lt;a href=&quot;http://www.sqlparser.com/sql-parser-java.php&quot;&gt;proprietary commercial product&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;our_ddl_parser_framework&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#our_ddl_parser_framework&quot;&gt;&lt;/a&gt;Our DDL parser framework&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Since we couldn’t find a useful 3rd party open source library, we chose to create our own DDL parser framework limited to our needs:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Parse DDL statements and update our in-memory model.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Focus on consuming those essential statements (e.g., create, alter, and drop tables and views), while completely ignoring other statements without having to parse them.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Structure the parser code similarly to the &lt;a href=&quot;http://dev.mysql.com/doc/refman/5.7/en/sql-syntax-data-definition.html&quot;&gt;MySQL DDL grammar documentation&lt;/a&gt; and use method names that mirror the rules in the grammar. This will make it easier to maintain over time.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Allow creation of parsers for PostgreSQL, Oracle, SQLServer, and other DBMSes as needed.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Support customization through subclassing: be able to easily override narrow portions of the logic without having to copy lots of code.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Make it easy to develop, debug, and test parsers.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The resulting framework includes a tokenizer that converts one or more DDL statements in a string into a rewindable sequence of tokens, where each token represents punctuation, quoted strings, case-insentivie words and symbols, numbers, keywords, comments, and terminating characters  (such as &lt;code&gt;;&lt;/code&gt; for MySQL). The DDL parser, then, walks the token stream looking for patterns using a simple and easy to read fluent API, calling methods on itself to process the various sets of tokens. The parser also uses an internal &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/relational/ddl/DataTypeParser.java&quot;&gt;data type parser&lt;/a&gt; for processing SQL data type expressions, such as &lt;code&gt;INT&lt;/code&gt;, &lt;code&gt;VARCHAR(64)&lt;/code&gt;, &lt;code&gt;NUMERIC(32,3)&lt;/code&gt;, &lt;code&gt;TIMESTAMP(8) WITH TIME ZONE&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/debezium-connector-mysql/src/main/java/io/debezium/connector/mysql/MySqlDdlParser.java&quot;&gt;MySqlDdlParser&lt;/a&gt; class extends a &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/relational/ddl/DdlParser.java&quot;&gt;base class&lt;/a&gt; and provides all of the MySQL-specific parsing logic. For example, the DDL statements:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;# Create and populate our products using a single insert with many rows
      CREATE TABLE products (
        id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
        name VARCHAR(255) NOT NULL,
        description VARCHAR(512),
        weight FLOAT
      );
      ALTER TABLE products AUTO_INCREMENT = 101;
      
      # Create and populate the products on hand using multiple inserts
      CREATE TABLE products_on_hand (
        product_id INTEGER NOT NULL PRIMARY KEY,
        quantity INTEGER NOT NULL,
        FOREIGN KEY (product_id) REFERENCES products(id)
      );&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;can be easily parsed with:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;String ddlStatements = ...
      DdlParser parser = new MySqlDdlParser();
      Tables tables = new Tables();
      parser.parse(ddl, tables);&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Here, the &lt;code&gt;Tables&lt;/code&gt; object is our in-memory representation of our named table definitions. The parser processes the DDL statements, applying each to the appropriate table definition inside the &lt;code&gt;Tables&lt;/code&gt; object.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;how_it_works&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#how_it_works&quot;&gt;&lt;/a&gt;How it works&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Each &lt;code&gt;DdlParser&lt;/code&gt; implementation has the following public method that will parse the statements in the supplied String:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;    public final void parse(String ddlContent, Tables databaseTables) {
              Tokenizer tokenizer = new DdlTokenizer(!skipComments(), this::determineTokenType);
              TokenStream stream = new TokenStream(ddlContent, tokenizer, false);
              stream.start();
              parse(stream, databaseTables);
          }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Here, the method creates a new &lt;code&gt;TokenStream&lt;/code&gt; from the content using a &lt;code&gt;DdlTokenizer&lt;/code&gt; that knows how to separate the characters in the string into the various typed token objects. It then calls another &lt;code&gt;parse&lt;/code&gt; method that does the bulk of the work:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;    public final void parse(TokenStream ddlContent, Tables databaseTables)
                                 throws ParsingException, IllegalStateException {
              this.tokens = ddlContent;
              this.databaseTables = databaseTables;
              Marker marker = ddlContent.mark();
              try {
                  while (ddlContent.hasNext()) {
                      parseNextStatement(ddlContent.mark());
                      // Consume the statement terminator if it is still there ...
                      tokens.canConsume(DdlTokenizer.STATEMENT_TERMINATOR);
                  }
              } catch (ParsingException e) {
                  ddlContent.rewind(marker);
                  throw e;
              } catch (Throwable t) {
                  parsingFailed(ddlContent.nextPosition(),
                                &quot;Unexpected exception (&quot; + t.getMessage() + &quot;) parsing&quot;, t);
              }
          }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This sets up some local state, marks the current starting point, and tries to parse DDL statements until no more can be found. If the parsing logic fails to find a match, it generates a &lt;code&gt;ParsingException&lt;/code&gt; with the offending line and column plus a message signaling what was found and what was expected. In such cases, this method rewinds the token stream (in case the caller wishes to try an alternative different parser).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Each time the &lt;code&gt;parseNextStatement&lt;/code&gt; method is called, the starting position of that statement is passed into the method, giving it the starting position of the statement. Our &lt;code&gt;MySqlDdlParser&lt;/code&gt; subclass overrides the &lt;code&gt;parseNextStatement&lt;/code&gt; method to use the first token in the statement to determine the kinds of statement allowed in the MySQL DDL grammar:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;    @Override
          protected void parseNextStatement(Marker marker) {
              if (tokens.matches(DdlTokenizer.COMMENT)) {
                  parseComment(marker);
              } else if (tokens.matches(&quot;CREATE&quot;)) {
                  parseCreate(marker);
              } else if (tokens.matches(&quot;ALTER&quot;)) {
                  parseAlter(marker);
              } else if (tokens.matches(&quot;DROP&quot;)) {
                  parseDrop(marker);
              } else if (tokens.matches(&quot;RENAME&quot;)) {
                  parseRename(marker);
              } else {
                  parseUnknownStatement(marker);
              }
          }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When a matching token is found, the method calls the appropriate method. For example, if the statement begins with &lt;code&gt;CREATE TABLE …​&lt;/code&gt;, then the &lt;code&gt;parseCreate&lt;/code&gt; method is called with the same marker that identifies the starting position of the statement:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;    @Override
          protected void parseCreate(Marker marker) {
              tokens.consume(&quot;CREATE&quot;);
              if (tokens.matches(&quot;TABLE&quot;) || tokens.matches(&quot;TEMPORARY&quot;, &quot;TABLE&quot;)) {
                  parseCreateTable(marker);
              } else if (tokens.matches(&quot;VIEW&quot;)) {
                  parseCreateView(marker);
              } else if (tokens.matchesAnyOf(&quot;DATABASE&quot;, &quot;SCHEMA&quot;)) {
                  parseCreateUnknown(marker);
              } else if (tokens.matchesAnyOf(&quot;EVENT&quot;)) {
                  parseCreateUnknown(marker);
              } else if (tokens.matchesAnyOf(&quot;FUNCTION&quot;, &quot;PROCEDURE&quot;)) {
                  parseCreateUnknown(marker);
              } else if (tokens.matchesAnyOf(&quot;UNIQUE&quot;, &quot;FULLTEXT&quot;, &quot;SPATIAL&quot;, &quot;INDEX&quot;)) {
                  parseCreateIndex(marker);
              } else if (tokens.matchesAnyOf(&quot;SERVER&quot;)) {
                  parseCreateUnknown(marker);
              } else if (tokens.matchesAnyOf(&quot;TABLESPACE&quot;)) {
                  parseCreateUnknown(marker);
              } else if (tokens.matchesAnyOf(&quot;TRIGGER&quot;)) {
                  parseCreateUnknown(marker);
              } else {
                  // It could be several possible things (including more
                  // elaborate forms of those matches tried above),
                  sequentially(this::parseCreateView,
                               this::parseCreateUnknown);
              }
          }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Here, the method first consumes the token with the &lt;code&gt;CREATE&lt;/code&gt; literal, and then tries to match the tokens with various patterns of token literals. If a match is found, this method delegates to other more specific parsing methods. Note how the fluent API of the framework makes it quite easy to understand the match patterns.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s go one step further. Assuming our DDL statement starts with &lt;code&gt;CREATE TABLE products (&lt;/code&gt;, then the parser will then invoke the &lt;code&gt;parseCreateTable&lt;/code&gt; method, again with the same marker denoting the start of the statement:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;    protected void parseCreateTable(Marker start) {
              tokens.canConsume(&quot;TEMPORARY&quot;);
              tokens.consume(&quot;TABLE&quot;);
              boolean onlyIfNotExists = tokens.canConsume(&quot;IF&quot;, &quot;NOT&quot;, &quot;EXISTS&quot;);
              TableId tableId = parseQualifiedTableName(start);
              if ( tokens.canConsume(&quot;LIKE&quot;)) {
                  TableId originalId = parseQualifiedTableName(start);
                  Table original = databaseTables.forTable(originalId);
                  if ( original != null ) {
                      databaseTables.overwriteTable(tableId, original.columns(),
                                                    original.primaryKeyColumnNames());
                  }
                  consumeRemainingStatement(start);
                  debugParsed(start);
                  return;
              }
              if (onlyIfNotExists &amp;amp;&amp;amp; databaseTables.forTable(tableId) != null) {
                  // The table does exist, so we should do nothing ...
                  consumeRemainingStatement(start);
                  debugParsed(start);
                  return;
              }
              TableEditor table = databaseTables.editOrCreateTable(tableId);
      
              // create_definition ...
              if (tokens.matches('(')) parseCreateDefinitionList(start, table);
              // table_options ...
              parseTableOptions(start, table);
              // partition_options ...
              if (tokens.matches(&quot;PARTITION&quot;)) {
                  parsePartitionOptions(start, table);
              }
              // select_statement
              if (tokens.canConsume(&quot;AS&quot;) || tokens.canConsume(&quot;IGNORE&quot;, &quot;AS&quot;)
                  || tokens.canConsume(&quot;REPLACE&quot;, &quot;AS&quot;)) {
                  parseAsSelectStatement(start, table);
              }
      
              // Update the table definition ...
              databaseTables.overwriteTable(table.create());
              debugParsed(start);
          }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This method tries to mirror the &lt;a href=&quot;http://dev.mysql.com/doc/refman/5.7/en/create-table.html&quot;&gt;MySQL &lt;code&gt;CREATE TABLE&lt;/code&gt; grammar rules&lt;/a&gt;, which start with:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name
          (create_definition,...)
          [table_options]
          [partition_options]
      
      CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name
          [(create_definition,...)]
          [table_options]
          [partition_options]
          select_statement
      
      CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name
          { LIKE old_tbl_name | (LIKE old_tbl_name) }
      
      create_definition:
          ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;code&gt;CREATE&lt;/code&gt; literal was already consumed before our &lt;code&gt;parseCreateTable&lt;/code&gt; begins, so it first tries to consume the &lt;code&gt;TEMPORARY&lt;/code&gt; literal if available, the &lt;code&gt;TABLE&lt;/code&gt; literal, the &lt;code&gt;IF NOT EXISTS&lt;/code&gt; fragment if avaialble, and then consumes and parses the qualified name of the table. If the statement includes &lt;code&gt;LIKE otherTable&lt;/code&gt;, it uses the &lt;code&gt;databaseTables&lt;/code&gt; (which is the reference to our &lt;code&gt;Tables&lt;/code&gt; object) to overwrite the definition of the named table with that of the referenced table. Otherwise, it obtains an editor for the new table, and then (like the grammar rules) parses a list of &lt;em&gt;create_definition&lt;/em&gt; fragments, followed by &lt;em&gt;table_options&lt;/em&gt;, &lt;em&gt;partition_options&lt;/em&gt;, and possibly a &lt;em&gt;select_statement&lt;/em&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Take a look at the full &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/debezium-connector-mysql/src/main/java/io/debezium/connector/mysql/MySqlDdlParser.java&quot;&gt;MySqlDdlParser&lt;/a&gt; class to see far more details.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;wrap_up&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#wrap_up&quot;&gt;&lt;/a&gt;Wrap up&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This post goes into some detail about why the MySQL connector uses the DDL statements in the binlog, though we only scratched the surface about &lt;em&gt;how&lt;/em&gt; the connector does the DDL parsing with its framework, and how that can be reused in future parsers for other DBMS dialects.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Try our &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt; to see the MySQL connector in action, and stay tuned for more connectors, releases, and news.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/04/14/Debezium-website/</id>
    <title>Debezium Website</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2016-04-14T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/04/14/Debezium-website/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="website"></category>
    <summary>
      
      As you may have noticed, we have a new website with documentation, a blog, and information about the Debezium community and how you can contribute. Let us know what you think, and contribute improvements.
      ...
    </summary>
    <content type="html">
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As you may have noticed, we have a &lt;a href=&quot;http://debezium.io&quot;&gt;new website&lt;/a&gt; with &lt;a href=&quot;http://debezium.io/docs&quot;&gt;documentation&lt;/a&gt;, a &lt;a href=&quot;http://debezium.io/blog&quot;&gt;blog&lt;/a&gt;, and information about the &lt;a href=&quot;http://debezium.io/community&quot;&gt;Debezium community&lt;/a&gt; and how you can &lt;a href=&quot;http://debezium.io/docs/contribute&quot;&gt;contribute&lt;/a&gt;. Let us know what you think, and &lt;a href=&quot;http://debezium.io/docs/contribute&quot;&gt;contribute improvements&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/03/18/Debezium-0-1-Released/</id>
    <title>Debezium 0.1 Released</title>
    <updated>2018-06-11T08:15:21+00:00</updated>
    <published>2016-03-18T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/03/18/Debezium-0-1-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      Debezium is a distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of Kafka and provides Kafka Connect compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is open source under the Apache...
    </summary>
    <content type="html">
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is a distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now the good news — &lt;strong&gt;&lt;em&gt;Debezium 0.1 is now available&lt;/em&gt;&lt;/strong&gt; and includes several significant features:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;A &lt;a href=&quot;http://debezium.io/docs/connectors/mysql&quot;&gt;connector for MySQL&lt;/a&gt; to monitor MySQL databases. It’s a Kafka Connect source connector, so simply install it into a Kafka Connect service (see below) and use the service’s REST API to configure and manage connectors to each DBMS server. The connector reads the MySQL binlog and generates data change events for every committed row-level modification in the monitored databases. The MySQL connector generates events based upon the tables' structure at the time the row is changed, and it automatically handles changes to the table structures.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;A small library so applications can &lt;a href=&quot;http://debezium.io/docs/embedded&quot;&gt;embed any Kafka Connect connector&lt;/a&gt; and consume data change events read directly from the source system. This provides a much lighter weight system (since Zookeeper, Kafka, and Kafka Connect services are not needed), but as a consequence is not as fault tolerant or reliable since the application must maintain state normally kept inside Kafka’s distributed and replicated logs. Thus the application becomes completely responsible for managing all state.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Although Debezium is really intended to be used as turnkey services, all of Debezium’s JARs and other artifacts are available in &lt;a href=&quot;http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22io.debezium%22&quot;&gt;Maven Central&lt;/a&gt;. Detailed information about the features, tasks, and bugs are outlined in our release notes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To make it easier to use a Debezium’s connector inside your own Kafka Connect service, we created a plugin archive (in both zip and tar.gz formats) that includes all JARs used by the connector not already included in Kafka Connect 0.9.0.1. Simply download, extract to your Kafka Connect 0.9.0.1 installation, and add all of the JARs to the service’s classpath. Once the service is restarted, you can then use the REST API to configure and manage connector instances that monitor the databases of your choice. &lt;a href=&quot;http://search.maven.org/#artifactdetails%7Cio.debezium%7Cdebezium-connector-mysql%7C0.1.0%7Cjar&quot;&gt;MySQL connector plugin archive&lt;/a&gt; is located in Maven Central, so it’s even possible to use Maven to build a customized Kafka Connect service. We’ll generate these plugins for future connectors, too.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium platform has a lot of moving parts in Zookeeper, Kafka, and Kafka Connect. To make it much easier for you to try it out and play with it, we created &lt;a href=&quot;https://hub.docker.com/u/debezium/&quot;&gt;Docker images&lt;/a&gt; and a &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;tutorial&lt;/a&gt; that walks you through using Debezium. First, it has you use Docker to start a container for each of these services and a MySQL server with an example &quot;inventory&quot; database. It shows you how to use the RESTful API to register a connector to monitor the inventory database, how to watch the streams of data changes for various tables, and how changing the database produces new change events with very low latency. It also walks you through shutting down the Kafka Connect service, changing data while the service is not monitoring the database, and then restarting the Kafka Connect service to see how all of the data changes that occurred while the service was not running are still captured correctly in the streams. This tutorial really is a great way to interactively learn the basics of Debezium and change data capture.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;. We plan to release 0.2 very soon with at least one additional connector.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Emmanuel, Chris, Akshath, James, and Paul for their help with the release, questions, and discussions!&lt;/p&gt;
      &lt;/div&gt;
    </content>
  </entry>
</feed>
