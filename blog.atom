<?xml version="1.0" encoding="utf-8" ?>
<feed xml:lang="en-US" xmlns="http://www.w3.org/2005/Atom">
  <id>http://debezium.io/</id>
  <title>Debezium Blog</title>
  <updated>2019-01-17T09:16:09+00:00</updated>
  <link href="http://debezium.io/blog.atom" rel="self" type="application/atom+xml" />
  <link href="http://debezium.io/" rel="alternate" type="text/html" />
  <entry>
    <id>http://debezium.io/blog/2018/12/19/debezium-0-9-0-beta2-released/</id>
    <title>Debezium 0.9.0.Beta2 Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-12-19T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/12/19/debezium-0-9-0-beta2-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="sqlserver"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      With only a few days left for the year, it&#8217;s about time for another Debezium release;
      so it&#8217;s with great pleasure that I&#8217;m announcing Debezium 0.9.0.Beta2!
      
      
      This release comes with support for MySQL 8 and Oracle 11g;
      it includes a first cut of metrics for monitoring the SQL Server and Oracle connectors,
      several improvements to the MongoDB event flattening SMT as well as a wide range of bug fixes.
      Overall, not less than 42 issues were addressed;
      very clearly, there has to be some deeper sense in that ;)
      
      
      A big shout out goes to the following members Debezium&#8217;s amazing community, who contributed to this release:
      Eero Koplimets,...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With only a few days left for the year, it’s about time for another Debezium release;
      so it’s with great pleasure that I’m announcing Debezium &lt;strong&gt;0.9.0.Beta2&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release comes with support for MySQL 8 and Oracle 11g;
      it includes a first cut of metrics for monitoring the SQL Server and Oracle connectors,
      several improvements to the MongoDB event flattening SMT as well as a wide range of bug fixes.
      Overall, not less than &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-9-0-beta2&quot;&gt;42 issues&lt;/a&gt; were addressed;
      very clearly, there has to be &lt;a href=&quot;https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#Answer_to_the_Ultimate_Question_of_Life%2C_the_Universe%2C_and_Everything_%2842%29&quot;&gt;some deeper sense&lt;/a&gt; in that ;)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A big shout out goes to the following members Debezium’s amazing community, who contributed to this release:
      &lt;a href=&quot;https://github.com/pimpelsang&quot;&gt;Eero Koplimets&lt;/a&gt;, &lt;a href=&quot;https://github.com/grzegorz8&quot;&gt;Grzegorz Kołakowski&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/ooooorz&quot;&gt;Hanlin Liu&lt;/a&gt;, &lt;a href=&quot;https://github.com/sweat123&quot;&gt;Lao Mei&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt;, &lt;a href=&quot;https://github.com/tautautau&quot;&gt;Tautvydas Januskevicius&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/wscheep&quot;&gt;Wout Scheepers&lt;/a&gt; and &lt;a href=&quot;https://github.com/wangzheng422&quot;&gt;Zheng Wang&lt;/a&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In the following, let’s take a closer look at some of the changes coming with the 0.9 Beta2 release.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;monitoring_and_metrics_for_the_sql_server_and_oracle_connectors&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#monitoring_and_metrics_for_the_sql_server_and_oracle_connectors&quot;&gt;&lt;/a&gt;Monitoring and Metrics for the SQL Server and Oracle Connectors&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Following the example of the MySQL connector, the connectors for &lt;a href=&quot;http://debezium.io/docs/connectors/sqlserver/&quot;&gt;SQL Server&lt;/a&gt; and &lt;a href=&quot;http://debezium.io/docs/connectors/oracle/&quot;&gt;Oracle&lt;/a&gt; now expose a range of metrics for monitoring purposes via JMX (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-978&quot;&gt;DBZ-978&lt;/a&gt;).
      This includes values like the time since the last CDC event, offset of the last event, the total number of events, remaining and already scanned tables while doing a snapshot and much more.
      Please see &lt;a href=&quot;http://debezium.io/docs/monitoring/&quot;&gt;the monitoring documentation&lt;/a&gt; for details on how to enable JMX.
      The following image shows an example of displaying the values in OpenJDK’s &lt;a href=&quot;https://openjdk.java.net/projects/jmc/&quot;&gt;Mission Control&lt;/a&gt; tool:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/monitoring_mission_control.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Monitoring the Debezium SQL Server connector&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re planning to expand the set of exposed metrics in future versions and also make them available for Postgres and MongoDB.
      Please let us know about the metrics you’d like to see by commenting on JIRA issue &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1040&quot;&gt;DBZ-1040&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As a bonus, we’ve also created a Grafana dashboard for visualizing all the relevant metrics:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/monitoring_dashboard.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Connector metrics in Grafana&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll blog about monitoring and the dashboard in more detail soon;
      but if you are interested, you already can take a look at &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/monitoring&quot;&gt;this demo&lt;/a&gt; in our examples repository.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;misc_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#misc_features&quot;&gt;&lt;/a&gt;Misc. Features&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &quot;snapshot.delay.ms&quot; option already known from the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;Debezium MySQL connector&lt;/a&gt; is now available for all other Debezium connectors, too (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-966&quot;&gt;DBZ-966&lt;/a&gt;).
      This comes in handy when deploying multiple connectors to a Kafka Connect cluster,
      which may cause rebalancing the connectors in the cluster,
      interrupting and restarting running snapshots of already deployed connector instances.
      This can be avoided by specifying a delay which allows to wait with the snapshotting until the rebalancing phase is completed.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;a href=&quot;http://debezium.io/docs/configuration/mongodb-event-flattening/&quot;&gt;MongoDB CDC Event Flattening&lt;/a&gt; transformation received a number of improvements:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Support for MongoDB’s &lt;code&gt;$unset&lt;/code&gt; operator (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-612&quot;&gt;DBZ-612&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Support for full document updates (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-987&quot;&gt;DBZ-987&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;New option for dropping delete and tombstone messages (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-563&quot;&gt;DBZ-563&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Option to convey the original type of operation as a header parameter (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-971&quot;&gt;DBZ-971&lt;/a&gt;);
      that option is also available for the &lt;a href=&quot;http://debezium.io/docs/configuration/event-flattening/&quot;&gt;Flattening SMT&lt;/a&gt; for the relational connectors and can be useful in case sink connectors need to differentiate between inserts and updates&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;bug_fixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bug_fixes&quot;&gt;&lt;/a&gt;Bug fixes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As always, we’ve also fixed a good number of bugs reported by Debezium users.
      The set of fixed issues includes:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Several bugs related to streaming changes from MySQL in GTID mode (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-923&quot;&gt;DBZ-923&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1005&quot;&gt;DBZ-1005&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1008&quot;&gt;DBZ-1008&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Handling of tables with reserved names in the SQL Server connector (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1031&quot;&gt;DBZ-1031&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Potential event loss after MySQL connector restart (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-1033&quot;&gt;DBZ-1033&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Unchanged values of TOASTed columns caused the Postgres connector to fail (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-842&quot;&gt;DBZ-842&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-9-0-beta2&quot;&gt;change log&lt;/a&gt; for the complete list of addressed issues.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;next_steps&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#next_steps&quot;&gt;&lt;/a&gt;Next Steps&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re planning to do a candidate release of Debezium 0.9 in early January.
      Provided no critical issues show up, Debezium 0.9.0.Final should be out by the end of January.
      For the CR we’ve mostly scheduled a number of further bug fixes, improvements to the SQL Server connector and the addition of further metrics.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In parallel, we’ll focus our attention on the Oracle connector again, finally getting back to the long-awaited LogMiner-based capture implementation (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-137&quot;&gt;DBZ-137&lt;/a&gt;).
      This will be a primary feature of Debezium 0.10.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In addition, we’ll spend some cycles on the blogging and demo side of things;
      namely we’re thinking about writing on and demoing the new monitoring and metrics support,
      HA architectures including failover with MySQL, HAProxy and Debezium,
      as well as enriching CDC events with contextual information such as the current user or use case identifiers.
      Stay tuned!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also going beyond 0.10, we got some &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;great plans&lt;/a&gt; for Debezium in the coming year.
      If you’d like to bring in your ideas, too, please let us know on the &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; or in the comments below,
      we’re looking forward to hearing from you.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;And with that, all that remains to be said, is &lt;a href=&quot;https://en.wikipedia.org/wiki/Festivus&quot;&gt;&quot;Happy Festivus for the rest of us!&quot;&lt;/a&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Happy change data streaming and see you in 2019!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/12/05/automating-cache-invalidation-with-change-data-capture/</id>
    <title>Automating Cache Invalidation With Change Data Capture</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-12-05T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/12/05/automating-cache-invalidation-with-change-data-capture/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="discussion"></category>
    <category term="examples"></category>
    <summary>
      
      
      
      The second-level cache of Hibernate ORM / JPA is a proven and efficient way to increase application performance:
      caching read-only or rarely modified entities avoids roundtrips to the database,
      resulting in improved response times of the application.
      
      
      Unlike the first-level cache, the second-level cache is associated with the session factory (or entity manager factory in JPA terms),
      so its contents are shared across transactions and concurrent sessions.
      Naturally, if a cached entity gets modified, the corresponding cache entry must be updated (or purged from the cache), too.
      As long as the data changes are done through Hibernate ORM, this is nothing to worry about: the ORM...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;a href=&quot;https://docs.jboss.org/hibernate/stable/orm/userguide/html_single/Hibernate_User_Guide.html#caching-config&quot;&gt;second-level cache&lt;/a&gt; of Hibernate ORM / JPA is a proven and efficient way to increase application performance:
      caching read-only or rarely modified entities avoids roundtrips to the database,
      resulting in improved response times of the application.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Unlike the first-level cache, the second-level cache is associated with the session factory (or entity manager factory in JPA terms),
      so its contents are shared across transactions and concurrent sessions.
      Naturally, if a cached entity gets modified, the corresponding cache entry must be updated (or purged from the cache), too.
      As long as the data changes are done through Hibernate ORM, this is nothing to worry about: the ORM will update the cache automatically.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Things get tricky, though, when bypassing the application, e.g. when modifying records directly in the database.
      Hibernate ORM then has no way of knowing that the cached data has become stale, and it’s necessary to invalidate the affected items explicitly.
      A common way for doing so is to foresee some admin functionality that allows to clear  an application’s caches.
      For this to work, it’s vital to not forget about calling that invalidation functionality, or the application will keep working with outdated cached data.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In the following we’re going to explore an alternative approach for cache invalidation, which works in a reliable and fully automated way:
      by employing Debezium and its &lt;a href=&quot;http://debezium.io/blog/2018/07/19/advantages-of-log-based-change-data-capture/&quot;&gt;change data capture&lt;/a&gt; (CDC) capabilities, you can track data changes in the database itself and react to any applied change.
      This allows to invalidate affected cache entries in near-realtime,
      without the risk of stale data due to missed changes.
      If an entry has been evicted from the cache, Hibernate ORM will load the latest version of the entity from the database the next time is requested.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;the_example_application&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_example_application&quot;&gt;&lt;/a&gt;The Example Application&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As an example, consider this simple model of two entities, &lt;code&gt;PurchaseOrder&lt;/code&gt; and &lt;code&gt;Item&lt;/code&gt;:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/cache_invalidation_class_diagram.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Example domain model&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A purchase order represents the order of an item, where its total price is the ordered quantity times the item’s base price.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;admonitionblock note&quot;&gt;
      &lt;table&gt;
      &lt;tr&gt;
      &lt;td class=&quot;icon&quot;&gt;
      &lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot;&gt;&lt;/i&gt;
      &lt;/td&gt;
      &lt;td class=&quot;content&quot;&gt;
      &lt;div class=&quot;title&quot;&gt;Source Code&lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/cache-invalidation/&quot;&gt;source code&lt;/a&gt; of this example is provided on GitHub.
      If you want to follow along and try out all the steps described in the following,
      clone the repo and follow the instructions in &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/cache-invalidation/_README.md&quot;&gt;README.md&lt;/a&gt; for building the project.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/td&gt;
      &lt;/tr&gt;
      &lt;/table&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Modelling order and item as JPA entities is straight-forward:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Entity
      public class PurchaseOrder {
      
          @Id
          @GeneratedValue(generator = &quot;sequence&quot;)
          @SequenceGenerator(
              name = &quot;sequence&quot;, sequenceName = &quot;seq_po&quot;, initialValue = 1001, allocationSize = 50
          )
          private long id;
          private String customer;
          @ManyToOne private Item item;
          private int quantity;
          private BigDecimal totalPrice;
      
          // ...
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As changes to items are rare, the &lt;code&gt;Item&lt;/code&gt; entity should be cached.
      This can be done by simply specifying JPA’s &lt;a href=&quot;https://docs.oracle.com/javaee/7/api/javax/persistence/Cacheable.html&quot;&gt;@Cacheable&lt;/a&gt; annotation:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Entity
      @Cacheable
      public class Item {
      
          @Id
          private long id;
          private String description;
          private BigDecimal price;
      
          // ...
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You also need to enable the second-level cache in the &lt;em&gt;META-INF/persistence.xml&lt;/em&gt; file.
      The property &lt;code&gt;hibernate.cache.use_second_level_cache&lt;/code&gt; activates the cache itself, and the &lt;code&gt;ENABLE_SELECTIVE&lt;/code&gt; cache mode
      causes only those entities to be put into the cache which are annotated with &lt;code&gt;@Cacheable&lt;/code&gt;.
      It’s also a good idea to enable SQL query logging and cache access statistics.
      That way you’ll be able to verify whether things work as expected by examining the application log:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt;
      &amp;lt;persistence xmlns=&quot;http://xmlns.jcp.org/xml/ns/persistence&quot;
          xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
          xsi:schemaLocation=&quot;...&quot;
          version=&quot;2.2&quot;&amp;gt;
      
          &amp;lt;persistence-unit name=&quot;orders-PU-JTA&quot; transaction-type=&quot;JTA&quot;&amp;gt;
              &amp;lt;jta-data-source&amp;gt;java:jboss/datasources/OrderDS&amp;lt;/jta-data-source&amp;gt;
              &amp;lt;shared-cache-mode&amp;gt;ENABLE_SELECTIVE&amp;lt;/shared-cache-mode&amp;gt;
              &amp;lt;properties&amp;gt;
                  &amp;lt;property name=&quot;hibernate.cache.use_second_level_cache&quot; value=&quot;true&quot; /&amp;gt;
      
                  &amp;lt;property name=&quot;hibernate.show_sql&quot; value=&quot;true&quot; /&amp;gt;
                  &amp;lt;property name=&quot;hibernate.format_sql&quot; value=&quot;true&quot; /&amp;gt;
                  &amp;lt;property name=&quot;hibernate.generate_statistics&quot; value=&quot;true&quot; /&amp;gt;
      
                  &amp;lt;!-- dialect etc. ... --&amp;gt;
              &amp;lt;/properties&amp;gt;
          &amp;lt;/persistence-unit&amp;gt;
      &amp;lt;/persistence&amp;gt;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When running on a &lt;a href=&quot;https://www.oracle.com/technetwork/java/javaee/overview/index.html&quot;&gt;Java EE&lt;/a&gt; application server
      (or &lt;a href=&quot;https://jakarta.ee/&quot;&gt;Jakarta EE&lt;/a&gt; how the stack is called after it has been donated to the Eclipse Foundation),
      that’s all you need to enable second-level caching.
      In the case of &lt;a href=&quot;http://wildfly.org/&quot;&gt;WildFly&lt;/a&gt; (which is what’s used in the example project), the &lt;a href=&quot;http://infinispan.org/&quot;&gt;Infinispan&lt;/a&gt; key/value store is used as the cache provider by default.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now try and see what happens when modifying an item’s price by running some SQL in the database,
      bypassing the application layer.
      If you’ve checked out the example source code, comment out the &lt;code&gt;DatabaseChangeEventListener&lt;/code&gt; class and start the application as described in the &lt;em&gt;README.md&lt;/em&gt;.
      You then can place purchase orders using curl like this
      (a couple of example items have been persisted at application start-up):&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&amp;gt; curl -H &quot;Content-Type: application/json&quot; \
        -X POST \
        --data '{ &quot;customer&quot; : &quot;Billy-Bob&quot;, &quot;itemId&quot; : 10003, &quot;quantity&quot; : 2 }' \
        http://localhost:8080/cache-invalidation/rest/orders&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;{
          &quot;id&quot; : 1002,
          &quot;customer&quot; : &quot;Billy-Bob&quot;,
          &quot;item&quot; : {
              &quot;id&quot; :10003,
              &quot;description&quot; : &quot;North By Northwest&quot;,
              &quot;price&quot; : 14.99
          },
          &quot;quantity&quot; : 2,
          &quot;totalPrice&quot; : 29.98
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The response is the expected one, as the item price is 14.99.
      Now update the item’s price directly in the database.
      The example uses Postgres, so you can use the &lt;em&gt;psql&lt;/em&gt; CLI utility to do so:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec postgres bash -c 'psql -U $POSTGRES_USER $POSTGRES_DB -c &quot;UPDATE item SET price = 20.99 where id = 10003&quot;'&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Placing another purchase order for the same item using curl,
      you’ll see that the calculated total price doesn’t reflect the update.
      Not good!
      But it’s not too surprising, given that the price update was applied completely bypassing the application layer and Hibernate ORM.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;the_change_event_handler&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_change_event_handler&quot;&gt;&lt;/a&gt;The Change Event Handler&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now let’s explore how to use Debezium and CDC to react to changes in the &lt;code&gt;item&lt;/code&gt; table and invalidate corresponding cache entries.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While Debezium most of the times is deployed into &lt;a href=&quot;https://kafka.apache.org/documentation/#connect&quot;&gt;Kafka Connect&lt;/a&gt; (thus streaming change events into Apache Kafka topics),
      it has another mode of operation that comes in very handy for the use case at hand.
      Using the &lt;a href=&quot;http://debezium.io/docs/embedded/&quot;&gt;embedded engine&lt;/a&gt;, you can run the Debezium connectors as a library directly within your application.
      For each change event received from the database, a configured callback method will be invoked, which in the case at hand will evict the affected item from the second-level cache.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The following picture shows the design of this approach:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/cache_invalidation_architecture.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Architecture Overview&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While this doesn’t come with the scalability and fault tolerance provided by Apache Kafka,
      it nicely fits the given requirements.
      As the second-level cache is bound to the application lifecycle, there is for instance no need for the offset management and restarting capabilities provided by the Kafka Connect framework.
      For the given use case it is enough to receive data change events while the application is running, and using the embedded engine enables exactly that.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;admonitionblock note&quot;&gt;
      &lt;table&gt;
      &lt;tr&gt;
      &lt;td class=&quot;icon&quot;&gt;
      &lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot;&gt;&lt;/i&gt;
      &lt;/td&gt;
      &lt;td class=&quot;content&quot;&gt;
      &lt;div class=&quot;title&quot;&gt;Clustered Applications&lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note that it still might make sense to use Apache Kafka and the regular deployment of Debezium into Kafka Connect when running a clustered application where each node has a local cache.
      Instead of registering a connector on each node, Kafka and Connect would allow you to deploy a single connector instance and have the application nodes listen to the topic(s) with the change events.
      This would result in less resource utilization in the database.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/td&gt;
      &lt;/tr&gt;
      &lt;/table&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Having added the dependencies of the Debezium embedded engine (&lt;em&gt;io.debezium:debezium-embedded:0.9.0.Beta1&lt;/em&gt;) and the Debezium Postgres connector (&lt;em&gt;io.debezium:debezium-connector-postgres:0.9.0.Beta1&lt;/em&gt;) to your project,
      a class &lt;code&gt;DatabaseChangeEventListener&lt;/code&gt; for listening to any changes in the database can be implemented like this:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
      public class DatabaseChangeEventListener {
      
          @Resource
          private ManagedExecutorService executorService;
      
          @PersistenceUnit private EntityManagerFactory emf;
      
          @PersistenceContext
          private EntityManager em;
      
          private EmbeddedEngine engine;
      
          public void startEmbeddedEngine(@Observes @Initialized(ApplicationScoped.class) Object init) {
              Configuration config = Configuration.empty()
                      .withSystemProperties(Function.identity()).edit()
                      .with(EmbeddedEngine.CONNECTOR_CLASS, PostgresConnector.class)
                      .with(EmbeddedEngine.ENGINE_NAME, &quot;cache-invalidation-engine&quot;)
                      .with(EmbeddedEngine.OFFSET_STORAGE, MemoryOffsetBackingStore.class)
                      .with(&quot;name&quot;, &quot;cache-invalidation-connector&quot;)
                      .with(&quot;database.hostname&quot;, &quot;postgres&quot;)
                      .with(&quot;database.port&quot;, 5432)
                      .with(&quot;database.user&quot;, &quot;postgresuser&quot;)
                      .with(&quot;database.password&quot;, &quot;postgrespw&quot;)
                      .with(&quot;database.server.name&quot;, &quot;dbserver1&quot;)
                      .with(&quot;database.dbname&quot;, &quot;inventory&quot;)
                      .with(&quot;database.whitelist&quot;, &quot;public&quot;)
                      .with(&quot;snapshot.mode&quot;, &quot;never&quot;)
                      .build();
      
              this.engine = EmbeddedEngine.create()
                      .using(config)
                      .notifying(this::handleDbChangeEvent)
                      .build();
      
              executorService.execute(engine);
          }
      
          @PreDestroy
          public void shutdownEngine() {
              engine.stop();
          }
      
          private void handleDbChangeEvent(SourceRecord record) {
              if (record.topic().equals(&quot;dbserver1.public.item&quot;)) {
                  Long itemId = ((Struct) record.key()).getInt64(&quot;id&quot;);
                  Struct payload = (Struct) record.value();
                  Operation op = Operation.forCode(payload.getString(&quot;op&quot;));
      
                  if (op == Operation.UPDATE || op == Operation.DELETE) {
                      emf.getCache().evict(Item.class, itemId);
                  }
              }
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Upon application start-up, this configures an instance of the &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql/&quot;&gt;Debezium Postgres connector&lt;/a&gt; and sets up the embedded engine for running the connector.
      The &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql/#connector-properties&quot;&gt;connector options&lt;/a&gt; (host name, credentials etc.) are mostly the same as when deploying the connector into Kafka Connect.
      There is no need for doing an initial snapshot of the existing data, hence the &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql/#snapshots&quot;&gt;snapshot mode&lt;/a&gt; is set to &quot;never&quot;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The offset storage option is used for controlling how connector offsets should be persisted.
      As it’s not necessary to process any change events occurring while the connector is not running
      (instead you’d just begin to read the log from the current location after the restart),
      the in-memory implementation provided by Kafka Connect is used.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once configured, the embedded engine must be run via an &lt;code&gt;Executor&lt;/code&gt; instance.
      As the example runs in WildFly, a managed executor can simply be obtained through &lt;code&gt;@Resource&lt;/code&gt; injection for that purpose (see &lt;a href=&quot;https://www.jcp.org/en/jsr/detail?id=236&quot;&gt;JSR 236&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The embedded engine is configured to invoke the &lt;code&gt;handleDbChangeEvent()&lt;/code&gt; method for each received data change event.
      In this method it first is checked whether the incoming event originates from the &lt;code&gt;item&lt;/code&gt; table.
      If that’s the case, and if the change event represents an &lt;code&gt;UPDATE&lt;/code&gt; or &lt;code&gt;DELETE&lt;/code&gt; statement,
      the affected &lt;code&gt;Item&lt;/code&gt; instance is evicted from the second-level cache.
      JPA 2.0 provides a &lt;a href=&quot;https://javaee.github.io/javaee-spec/javadocs/index.html?javax/persistence/Cache.html&quot;&gt;simple API&lt;/a&gt; for this purpose which is accessible via the &lt;code&gt;EntityManagerFactory&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With the &lt;code&gt;DatabaseChangeEventListener&lt;/code&gt; class in place, the cache entry will now automatically be evicted when doing another item update via &lt;em&gt;psql&lt;/em&gt;.
      When placing the first purchase order for that item after the update, you’ll see in the application log how Hibernate ORM executes a query &lt;code&gt;SELECT ... FROM item ...&lt;/code&gt; in order to load the item referenced by the order.
      Also the cache statistics will report one &quot;L2C miss&quot;.
      Upon subsequent orders of that same item it will be obtained from the cache again.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;admonitionblock note&quot;&gt;
      &lt;table&gt;
      &lt;tr&gt;
      &lt;td class=&quot;icon&quot;&gt;
      &lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot;&gt;&lt;/i&gt;
      &lt;/td&gt;
      &lt;td class=&quot;content&quot;&gt;
      &lt;div class=&quot;title&quot;&gt;Eventual Consistency&lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While the event handling happens in near-realtime, it’s important to point out that it still applies eventual consistency semantics.
      This means that there is a very short time window between the point in time where a transaction is committed
      and the point in time where the change event is streamed from the log to the event handler and the cache entry is invalidated.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/td&gt;
      &lt;/tr&gt;
      &lt;/table&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;avoiding_cache_invalidations_after_application_triggered_data_changes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#avoiding_cache_invalidations_after_application_triggered_data_changes&quot;&gt;&lt;/a&gt;Avoiding Cache Invalidations After Application-triggered Data Changes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The change event listener shown above satisfies the requirement of invalidating cached items after external data changes.
      But in its current form it is evicting cache items a bit too aggressively:
      cached items will also be purged when updating an &lt;code&gt;Item&lt;/code&gt; instance through the application itself.
      This is not only not needed (as the cached item already is the current version), but it’s even counter-productive:
      the superfluous cache evictions will cause additional database roundtrips, resulting in longer response times.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It is therefore necessary to distinguish between data changes performed by the application itself and external data changes.
      Only in the latter case the affected items should be evicted from the cache.
      In order to do so, you can leverage the fact that each Debezium data change event contains the id of the originating transaction.
      Keeping track of all transactions run by the application itself allows to trigger the cache eviction only for those items altered by external transactions.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Accounting for this change, the overall architecture looks like so:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
          &lt;img src=&quot;http://debezium.io/images/cache_invalidation_architecture_tx_registry.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Architecture Overview with Transaction Registry&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The first thing to implement is the transaction registry, i.e. a class for the transaction book keeping:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
      public class KnownTransactions {
      
          private final DefaultCacheManager cacheManager;
          private final Cache&amp;lt;Long, Boolean&amp;gt; applicationTransactions;
      
          public KnownTransactions() {
              cacheManager = new DefaultCacheManager();
              cacheManager.defineConfiguration(
                      &quot;tx-id-cache&quot;,
                      new ConfigurationBuilder()
                          .expiration()
                              .lifespan(60, TimeUnit.SECONDS)
                          .build()
                      );
      
              applicationTransactions = cacheManager.getCache(&quot;tx-id-cache&quot;);
          }
      
          @PreDestroy
          public void stopCacheManager() {
              cacheManager.stop();
          }
      
          public void register(long txId) {
              applicationTransactions.put(txId, true);
          }
      
          public boolean isKnown(long txId) {
              return Boolean.TRUE.equals(applicationTransactions.get(txId));
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This uses the Infinispan &lt;code&gt;DefaultCacheManager&lt;/code&gt; for creating and maintaining an in-memory cache of transaction ids encountered by the application.
      As data change events arrive in near-realtime, the TTL of the cache entries can be rather short
      (in fact, the value of one minute shown in the example is chosen very conservatively, usually events should be received within seconds).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The next step is to retrieve the current transaction id whenever a request is processed by the application and register it within &lt;code&gt;KnownTransactions&lt;/code&gt;.
      This should happen once per transaction.
      There are multiple ways for implementing this logic; in the following a Hibernate ORM &lt;code&gt;FlushEventListener&lt;/code&gt; is used for this purpose:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;class TransactionRegistrationListener implements FlushEventListener {
      
          private volatile KnownTransactions knownTransactions;
      
          public TransactionRegistrationListener() {
          }
      
          @Override
          public void onFlush(FlushEvent event) throws HibernateException {
              event.getSession().getActionQueue().registerProcess( session -&amp;gt; {
                  Number txId = (Number) event.getSession().createNativeQuery(&quot;SELECT txid_current()&quot;)
                          .setFlushMode(FlushMode.MANUAL)
                          .getSingleResult();
      
                  getKnownTransactions().register(txId.longValue());
              } );
          }
      
          private  KnownTransactions getKnownTransactions() {
              KnownTransactions value = knownTransactions;
      
              if (value == null) {
                  knownTransactions = value = CDI.current().select(KnownTransactions.class).get();
              }
      
              return value;
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As there’s no portable way to obtain the transaction id, this is done using a native SQL query.
      In the case of Postgres, the &lt;code&gt;txid_current()&lt;/code&gt; function can be called for that.
      Hibernate ORM event listeners are not subject to dependency injection via CDI.
      Hence the static &lt;code&gt;current()&lt;/code&gt; method is used to obtain a handle to the application’s CDI container and get a reference to the &lt;code&gt;KnownTransactions&lt;/code&gt; bean.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This listener will be invoked whenever Hibernate ORM is synchronizing its persistence context with the database (&quot;flushing&quot;),
      which usually happens exactly once when the transaction is committed.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;admonitionblock note&quot;&gt;
      &lt;table&gt;
      &lt;tr&gt;
      &lt;td class=&quot;icon&quot;&gt;
      &lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot;&gt;&lt;/i&gt;
      &lt;/td&gt;
      &lt;td class=&quot;content&quot;&gt;
      &lt;div class=&quot;title&quot;&gt;Manual Flushes&lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The session / entity manager can also be flushed manually, in which case the &lt;code&gt;txid_current()&lt;/code&gt; function would be invoked multiple times.
      That’s neglected here for the sake of simplicity.
      The actual code in the example repo contains a slightly extended version of this class which makes sure that the transaction id is obtained only once.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/td&gt;
      &lt;/tr&gt;
      &lt;/table&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To register the flush listener with Hibernate ORM, an &lt;code&gt;Integrator&lt;/code&gt; implementation must be created and declared in the &lt;em&gt;META-INF/services/org.hibernate.integrator.spi.Integrator&lt;/em&gt; file:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public class TransactionRegistrationIntegrator implements Integrator {
      
          @Override
          public void integrate(Metadata metadata, SessionFactoryImplementor sessionFactory,
                  SessionFactoryServiceRegistry serviceRegistry) {
              serviceRegistry.getService(EventListenerRegistry.class)
                  .appendListeners(EventType.FLUSH, new TransactionRegistrationListener());
          }
      
          @Override
          public void disintegrate(SessionFactoryImplementor sessionFactory,
                  SessionFactoryServiceRegistry serviceRegistry) {
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;io.debezium.examples.cacheinvalidation.persistence.TransactionRegistrationIntegrator&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;During bootstrap, Hibernate ORM will detect the integrator class (by means of the &lt;a href=&quot;https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/ServiceLoader.html&quot;&gt;Java service loader&lt;/a&gt;),
      invoke its &lt;code&gt;integrate()&lt;/code&gt; method which in turn will register the listener class for the &lt;code&gt;FLUSH&lt;/code&gt; event.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The last step is to exclude any events stemming from transactions run by the application itself in the database change event handler:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
      public class DatabaseChangeEventListener {
      
          // ...
      
          @Inject
          private KnownTransactions knownTransactions;
      
          private void handleDbChangeEvent(SourceRecord record) {
              if (record.topic().equals(&quot;dbserver1.public.item&quot;)) {
                  Long itemId = ((Struct) record.key()).getInt64(&quot;id&quot;);
                  Struct payload = (Struct) record.value();
                  Operation op = Operation.forCode(payload.getString(&quot;op&quot;));
                  Long txId = ((Struct) payload.get(&quot;source&quot;)).getInt64(&quot;txId&quot;);
      
                  if (!knownTransactions.isKnown(txId) &amp;amp;&amp;amp;
                          (op == Operation.UPDATE || op == Operation.DELETE)) {
                      emf.getCache().evict(Item.class, itemId);
                  }
              }
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;And with that, you got all the pieces in place: cached &lt;code&gt;Item&lt;/code&gt;s will only be evicted after external data changes, but not after changes done by the application itself.
      To confirm, you can invoke the example’s &lt;code&gt;items&lt;/code&gt; resource using curl:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&amp;gt; curl -H &quot;Content-Type: application/json&quot; \
        -X PUT \
        --data '{ &quot;description&quot; : &quot;North by Northwest&quot;, &quot;price&quot; : 20.99}' \
        http://localhost:8080/cache-invalidation/rest/items/10003&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When placing the next order for the item after this update, you should see that the &lt;code&gt;Item&lt;/code&gt; entity is obtained from the cache,
      i.e. the change event will not have caused the item’s cache entry to be evicted.
      In contrast, if you update the item’s price via &lt;em&gt;psql&lt;/em&gt; another time,
      the item should be removed from the cache and the order request will produce a cache miss, followed by a &lt;code&gt;SELECT&lt;/code&gt; against the &lt;code&gt;item&lt;/code&gt; table in the database.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot;&gt;&lt;/a&gt;Summary&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In this blog post we’ve explored how Debezium and change data capture can be employed to invalidate application-level caches after external data changes.
      Compared to manual cache invalidation, this approach works very reliably
      (by capturing changes directly from the database log, no events will be missed) and fast
      (cache eviction happens in near-realtime after the data changes).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As you have seen, not too much glue code is needed in order to implement this.
      While the shown implementation is somewhat specific to the entities of the example,
      it should be possible to implement the change event handler in a more generic fashion,
      so that it can handle a set of configured entity types
      (essentially, the database change listener would have to convert the primary key field(s) from the change events into the primary key type of the corresponding entities in a generic way).
      Also such generic implementation would have to provide the logic for obtaining the current transaction id for the most commonly used databases.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please let us know whether you think this would be an interesting extension to have for Debezium and Hibernate ORM.
      For instance this could be a new module under the Debezium umbrella,
      and it could also be a very great project to work on, should you be interested in contributing to Debezium.
      If you got any thoughts on this idea, please post a comment below or come to our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Many thanks to Guillaume Smet, Hans-Peter Grahsl and Jiri Pechanec for their feedback while writing this post!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/11/22/debezium-0-9-0-beta1-released/</id>
    <title>Debezium 0.9.0.Beta1 Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-11-22T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/11/22/debezium-0-9-0-beta1-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="sqlserver"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s my pleasure to announce the release of Debezium 0.9.0.Beta1!
      Oh, and to those of you who are celebrating it&#8201;&#8212;&#8201;Happy Thanksgiving!
      
      
      This new Debezium release comes with several great improvements to our work-in-progress SQL Server connector:
      
      
      
      
      Initial snapshots can be done using the snapshot isolation level if enabled in the DB (DBZ-941)
      
      
      Changes to the structures of captured tables after the connector has been set up are supported now (DBZ-812)
      
      
      New connector option decimal.handling.mode (DBZ-953) and pass-through of any database.* option to the JDBC driver (DBZ-964)
      
      
      
      
      Besides that, we spent some time on supporting the latest versions of the different databases.
      The Debezium connectors now support Postgres...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.9.0.Beta1&lt;/strong&gt;!
      Oh, and to those of you who are celebrating it — Happy Thanksgiving!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This new Debezium release comes with several great improvements to our work-in-progress SQL Server connector:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Initial snapshots can be done using the &lt;code&gt;snapshot&lt;/code&gt; isolation level if enabled in the DB (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-941&quot;&gt;DBZ-941&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Changes to the structures of captured tables after the connector has been set up are supported now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-812&quot;&gt;DBZ-812&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;New connector option &lt;code&gt;decimal.handling.mode&lt;/code&gt; (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-953&quot;&gt;DBZ-953&lt;/a&gt;) and pass-through of any &lt;code&gt;database.*&lt;/code&gt; option to the JDBC driver (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-964&quot;&gt;DBZ-964&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Besides that, we spent some time on supporting the latest versions of the different databases.
      The Debezium connectors now support Postgres 11 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-955&quot;&gt;DBZ-955&lt;/a&gt;) and MongoDB 4.0 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-974&quot;&gt;DBZ-974&lt;/a&gt;).
      We are also working on supporting MySQL 8.0, which should be completed in the next 0.9.x release.
      The Debezium container images have been updated to Kafka 2.0.1 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-979&quot;&gt;DBZ-979&lt;/a&gt;)
      and the Kafka Connect image now supports the &lt;code&gt;STATUS_STORAGE_TOPIC&lt;/code&gt; environment variable,
      bringing consistency with &lt;code&gt;CONFIG_STORAGE_TOPIC&lt;/code&gt; and &lt;code&gt;OFFSET_STORAGE_TOPIC&lt;/code&gt; that already were supported before (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-893&quot;&gt;DBZ-893&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As usual, several bugs were fixed, too.
      Several of them dealt with the new Antlr-based DDL parser for the MySQL connector.
      By now we feel confident about its implementation, so it’s the default DDL parser as of this release (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-757&quot;&gt;DBZ-757&lt;/a&gt;).
      If you would like to continue to use the legacy parser for some reason, you can do so by setting the &lt;code&gt;ddl.parser.mode&lt;/code&gt; connector option to &quot;legacy&quot;.
      This implementation will remain available in the lifetime of Debezium 0.9.x and is scheduled for removal after that.
      So please make sure to log issues in JIRA should you run into any problems with the Antlr parser.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Overall, this release contains &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-9-0-beta1&quot;&gt;21 fixes&lt;/a&gt;.
      Thanks a lot to all the community members who helped with making this happen:
      &lt;a href=&quot;https://github.com/anton-martynov&quot;&gt;Anton Martynov&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/deepakbarr&quot;&gt;Deepak Barr&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/grzegorz8&quot;&gt;Grzegorz Kołakowski&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/olavim&quot;&gt;Olavi Mustanoja&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/vamossagar12&quot;&gt;Sagar Rao&lt;/a&gt; and
      &lt;a href=&quot;https://github.com/shivamsharma&quot;&gt;Shivam Sharma&lt;/a&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_else&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_else&quot;&gt;&lt;/a&gt;What else?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While the work towards Debezium 0.9 continues, we’ve lately been quite busy with presenting Debezium at multiple conferences.
      You can find the slides and recordings from &lt;a href=&quot;https://kafka-summit.org/sessions/change-data-streaming-patterns-microservices-debezium/&quot;&gt;Kafka Summit&lt;/a&gt; San Francisco and &lt;a href=&quot;https://vxdms2018.confinabox.com/talk/INI-9172/Data_Streaming_for_Microservices_using_Debezium&quot;&gt;Voxxed Days Microservices&lt;/a&gt; on our list of &lt;a href=&quot;http://debezium.io/docs/online-resources/&quot;&gt;online resources&lt;/a&gt; around Debezium.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;There you also can find the links to the slides of the great talk &quot;The Why’s and How’s of Database Streaming&quot; by Joy Gao of WePay, a Debezium user of the first hour,
      as well as the link to a blog post by Hans-Peter Grahsl about setting up a CDC pipeline from MySQL into Cosmos DB running on Azure.
      If you know about other great articles, session recordings or similar on Debezium and change data capture which should be added there, please let us know.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/10/04/debezium-0-9-0-alpha2-released/</id>
    <title>Debezium 0.9.0.Alpha2 Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-10-04T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/10/04/debezium-0-9-0-alpha2-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="sqlserver"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s my pleasure to announce the release of Debezium 0.9.0.Alpha2!
      
      
      While the work on the connectors for SQL Server and Oracle continues, we decided to do another Alpha release,
      as lots of fixes and new features - many of them contributed by community members - have piled up,
      which we wanted to get into your hands as quickly as possible.
      
      
      This release supports Apache Kafka 2.0, comes with support for Postgres' HSTORE column type, allows to rename and filter fields from change data messages for MongoDB
      and contains multiple bug fixes and performance improvements.
      Overall, this release contains 55 fixes
      (note that a few of these have...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.9.0.Alpha2&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While the work on the connectors for SQL Server and Oracle continues, we decided to do another Alpha release,
      as lots of fixes and new features - many of them contributed by community members - have piled up,
      which we wanted to get into your hands as quickly as possible.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release supports Apache Kafka 2.0, comes with support for Postgres' HSTORE column type, allows to rename and filter fields from change data messages for MongoDB
      and contains multiple bug fixes and performance improvements.
      Overall, this release contains &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-9-0-alpha2&quot;&gt;55 fixes&lt;/a&gt;
      (note that a few of these have been merged back to 0.8.x and are contained in earlier 0.8 releases, too).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A big &quot;Thank You&quot; is in order to community members
      &lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/artiship&quot;&gt;Artiship Artiship&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/CliffWheadon&quot;&gt;Cliff Wheadon&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/deepakbarr&quot;&gt;Deepak Barr&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/ian-axelrod&quot;&gt;Ian Axelrod&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/ooooorz&quot;&gt;Liu Hanlin&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/maver1ck&quot;&gt;Maciej Bryński&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/oripwk&quot;&gt;Ori Popowski&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/PengLyu&quot;&gt;Peng Lyu&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/PSanetra&quot;&gt;Philip Sanetra&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/sagarrao&quot;&gt;Sagar Rao&lt;/a&gt; and
      &lt;a href=&quot;https://github.com/SyedMuhammadSufyian&quot;&gt;Syed Muhammad Sufyian&lt;/a&gt;
      for their contributions to this release.
      We salute you!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;kafka_upgrade&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#kafka_upgrade&quot;&gt;&lt;/a&gt;Kafka Upgrade&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium runs with and has been tested on top of the recently released Apache Kafka 2.0 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-858&quot;&gt;DBZ-858&lt;/a&gt;).
      The widely used version Kafka 1.x continues to be supported as well.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note that 0.10.x is not supported due to Debezium’s usage of the admin client API which is only available in later versions.
      It shouldn’t be too hard to work around this, so if someone is interested in helping out with this,
      this would be a great contribution (see &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-883&quot;&gt;DBZ-883&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;support_for_hstore_columns_in_postgres&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#support_for_hstore_columns_in_postgres&quot;&gt;&lt;/a&gt;Support for HSTORE columns in Postgres&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Postgres is an amazingly powerful and flexible RDBMS, not the least due to its wide range of column types which go far beyond what’s defined by the SQL standard.
      One of these types being &lt;a href=&quot;https://www.postgresql.org/docs/current/static/hstore.html&quot;&gt;HSTORE&lt;/a&gt;, which is a string-to-string map essentially.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium can capture changes to columns of this type now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-898&quot;&gt;DBZ-898&lt;/a&gt;).
      By default, the field values will be represented using Kafka Connect’s map data type.
      As this may not be supported by all sink connectors,
      you might alternatively represent them as a string-ified JSON by setting the new &lt;code&gt;hstore.handling.mode&lt;/code&gt; connector option to &lt;code&gt;json&lt;/code&gt;.
      In this case, you’d see HSTORE columns represented as values in change messages like so: &lt;code&gt;{ &quot;key1&quot; : &quot;val1&quot;, &quot;key2&quot; : &quot;val2&quot; }&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;field_filtering_and_renaming_for_mongodb&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#field_filtering_and_renaming_for_mongodb&quot;&gt;&lt;/a&gt;Field filtering and renaming for MongoDB&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Unlike the connectors for MySQL and Postgres, the Debezium MongoDB connector so far didn’t allow to exclude single fields of captured collections from CDC messages.
      Also renaming them wasn’t supported e.g. by means of Kafka’s &lt;code&gt;ReplaceField&lt;/code&gt; SMT.
      The reason being that MongoDB doesn’t mandate a fixed schema for the documents of a given collection,
      and documents therefore are represented in change messages using a single string-ified JSON field.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to the fantastic work of community member Andrey Pustovetov,
      this finally has changed, i.e. you can remove given fields (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-633&quot;&gt;DBZ-633&lt;/a&gt;) now from the CDC messages of given collections or have them renamed (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-881&quot;&gt;DBZ-881&lt;/a&gt;).
      Please refer to the description of the new connector options &lt;code&gt;field.blacklist&lt;/code&gt; and &lt;code&gt;field.renames&lt;/code&gt; in the &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/&quot;&gt;MongoDB connector documentation&lt;/a&gt; to learn more.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;extended_source_info&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#extended_source_info&quot;&gt;&lt;/a&gt;Extended source info&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Another contribution by Andrey is the new optional &lt;code&gt;connector&lt;/code&gt; field within the source info block of CDC messages
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-918&quot;&gt;DBZ-918&lt;/a&gt;).
      This tells the type of source connector that produced the messages (&quot;mysql&quot;, &quot;postgres&quot; etc.),
      which can come in handy in cases where specific semantics need to be applied on the consumer side depending on the type of source database.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;bug_fixes_and_version_upgrades&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bug_fixes_and_version_upgrades&quot;&gt;&lt;/a&gt;Bug fixes and version upgrades&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The new release contains a good number of bug fixes and other smaller improvements.
      Amongst them are&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;correct handling of invalid temporal default values with MySQL (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-927&quot;&gt;DBZ-927&lt;/a&gt;),&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;support for table/collection names with special characters for MySQL (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-878&quot;&gt;DBZ-878&lt;/a&gt;) and MongoDB (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-865&quot;&gt;DBZ-865&lt;/a&gt;) and&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;fixed handling of blacklisted tables with the new Antlr-based DDL parser (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-872&quot;&gt;DBZ-872&lt;/a&gt;).&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Community member Ian Axelrod provided a fix for a potential performance issue,
      where changes to tables with TOAST columns in Postgres would cause repeated updates to the connector’s internal schema metadata,
      which can be a costly operation (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-911&quot;&gt;DBZ-911&lt;/a&gt;).
      Please refer to the &lt;a href=&quot;http://debezium.io/docs/connectors/postgres/&quot;&gt;Postgres connector documentation&lt;/a&gt; for details on the new &lt;code&gt;schema.refresh.mode&lt;/code&gt; option,
      which deals with this issue.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In terms of version upgrades we migrated to the latest releases of the MySQL (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-763&quot;&gt;DBZ-763&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-764&quot;&gt;DBZ-764&lt;/a&gt;) and Postgres drivers (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-912&quot;&gt;DBZ-912&lt;/a&gt;).
      The former is part of a longer stream of work leading towards support of MySQL 8 which should be finished in one of the next Debezium releases.
      For Postgres we provide a Docker image with Debezium’s supported logical decoding plug-ins based on Alpine now,
      which might be interesting to those concerned about container size (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-705&quot;&gt;DBZ-705&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the change log for the complete list of fixed issues.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The work towards Debezium 0.9 continues, and we’ll focus mostly on improvements to the SQL Server and Oracle connectors.
      Other potential topics include support for MySQL 8 and native logical decoding as introduced with Postgres 10,
      which should greatly help with using the Debezium Postgres connectors in cloud environments such as Amazon RDS.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll also be talking about Debezium at the following conferences:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://kafka-summit.org/sessions/change-data-streaming-patterns-microservices-debezium/&quot;&gt;Kafka Summit&lt;/a&gt;; San Francisco, Cal.; Oct. 17&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://vxdms2018.confinabox.com/talk/INI-9172/Data_Streaming_for_Microservices_using_Debezium&quot;&gt;VoxxedDays Microservices&lt;/a&gt;; Paris, France; Oct. 29 - 31&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://cfp.devoxx.ma/2018/talk/AEY-4477/Change_Data_Streaming_Patterns_for_Microservices_With_Debezium&quot;&gt;Devoxx Morocco&lt;/a&gt;; Marrakesh, Morocco; Nov. 27 - 29&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Already last week I had the opportunity to present Debezium at &lt;a href=&quot;https://jug-saxony-day.org/programm/#!/P31&quot;&gt;JUG Saxony Day&lt;/a&gt;.
      If you are interested, you can find the (German) &lt;a href=&quot;https://speakerdeck.com/gunnarmorling/streaming-von-datenbankanderungen-mit-debezium-jug-saxony-day&quot;&gt;slideset of that talk&lt;/a&gt; on Speaker Deck.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/09/20/materializing-aggregate-views-with-hibernate-and-debezium/</id>
    <title>Materializing Aggregate Views With Hibernate and Debezium</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-09-20T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/09/20/materializing-aggregate-views-with-hibernate-and-debezium/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="discussion"></category>
    <category term="examples"></category>
    <summary>
      
      
      
      Updating external full text search indexes (e.g. Elasticsearch) after data changes is a very popular use case for change data capture (CDC).
      
      
      As we&#8217;ve discussed in a blog post a while ago,
      the combination of Debezium&#8217;s CDC source connectors and Confluent&#8217;s sink connector for Elasticsearch makes it straight forward to capture data changes in MySQL, Postgres etc. and push them towards Elastisearch in near real-time.
      This results in a 1:1 relationship between tables in the source database and a corresponding search index in Elasticsearch,
      which is perfectly fine for many use cases.
      
      
      It gets more challenging though if you&#8217;d like to put entire aggregates into...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Updating external full text search indexes (e.g. &lt;a href=&quot;https://www.elastic.co/products/elasticsearch&quot;&gt;Elasticsearch&lt;/a&gt;) after data changes is a very popular use case for change data capture (CDC).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As we’ve discussed in a &lt;a href=&quot;http://debezium.io/blog/2018/01/17/streaming-to-elasticsearch/&quot;&gt;blog post&lt;/a&gt; a while ago,
      the combination of Debezium’s CDC source connectors and Confluent’s &lt;a href=&quot;https://docs.confluent.io/current/connect/connect-elasticsearch/docs/index.html&quot;&gt;sink connector for Elasticsearch&lt;/a&gt; makes it straight forward to capture data changes in MySQL, Postgres etc. and push them towards Elastisearch in near real-time.
      This results in a 1:1 relationship between tables in the source database and a corresponding search index in Elasticsearch,
      which is perfectly fine for many use cases.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It gets more challenging though if you’d like to put entire aggregates into a single index.
      An example could be a customer and all their addresses;
      those would typically be stored in two separate tables in an RDBMS, linked by a foreign key,
      whereas you’d like to have just one index in Elasticsearch,
      containing documents of customers with their addresses embedded,
      allowing you to efficiently search for customers based on their address.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Following up to the &lt;a href=&quot;http://debezium.io/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/&quot;&gt;KStreams-based solution&lt;/a&gt; to this we described recently,
      we’d like to present in this post an alternative for materializing such aggregate views driven by the application layer.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;overview&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The idea is to materialize views in a separate table in the source database,
      right in the moment the original data is altered.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Aggregates are serialized as JSON structures (which naturally can represent any nested object structure) and stored in a specific table.
      This is done within the actual transaction altering the data,
      which means the aggregate view is always consistent with the primary data.
      In particular this approach isn’t prone to exposing intermediary aggregations as the KStreams-based solution discussed in the post linked above.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The following picture shows the overall architecture:&lt;/p&gt;
      &lt;/div&gt;
      &lt;img src=&quot;http://debezium.io/images/jpa_aggregations.png&quot; style=&quot;max-width:100%; margin-bottom:10px;&quot; class=&quot;responsive-image&quot; alt=&quot;Streaming Materialized Aggregate Views to Elastisearch&quot; /&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Here the aggregate views are materialized by means of a small extension to &lt;a href=&quot;http://hibernate.org/orm/&quot;&gt;Hibernate ORM&lt;/a&gt;,
      which stores the JSON aggregates within the source database
      (note &quot;aggregate views&quot; can be considered conceptually the same as &quot;materialized views&quot; as known from different RDBMS,
      as in that they materialize the result of a &quot;join&quot; operation,
      but technically we’re not using the latter to store aggregate views, but a regular table).
      Changes to that aggregate table are then captured by Debezium and streamed to one topic per aggregate type.
      The Elasticsearch sink connector can subscribe to these topics and update corresponding full-text indexes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You can find a proof-of-concept implementation (said Hibernate extension and related code) of this idea in our &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/jpa-aggregations&quot;&gt;examples repository&lt;/a&gt;.
      Of course the general idea isn’t limited to Hibernate ORM or JPA,
      you could implement something similar with any other API you’re using to access your data.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;creating_aggregate_views_via_hibernate_orm&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#creating_aggregate_views_via_hibernate_orm&quot;&gt;&lt;/a&gt;Creating Aggregate Views via Hibernate ORM&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For the following let’s assume we’re persisting a simple domain model
      (comprising a &lt;code&gt;Customer&lt;/code&gt; entity and a few related ones like &lt;code&gt;Address&lt;/code&gt;, (customer) &lt;code&gt;Category&lt;/code&gt; etc.) in a database.
      Using Hibernate for that allows us to make the creation of aggregates fully transparent to the actual application code using a &lt;a href=&quot;http://docs.jboss.org/hibernate/orm/current/userguide/html_single/Hibernate_User_Guide.html#events-events&quot;&gt;Hibernate event listener&lt;/a&gt;.
      Thanks to its extensible architecture, we can plug such listener into Hibernate just by adding it to the classpath,
      from where it will be picked up automatically when bootstrapping the entity manager / session factory.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Our example listener reacts to an annotation, &lt;code&gt;@MaterializeAggregate&lt;/code&gt;,
      which marks those entity types that should be the roots of materialized aggregates.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Entity
      @MaterializeAggregate(aggregateName=&quot;customers-complete&quot;)
      public class Customer {
      
          @Id
          private long id;
      
          private String firstName;
      
          @OneToMany(mappedBy = &quot;customer&quot;, fetch = FetchType.EAGER, cascade = CascadeType.ALL)
          private Set&amp;lt;Address&amp;gt; addresses;
      
          @ManyToOne
          private Category category;
      
          ...
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now if any entity annotated with &lt;code&gt;@MaterializeAggregate&lt;/code&gt; is inserted, updated or deleted via Hibernate,
      the listener will kick in and materialize a JSON view of the aggregate root (customer) and its associated entities (addresses, category).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Under the hood the &lt;a href=&quot;https://github.com/FasterXML/jackson&quot;&gt;Jackson API&lt;/a&gt; is used for serializing the model into JSON.
      This means you can use any of its annotations to customize the JSON output, e.g. &lt;code&gt;@JsonIgnore&lt;/code&gt;  to exclude the inverse relationship from &lt;code&gt;Address&lt;/code&gt; to &lt;code&gt;Customer&lt;/code&gt;:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Entity
      public class Address {
      
          @Id
          private long id;
      
          @ManyToOne
          @JoinColumn(name = &quot;customer_id&quot;)
          @JsonIgnore
          private Customer customer;
      
          private String street;
      
          private String city;
      
          ...
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note that &lt;code&gt;Address&lt;/code&gt; itself isn’t marked with &lt;code&gt;@MaterializeAggregate&lt;/code&gt;, i.e. it won’t be materialized into an aggregate view by itself.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;After using JPA’s &lt;code&gt;EntityManager&lt;/code&gt; to insert or update a few customers,
      let’s take a look at the &lt;code&gt;aggregates&lt;/code&gt; table which has been populated by the listener
      (value schema omitted for the sake of brevity):&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;&amp;gt; select * from aggregates;
      
      | rootType | keySchema | rootId | materialization | valueSchema |
      
      | customers-complete
      
      | {
        &quot;schema&quot; : {
          &quot;type&quot; : &quot;struct&quot;,
          &quot;fields&quot; : [ {
            &quot;type&quot; : &quot;int64&quot;,
            &quot;optional&quot; : false,
            &quot;field&quot; : &quot;id&quot;
          } ],
          &quot;optional&quot; : false,
          &quot;name&quot; : &quot;customers-complete.Key&quot;
        }
      }
      
      | { &quot;id&quot; : 1004 }
      
      | { &quot;schema&quot; : { ... } }
      
      | {
        &quot;id&quot; : 1004,
        &quot;firstName&quot; : &quot;Anne&quot;,
        &quot;lastName&quot; : &quot;Kretchmar&quot;,
        &quot;email&quot; : &quot;annek@noanswer.org&quot;,
        &quot;tags&quot; : [ &quot;long-term&quot;, &quot;vip&quot; ],
        &quot;birthday&quot; : 5098,
        &quot;category&quot; : {
          &quot;id&quot; : 100001,
          &quot;name&quot; : &quot;Retail&quot;
        },
        &quot;addresses&quot; : [ {
          &quot;id&quot; : 16,
          &quot;street&quot; : &quot;1289 University Hill Road&quot;,
          &quot;city&quot; : &quot;Canehill&quot;,
          &quot;state&quot; : &quot;Arkansas&quot;,
          &quot;zip&quot; : &quot;72717&quot;,
          &quot;type&quot; : &quot;SHIPPING&quot;
        } ]
      } |&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The table contains these columns:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;rootType&lt;/code&gt;: The name of the aggregate as given in the &lt;code&gt;@MaterializeAggregate&lt;/code&gt; annotation&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;rootId&lt;/code&gt;: The aggregate’s id as serialized JSON&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;materialization&lt;/code&gt;: The aggregate itself as serialized JSON; in this case a customer and their addresses, category etc.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;keySchema&lt;/code&gt;: The Kafka Connect schema of the row’s key&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;valueSchema&lt;/code&gt;: The Kafka Connect schema of the materialization&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s talk about the two schema columns for a bit.
      JSON itself is quite limited as far as its supported data types are concerned.
      So for instance we’d loose information about a numeric field’s value range (int vs. long etc.) without any additional information.
      Therefore the listener derives the corresponding schema information for key and aggregate view from the entity model and stores it within the aggregate records.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now Jackson itself only supports JSON Schema, which would be a bit too limited for our purposes.
      Hence the example implementation provides custom serializers for Jackson’s schema system,
      which allow us to emit Kafka Connect’s schema representation (with more precise type information) instead of plain JSON Schema.
      This will come in handy in the following when we’d like to expand the string-based JSON representations of key and value into properly typed Kafka Connect records.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;capturing_changes_to_the_aggregate_table&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#capturing_changes_to_the_aggregate_table&quot;&gt;&lt;/a&gt;Capturing Changes to the Aggregate Table&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We now have a mechanism in place which transparently persists aggregates into a separate table within the source database,
      whenever the application data is changed through Hibernate.
      Note that this happens within the boundaries of the source transaction,
      so if the same would be rolled back for some reason, also the aggregate view would not be updated.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Hibernate listener uses insert-or-update semantics when writing an aggregate view,
      i.e. for a given aggregate root there’ll always be exactly one corresponding entry in the aggregate table which reflects its current state.
      If an aggregate root entity is deleted, the listener will also drop the entry from the aggregate table.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So let’s set up Debezium now to capture any changes to the &lt;code&gt;aggregates&lt;/code&gt; table:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;curl -i -X POST \
        -H &quot;Accept:application/json&quot; \
        -H &quot;Content-Type:application/json&quot; \
        http://localhost:8083/connectors/ -d @- &amp;lt;&amp;lt;-EOF
        {
            &quot;name&quot;: &quot;inventory-connector&quot;,
            &quot;config&quot;: {
                &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
                &quot;tasks.max&quot;: &quot;1&quot;,
                &quot;database.hostname&quot;: &quot;mysql&quot;,
                &quot;database.port&quot;: &quot;3306&quot;,
                &quot;database.user&quot;: &quot;debezium&quot;,
                &quot;database.password&quot;: &quot;dbz&quot;,
                &quot;database.server.id&quot;: &quot;184054&quot;,
                &quot;database.server.name&quot;: &quot;dbserver1&quot;,
                &quot;database.whitelist&quot;: &quot;inventory&quot;,
                &quot;table.whitelist&quot;: &quot;.*aggregates&quot;,
                &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
                &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot;
            }
        }
      EOF&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This registers the MySQL connector with the &quot;inventory&quot; database
      (we’re using an expanded version of the schema from the &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;Debezium tutorial&lt;/a&gt;),
      capturing any changes to the &quot;aggregates&quot; table.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;expanding_json&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#expanding_json&quot;&gt;&lt;/a&gt;Expanding JSON&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If we now were to browse the corresponding Kafka topic, we’d see data change events in the known Debezium format for all the changes to the &lt;code&gt;aggregates&lt;/code&gt; table.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &quot;materialization&quot; field with the records' &quot;after&quot; state still is a single field containing a JSON string, though.
      What we’d rather like to have is a strongly typed Kafka Connect record, whose schema exactly describes the aggregate structure and the types of its fields.
      For that purpose the example project provides an SMT (single message transform) which takes the JSON materialization and the corresponding &lt;code&gt;valueSchema&lt;/code&gt; and converts this into a full-blown Kafka Connect record.
      The same is done for keys.
      DELETE events are rewritten into tombstone events.
      Finally, the SMT re-routes every record to a topic named after the aggregate root,
      allowing consumers to subscribe just to changes to specific aggregate types.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So let’s add that SMT when registering the Debezium CDC connector:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;...
      &quot;transforms&quot;:&quot;expandjson&quot;,
      &quot;transforms.expandjson.type&quot;:&quot;io.debezium.aggregation.smt.ExpandJsonSmt&quot;,
      ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When now browsing the &quot;customers-complete&quot; topic, we’ll see the strongly typed Kafka Connect records we’d expect:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
          &quot;schema&quot;: {
              &quot;type&quot;: &quot;struct&quot;,
              &quot;fields&quot;: [
                  {
                      &quot;type&quot;: &quot;int64&quot;,
                      &quot;optional&quot;: false,
                      &quot;field&quot;: &quot;id&quot;
                  }
              ],
              &quot;optional&quot;: false,
              &quot;name&quot;: &quot;customers-complete.Key&quot;
          },
          &quot;payload&quot;: {
              &quot;id&quot;: 1004
          }
      }
      {
          &quot;schema&quot;: {
              &quot;type&quot;: &quot;struct&quot;,
              &quot;fields&quot;: [ ... ],
              &quot;optional&quot;: true,
              &quot;name&quot;: &quot;urn:jsonschema:com:example:domain:Customer&quot;
          },
          &quot;payload&quot;: {
              &quot;id&quot;: 1004,
              &quot;firstName&quot;: &quot;Anne&quot;,
              &quot;lastName&quot;: &quot;Kretchmar&quot;,
              &quot;email&quot;: &quot;annek@noanswer.org&quot;,
              &quot;active&quot;: true,
              &quot;tags&quot; : [ &quot;long-term&quot;, &quot;vip&quot; ],
              &quot;birthday&quot; : 5098,
              &quot;category&quot;: {
                  &quot;id&quot;: 100001,
                  &quot;name&quot;: &quot;Retail&quot;
              },
              &quot;addresses&quot;: [
                  {
                      &quot;id&quot;: 16,
                      &quot;street&quot;: &quot;1289 University Hill Road&quot;,
                      &quot;city&quot;: &quot;Canehill&quot;,
                      &quot;state&quot;: &quot;Arkansas&quot;,
                      &quot;zip&quot;: &quot;72717&quot;,
                      &quot;type&quot;: &quot;LIVING&quot;
                  }
              ]
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To confirm that these are actual typed Kafka Connect records and not just a single JSON string field,
      you could for instance use the &lt;a href=&quot;http://debezium.io/docs/configuration/avro/&quot;&gt;Avro message converter&lt;/a&gt; and examine the message schemas in the schema registry.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;sinking_aggregate_messages_into_elasticsearch&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#sinking_aggregate_messages_into_elasticsearch&quot;&gt;&lt;/a&gt;Sinking Aggregate Messages Into Elasticsearch&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The last missing step is to register the Confluent Elasticsearch sink connector, hooking it up with the &quot;customers-complete&quot; topic and letting it push any changes to the corresponding index:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;curl -i -X POST \
        -H &quot;Accept:application/json&quot; \
        -H &quot;Content-Type:application/json&quot; \
        http://localhost:8083/connectors/ -d @- &amp;lt;&amp;lt;-EOF
        {
            &quot;name&quot;: &quot;es-customers&quot;,
            &quot;config&quot;: {
                &quot;connector.class&quot;: &quot;io.confluent.connect.elasticsearch.ElasticsearchSinkConnector&quot;,
                &quot;tasks.max&quot;: &quot;1&quot;,
                &quot;topics&quot;: &quot;customers-complete&quot;,
                &quot;connection.url&quot;: &quot;http://elastic:9200&quot;,
                &quot;key.ignore&quot;: &quot;false&quot;,
                &quot;schema.ignore&quot; : &quot;false&quot;,
                &quot;behavior.on.null.values&quot; : &quot;delete&quot;,
                &quot;type.name&quot;: &quot;customer-with-addresses&quot;,
                &quot;transforms&quot; : &quot;key&quot;,
                &quot;transforms.key.type&quot;: &quot;org.apache.kafka.connect.transforms.ExtractField$Key&quot;,
                &quot;transforms.key.field&quot;: &quot;id&quot;
            }
        }
      EOF&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This uses Connect’s &lt;code&gt;ExtractField&lt;/code&gt; transformation to obtain just the actual id value from the key struct and use it as key for the corresponding Elasticsearch documents.
      Specifying the &quot;behavior.on.null.values&quot; option will let the connector delete the corresponding document from the index when encountering a tombstone message (i.e. a message with a key but without value).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, we can use the Elasticsearch REST API to browse the index and of course use its powerful full-text query language to find customers by the address or any other property embedded into the aggregate structure:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&amp;gt; curl -X GET -H &quot;Accept:application/json&quot; \
        http://localhost:9200/customers-complete/_search?pretty
      
        {
            &quot;_shards&quot;: {
                &quot;failed&quot;: 0,
                &quot;successful&quot;: 5,
                &quot;total&quot;: 5
            },
            &quot;hits&quot;: {
                &quot;hits&quot;: [
                    {
                        &quot;_id&quot;: &quot;1004&quot;,
                        &quot;_index&quot;: &quot;customers-complete&quot;,
                        &quot;_score&quot;: 1.0,
                        &quot;_source&quot;: {
                            &quot;active&quot;: true,
                            &quot;addresses&quot;: [
                                {
                                    &quot;city&quot;: &quot;Canehill&quot;,
                                    &quot;id&quot;: 16,
                                    &quot;state&quot;: &quot;Arkansas&quot;,
                                    &quot;street&quot;: &quot;1289 University Hill Road&quot;,
                                    &quot;type&quot;: &quot;LIVING&quot;,
                                    &quot;zip&quot;: &quot;72717&quot;
                                }
                            ],
                            &quot;tags&quot; : [ &quot;long-term&quot;, &quot;vip&quot; ],
                            &quot;birthday&quot; : 5098,
                            &quot;category&quot;: {
                                &quot;id&quot;: 100001,
                                &quot;name&quot;: &quot;Retail&quot;
                            },
                            &quot;email&quot;: &quot;annek@noanswer.org&quot;,
                            &quot;firstName&quot;: &quot;Anne&quot;,
                            &quot;id&quot;: 1004,
                            &quot;lastName&quot;: &quot;Kretchmar&quot;,
                            &quot;scores&quot;: [],
                            &quot;someBlob&quot;: null,
                            &quot;tags&quot;: []
                        },
                        &quot;_type&quot;: &quot;customer-with-addresses&quot;
                    }
                ],
                &quot;max_score&quot;: 1.0,
                &quot;total&quot;: 1
            },
            &quot;timed_out&quot;: false,
            &quot;took&quot;: 11
        }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;And there you have it: a customer’s complete data, including their addresses, categories, tags etc., materialized into a single document within Elasticsearch.
      If you’re using JPA to update the customer, you’ll see the data in the index being updated accordingly in near-realtime.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;pros_and_cons&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#pros_and_cons&quot;&gt;&lt;/a&gt;Pros and Cons&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So what are the advantages and disadvantages of this approach for materializing aggregates from multiple source tables compared to the &lt;a href=&quot;http://debezium.io/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/&quot;&gt;KStreams-based approach&lt;/a&gt;?&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The big advantage is consistency and awareness of transactional boundaries,
      whereas the KStreams-based solution in its suggested form was prone to exposing intermediary aggregates.
      For instance, if you’re storing a customer and three addresses, it might happen that the streaming query first creates an aggregation of the customer and the two addresses inserted first, and shortly thereafter the complete aggregate with all three addresses.
      This not the case for the approach discussed here, as you’ll only ever stream complete aggregates to Kafka.
      Also this approach feels a bit more &quot;light-weight&quot;, i.e. a simple marker annotation (together with some Jackson annotations for fine-tuning the emitted JSON structures) is enough in order to materialize aggregates from your domain model,
      whereas some more effort was needed to set up the required streams, temporary tables etc. with the KStreams solution.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The downside of driving aggregations through the application layer is that it’s not fully agnostic to the way you access the primary data.
      If you bypass the application, e.g. by patching data directly in the database, naturally these updates would be missed, requiring a refresh of affected aggregates.
      Although this again could be done through change data capture and Debezium:
      change events to source tables could be captured and consumed by the application itself, allowing it to re-materialize aggregates after external data changes.
      You also might argue that running JSON serializations within source transactions and storing aggregates within the source database represents some overhead.
      This often may be acceptable, though.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Another question to ask is what’s the advantage of using change data capture on an intermediary aggregate table over simply posting REST requests to Elasticsearch.
      The answer is the highly increased robustness and fault tolerance.
      If the Elasticsearch cluster can’t be accessed for some reason, the machinery of Kafka and Kafka Connect will ensure that any change events will be propagated eventually, once the sink is up again.
      Also other consumers than Elasticsearch can subscribe to the aggregate topic, the log can be replayed from the beginning etc.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note that while we’ve been talking primarily about using Elasticsearch as a data sink, there are also other datastores and connectors that support complexly structured records.
      One example would be MongoDB and the &lt;a href=&quot;https://github.com/hpgrahsl/kafka-connect-mongodb&quot;&gt;sink connector&lt;/a&gt; maintained by Hans-Peter Grahsl,
      which one could use to sink customer aggregates into MongoDB, for instance enabling efficient retrieval of a customer and all their associated data with a single primary key look-up.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;outlook&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#outlook&quot;&gt;&lt;/a&gt;Outlook&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Hibernate ORM extension as well as the SMT discussed in this post can be found in our &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/jpa-aggregations&quot;&gt;examples repository&lt;/a&gt;.
      They should be considered to be at &quot;proof-of-concept&quot; level currently.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;That being said, we’re considering to make this a Debezium component proper,
      allowing you to employ this aggregation approach within your Hibernate-based applications just by pulling in this new component.
      For that we’d have to improve a few things first, though.
      Most importantly, an API is needed which will let you (re-)create aggregates on demand,
      e.g. for existing data or for data updated by bulk updates via the Criteria API / JPQL (which will be missed by listeners).
      Also aggregates should be re-created automatically, if any of the referenced entities change
      (with the current PoC, only a change to the customer instance itself will trigger its aggregate view to be rebuilt, but not a change to one of its addresses).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you like this idea, then let us know about it,
      so we can gauge the general interest in this.
      Also, this would be a great item to work on, if you’re interested in contributing to the Debezium project.
      Looking forward to hearing from you, e.g. in the comment section below or on our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks a lot to Hans-Peter Grahsl for his feedback on an earlier version of this post!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/09/19/debezium-0-8-3-final-released/</id>
    <title>Debezium 0.8.3.Final Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-09-19T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/09/19/debezium-0-8-3-final-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      As temperatures are cooling off, the Debezium team is getting into full swing again and we&#8217;re happy to announce the release of Debezium 0.8.3.Final!
      
      
      This is a bugfix release to the current stable release line of Debezium, 0.8.x, while the work on Debezium 0.9 goes on in parallel.
      There are 14 fixes in this release.
      As in earlier 0.8.x releases, we&#8217;ve further improved the new Antlr-based DDL parser used by the MySQL connector (see DBZ-901, DBZ-903 and DBZ-910).
      
      
      The Postgres connector saw a huge improvement to its start-up time for databases with lots of custom types (DBZ-899).
      The user reporting this issue had nearly 200K...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As temperatures are cooling off, the Debezium team is getting into full swing again and we’re happy to announce the release of Debezium &lt;strong&gt;0.8.3.Final&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This is a bugfix release to the current stable release line of Debezium, 0.8.x, while the work on Debezium 0.9 goes on in parallel.
      There are &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-8-3-final&quot;&gt;14 fixes&lt;/a&gt; in this release.
      As in earlier 0.8.x releases, we’ve further improved the new Antlr-based DDL parser used by the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL connector&lt;/a&gt; (see &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-901&quot;&gt;DBZ-901&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-903&quot;&gt;DBZ-903&lt;/a&gt; and &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-910&quot;&gt;DBZ-910&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;a href=&quot;http://debezium.io/docs/connectors/postgres/&quot;&gt;Postgres connector&lt;/a&gt; saw a huge improvement to its start-up time for databases with lots of custom types (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-899&quot;&gt;DBZ-899&lt;/a&gt;).
      The user reporting this issue had nearly 200K entries in pg_catalog.pg_type, and due to an N + 1 SELECT issue within the Postgres driver itself, this caused the connector to take 24 minutes to start.
      By using a custom query for obtaining the type metadata, we were able to cut down this time to 5 seconds!
      Right now we’re working with the maintainers of the Postgres driver to get this issue fixed upstream, too.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;more_flexible_propagation_of_deletes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#more_flexible_propagation_of_deletes&quot;&gt;&lt;/a&gt;More Flexible Propagation of DELETEs&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Besides those bug fixes we decided to also merge one new feature from the 0.9.x branch into the 0.8.3.Final release,
      which those of you may find useful who are using the &lt;a href=&quot;http://debezium.io/docs/configuration/event-flattening/&quot;&gt;SMT for extracting the &quot;after&quot; state&lt;/a&gt; from change events (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-857&quot;&gt;DBZ-857&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This SMT can be employed to stream changes to sink connectors which expect just a &quot;flat&quot; row representation of data instead of Debezium’s complex event structure.
      Not all sink connectors support the handling of deletions, though.
      E.g. some connectors will fail when encountering tombstone events.
      Therefore the SMT can now optionally rewrite delete events into updates of a special &quot;deleted&quot; marker field.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For that, set the &lt;code&gt;delete.handling.mode&lt;/code&gt; option of the SMT to &quot;rewrite&quot;:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;...
      &quot;transforms&quot; : &quot;unwrap&quot;,
      &quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.UnwrapFromEnvelope&quot;,
      &quot;transforms.unwrap.delete.handling.mode&quot; : &quot;rewrite&quot;,
      ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When a DELETE event is propagated, the &quot;__deleted&quot; field of outgoing records will be set to true.
      So when for instance consuming the events with the JDBC sink connector, you’d see this being reflected in a corresponding column in the sink tables:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;__deleted | last_name |  id  | first_name |         email
      -----------+-----------+------+------------+-----------------------
      false     | Thomas    | 1001 | Sally      | sally.thomas@acme.com
      false     | Bailey    | 1002 | George     | gbailey@foobar.com
      false     | Kretchmar | 1004 | Anne       | annek@noanswer.org
      true      | Walker    | 1003 | Edward     | ed@walker.com&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You then for instance can use a batch job running on your sink to remove all records flagged as deleted.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re continuing the work on Debezium 0.9, which will mostly be about improvements to the SQL Server and Oracle connectors.
      The current plan is to do the next 0.9 release (either Alpha2 or Beta1) in two weeks from now.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also it’s the beginning of the conference season, so we’ll spend some time with preparing demos and presenting Debezium at multiple locations.
      There will be sessions on change data capture with Debezium a these conferences:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://jug-saxony-day.org/programm/#!/P31&quot;&gt;JUG Saxony Day&lt;/a&gt;; Dresden, Germany; Sept. 28&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://kafka-summit.org/sessions/change-data-streaming-patterns-microservices-debezium/&quot;&gt;Kafka Summit&lt;/a&gt;; San Francisco, Cal.; Oct. 17&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://vxdms2018.confinabox.com/talk/INI-9172/Data_Streaming_for_Microservices_using_Debezium&quot;&gt;VoxxedDays Microservices&lt;/a&gt;; Paris, France; Oct. 29 - 31&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://cfp.devoxx.ma/2018/talk/AEY-4477/Change_Data_Streaming_Patterns_for_Microservices_With_Debezium&quot;&gt;Devoxx Morocco&lt;/a&gt;; Marrakesh, Morocco; Nov. 27 - 29&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you are at any of these conferences, come and say Hi;
      we’d love to exchange with you about your use cases, feature requests, feedback on our &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;roadmap&lt;/a&gt; and any other ideas around Debezium.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, a big &quot;Thank You&quot; goes to our fantastic community members &lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;, &lt;a href=&quot;https://github.com/maver1ck&quot;&gt;Maciej Bryński&lt;/a&gt; and &lt;a href=&quot;https://github.com/PengLyu&quot;&gt;Peng Lyu&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/08/30/streaming-mysql-data-changes-into-kinesis/</id>
    <title>Streaming MySQL Data Changes to Amazon Kinesis</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-08-30T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/08/30/streaming-mysql-data-changes-into-kinesis/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="discussion"></category>
    <category term="examples"></category>
    <summary>
      
      
      
      Most of the times Debezium is used to stream data changes into Apache Kafka.
      What though if you&#8217;re using another streaming platform such as Apache Pulsar or a cloud-based solution such as Amazon Kinesis, Azure Event Hubs and the like?
      Can you still benefit from Debezium&#8217;s powerful change data capture (CDC) capabilities  and ingest changes from databases such as MySQL, Postgres, SQL Server etc.?
      
      
      Turns out, with just a bit of glue code, you can!
      In the following we&#8217;ll discuss how to use Debezium to capture changes in a MySQL database and stream the change events into Kinesis,
      a fully-managed data streaming service available...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Most of the times Debezium is used to stream data changes into &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Apache Kafka&lt;/a&gt;.
      What though if you’re using another streaming platform such as &lt;a href=&quot;https://pulsar.incubator.apache.org/&quot;&gt;Apache Pulsar&lt;/a&gt; or a cloud-based solution such as &lt;a href=&quot;https://aws.amazon.com/kinesis/&quot;&gt;Amazon Kinesis&lt;/a&gt;, &lt;a href=&quot;https://azure.microsoft.com/services/event-hubs/&quot;&gt;Azure Event Hubs&lt;/a&gt; and the like?
      Can you still benefit from Debezium’s powerful change data capture (CDC) capabilities  and ingest changes from databases such as MySQL, Postgres, SQL Server etc.?&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Turns out, with just a bit of glue code, you can!
      In the following we’ll discuss how to use Debezium to capture changes in a MySQL database and stream the change events into Kinesis,
      a fully-managed data streaming service available on the Amazon cloud.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;introducing_the_debezium_embedded_engine&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#introducing_the_debezium_embedded_engine&quot;&gt;&lt;/a&gt;Introducing the Debezium Embedded Engine&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is implemented as a set of connectors for Kafka and thus usually is run via &lt;a href=&quot;https://kafka.apache.org/documentation/#connect&quot;&gt;Kafka Connect&lt;/a&gt;.
      But there’s one little gem in Debezium which isn’t as widely known yet, which is the &lt;a href=&quot;http://debezium.io/docs/embedded/&quot;&gt;embedded engine&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When using this engine, the Debezium connectors are not executed within Kafka Connect, but as a library embedded into your own Java application.
      For this purpose, the &lt;em&gt;debezium-embedded&lt;/em&gt; module provides a small runtime environment which performs the tasks that’d otherwise be handled by the Kafka Connect framework:
      requesting change records from the connector, committing offsets etc.
      Each change record produced by the connector is passed to a configured event handler method,
      which in our case will convert the record into its JSON representation and submit it to a Kinesis stream, using the Kinesis Java API.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The overall architecture looks like so:&lt;/p&gt;
      &lt;/div&gt;
      &lt;img src=&quot;http://debezium.io/images/debezium-embedded.png&quot; style=&quot;max-width:100%; margin-bottom:10px;&quot; class=&quot;responsive-image&quot; alt=&quot;Debezium Embedded Engine Streaming to Amazon Kinesis&quot; /&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now let’s walk through the relevant parts of the code required for that.
      A complete executable example can be found in the &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/kinesis&quot;&gt;debezium-examples&lt;/a&gt; repo on GitHub.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;set_up&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#set_up&quot;&gt;&lt;/a&gt;Set-Up&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In order to use Debezium’s embedded engine, add the &lt;em&gt;debezium-embedded&lt;/em&gt; dependency as well as the Debezium connector of your choice to your project’s &lt;em&gt;pom.xml&lt;/em&gt;.
      In the following we’re going to use the connector for MySQL.
      We also need to add a dependency to the &lt;a href=&quot;https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/kinesis/package-summary.html&quot;&gt;Kinesis Client API&lt;/a&gt;, so these are the dependencies needed:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;...
      &amp;lt;dependency&amp;gt;
          &amp;lt;groupId&amp;gt;io.debezium&amp;lt;/groupId&amp;gt;
          &amp;lt;artifactId&amp;gt;debezium-embedded&amp;lt;/artifactId&amp;gt;
          &amp;lt;version&amp;gt;0.8.3.Final&amp;lt;/version&amp;gt;
      &amp;lt;/dependency&amp;gt;
      &amp;lt;dependency&amp;gt;
          &amp;lt;groupId&amp;gt;io.debezium&amp;lt;/groupId&amp;gt;
          &amp;lt;artifactId&amp;gt;debezium-connector-mysql&amp;lt;/artifactId&amp;gt;
          &amp;lt;version&amp;gt;0.8.3.Final&amp;lt;/version&amp;gt;
      &amp;lt;/dependency&amp;gt;
      &amp;lt;dependency&amp;gt;
          &amp;lt;groupId&amp;gt;com.amazonaws&amp;lt;/groupId&amp;gt;
          &amp;lt;artifactId&amp;gt;amazon-kinesis-client&amp;lt;/artifactId&amp;gt;
          &amp;lt;version&amp;gt;1.9.0&amp;lt;/version&amp;gt;
      &amp;lt;/dependency&amp;gt;
      ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;configuring_the_embedded_engine&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#configuring_the_embedded_engine&quot;&gt;&lt;/a&gt;Configuring the Embedded Engine&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium embedded engine is configured through an instance of &lt;code&gt;io.debezium.config.Configuration&lt;/code&gt;.
      This class can obtain values from system properties or from a given config file,
      but for the sake of the example we’ll simply pass all required values via its fluent builder API:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;Configuration config = Configuration.create()
          .with(EmbeddedEngine.CONNECTOR_CLASS, &quot;io.debezium.connector.mysql.MySqlConnector&quot;)
          .with(EmbeddedEngine.ENGINE_NAME, &quot;kinesis&quot;)
          .with(MySqlConnectorConfig.SERVER_NAME, &quot;kinesis&quot;)
          .with(MySqlConnectorConfig.SERVER_ID, 8192)
          .with(MySqlConnectorConfig.HOSTNAME, &quot;localhost&quot;)
          .with(MySqlConnectorConfig.PORT, 3306)
          .with(MySqlConnectorConfig.USER, &quot;debezium&quot;)
          .with(MySqlConnectorConfig.PASSWORD, &quot;dbz&quot;)
          .with(MySqlConnectorConfig.DATABASE_WHITELIST, &quot;inventory&quot;)
          .with(MySqlConnectorConfig.TABLE_WHITELIST, &quot;inventory.customers&quot;)
          .with(EmbeddedEngine.OFFSET_STORAGE,
              &quot;org.apache.kafka.connect.storage.MemoryOffsetBackingStore&quot;)
          .with(MySqlConnectorConfig.DATABASE_HISTORY,
              MemoryDatabaseHistory.class.getName())
          .with(&quot;schemas.enable&quot;, false)
          .build();&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’ve ever set up the Debezium MySQL connector in Kafka Connect, most of the properties will look familiar to you.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;But let’s talk about the &lt;code&gt;OFFSET_STORAGE&lt;/code&gt; and &lt;code&gt;DATABASE_HISTORY&lt;/code&gt; options in a bit more detail.
      They deal with how connector offsets and the database history should be persisted.
      When running the connector via Kafka Connect, both would typically be stored in specific Kafka topics.
      But that’s not an option here, so an alternative is needed.
      For this example we’re simply going to keep the offsets and database history in memory.
      I.e. if the engine is restarted, this information will be lost and the connector will start from scratch, e.g. with a new initial snapshot.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While out of scope for this blog post, it wouldn’t be too difficult to create alternative implementations of the &lt;code&gt;OffsetBackingStore&lt;/code&gt; and &lt;code&gt;DatabaseHistory&lt;/code&gt; contracts, respectively.
      For instance if you’re fully committed into the AWS cloud services, you could think of storing offsets and database history in the DynamoDB NoSQL store.
      Note that, different from Kafka, a Kinesis stream wouldn’t be suitable for storing the database history.
      The reason being, that the maximum retention period for Kinesis data streams is seven days, whereas the database history must be kept for the entire lifetime of the connector.
      Another alternative could be to use the existing filesystem based implementations &lt;code&gt;FileOffsetBackingStore&lt;/code&gt; and &lt;code&gt;FileDatabaseHistory&lt;/code&gt;, respectively.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The next step is to build an &lt;code&gt;EmbeddedEngine&lt;/code&gt; instance from the configuration.
      Again this is done using a fluent API:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;EmbeddedEngine engine = EmbeddedEngine.create()
          .using(config)
          .using(this.getClass().getClassLoader())
          .using(Clock.SYSTEM)
          .notifying(this::sendRecord)
          .build();&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The most interesting part here is the &lt;code&gt;notifying&lt;/code&gt; call.
      The method passed here is the one which will be invoked by the engine for each emitted data change record.
      So let’s take a look at the implementation of this method.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;sending_change_records_to_kinesis&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#sending_change_records_to_kinesis&quot;&gt;&lt;/a&gt;Sending Change Records to Kinesis&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;code&gt;sendRecord()&lt;/code&gt; method is where the magic happens.
      We’ll convert the incoming &lt;code&gt;SourceRecord&lt;/code&gt; into an equivalent JSON representation and propagate it to a Kinesis stream.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For that, it’s important to understand some conceptual differences between Apache Kafka and Kinesis.
      Specifically, messages in Kafka have a &lt;em&gt;key&lt;/em&gt; and a &lt;em&gt;value&lt;/em&gt; (which both are arbitrary byte arrays).
      In case of Debezium, the key of data change events represents the primary key of the affected record and the value is a structure comprising of old and new row state as well as some additional metadata.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In Kinesis on the other hand a message contains a &lt;em&gt;data blob&lt;/em&gt; (again an arbitrary byte sequence) and a &lt;em&gt;partition key&lt;/em&gt;.
      Kinesis streams can be split up into multiple shards and the partition key is used to determine into which shard a given message should go.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now one could think of mapping the key from Debezium’s change data events to the Kinesis partition key,
      but partition keys are limited to a length of 256 bytes.
      Depending on the length of primary key column(s) in the captured tables, this might not be enough.
      So a safer option is to create a hash value from the change message key and use that as the partition key.
      This in turn means that the change message key structure should be added next to the actual value to the Kinesis message’s data blob.
      While the key column values themselves are part of the value structure, too, a consumer otherwise wouldn’t know which column(s) make up the primary key.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With that in mind, let’s take a look at the &lt;code&gt;sendRecord()&lt;/code&gt; implementation:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;private void sendRecord(SourceRecord record) {
          // We are interested only in data events not schema change events
          if (record.topic().equals(&quot;kinesis&quot;)) {
              return;
          }
      
          // create schema for container with key *and* value
          Schema schema = SchemaBuilder.struct()
              .field(&quot;key&quot;, record.keySchema())
              .field(&quot;value&quot;, record.valueSchema())
              .build();
      
          Struct message = new Struct(schema);
          message.put(&quot;key&quot;, record.key());
          message.put(&quot;value&quot;, record.value());
      
          // create partition key by hashing the record's key
          String partitionKey = String.valueOf(
              record.key() != null ? record.key().hashCode() : -1);
      
          // create data blob representing the container by using Kafka Connect's
          // JSON converter
          final byte[] payload = valueConverter.fromConnectData(
              &quot;dummy&quot;, schema, message);
      
          // Assemble the put-record request ...
          PutRecordRequest putRecord = new PutRecordRequest();
      
          putRecord.setStreamName(record.topic());
          putRecord.setPartitionKey(partitionKey);
          putRecord.setData(ByteBuffer.wrap(payload));
      
          // ... and execute it
          kinesisClient.putRecord(putRecord);
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The code is quite straight-forward; as discussed above it’s first creating a container structure containing key &lt;em&gt;and&lt;/em&gt; value of the incoming source record.
      This structure then is converted into a binary representation using the JSON converter provided by Kafka Connect (an instance of &lt;code&gt;JsonConverter&lt;/code&gt;).
      Then a &lt;code&gt;PutRecordRequest&lt;/code&gt; is assembled from that blob, the partition key and the change record’s topic name, which finally is sent to Kinesis.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Kinesis client object can be re-used and is set up once like so:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;// Uses the credentials from the local &quot;default&quot; AWS profile
      AWSCredentialsProvider credentialsProvider =
          new ProfileCredentialsProvider(&quot;default&quot;);
      
      this.kinesisClient = AmazonKinesisClientBuilder.standard()
          .withCredentials(credentialsProvider)
          .withRegion(&quot;eu-central-1&quot;) // use your AWS region here
          .build();&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With that, we’ve set up an instance of Debezium’s &lt;code&gt;EmbeddedEngine&lt;/code&gt; which runs the configured MySQL connector and passes each emitted change event to Amazon Kinesis.
      The last missing step is to actually run the engine.
      This is done on a separate thread using an &lt;code&gt;Executor&lt;/code&gt;, e.g. like so:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;ExecutorService executor = Executors.newSingleThreadExecutor();
      executor.execute(engine);&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note you also should make sure to properly shut down the engine eventually.
      How that can be done &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/kinesis/src/main/java/io/debezium/examples/kinesis/ChangeDataSender.java#L83-L88&quot;&gt;is shown&lt;/a&gt; in the accompanying example in the &lt;em&gt;debezium-examples&lt;/em&gt; repo.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;running_the_example&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#running_the_example&quot;&gt;&lt;/a&gt;Running the Example&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally let’s take a look at running the complete example and consuming the Debezium CDC events from the Kinesis stream.
      Start by cloning the examples repository and go to the &lt;em&gt;kinesis&lt;/em&gt; directory:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;git clone https://github.com/debezium/debezium-examples.git
      cd debezium-examples/kinesis&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Make sure you’ve met the &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/kinesis#prerequisites&quot;&gt;prerequisites&lt;/a&gt; described in the example’s &lt;em&gt;README.md&lt;/em&gt;;
      most notably you should have a local Docker installation and you’ll need to have set up an AWS account as well as have the AWS client tools installed.
      Note that Kinesis isn’t part of the free tier when registering with AWS, i.e. you’ll pay a (small) amount of money when executing the example.
      Don’t forget to delete the streams you’ve set up once done, we won’t pay your AWS bills :)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now run Debezium’s MySQL example database to have some data to play with:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker run -it --rm --name mysql -p 3306:3306 \
        -e MYSQL_ROOT_PASSWORD=debezium \
        -e MYSQL_USER=mysqluser \
        -e MYSQL_PASSWORD=mysqlpw \
        debezium/example-mysql:0.8&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Create a Kinesis stream for change events from the &lt;code&gt;customers&lt;/code&gt; table:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;aws kinesis create-stream --stream-name kinesis.inventory.customers \
        --shard-count 1&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Execute the Java application that runs the Debezium embedded engine
      (if needed, adjust the value of the &lt;code&gt;kinesis.region&lt;/code&gt; property in &lt;em&gt;pom.xml&lt;/em&gt; to your own region first):&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;mvn exec:java&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This will start up the engine and the MySQL connector, which takes an initial snapshot of the captured database.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In order to take a look at the CDC events in the Kinesis stream, the AWS CLI can be used
      (usually, you’d implement a Kinesis Streams application for consuming the events).
      To do so, set up a &lt;a href=&quot;https://docs.aws.amazon.com/streams/latest/dev/developing-consumers-with-sdk.html#kinesis-using-sdk-java-get-data-shard-iterators&quot;&gt;shard iterator&lt;/a&gt; first:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;ITERATOR=$(aws kinesis get-shard-iterator --stream-name kinesis.inventory.customers --shard-id 0 --shard-iterator-type TRIM_HORIZON | jq '.ShardIterator')&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note how the &lt;a href=&quot;https://stedolan.github.io/jq/&quot;&gt;jq&lt;/a&gt; utility is used to obtain the generated id of the iterator from the JSON structure returned by the Kinesis API.
      Next that iterator can be used to examine the stream:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;aws kinesis get-records --shard-iterator $ITERATOR&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You should receive an array of records like this:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
          &quot;Records&quot;: [
              {
                  &quot;SequenceNumber&quot;:
                      &quot;49587760482547027816046765529422807492446419903410339842&quot;,
                  &quot;ApproximateArrivalTimestamp&quot;: 1535551896.475,
                  &quot;Data&quot;: &quot;eyJiZWZvcm...4OTI3MzN9&quot;,
                  &quot;PartitionKey&quot;: &quot;eyJpZCI6MTAwMX0=&quot;
              },
              ...
          ]
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;code&gt;Data&lt;/code&gt; element is a Base64-encoded representation of the message’s data blob.
      Again &lt;em&gt;jq&lt;/em&gt; comes in handy: we can use it to just extract the &lt;code&gt;Data&lt;/code&gt; part of each record and decode the Base64 representation
      (make sure to use jq 1.6 or newer):&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;aws kinesis get-records --shard-iterator $ITERATOR | \
        jq -r '.Records[].Data | @base64d' | jq .&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now you should see the change events as JSON, each one with key and value:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
        &quot;key&quot;: {
          &quot;id&quot;: 1001
        },
        &quot;value&quot;: {
          &quot;before&quot;: null,
          &quot;after&quot;: {
            &quot;id&quot;: 1001,
            &quot;first_name&quot;: &quot;Sally&quot;,
            &quot;last_name&quot;: &quot;Thomas&quot;,
            &quot;email&quot;: &quot;sally.thomas@acme.com&quot;
          },
          &quot;source&quot;: {
            &quot;version&quot;: &quot;0.8.1.Final&quot;,
            &quot;name&quot;: &quot;kinesis&quot;,
            &quot;server_id&quot;: 0,
            &quot;ts_sec&quot;: 0,
            &quot;gtid&quot;: null,
            &quot;file&quot;: &quot;mysql-bin.000003&quot;,
            &quot;pos&quot;: 154,
            &quot;row&quot;: 0,
            &quot;snapshot&quot;: true,
            &quot;thread&quot;: null,
            &quot;db&quot;: &quot;inventory&quot;,
            &quot;table&quot;: &quot;customers&quot;,
            &quot;query&quot;: null
          },
          &quot;op&quot;: &quot;c&quot;,
          &quot;ts_ms&quot;: 1535555325628
        }
      }
      ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Next let’s try and update a record in MySQL:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;# Start MySQL CLI client
      docker run -it --rm --name mysqlterm --link mysql --rm mysql:5.7 \
        sh -c 'exec mysql -h&quot;$MYSQL_PORT_3306_TCP_ADDR&quot; \
        -P&quot;$MYSQL_PORT_3306_TCP_PORT&quot; -uroot -p&quot;$MYSQL_ENV_MYSQL_ROOT_PASSWORD&quot;'
      
      # In the MySQL client
      use inventory;
      update customers set first_name = 'Trudy' where id = 1001;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you now fetch the iterator again, you should see one more data change event representing that update:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;...
      
      {
        &quot;key&quot;: {
          &quot;id&quot;: 1001
        },
        &quot;value&quot;: {
          &quot;before&quot;: {
            &quot;id&quot;: 1001,
            &quot;first_name&quot;: &quot;Sally&quot;,
            &quot;last_name&quot;: &quot;Thomas&quot;,
            &quot;email&quot;: &quot;sally.thomas@acme.com&quot;
          },
          &quot;after&quot;: {
            &quot;id&quot;: 1001,
            &quot;first_name&quot;: &quot;Trudy&quot;,
            &quot;last_name&quot;: &quot;Thomas&quot;,
            &quot;email&quot;: &quot;sally.thomas@acme.com&quot;
          },
          &quot;source&quot;: {
            &quot;version&quot;: &quot;0.8.1.Final&quot;,
            &quot;name&quot;: &quot;kinesis&quot;,
            &quot;server_id&quot;: 223344,
            &quot;ts_sec&quot;: 1535627629,
            &quot;gtid&quot;: null,
            &quot;file&quot;: &quot;mysql-bin.000003&quot;,
            &quot;pos&quot;: 364,
            &quot;row&quot;: 0,
            &quot;snapshot&quot;: false,
            &quot;thread&quot;: 10,
            &quot;db&quot;: &quot;inventory&quot;,
            &quot;table&quot;: &quot;customers&quot;,
            &quot;query&quot;: null
          },
          &quot;op&quot;: &quot;u&quot;,
          &quot;ts_ms&quot;: 1535627622546
        }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once you’re done, stop the embedded engine application by hitting Ctrl + C,
      stop the MySQL server by running &lt;code&gt;docker stop mysql&lt;/code&gt; and delete the &lt;em&gt;kinesis.inventory.customers&lt;/em&gt; stream in Kinesis.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;summary_and_outlook&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary_and_outlook&quot;&gt;&lt;/a&gt;Summary and Outlook&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In this blog post we’ve demonstrated that Debezium cannot only be used to stream data changes into Apache Kafka, but also into other streaming platforms such as Amazon Kinesis.
      Leveraging its embedded engine and by implementing a bit of glue code, you can benefit from &lt;a href=&quot;http://debezium.io/docs/connectors/&quot;&gt;all the CDC connectors&lt;/a&gt; provided by Debezium and their capabilities and connect them to the streaming solution of your choice.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;And we’re thinking about even further simplifying this usage of Debezium.
      Instead of requiring you to implement your own application that invokes the embedded engine API,
      we’re considering to provide a small self-contained Debezium runtime which you can simply execute.
      It’d be configured with the source connector to run and make use of an outbound plug-in SPI with ready-to-use implementations for Kinesis, Apache Pulsar and others.
      Of course such runtime would also provide suitable implementations for safely persisting offsets and database history,
      and it’d offer means of monitoring, health checks etc.
      Meaning you could connect the Debezium source connectors with your preferred streaming platform in a robust and reliable way, without any manual coding required!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you like this idea, then please check out JIRA issue &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-651&quot;&gt;DBZ-651&lt;/a&gt; and let us know about your thoughts,
      e.g. by leaving a comment on the issue, in the comment section below or on our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/08/30/debezium-0-8-2-released/</id>
    <title>Debezium 0.8.2 Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-08-30T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/08/30/debezium-0-8-2-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      The Debezium team is back from summer holidays and we&#8217;re happy to announce the release of Debezium 0.8.2!
      
      
      This is a bugfix release to the current stable release line of Debezium, 0.8.x, while the work on Debezium 0.9 is continuing.
      
      
      Note: By accident the version of the release artifacts is 0.8.2 instead of 0.8.2.Final.
      This is not in line with our recently established convention of always letting release versions end with qualifiers such as Alpha1, Beta1, CR1 or Final.
      The next version in the 0.8 line will be 0.8.3.Final and we&#8217;ll improve our release pipeline to make sure that this situation doesn&#8217;t occur again.
      
      
      The...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium team is back from summer holidays and we’re happy to announce the release of Debezium &lt;strong&gt;0.8.2&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This is a bugfix release to the current stable release line of Debezium, 0.8.x, while the work on Debezium 0.9 is continuing.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; By accident the version of the release artifacts is &lt;em&gt;0.8.2&lt;/em&gt; instead of &lt;em&gt;0.8.2.Final&lt;/em&gt;.
      This is not in line with our recently established convention of always letting release versions end with qualifiers such as &lt;em&gt;Alpha1&lt;/em&gt;, &lt;em&gt;Beta1&lt;/em&gt;, &lt;em&gt;CR1&lt;/em&gt; or &lt;em&gt;Final&lt;/em&gt;.
      The next version in the 0.8 line will be &lt;em&gt;0.8.3.Final&lt;/em&gt; and we’ll improve our release pipeline to make sure that this situation doesn’t occur again.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The 0.8.2 release contains &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-8-2&quot;&gt;10 fixes&lt;/a&gt; overall, most of them dealing with issues related to DDL parsing as done by the Debezium &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL connector&lt;/a&gt;.
      For instance, implicit non-nullable primary key columns will be handled correctly now using the new Antlr-based DDL parser (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-860&quot;&gt;DBZ-860&lt;/a&gt;).
      Also the &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/&quot;&gt;MongoDB connector&lt;/a&gt; saw a bug fix (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-838&quot;&gt;DBZ-838&lt;/a&gt;): initial snapshots will be interrupted now if the connector is requested to stop
      (e.g. when shutting down Kafka Connect).
      More a useful improvement rather than a bug fix is the &lt;a href=&quot;http://debezium.io/docs/connectors/postgres/&quot;&gt;Postgres connector’s&lt;/a&gt; capability to add the table, schema and database names to the &lt;code&gt;source&lt;/code&gt; block of emitted CDC events (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-866&quot;&gt;DBZ-866&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks a lot to community members &lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;, &lt;a href=&quot;https://github.com/CliffWheadon&quot;&gt;Cliff Wheadon&lt;/a&gt; and &lt;a href=&quot;https://github.com/oripwk&quot;&gt;Ori Popowski&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re continuing the work on Debezium 0.9, which will mostly be about improvements to the SQL Server and Oracle connectors.
      Both will get support for handling structural changes to captured tables while the connectors are running.
      Also the exploration of alternatives to using the XStream API for the Oracle connector continues.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, a recurring theme of our work is to further consolidate the code bases of the different connectors,
      which will allow us to roll out new and improved features more quickly across all the Debezium connectors.
      The recently added Oracle and SQL Server connectors already share a lot of code,
      and in the next step we’ve planned to move the existing Postgres connector to the new basis established for these two connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to learn more about some middle and long term ideas, please check out our &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;roadmap&lt;/a&gt;.
      Also please get in touch with us if you got any ideas or suggestions for future development.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/07/26/debezium-0-9-0-alpha1-released/</id>
    <title>Debezium 0.9 Alpha1 and 0.8.1 Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-07-26T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/07/26/debezium-0-9-0-alpha1-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="postgres"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <category term="sqlserver"></category>
    <summary>
      
      
      
      Just two weeks after the Debezium 0.8 release, I&#8217;m very happy to announce the release of Debezium 0.9.0.Alpha1!
      
      
      The main feature of the new version is a first work-in-progress version of the long-awaited Debezium connector for MS SQL Server.
      Based on the CDC functionality available in the Enterprise and Standard editions,
      the new connector lets you stream data changes out of Microsoft&#8217;s popular RDBMS.
      
      
      Besides that we&#8217;ve continued the work on the Debezium Oracle connector.
      Most notably, it supports initial snapshots of captured tables now.
      We&#8217;ve also upgraded Apache Kafka in our Docker images to 1.1.1 (DBZ-829).
      
      
      Please take a look at the change log for the...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Just two weeks after the Debezium 0.8 release, I’m very happy to announce the release of Debezium &lt;strong&gt;0.9.0.Alpha1&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The main feature of the new version is a first work-in-progress version of the long-awaited Debezium connector for &lt;a href=&quot;https://www.microsoft.com/en-us/sql-server&quot;&gt;MS SQL Server&lt;/a&gt;.
      Based on the &lt;a href=&quot;https://docs.microsoft.com/en-us/sql/relational-databases/track-changes/about-change-data-capture-sql-server?view=sql-server-2017&quot;&gt;CDC functionality&lt;/a&gt; available in the Enterprise and Standard editions,
      the new connector lets you stream data changes out of Microsoft’s popular RDBMS.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Besides that we’ve continued the work on the Debezium &lt;a href=&quot;http://debezium.io/docs/connectors/oracle/&quot;&gt;Oracle connector&lt;/a&gt;.
      Most notably, it supports initial snapshots of captured tables now.
      We’ve also upgraded Apache Kafka in our Docker images to 1.1.1 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-829&quot;&gt;DBZ-829&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please take a look at the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-9-0-alpha1&quot;&gt;change log&lt;/a&gt; for the complete list of changes in 0.9.0.Alpha1 and general upgrade notes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; &lt;span class=&quot;line-through&quot;&gt;At the time of writing (2018-07-26), the release artifacts (connector archives) are available on &lt;a href=&quot;http://central.maven.org/maven2/io/debezium/&quot;&gt;Maven Central&lt;/a&gt;.
      We’ll upload the Docker images for 0.9.0.Alpha1 to &lt;a href=&quot;https://hub.docker.com/u/debezium/&quot;&gt;Docker Hub&lt;/a&gt; as soon as possible.&lt;/span&gt;
      The Docker images are already uplodaded and ready for use under tags &lt;code&gt;0.9.0.Alpha1&lt;/code&gt; and rolling &lt;code&gt;0.9&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;sql_server_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#sql_server_connector&quot;&gt;&lt;/a&gt;SQL Server Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Support for SQL Server had been on the wish list of Debezium users for a long time (the original issue was &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-40&quot;&gt;DBZ-40&lt;/a&gt;).
      Thanks to lots of basic infrastructure created while working on the Oracle connector,
      we were finally able to come up with a first preview of this new connector in comparatively short time of development.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Just as the Oracle connector, the one for SQL Server is under active development and should be considered an incubating feature at this point.
      So for instance the structure of emitted change messages may change in upcoming releases.
      In terms of features, it supports initial snapshotting and capturing changes via SQL Server’s CDC functionality.
      There’s support for the most common column types, table whitelisting/blacklisting and more.
      The most significant feature missing is support for structural changes of tables while the connector is running.
      This is the next feature we’ll work on and it’s planned to be delivered as part of the next 0.9 release (see &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-812&quot;&gt;DBZ-812&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’d be very happy to learn about any feedback you may have on this newest connector of the Debezium family.
      If you spot any bugs or have feature requests for it, please create a report in our &lt;a href=&quot;https://issues.jboss.org/browse/DBZ&quot;&gt;JIRA tracker&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;oracle_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#oracle_connector&quot;&gt;&lt;/a&gt;Oracle Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium connector for Oracle is able to take initial snapshots now.
      By means of the new connector option &lt;code&gt;snapshot.mode&lt;/code&gt; you can control whether &lt;em&gt;read&lt;/em&gt; events for all the records of all the captured tables should be emitted.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In addition the support for numeric data types has been honed (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-804&quot;&gt;DBZ-804&lt;/a&gt;);
      any integer columns (i.e. &lt;code&gt;NUMBER&lt;/code&gt; with a scale &amp;lt;\= 0) will be emitted using the corresponding &lt;code&gt;int8&lt;/code&gt;/&lt;code&gt;int16&lt;/code&gt;/&lt;code&gt;int32&lt;/code&gt;/&lt;code&gt;int64&lt;/code&gt; field type,
      if the columns precision allows for that.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also spent some time on expanding the Oracle &lt;a href=&quot;http://debezium.io/docs/connectors/oracle/&quot;&gt;connector documentation&lt;/a&gt;,
      which covers the structure of emitted change events and all the data type mappings in detail now.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;debezium_0_8_1_final&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#debezium_0_8_1_final&quot;&gt;&lt;/a&gt;Debezium 0.8.1.Final&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Together with Debezium 0.9.0.Alpha1 we also did another release of the current stable Debezium version 0.8.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While 0.9 at this point is more interesting to those eager to try out the latest developments in the Oracle and SQL Server connectors,
      0.8.1.Final is a recommended upgrade especially to the users of the Postgres connector.
      This release fixes an issue where it could happen that WAL segments on the server were retained longer than necessary,
      in case only records of non-whitelisted tables changed for a while.
      This has been addressed by means of supporting heartbeat messages (as already known from the MySQL connector) also for Postgres (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-800&quot;&gt;DBZ-800&lt;/a&gt;).
      This lets the connector regularly commit offsets to Kafka Connect which also serves as the hook to acknowledge processed LSNs with the Postgres server.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You can find the list of all changes done in Debezium 0.8.1.Final in the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-8-1-final&quot;&gt;change log&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As discussed above, we’ll work on supporting structural changes to captured tables while the SQL Server connector is running.
      The same applies to the Oracle connector.
      This will require some work on our DDL parsers, but thanks to the foundations provided by our recent migration of the MySQL DDL parser to Antlr, this should be manageable.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The other big focus of work with be to provide an alternative implementation for getting changes from Oracle which isn’t based on the XStream API.
      We’ve done some experiments with LogMiner and are also actively exploring further alternatives.
      While some details are still unclear, we are optimistic to have something to release in this area soon.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to learn more about some middle and long term ideas, please check out our &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;roadmap&lt;/a&gt;.
      Also please get in touch with us if you got any ideas or suggestions for future development.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/07/19/advantages-of-log-based-change-data-capture/</id>
    <title>Five Advantages of Log-Based Change Data Capture</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-07-19T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/07/19/advantages-of-log-based-change-data-capture/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="discussion"></category>
    <summary>
      
      
      
      Yesterday I had the opportunity to present Debezium and the idea of change data capture (CDC) to the Darmstadt Java User Group.
      It was a great evening with lots of interesting discussions and questions.
      One of the questions being the following: what is the advantage of using a log-based change data capturing tool such as Debezium over simply polling for updated records?
      
      
      So first of all, what&#8217;s the difference between the two approaches?
      With polling-based (or query-based) CDC you repeatedly run queries (e.g. via JDBC) for retrieving any newly inserted or updated rows from the tables to be captured.
      Log-based CDC in contrast works by...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Yesterday I had the opportunity to present Debezium and the idea of change data capture (CDC) to the &lt;a href=&quot;https://twitter.com/JUG_DA/status/1019634941020332032&quot;&gt;Darmstadt Java User Group&lt;/a&gt;.
      It was a great evening with lots of interesting discussions and questions.
      One of the questions being the following: what is the advantage of using a log-based change data capturing tool such as Debezium over simply polling for updated records?&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So first of all, what’s the difference between the two approaches?
      With polling-based (or query-based) CDC you repeatedly run queries (e.g. via JDBC) for retrieving any newly inserted or updated rows from the tables to be captured.
      Log-based CDC in contrast works by reacting to any changes to the database’s log files (e.g. MySQL’s binlog or MongoDB’s op log).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As this wasn’t the first time this question came up, I thought I could provide a more extensive answer also here on the blog.
      That way I’ll be able to refer to this post in the future, should the question come up again :)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So without further ado, here’s my list of five advantages of log-based CDC over polling-based approaches.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;dlist&quot;&gt;
      &lt;dl&gt;
      &lt;dt class=&quot;hdlist1&quot;&gt;All Data Changes Are Captured&lt;/dt&gt;
      &lt;dd&gt;
      &lt;p&gt;By reading the database’s log, you get the complete list of all data changes in their exact order of application.
      This is vital for many use cases where you are interested in the complete history of record changes.
      In contrast, with a polling-based approach you might miss intermediary data changes that happen between two runs of the poll loop.
      For instance it could happen that a record is inserted and deleted between two polls,
      in which case this record would never be captured by poll-based CDC.&lt;/p&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Related to this is the aspect of downtimes, e.g. when updating the CDC tool.
      With poll-based CDC, only the latest state of a given record would be captured once the CDC tool is back online,
      missing any earlier changes to the record that occurred during the downtime.
      A log-based CDC tool will be able to resume reading the database log from the point where it left off before it was shut down,
      causing the complete history of data changes to be captured.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/dd&gt;
      &lt;dt class=&quot;hdlist1&quot;&gt;Low Delays of Events While Avoiding Increased CPU Load&lt;/dt&gt;
      &lt;dd&gt;
      &lt;p&gt;With polling, you might be tempted to increase the frequency of polling attempts in order to reduce the chances of missing intermediary updates.
      While this works to some degree, polling too frequently may cause performance issues (as the queries used for polling cause load on the source database).
      On the other hand, expanding the polling interval will reduce the CPU load but may not only result in missed change events but also in a longer delay for propagating data changes.
      Log-based CDC allows you to react to data changes in near real-time without paying the price of spending CPU time on running polling queries repeatedly.&lt;/p&gt;
      &lt;/dd&gt;
      &lt;dt class=&quot;hdlist1&quot;&gt;No Impact on Data Model&lt;/dt&gt;
      &lt;dd&gt;
      &lt;p&gt;Polling requires some indicator to identify those records that have been changed since the last poll.
      So all the captured tables need to have some column like &lt;code&gt;LAST_UPDATE_TIMESTAMP&lt;/code&gt; which can be used to find changed rows.
      This can be fine in some cases, but in others such requirement might not be desirable.
      Specifically, you’ll need to make sure that the update timestamps are maintained correctly on all tables to be captured by the writing applications or e.g. through triggers.&lt;/p&gt;
      &lt;/dd&gt;
      &lt;dt class=&quot;hdlist1&quot;&gt;Can Capture Deletes&lt;/dt&gt;
      &lt;dd&gt;
      &lt;p&gt;Naturally, polling will not allow you to identify any records that have been deleted since the last poll.
      Often times that’s a problem for replication-like use cases where you’d like to have an identical data set on the source database and the replication targets,
      meaning you’d also like to delete records on the sink side if they have been removed in the source database.&lt;/p&gt;
      &lt;/dd&gt;
      &lt;dt class=&quot;hdlist1&quot;&gt;Can Capture Old Record State And Further Meta Data&lt;/dt&gt;
      &lt;dd&gt;
      &lt;p&gt;Depending on the source database’s capabilities, log-based CDC can provide the old record state for update and delete events.
      Whereas with polling, you’ll only get the current row state.
      Having the old row state handy in a single change event can be interesting for many use cases, e.g. if you’d like to display the complete data change with old and new column values to an application user for auditing purposes.&lt;/p&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In addition, log-based approaches often can provide streams of schema changes (e.g. in form of applied DDL statements) and expose additional metadata such as transaction ids or the user applying a certain change.
      These things may generally be doable with query-based approaches, too (depending on the capabilities of the database), I haven’t really seen it being done in practice, though.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/dd&gt;
      &lt;/dl&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot;&gt;&lt;/a&gt;Summary&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;And that’s it, five advantages of log-based change data capture.
      Note that this is not to say that polling-based CDC doesn’t have its applications.
      If for instance your use case can be satisfied by propagating changes once per hour and it’s not a problem to miss intermediary versions of records that were valid in between, it can be perfectly fine.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;But if you’re interested in capturing data changes in near real-time, making sure you don’t miss any change events (including deletions), then I’d recommend very much to explore the possibilities of log-based CDC as enabled by Debezium.
      The Debezium connectors do all the heavy-lifting for you, i.e. you don’t have to deal with all the low-level specifics of the individual databases and the means of getting changes from their logs.
      Instead, you can consume the generic and largely unified change data events produced by Debezium.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/07/12/debezium-0-8-0-final-released/</id>
    <title>Debezium 0.8 Final Is Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-07-12T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/07/12/debezium-0-8-0-final-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="mongodb"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m very happy to announce the release of Debezium 0.8.0.Final!
      
      
      The key features of Debezium 0.8 are the first work-in-progress version of our Oracle connector
      (based on the XStream API) and a brand-new parser for MySQL DDL statements.
      Besides that, there are plenty of smaller new features (e.g. propagation of default values to corresponding Connect schemas,
      optional propagation of source queries in CDC messages and a largely improved SMT for sinking changes from MongoDB into RDBMS)
      as well as lots of bug fixes (e.g. around temporal and numeric column types, large transactions with Postgres).
      
      
      Please see the previous announcements (Beta 1, CR 1)
      to learn about all...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m very happy to announce the release of Debezium &lt;strong&gt;0.8.0.Final&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The key features of Debezium 0.8 are the first work-in-progress version of our &lt;a href=&quot;http://debezium.io/docs/connectors/oracle/&quot;&gt;Oracle connector&lt;/a&gt;
      (based on the XStream API) and a brand-new parser for MySQL DDL statements.
      Besides that, there are plenty of smaller new features (e.g. propagation of default values to corresponding Connect schemas,
      optional propagation of source queries in CDC messages and a largely improved SMT for sinking changes from MongoDB into RDBMS)
      as well as lots of bug fixes (e.g. around temporal and numeric column types, large transactions with Postgres).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the previous announcements (&lt;a href=&quot;http://debezium.io/blog/2018/06/21/debezium-0-8-0-beta1-released/&quot;&gt;Beta 1&lt;/a&gt;, &lt;a href=&quot;http://debezium.io/blog/2018/07/04/debezium-0-8-0-cr1-released/&quot;&gt;CR 1&lt;/a&gt;)
      to learn about all the changes in more depth.
      The Final release largely resembles CR1;
      apart from further improvements to the Oracle connector (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-762&quot;&gt;DBZ-792&lt;/a&gt;) there’s one nice addition to the MySQL connector contributed by &lt;a href=&quot;https://github.com/pgoranss&quot;&gt;Peter Goransson&lt;/a&gt;:
      when doing a snapshot, it will now expose information about the processed rows via JMX (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-789&quot;&gt;DBZ-789&lt;/a&gt;), which is very handy when snapshotting larger tables.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please take a look at the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-8-0-final&quot;&gt;change log&lt;/a&gt; for the complete list of changes in 0.8.0.Final and general upgrade notes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re continuing our work on the Oracle connector.
      The work on initial snapshotting is well progressing and it should be part of the next release.
      Other improvements will be support for structural changes to captured tables after the initial snapshot has been made,
      more extensive source info metadata and more.
      Please track &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-716&quot;&gt;DBZ-716&lt;/a&gt; for this work; the improvements are planned to be released incrementally in the upcoming versions of Debezium.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also started to explore ingesting changes via LogMiner.
      This is more involved in terms of engineering efforts than using XStream, but it comes with the huge advantage of not requiring a separate license
      (LogMiner comes with the Oracle database itself).
      It’s not quite clear yet when we can release something on this front, and we’re also actively exploring further alternatives.
      But we are quite optimistic and hope to have something some time soon.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The other focus of work is a connector for SQL Server (see &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-40&quot;&gt;DBZ-40&lt;/a&gt;).
      Work on this has started as well, and there should be an Alpha1 release of Debezium 0.9 with a first drop of that connector within the next few weeks.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To find out about some more long term ideas, please check out our &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;roadmap&lt;/a&gt; and get in touch with us, if you got any ideas or suggestions for future development.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/07/04/debezium-0-8-0-cr1-released/</id>
    <title>Debezium 0.8.0.CR1 Is Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-07-04T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/07/04/debezium-0-8-0-cr1-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="mongodb"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      A fantastic Independence Day to all the Debezium users in the U.S.!
      But that&#8217;s not the only reason to celebrate: it&#8217;s also with great happiness that I&#8217;m announcing the release of Debezium 0.8.0.CR1!
      
      
      Following our new release scheme,
      the focus for this candidate release of Debezium 0.8 has been to fix bug reported for last week&#8217;s Beta release,
      accompanied by a small number of newly implemented features.
      
      
      Thanks a lot to everyone testing the new Antlr-based DDL parser for the MySQL connector;
      based on the issues you reported, we were able to fix a few bugs in it.
      As announced recently, for 0.8 the legacy parser will...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A fantastic Independence Day to all the Debezium users in the U.S.!
      But that’s not the only reason to celebrate: it’s also with great happiness that I’m announcing the release of Debezium &lt;strong&gt;0.8.0.CR1&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Following our new &lt;a href=&quot;http://debezium.io/blog/2018/06/21/debezium-0-8-0-beta1-released/&quot;&gt;release scheme&lt;/a&gt;,
      the focus for this candidate release of Debezium 0.8 has been to fix bug reported for last week’s Beta release,
      accompanied by a small number of newly implemented features.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks a lot to everyone testing the new Antlr-based DDL parser for the MySQL connector;
      based on the issues you reported, we were able to fix a few bugs in it.
      As announced recently, for 0.8 the legacy parser will remain the default implementation,
      but you are strongly encouraged to test out the new one
      (by setting the connector option &lt;code&gt;ddl.parser.mode&lt;/code&gt; to &lt;code&gt;antlr&lt;/code&gt;) and report any findings you may have.
      We’ve planned to switch to the new implementation by default in Debezium 0.9.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In terms of new features, the CR1 release brings support for &lt;code&gt;CITEXT&lt;/code&gt; columns in the Postgres connector (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-762&quot;&gt;DBZ-762&lt;/a&gt;).
      All the relational connectors support it now to convey the original name and length of captured columns using schema parameters in the emitted change messages (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-644&quot;&gt;DBZ-644&lt;/a&gt;).
      This can come in handy to properly size columns in a sink database for types such as &lt;code&gt;VARCHAR&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks a lot to the following community members who contributed to this release:
      &lt;a href=&quot;https://github.com/abergmeier&quot;&gt;Andreas Bergmeier&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/olavim&quot;&gt;Olavi Mustanoja&lt;/a&gt; and
      &lt;a href=&quot;https://github.com/orrganani&quot;&gt;Orr Ganani&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please take a look at the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-8-0-cr-1&quot;&gt;change log&lt;/a&gt; for the complete list of changes in 0.8.0.CR1 and general upgrade notes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Barring any unforeseen issues and critical bug reports, we’ll release Debezium 0.8.0.Final next week.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once that’s out, we’ll continue work on the Oracle connector (e.g. exploring alternatives to using XStream for ingesting changes from the database as well as initial snapshotting),
      which remains a &quot;tech preview&quot; component as of 0.8.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll also work towards a connector for SQL Server (see &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-40&quot;&gt;DBZ-40&lt;/a&gt;),
      for which the first steps just have been made today by preparing a Docker-based setup with a CDC-enabled SQL Server instance,
      allowing to implement and test the connector in the following.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To find out about some more long term ideas, please check out our &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;roadmap&lt;/a&gt; and get in touch with us, if you got any ideas or suggestions for future development.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/06/21/debezium-0-8-0-beta1-released/</id>
    <title>Debezium 0.8.0.Beta1 Is Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-06-21T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/06/21/debezium-0-8-0-beta1-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="mongodb"></category>
    <category term="oracle"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s with great excitement that I&#8217;m announcing the release of Debezium 0.8.0.Beta1!
      
      
      This release brings many exciting new features as well as bug fixes,
      e.g. the first drop of our new Oracle connector,
      a brand new DDL parser for the MySQL connector,
      support for MySQL default values and the update to Apache Kafka 1.1.
      
      
      Due to the big number of changes (the release contains exactly 42 issues overall),
      we decided to alter our versioning schema a little bit:
      going forward we may do one or more Beta and CR ("candidate release") releases before doing a final one.
      This will allow us to get feedback from the community early...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s with great excitement that I’m announcing the release of Debezium &lt;strong&gt;0.8.0.Beta1&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release brings many exciting new features as well as bug fixes,
      e.g. the first drop of our new Oracle connector,
      a brand new DDL parser for the MySQL connector,
      support for MySQL default values and the update to Apache Kafka 1.1.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Due to the big number of changes (the release contains exactly &lt;a href=&quot;https://issues.jboss.org/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.8.0.Beta1&quot;&gt;42 issues&lt;/a&gt; overall),
      we decided to alter our versioning schema a little bit:
      going forward we may do one or more Beta and CR (&quot;candidate release&quot;) releases before doing a final one.
      This will allow us to get feedback from the community early on,
      while still completing and polishing specific features.
      Final (stable) releases will be named like 0.8.0.Final etc.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release would not have been possible without our outstanding community;
      a huge &quot;thank you&quot; goes out to the following open source enthusiasts who all contributed to the new version:
      &lt;a href=&quot;https://github.com/echo-xu&quot;&gt;Echo Xu&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/vuckooo&quot;&gt;Ivan Vucina&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/glistman&quot;&gt;Listman Gamboa&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/omarsmak&quot;&gt;Omar Al-Safi&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/pgoranss&quot;&gt;Peter Goransson&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/kucharo2&quot;&gt;Roman Kuchar&lt;/a&gt; (who did a tremendous job with the new DDL parser implementation!),
      &lt;a href=&quot;https://github.com/sagarrao&quot;&gt;Sagar Rao&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/sauliusvl&quot;&gt;Saulius Valatka&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/sairam881990&quot;&gt;Sairam Polavarapu&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/Crim&quot;&gt;Stephen Powis&lt;/a&gt; and
      &lt;a href=&quot;https://github.com/sweat123&quot;&gt;WenZe Hu&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thank you all very much for your help!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now let’s take a closer look at some of the features new in Debezium 0.8.0.Beta1;
      as always, you can find the complete list of changes of this release in the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-8-0-beta-1&quot;&gt;change log&lt;/a&gt;.
      Plese take a special look at the breaking changes and the upgrade notes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;xstream_based_oracle_connector_tech_preview&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#xstream_based_oracle_connector_tech_preview&quot;&gt;&lt;/a&gt;XStream-based Oracle Connector (Tech Preview)&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Support for a Debezium Oracle connector has been one of the most asked for features for a long time
      (its original issue number is &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-20&quot;&gt;DBZ-20&lt;/a&gt;!).
      So we are very happy that we eventually can release a first work-in-progress version of that connector.
      At this point this code is still very much evolving, so it should be considered as a first tech preview.
      This means it’s not feature complete (most notably, there’s no support for initial snapshots yet),
      the emitted message format may still change etc.
      So while we don’t recommend using it in production quite yet,
      you should definitely give it a try and report back about your experiences.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;One challenge for the Oracle connector is how to get the actual change events out of the database.
      Unlike with MySQL and Postgres, there’s unfortunately no free-to-use and easy-to-work-with API which would allow to do the same for Oracle.
      After some exploration we decided to base this first version of the connector on the &lt;a href=&quot;https://docs.oracle.com/database/121/XSTRM/xstrm_intro.htm#XSTRM72647&quot;&gt;Oracle XStream&lt;/a&gt; API.
      While this (kinda) checks the box for &quot;easy-to-work-with&quot;, it doesn’t do so for &quot;free-to-use&quot;:
      using this API requires you to have a license for Oracle’s separate GoldenGate product.
      We’re fully aware of this being not ideal, but we decided to still go this route as a first step,
      allowing us to get some experiences with Oracle and also get a connector into the hands of those with the required license handy.
      Going forward, we are going to explore alternative approaches.
      We already have some ideas and discussions around this, so please stay tuned (the issue to track is &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-137&quot;&gt;DBZ-137&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Oracle connector is going to evolve within the next 0.8.x releases.
      To learn more about it, please check its &lt;a href=&quot;http://debezium.io/docs/connectors/oracle/&quot;&gt;connector documentation page&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;antlr_based_mysql_ddl_parser&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#antlr_based_mysql_ddl_parser&quot;&gt;&lt;/a&gt;Antlr-based MySQL DDL Parser&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In order to build up an internal meta-model of the captured database’s structure,
      the Debezium MySQL connector needs to parse all issued DDL statements (&lt;code&gt;CREATE TABLE&lt;/code&gt; etc.).
      This used to be done with a hand-written DDL parser which worked reasonably well,
      but over time it also revealed some shortcomings; as the DDL language is quite extensive,
      we saw repeatedly bug reports caused by some specific DDL constructs not being parseable.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So we decided to go back to the drawing board and came up with a brand new parser design.
      Thanks to the great work of Roman Kuchar, we now have a completely new DDL parser
      which is based on the proven and very mature &lt;a href=&quot;http://antlr.org/&quot;&gt;Antlr&lt;/a&gt; parser generator
      (luckily, the Antlr project provides a complete MySQL grammar).
      So we should see much less issue reports related to DDL parsing going forward.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For the time being, the old parser still is in place and remains to be the default parser for Debezium 0.8.x.
      You are very encouraged though to test the new implementation by setting the connector option &lt;code&gt;ddl.parser.mode&lt;/code&gt; to &lt;code&gt;antlr&lt;/code&gt;
      and report back if you run into any issues doing so.
      We plan to improve and polish the Antlr parser during the 0.8.x release line
      (specifically we’re going to measure its performance and optimize as needed)
      and switch to it by default as of Debezium 0.9.
      Eventually, the old parser will be removed in a future release after that.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;further_mysql_connector_changes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#further_mysql_connector_changes&quot;&gt;&lt;/a&gt;Further MySQL Connector Changes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The MySQL Connector propagates column default values to corresponding Kafka Connect schemas now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-191&quot;&gt;DBZ-191&lt;/a&gt;).
      That’s beneficial when using Avro as serialization format and the schema registry with compatibility checking enabled.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;By setting the &lt;code&gt;include.query&lt;/code&gt; connector option to true, you can add the original query that caused a data change to the corresponding CDC events (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-706&quot;&gt;DBZ-706&lt;/a&gt;).
      While disabled by default, this feature can be a useful tool for analyzing and interpreting data changes captured with Debezium.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Some other changes in the MySQL connector include configurability of the heartbeat topic name (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-668&quot;&gt;DBZ-668&lt;/a&gt;),
      fixes around timezone handling for &lt;code&gt;TIMESTAMP&lt;/code&gt; (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-578&quot;&gt;DBZ-578&lt;/a&gt;) and &lt;code&gt;DATETIME&lt;/code&gt; columns (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-741&quot;&gt;DBZ-741&lt;/a&gt;)
      and correct handling of &lt;code&gt;NUMERIC&lt;/code&gt; column without an explicit scale value (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-727&quot;&gt;DBZ-727&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;postgres_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#postgres_connector&quot;&gt;&lt;/a&gt;Postgres Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium Connector for Postgres has seen quite a number of bugfixes, including the following ones:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;wal2json can handle transactions now that are bigger than 1Gb (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-638&quot;&gt;DBZ-638&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;the transaction ID is consistently handled as long now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-673&quot;&gt;DBZ-673&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;multiple fixes related to temporal column types (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-681&quot;&gt;DBZ-681&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-696&quot;&gt;DBZ-696&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;OIDs are handled correctly as unsigned int now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-697&quot;&gt;DBZ-697&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-701&quot;&gt;DBZ-701&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mongodb_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mongodb_connector&quot;&gt;&lt;/a&gt;MongoDB Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also for the MongoDB Connector a number of small feature implementations and bugfixes has been done:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Tested against MongoDB 3.6 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-529&quot;&gt;DBZ-529&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Nested documents can be flattened using a provided SMT now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-561&quot;&gt;DBZ-561&lt;/a&gt;), which is useful when sinking changes from MongoDB into a relational database&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The &lt;a href=&quot;http://debezium.io/docs/configuration/mongodb-event-flattening/&quot;&gt;unwrapping SMT&lt;/a&gt; can be used together with Avro now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-650&quot;&gt;DBZ-650&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The unwrapping SMT can handle arrays with mixed element types (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-649&quot;&gt;DBZ-649&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;When interrupted during snapshotting before completion, the connector will redo the snapshot after restarting (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-712&quot;&gt;DBZ-712&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As per the new Beta/CR/Final release scheme, we hope to get some feedback by the community (i.e. you :) on this Beta release.
      Depending on the number of issues reported, we’ll either release another Beta or go to CR1 with the next version.
      The 0.8.0.Final version will be released within a few weeks.
      Note that the Oracle connector will remain a &quot;tech preview&quot; component also in the final version.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;After that, we’ve planned to do a few 0.8.x releases with bug fixes mostly,
      while work on Debezium 0.9 will commence in parallel.
      For that we’ve planned to work on a connector for SQL Server (see &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-40&quot;&gt;DBZ-40&lt;/a&gt;).
      We’d also like to explore means of creating consistent materializations of joins from multiple tables' CDC streams,
      based on the ids of originating transactions.
      Also there’s the idea and a first prototype of exposing Debezium change events as a reactive event stream (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-566&quot;&gt;DBZ-566&lt;/a&gt;),
      which might be shipped eventually.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please take a look at the &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;roadmap&lt;/a&gt; for some more long term ideas and get in touch with us,
      if you got thoughts around that.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/05/24/querying-debezium-change-data-eEvents-with-ksql/</id>
    <title>Querying Debezium Change Data Events With KSQL</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-05-24T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/05/24/querying-debezium-change-data-eEvents-with-ksql/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="mysql"></category>
    <category term="ksql"></category>
    <category term="example"></category>
    <summary>
      
      
      
      Last updated at Nov 21st 2018 (adjusted to new KSQL Docker images).
      
      
      Last year we have seen the inception of a new open-source project in the Apache Kafka universe, KSQL,
      which is a streaming SQL engine build on top of Kafka Streams.
      In this post, we are going to try out KSQL querying with data change events generated by Debezium from a MySQL database.
      
      
      As a source of data we will use the database and setup from our tutorial.
      The result of this exercise should be similar to the recent post about aggregation of events into domain driven aggregates.
      
      
      
      
      Entity diagram
      
      
      First let&#8217;s look at the entities...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;em&gt;Last updated at Nov 21st 2018 (adjusted to new KSQL Docker images)&lt;/em&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Last year we have seen the inception of a new open-source project in the &lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Apache Kafka&lt;/a&gt; universe, &lt;a href=&quot;https://github.com/confluentinc/ksql&quot;&gt;KSQL&lt;/a&gt;,
      which is a streaming SQL engine build on top of &lt;a href=&quot;https://kafka.apache.org/documentation/streams/&quot;&gt;Kafka Streams&lt;/a&gt;.
      In this post, we are going to try out KSQL querying with data change events generated by Debezium from a MySQL database.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As a source of data we will use the database and setup from our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;.
      The result of this exercise should be similar to the recent &lt;a href=&quot;http://debezium.io/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/&quot;&gt;post&lt;/a&gt; about aggregation of events into &lt;a href=&quot;https://martinfowler.com/bliki/DDD_Aggregate.html&quot;&gt;domain driven aggregates&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;entity_diagram&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#entity_diagram&quot;&gt;&lt;/a&gt;Entity diagram&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;First let’s look at the entities in the database and the relations between them.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;http://debezium.io/images/tutorial-erd.svg&quot; alt=&quot;Entity diagram&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;title&quot;&gt;Figure 1: Entity diagram of the example entities&lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt; &lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The picture above shows the full ER diagram for the inventory database in the example MySQL instance.
      We are going to focus on two entities:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;customers&lt;/code&gt; - the list of customers in the system&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;code&gt;orders&lt;/code&gt; - the list of orders in the system&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;There is a &lt;code&gt;1:n&lt;/code&gt; relation between &lt;code&gt;customers&lt;/code&gt; and &lt;code&gt;orders&lt;/code&gt;, modelled by the &lt;code&gt;purchaser&lt;/code&gt; column in the &lt;code&gt;orders&lt;/code&gt; table, which is a foreign key to the &lt;code&gt;customers&lt;/code&gt; table.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;configuration&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#configuration&quot;&gt;&lt;/a&gt;Configuration&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We are going to use a &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/ksql/docker-compose.yaml&quot;&gt;Docker Compose file&lt;/a&gt; for the deployment of the environment.
      The deployment consists of the following Docker images:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://hub.docker.com/r/debezium/zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://hub.docker.com/r/debezium/kafka/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Kafka Connect including the Debezium connectors &lt;a href=&quot;https://hub.docker.com/r/debezium/connect/&quot;&gt;image&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;A pre-populated MySQL database as used in our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The &lt;a href=&quot;https://hub.docker.com/r/confluentinc/cp-ksql-server/&quot;&gt;KSQL server&lt;/a&gt; and &lt;a href=&quot;https://hub.docker.com/r/confluentinc/cp-ksql-cli/&quot;&gt;CLI client&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;example&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#example&quot;&gt;&lt;/a&gt;Example&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;First we need to start the Debezium and Kafka infrastructure.
      To do so, clone the &lt;a href=&quot;https://github.com/debezium/debezium-examples/&quot;&gt;debezium-examples&lt;/a&gt; GitHub repository and start the required components using the provided Compose file:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;export DEBEZIUM_VERSION=0.8
      git clone https://github.com/debezium/debezium-examples.git
      cd debezium-examples/ksql/
      docker-compose up&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Next we must register an instance of the Debezium MySQL connector to listen to changes in the database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ -d @- &amp;lt;&amp;lt;-EOF
      {
          &quot;name&quot;: &quot;inventory-connector&quot;,
          &quot;config&quot;: {
              &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
              &quot;tasks.max&quot;: &quot;1&quot;,
              &quot;database.hostname&quot;: &quot;mysql&quot;,
              &quot;database.port&quot;: &quot;3306&quot;,
              &quot;database.user&quot;: &quot;debezium&quot;,
              &quot;database.password&quot;: &quot;dbz&quot;,
              &quot;database.server.id&quot;: &quot;184055&quot;,
              &quot;database.server.name&quot;: &quot;dbserver&quot;,
              &quot;database.whitelist&quot;: &quot;inventory&quot;,
              &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
              &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot;,
              &quot;transforms&quot;: &quot;unwrap&quot;,
              &quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.UnwrapFromEnvelope&quot;,
              &quot;key.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;,
              &quot;key.converter.schemas.enable&quot;: &quot;false&quot;,
              &quot;value.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;,
              &quot;value.converter.schemas.enable&quot;: &quot;false&quot;
          }
      }
      EOF&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now we should have all components up and running and initial data change events are already streamed into Kafka topics.
      There are multiple properties that are especially important for our use case:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;The &lt;a href=&quot;http://debezium.io/docs/configuration/event-flattening/&quot;&gt;UnwrapFromEnvelope SMT&lt;/a&gt; is used.
      This allows us to directly map fields from the &lt;code&gt;after&lt;/code&gt; part of change records into KSQL statements.
      Without it, we would need to use &lt;code&gt;EXTRACTJSONFIELD&lt;/code&gt; for each field to be extracted from the &lt;code&gt;after&lt;/code&gt; part of messages.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Schemas are disabled for the JSON converter.
      The reason is the same as above.
      With schemas enabled, for JSON the record is encapsulated in a JSON structure that contains the fields &lt;code&gt;schema&lt;/code&gt; (with schema information) and &lt;code&gt;payload&lt;/code&gt; (with the actual data itself).
      We would again need to use &lt;code&gt;EXTRACTJSONFIELD&lt;/code&gt; to get to the relevant fields.
      There is no such issue with Avro converter so this option does not need to be set when Avro is used.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Next we are going to start the KSQL command shell.
      We will run a local engine in the CLI.
      Also please note &lt;code&gt;--net&lt;/code&gt; parameter. This guarantees that KSQL container runs in the same network as Debezium containers and allows proper DNS resolution.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec ksql-cli ksql http://ksql-server:8088&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;First we will list all Kafka topics that exist in the broker:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; LIST TOPICS;
      
       Kafka Topic                         | Registered | Partitions | Partition Replicas
      ------------------------------------------------------------------------------------
       connect-status                      | false      | 5          | 1
       dbserver                            | false      | 1          | 1
       dbserver.inventory.addresses        | false      | 1          | 1
       dbserver.inventory.customers        | false      | 1          | 1
       dbserver.inventory.orders           | false      | 1          | 1
       dbserver.inventory.products         | false      | 1          | 1
       dbserver.inventory.products_on_hand | false      | 1          | 1
       ksql__commands                      | true       | 1          | 1
       my_connect_configs                  | false      | 1          | 1
       my_connect_offsets                  | false      | 25         | 1
       schema-changes.inventory            | false      | 1          | 1&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The topics we are interested in are &lt;code&gt;dbserver.inventory.orders&lt;/code&gt; and &lt;code&gt;dbserver.inventory.customers&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;KSQL processing by default starts with &lt;code&gt;latest&lt;/code&gt; offsets.
      We want to process the events already in the topics so we switch processing from &lt;code&gt;earliest&lt;/code&gt; offsets.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; SET 'auto.offset.reset' = 'earliest';
      Successfully changed local property 'auto.offset.reset' from 'null' to 'earliest'&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;First we need to create streams from the topics containing the Debezium data change events.
      A &lt;em&gt;stream&lt;/em&gt; in KSQL and Kafka Streams terminology is an unbounded incoming data set with no state.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; CREATE STREAM orders_from_debezium (order_number integer, order_date string, purchaser integer, quantity integer, product_id integer) WITH (KAFKA_TOPIC='dbserver.inventory.orders',VALUE_FORMAT='json');
      
       Message
      ----------------
       Stream created
      ksql&amp;gt;
      ksql&amp;gt; CREATE STREAM customers_from_debezium (id integer, first_name string, last_name string, email string) WITH (KAFKA_TOPIC='dbserver.inventory.customers',VALUE_FORMAT='json');
      
       Message
      ----------------
       Stream created&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;partitioning&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#partitioning&quot;&gt;&lt;/a&gt;Partitioning&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Our deployment uses only one partition per topic.
      In a production system there will likely be multiple partitions per topic and we need to ensure that all events belonging to our aggregated object end up in the same partition.
      The natural partioning in our case is per customer id.
      We are going to repartition the &lt;code&gt;orders_from_debezium&lt;/code&gt; stream according to the &lt;code&gt;purchaser&lt;/code&gt; field that contains the customer id.
      The repartitioned data are written into a new topic &lt;code&gt;ORDERS_REPART&lt;/code&gt;:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; CREATE STREAM orders WITH (KAFKA_TOPIC='ORDERS_REPART',VALUE_FORMAT='json',PARTITIONS=1) as SELECT * FROM orders_from_debezium PARTITION BY PURCHASER;
      
       Message
      ----------------------------
       Stream created and running
      ksql&amp;gt; LIST TOPICS;
      
       Kafka Topic                         | Registered | Partitions | Partition Replicas
      ------------------------------------------------------------------------------------
      ...
       ORDERS_REPART                       | true       | 1          | 1
      ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We are going to execute the same operation for customers too.
      It is necessary for two reasons:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;The current key is a struct that contains a field named &lt;code&gt;id&lt;/code&gt; with the customer id.
      This is different from the repartitioned order topic which contains only the &lt;code&gt;id&lt;/code&gt; value as the key, so the partitions would not match.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;When we will create a JOIN later, there is a limitation that requires the key to have the same value as a key field in the table.
      The table field contains a plain value but the key contains a struct so they would not match.
      See &lt;a href=&quot;https://github.com/confluentinc/ksql/issues/749&quot;&gt;this KSQL issue&lt;/a&gt; for more details.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; CREATE STREAM customers_stream WITH (KAFKA_TOPIC='CUSTOMERS_REPART',VALUE_FORMAT='json',PARTITIONS=1) as SELECT * FROM customers_from_debezium PARTITION BY ID;
      
       Message
      ----------------------------
       Stream created and running
      ksql&amp;gt; LIST TOPICS;
      
       Kafka Topic                         | Registered | Partitions | Partition Replicas
      ------------------------------------------------------------------------------------
      ...
       CUSTOMERS_REPART                    | true       | 1          | 1
      ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To verify that records have a new key and are thus repartioned we can issue few statements to compare the results:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; SELECT * FROM orders_from_debezium LIMIT 1;
      1524034842810 | {&quot;order_number&quot;:10001} | 10001 | 16816 | 1001 | 1 | 102
      LIMIT reached for the partition.
      Query terminated
      ksql&amp;gt; SELECT * FROM orders LIMIT 1;
      1524034842810 | 1001 | 10001 | 16816 | 1001 | 1 | 102
      LIMIT reached for the partition.
      Query terminated&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The second column contains &lt;code&gt;ROWKEY&lt;/code&gt; which is the key of the message.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect3&quot;&gt;
      &lt;h4 id=&quot;customer_order_join&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#customer_order_join&quot;&gt;&lt;/a&gt;Customer/order join&lt;/h4&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So far we were only declaring streams as an unbounded stateless data set.
      In our use case the &lt;code&gt;order&lt;/code&gt; is really an event that comes and goes.
      But &lt;code&gt;customer&lt;/code&gt; is an entity that can be updated and generally is a part of a state fo the system.
      Such quality is represented in KSQL or Kafka Streams as table.
      We are going to create a table of customers from the topic containing repartitioned customers.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; CREATE TABLE customers (id integer, first_name string, last_name string, email string) WITH (KAFKA_TOPIC='CUSTOMERS_REPART',VALUE_FORMAT='json',KEY='id');
      
       Message
      ---------------
       Table created&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now we have everything in place to make a join between customer and its orders and create a query that will monitor incoming orders and list them with associated customer fields.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;ksql&amp;gt; SELECT order_number,quantity,customers.first_name,customers.last_name FROM orders left join customers on orders.purchaser=customers.id;
      10001 | 1 | Sally | Thomas
      10002 | 2 | George | Bailey
      10003 | 2 | George | Bailey
      10004 | 1 | Edward | Walker&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s apply a few changes to the database, which will result in corresponding CDC events being emitted by Debezium:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER -p$MYSQL_PASSWORD inventory'
      
      mysql&amp;gt; INSERT INTO orders VALUES(default,NOW(), 1003,5,101);
      Query OK, 1 row affected, 1 warning (0.02 sec)
      
      mysql&amp;gt; UPDATE customers SET first_name='Annie' WHERE id=1004;
      Query OK, 1 row affected (0.02 sec)
      Rows matched: 1  Changed: 1  Warnings: 0
      
      mysql&amp;gt; UPDATE orders SET quantity=20 WHERE order_number=10004;
      Query OK, 1 row affected (0.02 sec)
      Rows matched: 1  Changed: 1  Warnings: 0&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You may notice that only changes in the &lt;code&gt;orders&lt;/code&gt; table have triggered changes in the joined stream.
      This is a product of the stream/table join.
      We would need a stream/stream join to trigger changes if any of input streams is modified.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So the final result of the select after the database is modified is&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;10001 | 1 | Sally | Thomas
      10002 | 2 | George | Bailey
      10003 | 2 | George | Bailey
      10004 | 1 | Edward | Walker
      10005 | 5 | Edward | Walker
      10004 | 20 | Edward | Walker&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot;&gt;&lt;/a&gt;Summary&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We have successfully started a KSQL instance. We have mapped KSQL streams to Debezium topics filled by Debezium and made a join between them.
      We have also discussed the problem of repartioning in streaming applications.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to try out this example with Avro encoding and schema registry then you can use our &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/tutorial/docker-compose-mysql-avro.yaml&quot;&gt;Avro example&lt;/a&gt;.
      Also for further details and more advanced usages just refer to the KSQL &lt;a href=&quot;https://github.com/confluentinc/ksql/blob/master/docs/syntax-reference.md&quot;&gt;syntax reference&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In case you need help, have feature requests or would like to share your experiences with this example, please let us know in the comments below.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/03/20/debezium-0-7-5-released/</id>
    <title>Debezium 0.7.5 Is Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-03-20T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/03/20/debezium-0-7-5-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="mongodb"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s my pleasure to announce the release of Debezium 0.7.5!
      
      
      This is a bugfix release to the 0.7 release line, which we decided to do while working towards Debezium 0.8.
      Most notably it fixes an unfortunate bug introduced in 0.7.3 (DBZ-663),
      where the internal database history topic of the Debezium MySQL connector could be partly deleted under some specific conditions.
      Please see the dedicated blog post on this issue to find out whether this affects you and what you should do to prevent this issue.
      
      
      Together with this, we released a couple of other fixes and improvements.
      Thanks to Maciej Brynski, the performance of the logical...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.7.5&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This is a bugfix release to the 0.7 release line, which we decided to do while working towards Debezium 0.8.
      Most notably it fixes an unfortunate bug introduced in 0.7.3 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-663&quot;&gt;DBZ-663&lt;/a&gt;),
      where the internal database history topic of the Debezium MySQL connector could be partly deleted under some specific conditions.
      Please see the &lt;a href=&quot;http://debezium.io/2018/03/16/note-on-database-history-topic-configuration/&quot;&gt;dedicated blog post&lt;/a&gt; on this issue to find out whether this affects you and what you should do to prevent this issue.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Together with this, we released a couple of other fixes and improvements.
      Thanks to &lt;a href=&quot;https://github.com/maver1ck&quot;&gt;Maciej Brynski&lt;/a&gt;, the performance of the &lt;a href=&quot;http://debezium.io/docs/configuration/topic-routing/&quot;&gt;logical table routing SMT&lt;/a&gt; has been improved significantly (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-655&quot;&gt;DBZ-655&lt;/a&gt;).
      Another fix contributed by Maciej is for &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-646&quot;&gt;DBZ-646&lt;/a&gt; which lets the MySQL connector handle &lt;code&gt;CREATE TABLE&lt;/code&gt; statements for the TokuDB storage engine now.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;And we got some more bugfixes by our fantastic community:
      Long-term community member &lt;a href=&quot;https://github.com/pgoranss&quot;&gt;Peter Goransson&lt;/a&gt; fixed an issue about the snapshot JMX metrics of the MySQL connector,
      which are now also accessible after the snapshot has been completed (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-640&quot;&gt;DBZ-640&lt;/a&gt;).
      &lt;a href=&quot;https://github.com/atongen&quot;&gt;Andrew Tongen&lt;/a&gt; spotted and fixed an issue for the Debezium embedded engine (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-665&quot;&gt;DBZ-665&lt;/a&gt;) which caused offsets to be committed more often than needed.
      And &lt;a href=&quot;https://github.com/matzew&quot;&gt;Matthias Wessendorf&lt;/a&gt; upgraded the Debezium dependencies and Docker images to Apache Kafka 1.0.1 (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-647&quot;&gt;DBZ-647&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thank you all for your help!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please refer to the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-4&quot;&gt;change log&lt;/a&gt; for the complete list of changes in Debezium 0.7.5.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/blog/2018/03/07/debezium-0-7-4-released/&quot;&gt;previous release announcement&lt;/a&gt; for the next planned features.
      Due to the unplanned 0.7.5 release, though, the schedule of the next one will likely be extended a little bit.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/03/16/note-on-database-history-topic-configuration/</id>
    <title>A Note On Database History Topic Configuration</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-03-16T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/03/16/note-on-database-history-topic-configuration/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="mysql"></category>
    <summary>
      
      
      
      A user of the Debezium connector for MySQL informed us about a potential issue with the configuration of the connector&#8217;s internal database history topic,
      which may cause the deletion of parts of that topic (DBZ-663).
      Please continue reading if you&#8217;re using the Debezium MySQL connector in versions 0.7.3 or 0.7.4.
      
      
      
      
      What is the issue about?
      
      
      In Debezium 0.7.3 we rolled out a feature for creating the database history automatically if it doesn&#8217;t exist yet (DBZ-278).
      While this feature sets the retention time for the topic to an "infinite" period, it doesn&#8217;t specify the "retention.bytes" option for the history topic.
      This may cause parts of the history...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A user of the Debezium connector for MySQL informed us about a potential issue with the configuration of the connector’s internal database history topic,
      which may cause the deletion of parts of that topic (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-663&quot;&gt;DBZ-663&lt;/a&gt;).
      Please continue reading if you’re using the Debezium MySQL connector in versions 0.7.3 or 0.7.4.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_is_the_issue_about&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_is_the_issue_about&quot;&gt;&lt;/a&gt;What is the issue about?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In Debezium 0.7.3 we rolled out a feature for creating the database history automatically if it doesn’t exist yet (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-278&quot;&gt;DBZ-278&lt;/a&gt;).
      While this feature sets the retention time for the topic to an &quot;infinite&quot; period, it doesn’t specify the &quot;retention.bytes&quot; option for the history topic.
      This may cause parts of the history topic to be deleted in case all of the following conditions are met:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;You are using versions 0.7.3 or 0.7.4 of the Debezium connector for MySQL&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The database history topic has been created by the connector (i.e. you haven’t created it yourself)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The broker level option &quot;log.retention.bytes&quot; is set to another value than -1
      (note that the default &lt;strong&gt;is&lt;/strong&gt; -1, in which case things work as intended)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The database history topic grows beyond the threshold configured via &quot;log.retention.bytes&quot;&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If the history topic is incomplete, the connector will fail to recover the database history after a restart of the connector and will not continue with reading the MySQL binlog.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;how_to_prevent_the_issue&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#how_to_prevent_the_issue&quot;&gt;&lt;/a&gt;How to prevent the issue?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You should either create the database history topic yourself with an infinite retention
      or alternatively override the &quot;retention.bytes&quot; configuration for the history topic created by the connector:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;&amp;lt;KAFKA_DIR&amp;gt;/bin/kafka-configs.sh \
        --zookeeper zookeeper:2181 \
        --entity-type topics \
        --entity-name &amp;lt;DB_HISTORY_TOPIC&amp;gt; \
        --alter \
        --add-config retention.bytes=-1&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In case parts of the history topic were removed already,
      you can use the snapshot mode &lt;code&gt;schema_only_recovery&lt;/code&gt; for re-creating the history topic in case no schema changes have happened since the last committed offset of the connector.
      Alternatively, a complete new snapshot should be taken, e.g. by setting up a new connector instance.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;next_steps&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#next_steps&quot;&gt;&lt;/a&gt;Next steps&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll release Debezium 0.7.5 with a fix for this issue early next week.
      Note that previously created database history topics should be re-configured as described above.
      Please don’t hesitate to get in touch in the comments below, the chat room or the mailing list in case you have any further questions on this issue.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/</id>
    <title>Creating DDD aggregates with Debezium and Kafka Streams</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-03-08T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/" rel="alternate" type="text/html" />
    <author>
      <name>Hans-Peter Grahsl, Gunnar Morling</name>
    </author>
    <category term="discussion"></category>
    <category term="examples"></category>
    <summary>
      
      
      
      Microservice-based architectures can be considered an industry trend and are thus
      often found in enterprise applications lately. One possible way to keep data
      synchronized across multiple services and their backing data stores is to make us of an approach
      called change data capture, or CDC for short.
      
      
      Essentially CDC allows to listen to any modifications which are occurring at one end of a data flow (i.e. the data source)
      and communicate them as change events to other interested parties or storing them into a data sink.
      Instead of doing this in a point-to-point fashion, it&#8217;s advisable to decouple this flow of events
      between data sources and data...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Microservice-based architectures can be considered an industry trend and are thus
      often found in enterprise applications lately. One possible way to keep data
      synchronized across multiple services and their backing data stores is to make us of an approach
      called &lt;a href=&quot;https://vladmihalcea.com/a-beginners-guide-to-cdc-change-data-capture/&quot;&gt;change data capture&lt;/a&gt;, or CDC for short.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Essentially CDC allows to listen to any modifications which are occurring at one end of a data flow (i.e. the data source)
      and communicate them as change events to other interested parties or storing them into a data sink.
      Instead of doing this in a point-to-point fashion, it’s advisable to decouple this flow of events
      between data sources and data sinks. Such a scenario can be implemented based on &lt;a href=&quot;http://debezium.io/&quot;&gt;Debezium&lt;/a&gt;
      and &lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Apache Kafka&lt;/a&gt; with relative ease and effectively no coding.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As an example, consider the following microservice-based architecture of an order management system:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock centered-image&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;http://debezium.io/images/msa_streaming.png&quot; alt=&quot;Microservice-based architecture of an order management system&quot; /&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This system comprises three services, &lt;em&gt;Order&lt;/em&gt;, &lt;em&gt;Item&lt;/em&gt; and &lt;em&gt;Stock&lt;/em&gt;.
      If the &lt;em&gt;Order&lt;/em&gt; service receives an order request, it will need information from the other two,
      such as item definitions or the stock count for specific items.
      Instead of making synchronous calls to these services to obtain this information,
      CDC can be used to set up change event streams for the data managed by the &lt;em&gt;Item&lt;/em&gt; and &lt;em&gt;Stock&lt;/em&gt; services.
      The &lt;em&gt;Order&lt;/em&gt; service can subscribe to these event streams and keep a local copy of the relevant item and stock data in its own database.
      This approach helps to decouple the services
      (e.g. no direct impact by service outages)
      and can also be beneficial for overall performance,
      as each service can hold optimized views just of those data items owned by other services which it is interested in.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;how_to_handle_aggregate_objects&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#how_to_handle_aggregate_objects&quot;&gt;&lt;/a&gt;How to Handle Aggregate Objects?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;There are use cases however, where things are a bit more tricky. It is sometimes
      useful to share information across services and data stores by means of so-called
      aggregates, which are a concept/pattern defined by domain-driven design (DDD).
      In general, a &lt;a href=&quot;https://martinfowler.com/bliki/DDD_Aggregate.html&quot;&gt;DDD aggregate&lt;/a&gt; is used
      to transfer state which can be comprised of multiple different domain objects that are
      together treated as a single unit of information.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Concrete examples are:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;strong&gt;customers and their addresses&lt;/strong&gt; which are represented as a customer record &lt;em&gt;aggregate&lt;/em&gt;
      storing a customer and a list of addresses&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;strong&gt;orders and corresponding line items&lt;/strong&gt; which are represented as an order record
      &lt;em&gt;aggregate&lt;/em&gt; storing an order and all its line items&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Chances are that the data of the involved domain objects backing these DDD aggregates are stored in
      separate relations of an RDBMS. When making use of the CDC capabilities currently found
      in Debezium, all changes to domain objects will be independently captured and by default eventually
      reflected in separate Kafka topics, one per RDBMS relation. While this behaviour
      is tremendously helpful for a lot of use cases it can be pretty limiting to others,
      like the DDD aggregate scenario described above.
      Therefore, this blog post explores how DDD aggregates can be built based on Debezium CDC events,
      using the &lt;a href=&quot;https://kafka.apache.org/documentation/streams/&quot;&gt;Kafka Streams API&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;capturing_change_events_from_a_data_source&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#capturing_change_events_from_a_data_source&quot;&gt;&lt;/a&gt;Capturing Change Events from a Data Source&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The complete source code for this blog post is provided in the Debezium &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/kstreams&quot;&gt;examples repository&lt;/a&gt; on GitHub.
      Begin by cloning this repository and changing into the &lt;em&gt;kstreams&lt;/em&gt; directory:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;git clone https://github.com/debezium/debezium-examples.git
      cd kstreams&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The project provides a Docker Compose file with services for all the components you may already know from the &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;Debezium tutorial&lt;/a&gt;:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://zookeeper.apache.org/&quot;&gt;Apache ZooKeeper&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;A &lt;a href=&quot;https://kafka.apache.org/documentation/#connect&quot;&gt;Kafka Connect&lt;/a&gt; instance with the Debezium CDC connectors&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.mysql.com/&quot;&gt;MySQL&lt;/a&gt; (populated with some test data)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In addition it declares the following services:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.mongodb.com/&quot;&gt;MongoDB&lt;/a&gt; which will be used as a data sink&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Another Kafka Connect instance which will host the MongoDB sink connector&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;A service for running the DDD aggregation process we’re going to build in the following&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll get to those three in a bit, for now let’s prepare the source side of our pipeline:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;export DEBEZIUM_VERSION=0.7
      docker-compose up mysql zookeeper kafka connect_source&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once all services have been started, register an instance of the Debezium MySQL connector by submitting the following JSON document:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
          &quot;name&quot;: &quot;mysql-source&quot;,
          &quot;config&quot;: {
              &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
              &quot;tasks.max&quot;: &quot;1&quot;,
              &quot;database.hostname&quot;: &quot;mysql&quot;,
              &quot;database.port&quot;: &quot;3306&quot;,
              &quot;database.user&quot;: &quot;debezium&quot;,
              &quot;database.password&quot;: &quot;dbz&quot;,
              &quot;database.server.id&quot;: &quot;184054&quot;,
              &quot;database.server.name&quot;: &quot;dbserver1&quot;,
              &quot;table.whitelist&quot;: &quot;inventory.customers,inventory.addresses&quot;,
              &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
              &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot;,
              &quot;transforms&quot;: &quot;unwrap&quot;,
              &quot;transforms.unwrap.type&quot;:&quot;io.debezium.transforms.UnwrapFromEnvelope&quot;,
              &quot;transforms.unwrap.drop.tombstones&quot;:&quot;false&quot;
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To do so, run the following curl command:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ -d @mysql-source.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This sets up the connector for the specified database, using the given credentials.
      For our purposes we’re only interested in changes to the &lt;code&gt;customers&lt;/code&gt; and &lt;code&gt;addresses&lt;/code&gt; tables,
      hence the &lt;code&gt;table.whitelist&lt;/code&gt; property is given to just select these two tables.
      Another noteworthy thing is the &quot;unwrap&quot; transform that is applied.
      By default, Debezium’s CDC events would contain the old and new state of changed rows and some additional metadata on the source of the change.
      By applying the &lt;a href=&quot;http://debezium.io/docs/configuration/event-flattening/&quot;&gt;UnwrapFromEnvelope&lt;/a&gt; SMT (single message transformation),
      only the new state will be propagated into the corresponding Kafka topics.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We can take a look at them once the connector has been deployed and finished its initial snapshot of the two captured tables:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker-compose exec kafka /kafka/bin/kafka-console-consumer.sh \
          --bootstrap-server kafka:9092 \
          --from-beginning \
          --property print.key=true \
          --topic dbserver1.inventory.customers # or dbserver1.inventory.addresses&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;E.g. you should see the following output&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;(formatted and omitting the schema information for the sake of readability) for the topic with customer changes:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;{
          &quot;schema&quot;: { ... },
          &quot;payload&quot;: {
              &quot;id&quot;: 1001
          }
      }
      {
          &quot;schema&quot;: { ... },
          &quot;payload&quot;: {
              &quot;id&quot;: 1001,
              &quot;first_name&quot;: &quot;Sally&quot;,
              &quot;last_name&quot;: &quot;Thomas&quot;,
              &quot;email&quot;: &quot;sally.thomas@acme.com&quot;
          }
      }
      ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;building_ddd_aggregates&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#building_ddd_aggregates&quot;&gt;&lt;/a&gt;Building DDD Aggregates&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The KStreams application is going to process data from the two Kafka topics. These topics
      receive CDC events based on the customers and addresses relations found in MySQL, each of which has its
      corresponding Jackson-annotated POJO (Customer and Address), enriched by a field holding the CDC event type (i.e. UPSERT/DELETE).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Since the Kafka topic records are in Debezium JSON format with unwrapped envelopes, a special &lt;strong&gt;SerDe&lt;/strong&gt;
      has been written in order to be able to read/write these records using their POJO or Debezium event representation respectively.
      While the serializer simply converts the POJOs into JSON using Jackson, the deserializer is a &quot;hybrid&quot;
      one, being able to deserialize from either Debezium CDC events or jsonified POJOs.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With that in place, the KStreams topology to create and maintain DDD aggregates on-the-fly can be built as follows:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;customers_topic_parent&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#customers_topic_parent&quot;&gt;&lt;/a&gt;Customers Topic (&quot;parent&quot;)&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;All the customer records are simply read from the customer topic into a &lt;strong&gt;KTable&lt;/strong&gt; which will automatically maintain
      the latest state per customer according to the record key (i.e. the customer’s PK)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;KTable&amp;lt;DefaultId, Customer&amp;gt; customerTable =
              builder.table(parentTopic, Consumed.with(defaultIdSerde,customerSerde));&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;addresses_topic_children&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#addresses_topic_children&quot;&gt;&lt;/a&gt;Addresses Topic (&quot;children&quot;)&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For the address records the processing is a bit more involved and needs several steps. First, all the address
      records are read into a &lt;strong&gt;KStream&lt;/strong&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;KStream&amp;lt;DefaultId, Address&amp;gt; addressStream = builder.stream(childrenTopic,
              Consumed.with(defaultIdSerde, addressSerde));&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Second, a 'pseudo' grouping of these address records is done based on their keys (the original primary key in the relation),
      During this step the relationships towards the corresponding customer records are maintained. This effectively allows to keep
      track which address record belongs to which customer record, even in the light of address record deletions.
      To achieve this an additional &lt;em&gt;LatestAddress&lt;/em&gt; POJO is introduced which allows to store the latest known PK &amp;lt;→ FK
      relation in addition to the &lt;em&gt;Address&lt;/em&gt; record itself.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;KTable&amp;lt;DefaultId,LatestAddress&amp;gt; tempTable = addressStream
              .groupByKey(Serialized.with(defaultIdSerde, addressSerde))
              .aggregate(
                      () -&amp;gt; new LatestAddress(),
                      (DefaultId addressId, Address address, LatestAddress latest) -&amp;gt; {
                          latest.update(
                              address, addressId, new DefaultId(address.getCustomer_id()));
                          return latest;
                      },
                      Materialized.&amp;lt;DefaultId,LatestAddress,KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;
                              as(childrenTopic+&quot;_table_temp&quot;)
                                  .withKeySerde(defaultIdSerde)
                                      .withValueSerde(latestAddressSerde)
              );&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Third, the intermediate &lt;strong&gt;KTable&lt;/strong&gt; is again converted to a &lt;strong&gt;KStream&lt;/strong&gt;. The &lt;em&gt;LatestAddress&lt;/em&gt; records are transformed
      to have the customer id (FK relationship) as their new key in order to group them per customer.
      During the grouping step, customer specific addresses are updated which can result in an address
      record being added or deleted. For this purpose, another POJO called &lt;em&gt;Addresses&lt;/em&gt; is introduced, which
      holds a map of address records that gets updated accordingly. The result is a &lt;strong&gt;KTable&lt;/strong&gt; holding the
      most recent &lt;em&gt;Addresses&lt;/em&gt; per customer id.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;KTable&amp;lt;DefaultId, Addresses&amp;gt; addressTable = tempTable.toStream()
              .map((addressId, latestAddress) -&amp;gt;
                  new KeyValue&amp;lt;&amp;gt;(latestAddress.getCustomerId(),latestAddress))
              .groupByKey(Serialized.with(defaultIdSerde,latestAddressSerde))
              .aggregate(
                      () -&amp;gt; new Addresses(),
                      (customerId, latestAddress, addresses) -&amp;gt; {
                          addresses.update(latestAddress);
                          return addresses;
                      },
                      Materialized.&amp;lt;DefaultId,Addresses,KeyValueStore&amp;lt;Bytes, byte[]&amp;gt;&amp;gt;
                              as(childrenTopic+&quot;_table_aggregate&quot;)
                                  .withKeySerde(defaultIdSerde)
                                      .withValueSerde(addressesSerde)
              );&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;combining_customers_with_addresses&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#combining_customers_with_addresses&quot;&gt;&lt;/a&gt;Combining Customers With Addresses&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, it’s easy to bring customers and addresses together by &lt;strong&gt;joining the customers KTable with
      the addresses KTable&lt;/strong&gt; and thereby building the DDD aggregates which are represented by the &lt;em&gt;CustomerAddressAggregate&lt;/em&gt; POJO.
      At the end, the KTable changes are written to a KStream, which in turn gets saved into a kafka topic.
      This allows to make use of the resulting DDD aggregates in manifold ways.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;KTable&amp;lt;DefaultId,CustomerAddressAggregate&amp;gt; dddAggregate =
                customerTable.join(addressTable, (customer, addresses) -&amp;gt;
                    customer.get_eventType() == EventType.DELETE ?
                            null :
                            new CustomerAddressAggregate(customer,addresses.getEntries())
                );
      
        dddAggregate.toStream().to(&quot;final_ddd_aggregates&quot;,
                                    Produced.with(defaultIdSerde,(Serde)aggregateSerde));&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;admonitionblock note&quot;&gt;
      &lt;table&gt;
      &lt;tr&gt;
      &lt;td class=&quot;icon&quot;&gt;
      &lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot;&gt;&lt;/i&gt;
      &lt;/td&gt;
      &lt;td class=&quot;content&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Records in the customers KTable might receive a CDC delete event. If so, this can be detected by
      checking the event type field of the customer POJO and e.g. return 'null' instead of a DDD aggregate.
      Such a convention can be helpful whenever consuming parties also need to act to deletions accordingly._&lt;/p&gt;
      &lt;/div&gt;
      &lt;/td&gt;
      &lt;/tr&gt;
      &lt;/table&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;running_the_aggregation_pipeline&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#running_the_aggregation_pipeline&quot;&gt;&lt;/a&gt;Running the Aggregation Pipeline&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Having implemented the aggregation pipeline, it’s time to give it a test run.
      To do so, build the &lt;em&gt;poc-ddd-aggregates&lt;/em&gt; Maven project which contains the complete implementation:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;mvn clean package -f poc-ddd-aggregates/pom.xml&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Then run the &lt;code&gt;aggregator&lt;/code&gt; service from the Compose file which takes the JAR built by this project
      and launches it using the &lt;a href=&quot;https://hub.docker.com/r/fabric8/java-jboss-openjdk8-jdk/&quot;&gt;java-jboss-openjdk8-jdk&lt;/a&gt; base image:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker-compose up -d aggregator&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once the aggregation pipeline is running, we can take a look at the aggregated events using the console consumer:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker-compose exec kafka /kafka/bin/kafka-console-consumer.sh \
          --bootstrap-server kafka:9092 \
          --from-beginning \
          --property print.key=true \
          --topic final_ddd_aggregates&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;transferring_ddd_aggregates_to_data_sinks&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#transferring_ddd_aggregates_to_data_sinks&quot;&gt;&lt;/a&gt;Transferring DDD Aggregates to Data Sinks&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We originally set out to build these DDD aggregates in order to transfer data and synchronize changes between
      a data source (MySQL tables in this case) and a convenient data sink. By definition,
      DDD aggregates are typically complex data structures and therefore it makes perfect sense to write them
      to data stores which offer flexible ways and means to query and/or index them. Talking about NoSQL databases, a
      document store seems the most natural choice with &lt;a href=&quot;https://www.mongodb.com/&quot;&gt;MongoDB&lt;/a&gt; being the leading database
      for such use cases.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to &lt;a href=&quot;https://kafka.apache.org/documentation/#connect&quot;&gt;Kafka Connect&lt;/a&gt; and numerous turn-key ready
      &lt;a href=&quot;https://www.confluent.io/product/connectors/&quot;&gt;connectors&lt;/a&gt; it is almost effortless to get this done.
      Using a &lt;a href=&quot;https://github.com/hpgrahsl/kafka-connect-mongodb&quot;&gt;MongoDB sink connector&lt;/a&gt; from the open-source community,
      it is easy to have the DDD aggregates written into MongoDB. All it needs is a proper configuration which can be posted
      to the &lt;a href=&quot;https://docs.confluent.io/current/connect/restapi.html&quot;&gt;REST API&lt;/a&gt; of Kafka Connect in order to run the connector.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So let’s start MongoDb and another Kafka Connect instance for hosting the sink connector:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker-compose up -d mongodb connect_sink&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In case the DDD aggregates should get written unmodified into MongoDB, a configuration may look as simple as follows:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
          &quot;name&quot;: &quot;mongodb-sink&quot;,
          &quot;config&quot;: {
              &quot;connector.class&quot;: &quot;at.grahsl.kafka.connect.mongodb.MongoDbSinkConnector&quot;,
              &quot;tasks.max&quot;: &quot;1&quot;,
              &quot;topics&quot;: &quot;final_ddd_aggregates&quot;,
              &quot;mongodb.connection.uri&quot;: &quot;mongodb://mongodb:27017/inventory?w=1&amp;amp;journal=true&quot;,
              &quot;mongodb.collection&quot;: &quot;customers_with_addresses&quot;,
              &quot;mongodb.document.id.strategy&quot;: &quot;at.grahsl.kafka.connect.mongodb.processor.id.strategy.FullKeyStrategy&quot;,
              &quot;mongodb.delete.on.null.values&quot;: true
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As with the source connector, deploy the connector using curl:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; -H  &quot;Content-Type:application/json&quot; http://localhost:8084/connectors/ -d @mongodb-sink.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This connector will consume messages from the &quot;final_ddd_aggregates&quot; Kafka topic and
      write them as &lt;strong&gt;MongoDB documents&lt;/strong&gt; into the &quot;customers_with_addresses&quot; collection.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You can take a look by firing up a Mongo shell and querying the collection’s contents:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker-compose exec mongodb bash -c 'mongo inventory'
      
      &amp;gt; db.customers_with_addresses.find().pretty()&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
          &quot;_id&quot;: {
              &quot;id&quot;: &quot;1001&quot;
          },
          &quot;addresses&quot;: [
              {
                  &quot;zip&quot;: &quot;76036&quot;,
                  &quot;_eventType&quot;: &quot;UPSERT&quot;,
                  &quot;city&quot;: &quot;Euless&quot;,
                  &quot;street&quot;: &quot;3183 Moore Avenue&quot;,
                  &quot;id&quot;: &quot;10&quot;,
                  &quot;state&quot;: &quot;Texas&quot;,
                  &quot;customer_id&quot;: &quot;1001&quot;,
                  &quot;type&quot;: &quot;SHIPPING&quot;
              },
              {
                  &quot;zip&quot;: &quot;17116&quot;,
                  &quot;_eventType&quot;: &quot;UPSERT&quot;,
                  &quot;city&quot;: &quot;Harrisburg&quot;,
                  &quot;street&quot;: &quot;2389 Hidden Valley Road&quot;,
                  &quot;id&quot;: &quot;11&quot;,
                  &quot;state&quot;: &quot;Pennsylvania&quot;,
                  &quot;customer_id&quot;: &quot;1001&quot;,
                  &quot;type&quot;: &quot;BILLING&quot;
              }
          ],
          &quot;customer&quot;: {
              &quot;_eventType&quot;: &quot;UPSERT&quot;,
              &quot;last_name&quot;: &quot;Thomas&quot;,
              &quot;id&quot;: &quot;1001&quot;,
              &quot;first_name&quot;: &quot;Sally&quot;,
              &quot;email&quot;: &quot;sally.thomas@acme.com&quot;
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Due to the combination of the data in a single document some parts aren’t needed or redundant. To get rid of any
      unwanted data (e.g. _eventType, customer_id of each address sub-document) it would also be possible
      to adapt the configuration in order to blacklist said fields.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, you update some customer or address data in the MySQL source database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER -p$MYSQL_PASSWORD inventory'
      
      mysql&amp;gt; update customers set first_name= &quot;Sarah&quot; where id = 1001;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Shortly thereafter, you should see that the corresponding aggregate document in MongoDB has been updated accordingly.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;drawbacks_and_limitations&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#drawbacks_and_limitations&quot;&gt;&lt;/a&gt;Drawbacks and Limitations&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While this first version for creating DDD aggregates from table-based CDC events basically works, it is very important to understand its current limitations:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;not generically applicable thus needs custom code for POJOs and intermediate types&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;cannot be scaled across multiple instances as is due to missing but necessary data repartitioning prior to processing&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;limited to building aggregates based on a single JOIN between 1:N relationships&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;resulting DDD aggregates are eventually consistent, meaning that it is possible for them to temporarily exhibit intermediate state before converging&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The first few can be addressed with a reasonable amount of work on the KStreams application. The last one,
      dealing with the eventually consistent nature of resulting DDD aggregates is much harder to correct
      and will require some efforts at Debezium’s own CDC mechanism.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;outlook&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#outlook&quot;&gt;&lt;/a&gt;Outlook&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In this post we described an approach for creating aggregated events from Debezium’s CDC events.
      In a follow-up blog post we may dive a bit more into the topic of how to be able to horizontally scale
      the DDD creation by running multiple KStreams aggregator instances. For that purpose, the data needs proper
      re-partitioning before running the topology. In addition, it could be interesting to look into
      a somewhat more generic version which only needs custom classes to the describe the two main POJOs involved.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We also thought about providing a ready-to-use component which would work in a generic way
      (based on Connect records, i.e. not tied to a specific serialization format such as JSON) and
      could be set up as a configurable stand-alone process running given aggregations.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also on the topic of dealing with eventual consistency we got some ideas,
      but those will need some more exploration and investigation for sure.
      Stay tuned!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’d love to hear about your feedback on the topic of event aggreation.
      If you got any ideas or thoughts on the subject,
      please get in touch by posting a comment below or sending a message to our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/03/07/debezium-0-7-4-released/</id>
    <title>Debezium 0.7.4 Is Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-03-07T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/03/07/debezium-0-7-4-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s my pleasure to announce the release of Debezium 0.7.4!
      
      
      Continuing the 0.7 release line, this new version brings several bug fixes and a handful of new features.
      We recommend this upgrade to all users.
      When upgrading from earlier versions,
      please check out the release notes of all versions between the one you&#8217;re currently on and 0.7.4 in order to learn about any steps potentially required for upgrading.
      
      
      
      
      New features
      
      
      In terms of new features, there&#8217;s a new mode for handling decimal columns in Postgres and MySQL (DBZ-611).
      By setting the decimal.handling.mode connector option to string, Debezium will emit decimal and numeric columns as Strings.
      That oftentimes is...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.7.4&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Continuing the 0.7 release line, this new version brings several bug fixes and a handful of new features.
      We recommend this upgrade to all users.
      When upgrading from earlier versions,
      please check out the &lt;a href=&quot;http://debezium.io/docs/releases/&quot;&gt;release notes&lt;/a&gt; of all versions between the one you’re currently on and 0.7.4 in order to learn about any steps potentially required for upgrading.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;new_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_features&quot;&gt;&lt;/a&gt;New features&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In terms of new features, there’s a new mode for handling decimal columns in Postgres and MySQL (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-611&quot;&gt;DBZ-611&lt;/a&gt;).
      By setting the &lt;code&gt;decimal.handling.mode&lt;/code&gt; connector option to &lt;code&gt;string&lt;/code&gt;, Debezium will emit decimal and numeric columns as Strings.
      That oftentimes is easier to handle for consumers than the byte-array based representation used by default, while keeping the full precision.
      As a bonus, &lt;code&gt;string&lt;/code&gt; also allows to convey the special numeric values &lt;code&gt;NaN&lt;/code&gt; and &lt;code&gt;Infinity&lt;/code&gt; as supported by Postgres.
      Note that this functionality required an update to Debezium’s logical decoding plug-in which runs within the Postgres database server.
      This plug-in must be upgraded to the new version &lt;em&gt;before&lt;/em&gt; upgrading the Debezium Postgres connector.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Speaking of byte arrays, the &lt;code&gt;BYTEA&lt;/code&gt; column type in Postgres is now also supported (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-605&quot;&gt;DBZ-605&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For the MySQL connector, there’s a new option to the snapshotting routine: &lt;code&gt;snapshot.locking.mode&lt;/code&gt; (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-602&quot;&gt;DBZ-602&lt;/a&gt;).
      By setting this to &lt;code&gt;NONE&lt;/code&gt;, this option allows to skip any table locks during snapshotting.
      This should be used if and only if you’re absolutely sure that the tables don’t undergo structural changes (columns added, removed etc.)
      while the snapshot is taken.
      But if that’s guaranteed, the new mode can be a useful tool for increasing overall system performance, as writes by concurrent processes won’t be blocked.
      That’s especially useful on environments such as Amazon RDS, where the connector otherwise would be required to keep a lock for the entirety of the snapshot.
      The new option supersedes the existing &lt;code&gt;snapshot.minimal.locks&lt;/code&gt; option.
      Please see the connector documentation for &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/#connector-properties&quot;&gt;the details&lt;/a&gt;.
      This feature was contributed by our community member &lt;a href=&quot;https://github.com/Crim&quot;&gt;Stephen Powis&lt;/a&gt;; many thanks to you!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;bug_fixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bug_fixes&quot;&gt;&lt;/a&gt;Bug Fixes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;0.7.4 brings multiple fixes related to how numeric columns are handled.
      E.g. columns without scale couldn’t correctly be processed by the MySQL connector during binlog reading (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-615&quot;&gt;DBZ-615&lt;/a&gt;).
      That’s fixed now.
      And when using the Postgres connector, arbitrary precision column values are correctly converted into change data message fields now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-351&quot;&gt;DBZ-351&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We also noticed a regression introduced in Debezium 0.6:
      the field schema for &lt;code&gt;NUMERIC&lt;/code&gt; columns was always marked as optional, also if that column was actually declared as &lt;code&gt;NOT NULL&lt;/code&gt;.
      The same affected geo-spatial array types on Postgres as supported as of Debezium 0.7.
      This has been fixed with &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-635&quot;&gt;DBZ-635&lt;/a&gt;.
      We don’t expect any impact on consumers by this change
      (just as before, they’ll always get a value for such field, only its schema won’t be incorrectly marked as optional any more).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-4&quot;&gt;full change log&lt;/a&gt; for more details and the complete list of issues fixed in Debezium 0.7.4.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Following our three weeks release cadence, the next Debezium release is planned for March 28th.
      We got some exciting changes in the works for that:
      if things go as planned, we’ll release the first version of our Oracle connector (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-20&quot;&gt;DBZ-20&lt;/a&gt;).
      This will be based on the Oracle XStream API in the first iteration and not support snapshots yet.
      But we felt it’d make sense to roll out this connector incrementally, so to get out the new feature early on and collect feedback on it.
      We’ve also planned to explore alternatives to using the XStream API in future releases.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Another great new feature will be &lt;a href=&quot;http://www.reactive-streams.org/&quot;&gt;Reactive Streams&lt;/a&gt; support (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-566&quot;&gt;DBZ-566&lt;/a&gt;).
      Based on top of the existing &lt;a href=&quot;http://debezium.io/docs/embedded/&quot;&gt;embedded mode&lt;/a&gt;,
      this will make it very easy to consume change data events using Reactive Streams implementations such as RxJava 2, the Java 9 Flow API and many more.
      It’ll also be very useful to consume change events in reactive frameworks such as Vert.x.
      We’re really looking forward to shipping this feature and already have a pending &lt;a href=&quot;https://github.com/debezium/debezium/pull/458&quot;&gt;pull request&lt;/a&gt; for it.
      If you like, take a look and let us know about your feedback!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please also check out our &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;roadmap&lt;/a&gt; for the coming months of Debezium’s development.
      This is our current plan for the things we’ll work on,
      but it’s not cast in stone, so please tell us about your feature requests by sending a message to our Google group.
      We’re looking forward to your feedback!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/02/15/debezium-0-7-3-released/</id>
    <title>Debezium 0.7.3 Is Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-02-15T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/02/15/debezium-0-7-3-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m very happy to announce the release of Debezium 0.7.3!
      
      
      This is primarily a bugfix release, but we&#8217;ve also added a handful of smaller new features.
      It&#8217;s a recommended upgrade for all users.
      When upgrading from earlier versions,
      please check out the release notes of all versions between the one your&#8217;re currently on and 0.7.3 in order to learn about any steps potentially required for upgrading.
      
      
      Let&#8217;s take a closer look at some of the new features.
      
      
      
      
      All Connectors
      
      
      Using the new connector option tombstones.on.delete you can now control whether upon record deletions a tombstone event should be emitted or not
      (DBZ-582).
      Doing so is usually the right thing...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m very happy to announce the release of Debezium &lt;strong&gt;0.7.3&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This is primarily a bugfix release, but we’ve also added a handful of smaller new features.
      It’s a recommended upgrade for all users.
      When upgrading from earlier versions,
      please check out the &lt;a href=&quot;http://debezium.io/docs/releases/&quot;&gt;release notes&lt;/a&gt; of all versions between the one your’re currently on and 0.7.3 in order to learn about any steps potentially required for upgrading.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s take a closer look at some of the new features.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;all_connectors&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#all_connectors&quot;&gt;&lt;/a&gt;All Connectors&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Using the new connector option &lt;code&gt;tombstones.on.delete&lt;/code&gt; you can now control whether upon record deletions a tombstone event should be emitted or not
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-582&quot;&gt;DBZ-582&lt;/a&gt;).
      Doing so is usually the right thing and thus remains the default behaviour.
      But disabling tombstones may be desirable in certain situations,
      and this gets a bit easier now using that option
      (before you’d have to use an SMT - single message transform -, which for instance isn’t supported when using Debezium’s embedded mode).
      This feature was contributed by our community member &lt;a href=&quot;https://github.com/rliwoch&quot;&gt;Raf Liwoch&lt;/a&gt;. Thanks!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also spent some time on a few operational aspects:
      The &lt;code&gt;sourceInfo&lt;/code&gt; element of Debezium’s change data messages contains a new field representing the version of the connector that created the message
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-593&quot;&gt;DBZ-593&lt;/a&gt;).
      This lets message consumers take specific action based on the version.
      For instance this can be helpful where a new Debezium release fixes a bug, which consumers could work around so far.
      Now, after the update to that new Debezium version, that workaround should not be applied anymore.
      The version field will allow consumers to decide whether to apply the workaround or not.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The names of all the threads managed by Debezium are now structured in the form of &quot;debezium-&amp;lt;connector&amp;gt;-…​&quot;
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-587&quot;&gt;DBZ-587&lt;/a&gt;).
      This helps with identifying Debezium’s threads when analyzing thread dumps for instance.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;postgres_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#postgres_connector&quot;&gt;&lt;/a&gt;Postgres Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Here we’ve focused on improving the support for array types:
      besides fixing a bug related to numeric arrays (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-577&quot;&gt;DBZ-577&lt;/a&gt;)
      we’ve also completed the support for the PostGIS types (which was introduced in 0.7.2),
      allowing you to capture array columns of types &lt;code&gt;GEOMETRY&lt;/code&gt; and &lt;code&gt;GEOGRAPHY&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Snapshots are now correctly interruptable (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-586&quot;&gt;DBZ-586&lt;/a&gt;)
      and the connector will correctly handle the case where after a restart it should continue from a WAL position which isn’t available any more:
      it’ll stop, requiring you to do a new snapshot (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-590&quot;&gt;DBZ-590&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mysql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mysql_connector&quot;&gt;&lt;/a&gt;MySQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The MySQL connector can create the DB history topic automatically, if needed
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-278&quot;&gt;DBZ-278&lt;/a&gt;).
      This means you don’t have to create that topic yourself and you also don’t need to rely on Kafka’s automatic topic creation any longer
      (any change data topics will automatically be created by Kafka Connect).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also the connector can optionally emit messages to a dedicated heartbeat topic in a configurable interval
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-220&quot;&gt;DBZ-220&lt;/a&gt;).
      This comes in handy in situations where you only want to capture tables with low traffic,
      while other tables in the database are changed more frequently.
      In that case, no messages would have been emitted to Kafka Connect for a long time,
      and thus no offset would have been committed either.
      This could have caused trouble when restarting the connector: it wanted to resume from the last comitted offset,
      which may not be available in the binlogs any longer.
      But as the captured tables didn’t change, it actually wouldn’t be necessary to resume from such old binlog position.
      This all is avoided by emitting messages to the heartbeat topic regularly, which causes the last offset the connector has seen to be committed.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll roll out this change to the other connectors, too, in future releases.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-3&quot;&gt;full change log&lt;/a&gt; for more details and the complete list of issues fixed in Debezium 0.7.3.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The next release is scheduled for March 7th.
      We’ll still have to decide whether that will be 0.7.4 or 0.8.0, depending on how far we are by then with our work on the Oracle connector
      (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-137&quot;&gt;DBZ-137&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please also our &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;roadmap&lt;/a&gt; describing our ideas for future development of Debezium.
      This is our current thinking of the things we’d like to tackle in the coming months,
      but it’s not cast in stone, so please let us know about your feature requests by sending a message to our Google group.
      We’re looking forward to your feedback!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/01/25/debezium-0-7-2-released/</id>
    <title>Debezium 0.7.2 Is Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-01-25T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/01/25/debezium-0-7-2-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s my pleasure to announce the release of Debezium 0.7.2!
      
      
      Amongst the new features there&#8217;s support for geo-spatial types,
      a new snapshotting mode for recovering a lost DB history topic for the MySQL connector,
      and a message transformation for converting MongoDB change events into a structure which can be consumed by many more sink connectors.
      And of course we fixed a whole lot of bugs, too.
      
      
      Debezium 0.7.2 is a drop-in replacement for previous 0.7.x versions.
      When upgrading from versions earlier than 0.7.0,
      please check out the release notes of all 0.7.x releases to learn about any steps potentially required for upgrading.
      
      
      A big thank you goes out...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.7.2&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Amongst the new features there’s support for geo-spatial types,
      a new snapshotting mode for recovering a lost DB history topic for the MySQL connector,
      and a message transformation for converting MongoDB change events into a structure which can be consumed by many more sink connectors.
      And of course we fixed a whole lot of bugs, too.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium 0.7.2 is a drop-in replacement for previous 0.7.x versions.
      When upgrading from versions earlier than 0.7.0,
      please check out the &lt;a href=&quot;http://debezium.io/docs/releases/&quot;&gt;release notes&lt;/a&gt; of all 0.7.x releases to learn about any steps potentially required for upgrading.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A big thank you goes out to our fantastic community members for their hard work on this release:
      &lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/notxcain&quot;&gt;Denis Mikhaylov&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/pgoranss&quot;&gt;Peter Goransson&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/rcoup&quot;&gt;Robert Coup&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/sairam881990&quot;&gt;Sairam Polavarapu&lt;/a&gt; and
      &lt;a href=&quot;https://github.com/tombentley&quot;&gt;Tom Bentley&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now let’s take a closer look at some of new features.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mysql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mysql_connector&quot;&gt;&lt;/a&gt;MySQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The biggest change of the MySQL connector is &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-507&quot;&gt;support for geo-spatial column types&lt;/a&gt; such as &lt;code&gt;GEOMETRY&lt;/code&gt;, &lt;code&gt;POLYGON&lt;/code&gt;, &lt;code&gt;MULTIPOINT&lt;/code&gt; etc.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;There are two new logical field types — &lt;code&gt;io.debezium.data.geometry.Geometry&lt;/code&gt; and &lt;code&gt;io.debezium.data.geometry.Geography&lt;/code&gt; — for representing geo-spatial columns in change data messages.
      These types represent geo-spatial data via WKB (&quot;well-known binary&quot;) and SRID (coordinate reference system identifier),
      allowing downstream consumers to interpret the change events using any existing library with support for parsing WKB.
      A blog post with more details on this will follow soon.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-443&quot;&gt;new snapshotting mode&lt;/a&gt; &lt;code&gt;schema_only_recovery&lt;/code&gt; comes in handy
      when for some reason you lost (parts of) the DB history topic used by the MySQL connector.
      It’s also useful if you’d like to compact that topic by re-creating it.
      Please refer to the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;connector documentation&lt;/a&gt; for the details of this mode,
      esp. when it’s safe (and when not) to make use of it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Another new feature related to managing the size of the DB history topic is &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-541&quot;&gt;the option&lt;/a&gt; to control
      whether to include all DDL events or only those pertaining to tables captured as per the whitelist/blacklist configuration.
      Again, check out the connector docs to learn more about the specifics of that setting.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, we fixed a few shortcomings of the MySQL DDL parser (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-524&quot;&gt;DBZ-524&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-530&quot;&gt;DBZ-530&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;postgresql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#postgresql_connector&quot;&gt;&lt;/a&gt;PostgreSQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Similar to the MySQL connector, there’s largely improved support for geo-spatial columns in Postgres now.
      More specifically, PostGIS column types can be represented in change data events now.
      Thanks a lot for Robert Coup who contributed this feature!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also the support for Postgres &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-506&quot;&gt;array columns&lt;/a&gt; has been expanded,
      e.g. we now support to track changes to &lt;code&gt;VARCHAR&lt;/code&gt; and &lt;code&gt;DATE&lt;/code&gt; array columns.
      Note that the connector doesn’t yet work with  geo-spatial array columns (should you ever have those),
      but this &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-571&quot;&gt;should be added&lt;/a&gt; soon, too.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to include just a subset of the rows of a captured table in snapshots, you may like the ability to &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-506&quot;&gt;specify
      dedicated SELECT statements&lt;/a&gt; to do so.
      For instance this can be used to exclude any logically deleted records — which you can recognize based on some flag in that table — from the snapshot.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A few bugs in this connector where reported and fixed by community members, too,
      e.g. the connector can be &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-519&quot;&gt;correctly paused&lt;/a&gt; now (thanks, Andrey Pustovetov),
      and we fixed an issue which could potentially have committed an &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-521&quot;&gt;incorrect offset&lt;/a&gt; to Kafka Connect (thanks, Thon Mekathikom).&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mongodb_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mongodb_connector&quot;&gt;&lt;/a&gt;MongoDB Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’ve ever compared the structures of change events emitted by the Debezium RDBMS connectors (MySQL, Postgres) and the MongoDB connector,
      you’ll know that the message structure of the latter is a bit different than the others.
      Due to the schemaless nature of MongoDB, the change events essentially contain a String with a JSON representation of the applied insert or patch.
      This structure cannot be consumed by existing sink connectors, such as the Confluent connectors for JDBC or Elasticsearch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This gets &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-409&quot;&gt;possible now&lt;/a&gt; by means of a newly added single message transformation (SMT),
      which parses these JSON strings and creates a structured Kafka Connect record from it (thanks, Sairam Polavarapu!).
      When applying this SMT to the JDBC sink connector, you can now stream data changes from MongoDB to any supported relational database.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note that this SMT is work-in-progress, details of its emitted message structure may still change.
      Also there are some inherent limitations to what can be achieved with it, if you e.g. have arrays in your MongoDB documents,
      the record created by this SMT will be structured accordingly, but many sink connectors cannot process such structure.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We have some ideas for further development here, e.g. there could be &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-561&quot;&gt;an option&lt;/a&gt; for flattening out (non-array) nested structures,
      so that e.g. &lt;code&gt;{ &quot;address&quot; { &quot;street&quot; : &quot;...&quot; } }&lt;/code&gt; would be represented as &lt;code&gt;address_street&lt;/code&gt;,
      which then could be consumed by sink connectors expecting a flat structure.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The new SMT is described in detail in &lt;a href=&quot;http://debezium.io/docs/configuration/mongodb-event-flattening/&quot;&gt;our docs&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-2&quot;&gt;full change log&lt;/a&gt; for more details and the complete list of issues fixed in Debezium 0.7.2.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The 0.7.3 release is scheduled for February 14th.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll focus on some more bug fixes, also we’re working on having Debezium regulary emit &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-409&quot;&gt;heartbeat messages&lt;/a&gt; to a dedicated topic.
      This will be practical for diagnostic purposes but also help to regularly trigger commits of the offset in Kafka Connect.
      That’s beneficial in certain situations when capturing tables which only very infrequently change.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also worked out &lt;a href=&quot;http://debezium.io/docs/roadmap/&quot;&gt;a roadmap&lt;/a&gt; describing our ideas for future work on Debezium, going beyond the next bugfix releases.
      While nothing is cast in stone, this is our idea of the features to add in the coming months.
      If you miss anything important on this roadmap, please tell us either in the comments below or send a message to our Google group.
      Looking forward to your feedback!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2018/01/17/streaming-to-elasticsearch/</id>
    <title>Streaming Data Changes from Your Database to Elasticsearch</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2018-01-17T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2018/01/17/streaming-to-elasticsearch/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="elasticsearch"></category>
    <category term="smt"></category>
    <category term="example"></category>
    <summary>
      
      
      
      We wish all the best to the Debezium community for 2018!
      
      
      While we&#8217;re working on the 0.7.2 release, we thought we&#8217;d publish another post describing an end-to-end data streaming use case based on Debezium.
      We have seen how to set up a change data stream to a downstream database a few weeks ago.
      In this blog post we will follow the same approach to stream the data to an Elasticsearch server to leverage its excellent capabilities for full-text search on our data.
      But to make the matter a little bit more interesting, we will stream the data to both, a PostgreSQL database and Elasticsearch,...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We wish all the best to the Debezium community for 2018!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While we’re working on the 0.7.2 release, we thought we’d publish another post describing an end-to-end data streaming use case based on Debezium.
      We have seen how to set up a change data stream to a downstream database &lt;a href=&quot;http://debezium.io/blog/2017/09/25/streaming-to-another-database/&quot;&gt;a few weeks ago&lt;/a&gt;.
      In this blog post we will follow the same approach to stream the data to an &lt;a href=&quot;https://www.elastic.co/&quot;&gt;Elasticsearch&lt;/a&gt; server to leverage its excellent capabilities for full-text search on our data.
      But to make the matter a little bit more interesting, we will stream the data to both, a PostgreSQL database and Elasticsearch, so we will optimize access to the data via the SQL query language as well as via full-text search.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;topology&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#topology&quot;&gt;&lt;/a&gt;Topology&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Here’s a diagram that shows how the data is flowing through our distributed system.
      First, the Debezium MySQL connector is continuously capturing the changes from the MySQL database, and sending the changes for each table to separate Kafka topics.
      Then, the Confluent &lt;a href=&quot;https://docs.confluent.io/current/connect/connect-jdbc/docs/sink_connector.html&quot;&gt;JDBC sink connector&lt;/a&gt; is continuously reading those topics and writing the events into the PostgreSQL database.
      And, at the same time, the Confluent &lt;a href=&quot;https://github.com/confluentinc/kafka-connect-elasticsearch&quot;&gt;Elasticsearch connector&lt;/a&gt; is continuously reading those same topics and writing the events into Elasticsearch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt; &lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div id=&quot;img-general&quot; class=&quot;imageblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;http://debezium.io/images/dbz-to-multiple.svg&quot; alt=&quot;Scenario topology&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;title&quot;&gt;Figure 1: A general topology&lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt; &lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We are going to deploy these components into several different processes.
      In this example, we’ll deploy all three connectors to a single Kafka Connect instance that will write to and read from Kafka on behalf of all of the connectors
      (in production you might need to keep the connectors separated to achieve better performance).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt; &lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div id=&quot;img-general&quot; class=&quot;imageblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;http://debezium.io/images/dbz-to-multiple-simplified.svg&quot; alt=&quot;Scenario topology&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;title&quot;&gt;Figure 2: A simplified topology&lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;configuration&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#configuration&quot;&gt;&lt;/a&gt;Configuration&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We will use this &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/unwrap-smt&quot;&gt;Docker Compose file&lt;/a&gt; for a fast deployment of the demo.
      The deployment consists of the following Docker images:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://hub.docker.com/r/debezium/zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://hub.docker.com/r/debezium/kafka/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;An &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/unwrap-smt/debezium-jdbc&quot;&gt;enriched&lt;/a&gt; Kafka Connect / Debezium &lt;a href=&quot;https://hub.docker.com/r/debezium/connect/&quot;&gt;image&lt;/a&gt; with a few changes:&lt;/p&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;PostgreSQL JDBC driver placed into &lt;em&gt;/kafka/libs&lt;/em&gt; directory&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The Confluent JDBC connector placed into &lt;em&gt;/kafka/connect/kafka-connect-jdbc&lt;/em&gt; directory&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Pre-populated MySQL as used in our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Empty PostgreSQL&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Empty Elasticsearch&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The message format is not the same for the Debezium source connector and the JDBC and Elasticsearch connectors as they are developed separately and each focuses on slightly different objectives.
      Debezium emits a more complex event structure so that it captures all of the information available.
      In particular, the change events contain the old and the new state of a changed record.
      Both sink connectors on the other hand expect a simple message that just represents the record state to be written.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium’s &lt;a href=&quot;http://debezium.io/docs/configuration/event-flattening/&quot;&gt;UnwrapFromEnvelope&lt;/a&gt; single message transformation (SMT) collapses the complex change event structure into the same row-based format expected by the two sink connectors and effectively acts as a &lt;a href=&quot;http://www.enterpriseintegrationpatterns.com/patterns/messaging/MessageTranslator.html&quot;&gt;message translator&lt;/a&gt; between the two aforementioned formats.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;example&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#example&quot;&gt;&lt;/a&gt;Example&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s move directly to our example as that’s where the changes are visible.
      First of all we need to deploy all components:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;export DEBEZIUM_VERSION=0.7
      docker-compose up&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When all components are started we are going to register the Elasticsearch Sink connector writing into the Elasticsearch instance.
      We want to use the same key (primary id) in the source and both PostgreSQL and Elasticsearch:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; \
          -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ \
          -d @es-sink.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re using this registration request:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
        {
          &quot;name&quot;: &quot;elastic-sink&quot;,
          &quot;config&quot;: {
            &quot;connector.class&quot;:
                &quot;io.confluent.connect.elasticsearch.ElasticsearchSinkConnector&quot;,
            &quot;tasks.max&quot;: &quot;1&quot;,
            &quot;topics&quot;: &quot;customers&quot;,
            &quot;connection.url&quot;: &quot;http://elastic:9200&quot;,
            &quot;transforms&quot;: &quot;unwrap,key&quot;,
            &quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.UnwrapFromEnvelope&quot;,        (1)
            &quot;transforms.key.type&quot;: &quot;org.apache.kafka.connect.transforms.ExtractField$Key&quot;,(2)
            &quot;transforms.key.field&quot;: &quot;id&quot;,                                                 (2)
            &quot;key.ignore&quot;: &quot;false&quot;,                                                        (3)
            &quot;type.name&quot;: &quot;customer&quot;                                                       (4)
          }
        }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The request configures these options:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;extracting only the new row’s state from Debezium’s change data message&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;extracting the &lt;code&gt;id&lt;/code&gt; field from the key &lt;code&gt;struct&lt;/code&gt;, then the same key is used for the source and both destinations.
      This is to address the fact that the Elasticsearch connector only supports numeric types and &lt;code&gt;string&lt;/code&gt; as keys. If we do not extract the &lt;code&gt;id&lt;/code&gt; the messages will be filtered out by the connector because of unknown key type.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;use key from the event instead of generating a synthetic one&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;type under which the events will be registered in Elasticsearch&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Next we are going to register the JDBC Sink connector writing into PostgreSQL database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; \
          -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ \
          -d @jdbc-sink.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, the source connector must be set up:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; \
          -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ \
          -d @source.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s check if the databases and the search server are synchronized.
      All the rows of the &lt;code&gt;customers&lt;/code&gt; table should be found in the source database (MySQL) as well as the target database (Postgres) and Elasticsearch:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER  -p$MYSQL_PASSWORD inventory -e &quot;select * from customers&quot;'
      +------+------------+-----------+-----------------------+
      | id   | first_name | last_name | email                 |
      +------+------------+-----------+-----------------------+
      | 1001 | Sally      | Thomas    | sally.thomas@acme.com |
      | 1002 | George     | Bailey    | gbailey@foobar.com    |
      | 1003 | Edward     | Walker    | ed@walker.com         |
      | 1004 | Anne       | Kretchmar | annek@noanswer.org    |
      +------+------------+-----------+-----------------------+&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec postgres bash -c 'psql -U $POSTGRES_USER $POSTGRES_DB -c &quot;select * from customers&quot;'
       last_name |  id  | first_name |         email
      -----------+------+------------+-----------------------
       Thomas    | 1001 | Sally      | sally.thomas@acme.com
       Bailey    | 1002 | George     | gbailey@foobar.com
       Walker    | 1003 | Edward     | ed@walker.com
       Kretchmar | 1004 | Anne       | annek@noanswer.org&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl 'http://localhost:9200/customers/_search?pretty'
      {
        &quot;took&quot; : 42,
        &quot;timed_out&quot; : false,
        &quot;_shards&quot; : {
          &quot;total&quot; : 5,
          &quot;successful&quot; : 5,
          &quot;failed&quot; : 0
        },
        &quot;hits&quot; : {
          &quot;total&quot; : 4,
          &quot;max_score&quot; : 1.0,
          &quot;hits&quot; : [
            {
              &quot;_index&quot; : &quot;customers&quot;,
              &quot;_type&quot; : &quot;customer&quot;,
              &quot;_id&quot; : &quot;1001&quot;,
              &quot;_score&quot; : 1.0,
              &quot;_source&quot; : {
                &quot;id&quot; : 1001,
                &quot;first_name&quot; : &quot;Sally&quot;,
                &quot;last_name&quot; : &quot;Thomas&quot;,
                &quot;email&quot; : &quot;sally.thomas@acme.com&quot;
              }
            },
            {
              &quot;_index&quot; : &quot;customers&quot;,
              &quot;_type&quot; : &quot;customer&quot;,
              &quot;_id&quot; : &quot;1004&quot;,
              &quot;_score&quot; : 1.0,
              &quot;_source&quot; : {
                &quot;id&quot; : 1004,
                &quot;first_name&quot; : &quot;Anne&quot;,
                &quot;last_name&quot; : &quot;Kretchmar&quot;,
                &quot;email&quot; : &quot;annek@noanswer.org&quot;
              }
            },
            {
              &quot;_index&quot; : &quot;customers&quot;,
              &quot;_type&quot; : &quot;customer&quot;,
              &quot;_id&quot; : &quot;1002&quot;,
              &quot;_score&quot; : 1.0,
              &quot;_source&quot; : {
                &quot;id&quot; : 1002,
                &quot;first_name&quot; : &quot;George&quot;,
                &quot;last_name&quot; : &quot;Bailey&quot;,
                &quot;email&quot; : &quot;gbailey@foobar.com&quot;
              }
            },
            {
              &quot;_index&quot; : &quot;customers&quot;,
              &quot;_type&quot; : &quot;customer&quot;,
              &quot;_id&quot; : &quot;1003&quot;,
              &quot;_score&quot; : 1.0,
              &quot;_source&quot; : {
                &quot;id&quot; : 1003,
                &quot;first_name&quot; : &quot;Edward&quot;,
                &quot;last_name&quot; : &quot;Walker&quot;,
                &quot;email&quot; : &quot;ed@walker.com&quot;
              }
            }
          ]
        }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With the connectors still running, we can add a new row to the MySQL database and then check that it was replicated into both the PostgreSQL database and Elasticsearch:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER  -p$MYSQL_PASSWORD inventory'
      
      mysql&amp;gt; insert into customers values(default, 'John', 'Doe', 'john.doe@example.com');
      Query OK, 1 row affected (0.02 sec)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec -postgres bash -c 'psql -U $POSTGRES_USER $POSTGRES_DB -c &quot;select * from customers&quot;'
       last_name |  id  | first_name |         email
      -----------+------+------------+-----------------------
      ...
      Doe        | 1005 | John       | john.doe@example.com
      (5 rows)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl 'http://localhost:9200/customers/_search?pretty'
      ...
      {
        &quot;_index&quot; : &quot;customers&quot;,
        &quot;_type&quot; : &quot;customer&quot;,
        &quot;_id&quot; : &quot;1005&quot;,
        &quot;_score&quot; : 1.0,
        &quot;_source&quot; : {
          &quot;id&quot; : 1005,
          &quot;first_name&quot; : &quot;John&quot;,
          &quot;last_name&quot; : &quot;Doe&quot;,
          &quot;email&quot; : &quot;john.doe@example.com&quot;
        }
      }
      ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot;&gt;&lt;/a&gt;Summary&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We set up a complex streaming data pipeline to synchronize a MySQL database with another database and also with an Elasticsearch instance.
      We managed to keep the same identifier across all systems which allows us to correlate records across the system as a whole.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Propagating data changes from a primary database in near realtime to a search engine such as Elasticsearch enables many interesting use cases.
      Besides different applications of fulltext search one could for instance also think about creating dashboards and all kinds of visualizations using &lt;a href=&quot;https://www.elastic.co/de/products/kibana&quot;&gt;Kibana&lt;/a&gt;, to gain further insight into the data.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to try out this set-up yourself, just clone the project from our &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/unwrap-smt&quot;&gt;examples repo&lt;/a&gt;.
      In case you need help, have feature requests or would like to share your experiences with this pipeline, please let us know in the comments below.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/12/20/debezium-0-7-1-released/</id>
    <title>Debezium 0.7.1 Is Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2017-12-20T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/12/20/debezium-0-7-1-released/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      Just last few days before Christmas we are releasing Debezium  0.7.1!
      This is a bugfix release that fixes few annoying issues that were found during first rounds of use of Debezium 0.7 by our community.
      All issues relate to either newly provided wal2json support or reduced risk of internal race condition improvement.
      
      
      Robert Coup has found a performance regression in situations when 0.7.0 was used with old version of Protobuf decoder.
      
      
      Suraj Savita (and others) has found an issue when our code failed to correctly detect it runs with Amazon RDS wal2json plug-in.
      We are outsmarted by the JDBC driver internals and included a...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Just last few days before Christmas we are releasing Debezium  &lt;strong&gt;0.7.1&lt;/strong&gt;!
      This is a bugfix release that fixes few annoying issues that were found during first rounds of use of Debezium 0.7 by our community.
      All issues relate to either newly provided wal2json support or reduced risk of internal race condition improvement.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/rcoup&quot;&gt;Robert Coup&lt;/a&gt; has found a &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-512&quot;&gt;performance regression&lt;/a&gt; in situations when 0.7.0 was used with old version of Protobuf decoder.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Suraj Savita (and others) has found an issue when our code failed to &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-513&quot;&gt;correctly detect&lt;/a&gt; it runs with Amazon RDS wal2json plug-in.
      We are outsmarted by the JDBC driver internals and included a distinct plugin decoder name &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-517&quot;&gt;wal2json_rds&lt;/a&gt; that bypasses detection routine and by default expects it runs against Amazon RDS instance. This mode should be used only with RDS instances.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We have also gathered feedback from first tries to run with Amazon RDS and included &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql/#amazon-rds&quot;&gt;a short section&lt;/a&gt; in our documentation on this topic.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/12/15/debezium-0-7-0-released/</id>
    <title>Debezium 0.7.0 Is Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2017-12-15T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/12/15/debezium-0-7-0-released/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s not Christmas yet, but we already got a present for you: Debezium  0.7.0 is here, full of new features as well as many bug fixes!
      A big thank you goes out to all the community members who contributed to this release.
      It is very encouraging for us to see not only more and more issues and feature requests being reported, but also pull requests coming in.
      
      
      Note that this release comes with a small number of changes to the default mappings for some data types.
      We try to avoid this sort of changes as far as possible, but in some cases it...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s not Christmas yet, but we already got a present for you: Debezium  &lt;strong&gt;0.7.0&lt;/strong&gt; is here, full of new features as well as many bug fixes!
      A big thank you goes out to all the community members who contributed to this release.
      It is very encouraging for us to see not only more and more issues and feature requests being reported, but also pull requests coming in.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note that this release comes with a small number of changes to the default mappings for some data types.
      We try to avoid this sort of changes as far as possible, but in some cases it is required,
      e.g. if the previous mapping could have caused potential value losses.
      Please see below for the details and also make sure to check out the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-0&quot;&gt;full change log&lt;/a&gt; which describes these changes in detail.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now let’s take a closer look at some of new features.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;based_on_apache_kafka_1_0&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#based_on_apache_kafka_1_0&quot;&gt;&lt;/a&gt;Based on Apache Kafka 1.0&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A few weeks ago the Apache Kafka team has &lt;a href=&quot;https://www.confluent.io/blog/apache-kafka-goes-1-0/&quot;&gt;released version 1.0.0&lt;/a&gt;.
      This was an important milestone for the Kafka community,
      and we now can happily declare that Debezium is &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-432&quot;&gt;built&lt;/a&gt; against and runs on that Apache Kafka version.
      Our Docker images were also &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-433&quot;&gt;promoted&lt;/a&gt; to contain Apache Kafka and Kafka Connect 1.0.0.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;postgresql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#postgresql_connector&quot;&gt;&lt;/a&gt;PostgreSQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The big news for the PostgreSQL connector is that it now &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-258&quot;&gt;supports&lt;/a&gt; the &lt;a href=&quot;https://github.com/eulerto/wal2json&quot;&gt;wal2json&lt;/a&gt; logical decoding plugin as an alternative to the existing &lt;a href=&quot;https://github.com/debezium/postgres-decoderbufs&quot;&gt;DecoderBufs plug-in&lt;/a&gt;.
      This means that you now can use Debezium to stream changes out of PostgreSQL on &lt;a href=&quot;https://aws.amazon.com/rds/postgresql/&quot;&gt;Amazon RDS&lt;/a&gt;, as wal2json is the logical decoding plugin used in this environment.
      Many thanks to &lt;a href=&quot;https://github.com/rcoup&quot;&gt;Robert Coup&lt;/a&gt; who significantly contributed to this feature.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Working on this plug-in, we noticed that there was a &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-379&quot;&gt;potential race condition&lt;/a&gt; when it comes to applying changes to the schema of captured tables.
      In that case it could have happened that a number of messages pertaining to data changes done before the schema change were emitted using the new schema.
      With the exception of a few corner cases (which are described &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-0&quot;&gt;here&lt;/a&gt;), this has been addressed when using Debezium’s own DecoderBufs plug-in.
      So it’s highly recommended to upgrade the DecoderBufs plug-in to the new version before upgrading the Debezium connector.
      We’ve also worked closely with the author of the wal2json plug-in (big thanks for the quick help!) to prevent the issue when using the wal2json plug-in.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While the Debezium Docker images for Postgres already come with the latest version of DecoderBufs and wal2json,
      RDS for now is still using an older version of wal2json.
      Until this has been updated, special attention must be paid when applying schema changes to captured tables.
      Please see &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-0&quot;&gt;the changelog&lt;/a&gt; for a in-depth description of this issue and ways to mitigate it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;There are new daily running &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-495&quot;&gt;CI jobs&lt;/a&gt; that verify that the wal2json plugin passes our test suite.
      For the foreseeable future we’ll support both, wal2json as well as the existing DecoderBufs plug-in.
      The latter should be more efficient due to the usage of the Protocol Buffers binary format,
      whereas the former comes in handy for RDS or other cloud environments where you don’t have control over the installed logical decoding plug-ins, but wal2json is available.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In other news on the Postgres connector, &lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt; discovered and proposed a fix for a &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-501&quot;&gt;multi-threading bug&lt;/a&gt; that could have put the connector into an undefined state if a rebalance in the Connect cluster was triggered during snapshotting.
      Thanks, Andrey!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mysql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mysql_connector&quot;&gt;&lt;/a&gt;MySQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In the MySQL connector we’ve fixed two issues which affect the default mapping of certain column types.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Following up to the new &lt;code&gt;BIGINT UNSIGNED&lt;/code&gt; mapping introduced in &lt;a href=&quot;http://debezium.io/blog/2017/10/26/debezium-0-6-1-released/&quot;&gt;Debezium 0.6.1&lt;/a&gt;, this type is now encoded as &lt;code&gt;int64&lt;/code&gt; in Debezium messages &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-461&quot;&gt;by default&lt;/a&gt; as it is easier for (polyglot) clients to work with.
      This is a reasonable mapping for the vast majority of cases.
      Only when using values &amp;gt; 2^63, you should switch it back to the &lt;code&gt;Decimal&lt;/code&gt; logical type
      which is a bit more cumbersome to handle, though.
      This should be a rare situation, as MySQL &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/numeric-type-overview.html&quot;&gt;advices against&lt;/a&gt; using unsigned values &amp;gt; 2^63 due to potential value losses when performing DB-side calculations.
      Please see the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;connector documentation&lt;/a&gt; for the details.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/rk3rn3r&quot;&gt;Rene Kerner&lt;/a&gt; has improved the support for the MySQL &lt;code&gt;TIME&lt;/code&gt; type.
      MySQL allows to store values larger than &lt;code&gt;23:59:59&lt;/code&gt; in such columns, and the type &lt;code&gt;int32&lt;/code&gt; which was previously used for &lt;code&gt;TIME(0-3)&lt;/code&gt; columns isn’t enough to convey the entire possible value range.
      Therefore all &lt;code&gt;TIME&lt;/code&gt; columns in MySQL are by default represented as &lt;code&gt;int64&lt;/code&gt; now,
      using the &lt;code&gt;io.debezium.time.MicroTime&lt;/code&gt; logical type, i.e. the value represents micro-seconds.
      If needed, you can switch to the previous mapping by setting &lt;code&gt;time.precision.mode&lt;/code&gt; to &lt;code&gt;adaptive&lt;/code&gt;,
      but you should only do so if you’re sure that you only ever will have values that fit into &lt;code&gt;int32&lt;/code&gt;.
      This option is only kept for a transitioning period and will be removed in a future release.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Recently we got a &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-390&quot;&gt;report&lt;/a&gt; that MySQL’s binlog can contain &lt;code&gt;ROLLBACK&lt;/code&gt; statements and thus transactions that are actually not committed.
      Of course no data change messages should be emitted in this situation.
      This e.g. can be the case when temporary tables are dropped.
      So we introduced a &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-406&quot;&gt;look-ahead buffer&lt;/a&gt; functionality that reads the binlog by transaction and excludes those that were rolled back.
      This feature should be considered incubating and is disabled by default for the time being.
      We’d like to gather your feedback on this, so if you’d benefit from this feature, please give it a try and let us know if you run into any issues.
      For further details please refer to the &lt;code&gt;binlog.buffer.size&lt;/code&gt; setting in the MySQL connector docs.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/ainagy&quot;&gt;Andras Istvan Nagy&lt;/a&gt; came with the idea and &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-349&quot;&gt;implemented&lt;/a&gt; a way for explicitly selecting the rows from each table that will be part of the snapshotting process.
      This can for instance be very useful if you work with soft deletes and would like to exclude all logically deleted records from snapshotting.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-7-0&quot;&gt;full change log&lt;/a&gt; for more details and the complete list of fixed issues.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium 0.7.1 release is planned to be out roughly two weeks after Christmas.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It will contain a new SMT that will unwind MongoDB change events into a regular JSON consumable by sink connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A big overhaul of &lt;code&gt;GEOMETRY&lt;/code&gt; types is in progress.
      When completed, all &lt;code&gt;GEOMETRY&lt;/code&gt; types will be supported by both MySQL and PostgreSQL connectors and they will be available in standard &lt;code&gt;WKB&lt;/code&gt; format for easy consumption by polyglot clients.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;There is ongoing work for the MySQL connector to allow dynamic update of &lt;code&gt;table.whitelist&lt;/code&gt; option.
      This will allow the user to re-configure the set of tables captured without need to re-create connector.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to contribute, please let us know.
      We’re happy about any help and will work with you to get you started quickly.
      Check out the details below on how to get in touch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/11/15/debezium-0-6-2-released/</id>
    <title>Debezium 0.6.2 Is Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2017-11-15T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/11/15/debezium-0-6-2-released/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We are accelerating! Three weeks after the 0.6.1 release, the Debezium team is bringing Debezium 0.6.2 to you!
      
      
      This release revolves mostly around bug fixes, but there are a few new features, too.
      Let&#8217;s take a closer look at some of the changes.
      
      
      
      
      PostgreSQL Connector
      
      
      The big news for the Postgres connector is that Debezium now runs against PostgreSQL 10 thanks to a contribution from Scofield Xu.
      As a part of this change we are providing a Docker Image with PostgreSQL 10, too, and we have set up a daily run of our integration tests against it.
      
      
      If you are building Postgres yourself using the Debezium...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We are accelerating! Three weeks after the 0.6.1 release, the Debezium team is bringing &lt;strong&gt;Debezium 0.6.2&lt;/strong&gt; to you!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release revolves mostly around bug fixes, but there are a few new features, too.
      Let’s take a closer look at some of the changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;postgresql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#postgresql_connector&quot;&gt;&lt;/a&gt;PostgreSQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The big news for the Postgres connector is that Debezium now runs against &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-424&quot;&gt;PostgreSQL 10&lt;/a&gt; thanks to a contribution from &lt;a href=&quot;https://github.com/ScofieldXu&quot;&gt;Scofield Xu&lt;/a&gt;.
      As a part of this change we are providing a &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-426&quot;&gt;Docker Image&lt;/a&gt; with PostgreSQL 10, too, and we have set up a &lt;a href=&quot;http://ci.hibernate.org/view/Debezium/job/debezium-postgresql-10-test/&quot;&gt;daily run&lt;/a&gt; of our integration tests against it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you are building Postgres yourself using the Debezium &lt;a href=&quot;https://github.com/debezium/postgres-decoderbufs&quot;&gt;logical decoding plug-in&lt;/a&gt;,
      you can save quite some megabytes if you don’t need the PostGIS geometric extension:
      thanks to the work by &lt;a href=&quot;https://github.com/QazerLab&quot;&gt;Danila Kiver&lt;/a&gt;, it’s now possible to omit that extension.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mysql_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mysql_connector&quot;&gt;&lt;/a&gt;MySQL Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve received multiple reports related to parsing MySQL DDL statements, e.g. there were a few specific invocations of the &lt;code&gt;ALTER TABLE&lt;/code&gt; statement which weren’t handled correctly.
      Those as well as a few other parser bugs have been fixed.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you work with the &lt;code&gt;TIMESTAMP&lt;/code&gt; column type and your Kafka Connect server isn’t using UTC as timezone, then the fix for &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-260&quot;&gt;DBZ-260&lt;/a&gt; is applying to you.
      In that case, the ISO 8601 formatted String emitted by Debezium would have, incorrectly, contained the UTC date and time plus the zone offset (as per the time zone the Kafka Connect server is located in) before.
      Whereas now it will contain the date and time adjusted to the zone offset.
      This may require adjustments to to downstream consumers if they were relying on the previous, incorrect behavior.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-217&quot;&gt;DBZ-217&lt;/a&gt; gives you more flexibility for handling corrupt events encountered in the MySQL binlog.
      By default, the connector will stop at the problematic event in such case.
      But you now also have the option to just log the event and its position and continue the processing after it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Another nice improvement for the MySQL connector is a much reduced CPU load after the snapshot has been completed, when using the &quot;snapshot only&quot; mode (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-396&quot;&gt;DBZ-396&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;mongodb_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mongodb_connector&quot;&gt;&lt;/a&gt;MongoDB Connector&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This connector received an important fix applying when more than one thread is used to performing the initial snapshot (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-438&quot;&gt;DBZ-438&lt;/a&gt;).
      Before, it could happen that single messages got lost during snapshotting which is fixed now.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;examples_and_docker_images&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#examples_and_docker_images&quot;&gt;&lt;/a&gt;Examples and Docker Images&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We have expanded our examples repository with &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/tutorial#using-mysql-and-the-avro-message-format&quot;&gt;an Avro example&lt;/a&gt;,
      which may be interesting to you if you’d like to not work with JSON messages but rather the compact Avro binary format and the Confluent schema registry.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As a part of our release process we are now creating &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-418&quot;&gt;micro tags&lt;/a&gt; for our Docker images for every released version.
      While tags in the format &lt;code&gt;x.y.z&lt;/code&gt; are fixed in time, tags in the format &lt;code&gt;x.y&lt;/code&gt; are rolling updates and always point to the latest micro release of that image.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-6-2&quot;&gt;full change log&lt;/a&gt; for more details and the complete list of fixed issues.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium 0.7 release is planned to be out in two to three weeks from now.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It will contain the move to Apache Kafka 1.0.0 and bring support for the wal2json logical decoding plug-in for Postgres.
      This will eventually allow to use the Debezium Postgres connector on Amazon RDS (once the correct wal2json version is available there).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In parallel, the work around handling updates to the whitelist configuration of the MySQL connector continues (it may be ready for 0.7.0),
      and so does the work on the Oracle connector (which will be shipping in a future release).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to contribute, please let us know.
      We’re happy about any help and will work with you to get you started quickly.
      Check out the details below on how to get in touch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/11/11/debezium-at-devoxx-belgium/</id>
    <title>Debezium at Devoxx Belgium</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2017-11-11T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/11/11/debezium-at-devoxx-belgium/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="introduction"></category>
    <category term="presentation"></category>
    <summary>
      
      
      
      Debezium&#8217;s project lead Gunnar Morling gave a few talks during recent Devoxx Belgium 2017.
      One of his talks was dedicated to Debezium and change data capture in general.
      
      
      If you are interested in those topics and you want to obtain a fast and simple introduction to it, do not hesitate and watch the talk.
      Batteries and demo included!
      
      
      
      
      
      The slide deck is available, too:
      
      
      
      
      
      
      
      About Debezium
      
      
      Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of Kafka and provides Kafka Connect...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium’s project lead &lt;a href=&quot;https://twitter.com/gunnarmorling&quot;&gt;Gunnar Morling&lt;/a&gt; gave a few talks during recent &lt;a href=&quot;https://cfp.devoxx.be/2017/index.html&quot;&gt;Devoxx Belgium 2017&lt;/a&gt;.
      One of his talks was dedicated to Debezium and change data capture in general.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you are interested in those topics and you want to obtain a fast and simple introduction to it, do not hesitate and watch the talk.
      Batteries and demo included!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;responsive-video&quot;&gt;
      &lt;iframe width=&quot;1600&quot; height=&quot;900&quot; src=&quot;https://www.youtube.com/embed/IOZ2Um6e430?rel=0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The slide deck is &lt;a href=&quot;https://speakerdeck.com/gunnarmorling/streaming-database-changes-with-debezium&quot;&gt;available&lt;/a&gt;, too:&lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div style=&quot;text-align-center&quot;&gt;
      &lt;script async=&quot;&quot; class=&quot;speakerdeck-embed&quot; data-id=&quot;4fb7aa5af1c54d7ea807c9d46fb5b1fa&quot; data-ratio=&quot;1.77777777777778&quot; src=&quot;//speakerdeck.com/assets/embed.js&quot;&gt;&lt;/script&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/10/26/debezium-0-6-1-released/</id>
    <title>Debezium 0.6.1 Is Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2017-10-26T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/10/26/debezium-0-6-1-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      Just shy of a month after the 0.6.0 release, I&#8217;m happy to announce the release of Debezium 0.6.1!
      
      
      This release contains several bugfixes, dependency upgrades and a new option for controlling how BIGINT UNSIGNED columns are conveyed.
      We also expanded the set of Docker images and Docker Compose files accompanying our tutorial, so you can run it now with all the databases we support.
      
      
      Let&#8217;s take a closer look at some of the changes.
      
      
      
      
      New connector option for controlling BIGINT UNSIGNED representation
      
      
      BIGINT UNSIGNED columns from MySQL databases have been represented using Kafka Connect&#8217;s Decimal type until now.
      This type allows to represent all possible values...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Just shy of a month after the 0.6.0 release, I’m happy to announce the release of &lt;strong&gt;Debezium 0.6.1&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release contains several bugfixes, dependency upgrades and a new option for controlling how &lt;code&gt;BIGINT UNSIGNED&lt;/code&gt; columns are conveyed.
      We also expanded the set of Docker images and Docker Compose files accompanying &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;our tutorial&lt;/a&gt;, so you can run it now with all the databases we support.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s take a closer look at some of the changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;new_connector_option_for_controlling_bigint_unsigned_representation&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_connector_option_for_controlling_bigint_unsigned_representation&quot;&gt;&lt;/a&gt;New connector option for controlling BIGINT UNSIGNED representation&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;code&gt;BIGINT UNSIGNED&lt;/code&gt; columns from MySQL databases have been represented using Kafka Connect’s &lt;code&gt;Decimal&lt;/code&gt; type until now.
      This type allows to represent all possible values of such columns, but its based on a byte array, so it can be a bit cumbersome to handle for consumers.
      Therefore we added a new option named &lt;code&gt;bigint.unsigned.handling.mode&lt;/code&gt; to the MySQL connector that allows to represent such columns using &lt;code&gt;long&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For the very most cases that’s the preferable option, only if your column contains values larger than 2^63
      (which &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/numeric-type-overview.html&quot;&gt;MySQL doesn’t recommend&lt;/a&gt; due to potential value losses when performing calculations),
      you should stick to the &lt;code&gt;Decimal&lt;/code&gt; representation.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Using &lt;code&gt;long&lt;/code&gt; will be the default as of Debezium 0.7, for the 0.6.x timeline we decided to go with the previous behavior (i.e. using &lt;code&gt;Decimal&lt;/code&gt;) for the sake of backwards compatibility.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks a lot to &lt;a href=&quot;https://github.com/vultron81&quot;&gt;Ben Williams&lt;/a&gt; who contributed this feature!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;new_example_docker_images_and_docker_compose_files&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_example_docker_images_and_docker_compose_files&quot;&gt;&lt;/a&gt;New example Docker images and Docker Compose files&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In the &lt;a href=&quot;https://github.com/debezium/debezium-examples/&quot;&gt;Debezium examples repository&lt;/a&gt; we now provide &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/tutorial&quot;&gt;Docker Compose files&lt;/a&gt; which let you run the tutorial with all the three databases we currently support, MySQL, Postgres and MongoDB.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Just choose the Compose file for your preferred database and get a all the required components (ZooKeeper, Apache Kafka, Kafka Connect and the database) running within a few seconds.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also deployed Docker images for Postgres and MongoDB to the &lt;a href=&quot;https://hub.docker.com/u/debezium/&quot;&gt;Debezium organization&lt;/a&gt; on Docker Hub, so you got some data to play with.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;version_upgrades&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#version_upgrades&quot;&gt;&lt;/a&gt;Version upgrades&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve upgraded our images from Kafka 0.11.0.0 to &lt;a href=&quot;https://issues.apache.org/jira/projects/KAFKA/versions/12340632&quot;&gt;0.11.0.1&lt;/a&gt;.
      Also the &lt;a href=&quot;https://github.com/shyiko/mysql-binlog-connector-java&quot;&gt;binlog client library&lt;/a&gt; used by the MySQL connector was upgraded from 0.9.0 to 0.13.0.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;bugfixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bugfixes&quot;&gt;&lt;/a&gt;Bugfixes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, several bugs were fixed in 0.6.1.
      E.g. you can now name a column &lt;code&gt;column&lt;/code&gt; in MySQL (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-408&quot;&gt;DBZ-408&lt;/a&gt;),
      generated &lt;code&gt;DROP TEMP TABLE&lt;/code&gt; statements won’t flood the DB history topic (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-395&quot;&gt;DBZ-295&lt;/a&gt;)
      and we’ve fixed a case where the Postgres connector would stop working due to an internal error but fail to report though via the task/connector status (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-380&quot;&gt;DBZ-380&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please see the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-6-1&quot;&gt;full change log&lt;/a&gt; for more details and the complete list of fixed issues.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The work on Debezium 0.7 has already begun and we’ve merged the first set of changes.
      You can expect to see support for using the &lt;a href=&quot;https://github.com/eulerto/wal2json&quot;&gt;wal2json&lt;/a&gt; logical decoding plug-in with the Postgres connector, which will finally allow it to use Debezium with Postgres on Amazon RDS!
      We’ve also started our explorations of providing a connector for Oracle (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-20&quot;&gt;DBZ-20&lt;/a&gt;) and hope to report some progress here soon.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While the work on Debezium 0.7 continues, you will likely continue to see one or more 0.6.x bugfix releases.
      We’ve automated the release process as much as possible, making it a breeze to ship a new release and getting fixes into your hands quickly.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’d like to contribute, please let us know.
      We’re happy about any help and will work with you to get you started quickly.
      Check out the details below on how to get in touch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/09/25/streaming-to-another-database/</id>
    <title>Streaming data to a downstream database</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2017-09-25T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/09/25/streaming-to-another-database/" rel="alternate" type="text/html" />
    <author>
      <name>Jiri Pechanec</name>
    </author>
    <category term="mysql"></category>
    <category term="postgres"></category>
    <category term="smt"></category>
    <category term="example"></category>
    <summary>
      
      
      
      In this blog post we will create a simple streaming data pipeline to continuously capture the changes in a MySQL database and replicate them in near real-time into a PostgreSQL database.
      We&#8217;ll show how to do this without writing any code, but instead by using and configuring Kafka Connect, the Debezium MySQL source connector, the Confluent JDBC sink connector, and a few single message transforms (SMTs).
      
      
      This approach of replicating data through Kafka is really useful on its own, but it becomes even more advantageous when we can combine our near real-time streams of data changes with other streams, connectors, and stream...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In this blog post we will create a simple streaming data pipeline to continuously capture the changes in a MySQL database and replicate them in near real-time into a PostgreSQL database.
      We’ll show how to do this without writing any code, but instead by using and configuring Kafka Connect, the Debezium MySQL source connector, the Confluent JDBC sink connector, and a few single message transforms (SMTs).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This approach of replicating data through Kafka is really useful on its own, but it becomes even more advantageous when we can combine our near real-time streams of data changes with other streams, connectors, and stream processing applications.
      A recent &lt;a href=&quot;https://www.confluent.io/blog/simplest-useful-kafka-connect-data-pipeline-world-thereabouts-part-1/&quot;&gt;Confluent blog post series&lt;/a&gt; shows a similar streaming data pipeline but using different connectors and SMTs.
      What’s great about Kafka Connect is that you can mix and match connectors to move data between multiple systems.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We will also demonstrate a new functionality that was released with &lt;a href=&quot;2017/09/21/debezium-0-6-0-released/&quot;&gt;Debezium 0.6.0&lt;/a&gt;: a single message transform for &lt;a href=&quot;http://debezium.io/docs/configuration/event-flattening/&quot;&gt;CDC Event Flattening&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;topology&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#topology&quot;&gt;&lt;/a&gt;Topology&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The general topology for this scenario is displayed on the following picture:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div id=&quot;img-general&quot; class=&quot;imageblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;http://debezium.io/images/dbz-to-jdbc.svg&quot; alt=&quot;Scenario topology&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;title&quot;&gt;Figure 1: A General topology&lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt; &lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To simplify the setup a little bit, we will use only one Kafka Connect instance that will contain all connectors.
      I.e. this instance will serve as an event producer and an event consumer:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt; &lt;br /&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div id=&quot;img-general&quot; class=&quot;imageblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;http://debezium.io/images/dbz-to-jdbc-simplified.svg&quot; alt=&quot;Scenario topology&quot; /&gt;
      &lt;/div&gt;
      &lt;div class=&quot;title&quot;&gt;Figure 2: A Simplified topology&lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;configuration&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#configuration&quot;&gt;&lt;/a&gt;Configuration&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We will use this &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/unwrap-smt&quot;&gt;compose&lt;/a&gt; for a fast deployment of the demo.
      The deployment consists of following Docker images:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://hub.docker.com/r/debezium/zookeeper/&quot;&gt;Apache ZooKeeper&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://hub.docker.com/r/debezium/kafka/&quot;&gt;Apache Kafka&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;An &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/unwrap-smt/debezium-jdbc&quot;&gt;enriched&lt;/a&gt; Kafka Connect / Debezium &lt;a href=&quot;https://hub.docker.com/r/debezium/connect/&quot;&gt;image&lt;/a&gt; with changes&lt;/p&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;PostgreSQL JDBC driver placed into &lt;code&gt;/kafka/libs&lt;/code&gt; directory&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;https://docs.confluent.io/current/connect/connect-jdbc/docs/index.html&quot;&gt;Kafka Connect JDBC Connector&lt;/a&gt; (developed by &lt;a href=&quot;https://www.confluent.io/&quot;&gt;Confluent&lt;/a&gt;) placed into &lt;code&gt;/kafka/connect/kafka-connect-jdbc&lt;/code&gt; directory&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Pre-populated MySQL used in our &lt;a href=&quot;docs/tutorial&quot;&gt;tutorial&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Empty PostgreSQL&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium MySQL Connector was designed to specifically capture database changes and provide as much information as possible about those events beyond just the new state of each row.
      Meanwhile, the Confluent JDBC Sink Connector was designed to simply convert each message into a database insert/upsert based upon the structure of the message.
      So, the two connectors have different structures for the messages, but they also use different topic naming conventions and behavior of representing deleted records.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;These mismatches in structure and behavior will be common when using connectors that were not designed to work together. But this is something that we can easily deal with, and we discuss how in the next few sections.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;event_format&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#event_format&quot;&gt;&lt;/a&gt;Event format&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium emits events in a complex format that contains all of the information about the captured data change:
      the type of operation, source metadata, the timestamp the event was processed by the connector, and state of the row before and after the change was made.
      Debezium calls this structure an &lt;em&gt;&quot;envelope&quot;&lt;/em&gt;:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
      	&quot;op&quot;: &quot;u&quot;,
      	&quot;source&quot;: {
      		...
      	},
      	&quot;ts_ms&quot; : &quot;...&quot;,
      	&quot;before&quot; : {
      		&quot;field1&quot; : &quot;oldvalue1&quot;,
      		&quot;field2&quot; : &quot;oldvalue2&quot;
      	},
      	&quot;after&quot; : {
      		&quot;field1&quot; : &quot;newvalue1&quot;,
      		&quot;field2&quot; : &quot;newvalue2&quot;
      	}
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Many other Kafka Connect source connectors don’t have the luxury of knowing this much about the changes, and instead use a simpler model where each message directly represents the after state of the row.
      This is also what many sink connectors expect, and the Confluent JDBC Sink Connector is not different:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
      	&quot;field1&quot; : &quot;newvalue1&quot;,
      	&quot;field2&quot; : &quot;newvalue2&quot;
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While we think it’s actually a great thing that Debezium CDC connectors provide as much detail as possible, we also make it easy for you to transform Debezium’s &lt;em&gt;&quot;envelope&quot;&lt;/em&gt; format into the &lt;em&gt;&quot;row&quot;&lt;/em&gt; format that is expected by many other connectors.
      Debezium provides a bridge between those two formats in a form of a &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-66%3A+Single+Message+Transforms+for+Kafka+Connect&quot;&gt;single message transform&lt;/a&gt;.
      The &lt;code&gt;UnwrapFromEnvelope&lt;/code&gt; transformation automatically extracts a new row record and thus effectively &lt;em&gt;flattens&lt;/em&gt; the complex record into a simple one consumable by other connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You can use this SMT on the source connector to transform the message &lt;em&gt;before&lt;/em&gt; it is written to Kafka, or you can instead store the source connector’s richer &lt;em&gt;&quot;envelope&quot;&lt;/em&gt; form of the message in Kafka and use this SMT on the sink connector to transform the message &lt;em&gt;after&lt;/em&gt; it is read from Kafka and before it is passed to the sink connector.
      Both options work, and it just depends on whether you find the envelope form of the message useful for other purposes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In our example we apply the SMT at the sink connector using these configuration properties:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;transforms&quot;: &quot;unwrap&quot;,
      &quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.UnwrapFromEnvelope&quot;,&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;delete_records&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#delete_records&quot;&gt;&lt;/a&gt;Delete records&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When the Debezium connector detects a row is deleted, it creates two event messages: a &lt;em&gt;delete&lt;/em&gt; event and a &lt;em&gt;tombstone&lt;/em&gt; message.
      The &lt;em&gt;delete&lt;/em&gt; message has an envelope with the state of the deleted row in the &lt;code&gt;before&lt;/code&gt; field, and an &lt;code&gt;after&lt;/code&gt; field that is &lt;code&gt;null&lt;/code&gt;.
      The &lt;em&gt;tombstone&lt;/em&gt; message contains same key as the &lt;em&gt;delete&lt;/em&gt; message, but the entire message value is &lt;code&gt;null&lt;/code&gt;, and Kafka’s log compaction utilizes this to know that it can remove any earlier messages with the same key.
      A number of sink connectors, including the Confluent’s JDBC Sink Connector, are not expecting these messages and will instead fail if they see either kind of message.
      The &lt;code&gt;UnwrapFromEnvelope&lt;/code&gt; SMT will by default filter out both &lt;em&gt;delete&lt;/em&gt; and &lt;em&gt;tombstone&lt;/em&gt; records, though you can change this if you’re using the SMT and want to keep one or both of these kinds of messages.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;topic_naming&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#topic_naming&quot;&gt;&lt;/a&gt;Topic naming&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Last but not least there is a difference in naming of topics.
      Debezium uses fully qualified naming for target topics representing each table it manages.
      The naming follows the pattern &lt;code&gt;&amp;lt;logical-name&amp;gt;.&amp;lt;database-name&amp;gt;.&amp;lt;table-name&amp;gt;&lt;/code&gt;.
      Kafka Connect JDBC Connector works with simple names &lt;code&gt;&amp;lt;table-name&amp;gt;&lt;/code&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In more complex scenarios the user may deploy the &lt;a href=&quot;https://kafka.apache.org/documentation/streams/&quot;&gt;Kafka Streams&lt;/a&gt; framework to establish elaborated routing between source and target routes.
      In our example we will use a stock &lt;code&gt;RegexRouter&lt;/code&gt; SMT that would route records created by Debezium into topics named according to JDBC Connector schema.
      Again, we could use this SMT in either the source or sink connectors, but for this example we’re going to use it in the source connector so we can choose the names of the Kafka topics where the records will be written.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;transforms&quot;: &quot;route&quot;,
      &quot;transforms.route.type&quot;: &quot;org.apache.kafka.connect.transforms.RegexRouter&quot;,
      &quot;transforms.route.regex&quot;: &quot;([^.]+)\\.([^.]+)\\.([^.]+)&quot;,
      &quot;transforms.route.replacement&quot;: &quot;$3&quot;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;example&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#example&quot;&gt;&lt;/a&gt;Example&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kick the tires and let’s try our example!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;First of all we need to deploy all components.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;export DEBEZIUM_VERSION=0.6
      docker-compose up&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When all components are started we are going to register the JDBC Sink connector writing into PostgreSQL database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ -d @jdbc-sink.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Using this registration request:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
          &quot;name&quot;: &quot;jdbc-sink&quot;,
          &quot;config&quot;: {
              &quot;connector.class&quot;: &quot;io.confluent.connect.jdbc.JdbcSinkConnector&quot;,
              &quot;tasks.max&quot;: &quot;1&quot;,
              &quot;topics&quot;: &quot;customers&quot;,
              &quot;connection.url&quot;: &quot;jdbc:postgresql://postgres:5432/inventory?user=postgresuser&amp;amp;password=postgrespw&quot;,
              &quot;transforms&quot;: &quot;unwrap&quot;,                                                  (1)
              &quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.UnwrapFromEnvelope&quot;,   (1)
              &quot;auto.create&quot;: &quot;true&quot;,                                                   (2)
              &quot;insert.mode&quot;: &quot;upsert&quot;,                                                 (3)
              &quot;pk.fields&quot;: &quot;id&quot;,                                                       (4)
              &quot;pk.mode&quot;: &quot;record_value&quot;                                                (4)
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The request configures these options:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;unwrapping Debezium’s complex format into a simple one&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;automatically create target tables&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;insert a row if it does not exist or update an existing one&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;identify the primary key stored in Kafka’s record value field&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Then the source connector must be set up:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;curl -i -X POST -H &quot;Accept:application/json&quot; -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ -d @source.json&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Using this registration request:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
          &quot;name&quot;: &quot;inventory-connector&quot;,
          &quot;config&quot;: {
              &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
              &quot;tasks.max&quot;: &quot;1&quot;,
              &quot;database.hostname&quot;: &quot;mysql&quot;,
              &quot;database.port&quot;: &quot;3306&quot;,
              &quot;database.user&quot;: &quot;debezium&quot;,
              &quot;database.password&quot;: &quot;dbz&quot;,
              &quot;database.server.id&quot;: &quot;184054&quot;,
              &quot;database.server.name&quot;: &quot;dbserver1&quot;,                                         (1)
              &quot;database.whitelist&quot;: &quot;inventory&quot;,                                           (2)
              &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
              &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot;,
              &quot;transforms&quot;: &quot;route&quot;,                                                       (3)
              &quot;transforms.route.type&quot;: &quot;org.apache.kafka.connect.transforms.RegexRouter&quot;,  (3)
              &quot;transforms.route.regex&quot;: &quot;([^.]+)\\.([^.]+)\\.([^.]+)&quot;,                     (3)
              &quot;transforms.route.replacement&quot;: &quot;$3&quot;                                         (3)
          }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The request configures these options:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;logical name of the database&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;the database we want to monitor&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;an SMT which defines a regular expression matching the topic name &lt;code&gt;&amp;lt;logical-name&amp;gt;.&amp;lt;database-name&amp;gt;.&amp;lt;table-name&amp;gt;&lt;/code&gt; and extracts the third part of it as the final topic name&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Let’s check if the databases are synchronized.
      All the rows of the &lt;code&gt;customers&lt;/code&gt; table should be found in the source database (MySQL) as well as the target database (Postgres):&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER  -p$MYSQL_PASSWORD inventory -e &quot;select * from customers&quot;'
      +------+------------+-----------+-----------------------+
      | id   | first_name | last_name | email                 |
      +------+------------+-----------+-----------------------+
      | 1001 | Sally      | Thomas    | sally.thomas@acme.com |
      | 1002 | George     | Bailey    | gbailey@foobar.com    |
      | 1003 | Edward     | Walker    | ed@walker.com         |
      | 1004 | Anne       | Kretchmar | annek@noanswer.org    |
      +------+------------+-----------+-----------------------+
      
      docker-compose exec postgres bash -c 'psql -U $POSTGRES_USER $POSTGRES_DB -c &quot;select * from customers&quot;'
       last_name |  id  | first_name |         email
      -----------+------+------------+-----------------------
       Thomas    | 1001 | Sally      | sally.thomas@acme.com
       Bailey    | 1002 | George     | gbailey@foobar.com
       Walker    | 1003 | Edward     | ed@walker.com
       Kretchmar | 1004 | Anne       | annek@noanswer.org&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With the connectors still running, we can add a new row to the MySQL database and then check that it was replicated into the PostgreSQL database:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER  -p$MYSQL_PASSWORD inventory'
      mysql&amp;gt; insert into customers values(default, 'John', 'Doe', 'john.doe@example.com');
      Query OK, 1 row affected (0.02 sec)
      
      docker-compose exec -postgres bash -c 'psql -U $POSTGRES_USER $POSTGRES_DB -c &quot;select * from customers&quot;'
       last_name |  id  | first_name |         email
      -----------+------+------------+-----------------------
      ...
      Doe        | 1005 | John       | john.doe@example.com
      (5 rows)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot;&gt;&lt;/a&gt;Summary&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We set up a simple streaming data pipeline to replicate data in near real-time from a MySQL database to a PostgreSQL database. We accomplished this using Kafka Connect, the Debezium MySQL source connector, the Confluent JDBC sink connector, and a few SMTs — all without having to write any code.
      And since it is a streaming system, it will continue to capture all changes made to the MySQL database and replicating them in near real time.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In a future blog post we will reproduce the same scenario with Elasticsearch as a target for events.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/09/21/debezium-0-6-0-released/</id>
    <title>Debezium 0.6 Is Out</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2017-09-21T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/09/21/debezium-0-6-0-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      What&#8217;s better than getting Java 9?
      Getting Java 9 and a new version of Debezium at the same time!
      So it&#8217;s with great happiness that I&#8217;m announcing the release of Debezium 0.6 today.
      
      
      
      
      What&#8217;s in it?
      
      
      Debezium is now built against and tested with Apache Kafka 0.11.0.
      Also the Debezium Docker images have been updated do that version (DBZ-305).
      You should make sure to read the Kafka update guide when upgrading from an earlier version.
      
      
      To improve integration with existing Kafka sink connectors such as the JDBC sink connector or the Elasticsearch connector,
      Debezium provides a new single message transformation (DBZ-226).
      This SMT converts Debezium&#8217;s CDC event structure into...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;What’s better than getting &lt;a href=&quot;http://openjdk.java.net/projects/jdk9/&quot;&gt;Java 9&lt;/a&gt;?
      Getting Java 9 and a new version of Debezium at the same time!
      So it’s with great happiness that I’m announcing the release of &lt;strong&gt;Debezium 0.6&lt;/strong&gt; today.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_in_it&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_in_it&quot;&gt;&lt;/a&gt;What’s in it?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is now built against and tested with Apache Kafka 0.11.0.
      Also the Debezium Docker images have been updated do that version (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-305&quot;&gt;DBZ-305&lt;/a&gt;).
      You should make sure to read the Kafka &lt;a href=&quot;https://kafka.apache.org/documentation/#upgrade&quot;&gt;update guide&lt;/a&gt; when upgrading from an earlier version.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;To improve integration with existing Kafka sink connectors such as the &lt;a href=&quot;https://docs.confluent.io/current/connect/connect-jdbc/docs/sink_connector.html&quot;&gt;JDBC sink connector&lt;/a&gt; or the &lt;a href=&quot;https://docs.confluent.io/current/connect/connect-elasticsearch/docs/elasticsearch_connector.html&quot;&gt;Elasticsearch&lt;/a&gt; connector,
      Debezium provides a new &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/debezium-core/src/main/java/io/debezium/transforms/UnwrapFromEnvelope.java&quot;&gt;single message transformation&lt;/a&gt; (&lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-226&quot;&gt;DBZ-226&lt;/a&gt;).
      This SMT converts Debezium’s CDC event structure into a more conventional structure commonly used in other sink and non-CDC source connectors where the message represents the state of the inserted or updated row, or null in the case of a deleted row.
      This lets your for instance capture the changes from a table in MySQL and update a corresponding table in a Postgres database accordingly.
      We’ll provide a complete example showing the usage of that new SMT in the next few days.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you are doing the Debezium &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;, you will like the new &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/tutorial&quot;&gt;Docker Compose set-up&lt;/a&gt; provided in the examples repo (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-127&quot;&gt;DBZ-127&lt;/a&gt;).
      This lets you start all the required Docker containers with a single command.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;new_connector_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_connector_features&quot;&gt;&lt;/a&gt;New connector features&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Now let’s take a look at some of the changes around the specific Debezium connectors.
      The &lt;strong&gt;MySQL connector&lt;/strong&gt; has seen multiple improvements, e.g.:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Snapshot consistency wasn’t guaranteed before in some corner cases (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-210&quot;&gt;DBZ-210&lt;/a&gt;); that’s fixed now&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;DEC and FIXED types supported in the DDL parser (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-359&quot;&gt;DBZ-359&lt;/a&gt;; thanks to &lt;a href=&quot;https://github.com/ooooorz&quot;&gt;Liu Hanlin&lt;/a&gt;!)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;UNION clause supported for ALTER TABLE (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-346&quot;&gt;DBZ-346&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For the &lt;strong&gt;MongoDB connector&lt;/strong&gt;, the way of serializing ids into the key payload of CDC events has changed (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-306&quot;&gt;DBZ-306&lt;/a&gt;).
      The new format allows to read back ids into the correct type.
      We also took the opportunity and made the id field name consistent with the other connectors, i.e. it’s &quot;id&quot; now.
      &lt;strong&gt;Note:&lt;/strong&gt; that change may break existing consumers, so some work on your end may be required, depending on the implementation of your consumer.
      The details are discussed in the &lt;a href=&quot;http://debezium.io/docs/releases/#_breaking_changes&quot;&gt;release notes&lt;/a&gt; and the format of message keys is described in depth in the &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/#change-events-key&quot;&gt;connector documentation&lt;/a&gt;.
      Kudos to &lt;a href=&quot;https://github.com/hpgrahsl&quot;&gt;Hans-Peter Grahsl&lt;/a&gt; who contributed on this feature!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Another nice improvement for this connector is support for SSL connections (&lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-343&quot;&gt;DBZ-343&lt;/a&gt;).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, the &lt;strong&gt;Postgres connector&lt;/strong&gt; learned some new tricks, too:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Support for variable-width numeric columns (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-318&quot;&gt;DBZ-318&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Views won’t stop the connector any more (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-319&quot;&gt;DBZ-319&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Warnings and notifications emitted by the server are correctly forwarded to the log (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-279&quot;&gt;DBZ-279&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please refer to the &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-6-0&quot;&gt;changelog&lt;/a&gt; for an overview of all the 20 issues fixed in Debezium 0.6.0.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next?&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;High on our agenda is exploring support for Oracle (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-20&quot;&gt;DBZ-20&lt;/a&gt;).
      We are also looking into using another logical decoding plug-in (wal2json) for the Postgres connector, which would enable to use Debezium with Postgres instances running on Amazon RDS.
      Another feature being worked on by community member &lt;a href=&quot;https://github.com/mtagle&quot;&gt;Moira Tagle&lt;/a&gt; is support for updates to the &lt;code&gt;table.whitelist&lt;/code&gt; for existing connector instances.
      Finally, we’ve planned to test and adapt the existing MySQL connector for providing CDC functionality to MariaDB.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium 0.7 with one or more out of those features as well as hopefully some others will be released later this year.
      We’ll likely also do further 0.6.x releases with bug fixes as required.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;You’d like to contribute?
      That’s great - let us know and we’ll get you started.
      Check out the details below on how to get in touch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/08/17/debezium-0-5-2-is-out/</id>
    <title>Debezium 0.5.2 Is Out</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2017-08-17T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/08/17/debezium-0-5-2-is-out/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m very happy to announce the release of Debezium 0.5.2!
      
      
      As the previous release, the 0.5.2 release fixes several bugs in the MySQL, Postgres and MongoDB connectors.
      But there are also several new features and options:
      
      
      
      
      The decimal.handling.mode option already known from the MySQL connector is now also supported for PostgreSQL (DBZ-337).
      It lets you control how NUMERIC and DECIMAL columns are represented in change events (either using Kafka&#8217;s Decimal type or as double).
      
      
      The MongoDB connector supports the options database.whitelist and database.blacklist now (DBZ-302)
      
      
      The PostgreSQL connector can deal with array-typed columns as well as with quoted identifiers for tables, schemas etc. (DBZ-297, DBZ-298)
      
      
      The Debezium...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m very happy to announce the release of &lt;strong&gt;Debezium 0.5.2&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As the previous release, the 0.5.2 release fixes several bugs in the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL&lt;/a&gt;, &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql/&quot;&gt;Postgres&lt;/a&gt; and &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/&quot;&gt;MongoDB&lt;/a&gt; connectors.
      But there are also several new features and options:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;The &lt;code&gt;decimal.handling.mode&lt;/code&gt; option already known from the MySQL connector is now also supported for PostgreSQL (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-337&quot;&gt;DBZ-337&lt;/a&gt;).
      It lets you control how &lt;code&gt;NUMERIC&lt;/code&gt; and &lt;code&gt;DECIMAL&lt;/code&gt; columns are represented in change events (either using Kafka’s &lt;code&gt;Decimal&lt;/code&gt; type or as &lt;code&gt;double&lt;/code&gt;).&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The MongoDB connector supports the options &lt;code&gt;database.whitelist&lt;/code&gt; and &lt;code&gt;database.blacklist&lt;/code&gt; now (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-302&quot;&gt;DBZ-302&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The PostgreSQL connector can deal with array-typed columns as well as with quoted identifiers for tables, schemas etc. (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-297&quot;&gt;DBZ-297&lt;/a&gt;, &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-298&quot;&gt;DBZ-298&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The Debezium Docker images run on Red Hat’s &lt;a href=&quot;https://www.openshift.com/&quot;&gt;OpenShift&lt;/a&gt; cloud environment (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-267&quot;&gt;DBZ-267&lt;/a&gt;)&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Speaking about the Docker images, we’ve set up &lt;em&gt;nightly&lt;/em&gt; tags for the &lt;a href=&quot;https://hub.docker.com/u/debezium/&quot;&gt;Debezium images on Docker Hub&lt;/a&gt;,
      allowing you to grab the latest improvements even before an official release has been cut.
      The connector archives are also deployed to the &lt;a href=&quot;https://oss.sonatype.org/content/repositories/snapshots/io/debezium/&quot;&gt;Sonatype OSS Maven repository&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, we’ve spent some time to extend the documentation on some things not covered before:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;http://debezium.io/docs/configuration/avro/&quot;&gt;Avro Serialization&lt;/a&gt; describes how to use the use the Avro converter and the Confluent Schema Registry instead of the JSON converter instead of the default JSON converter for serializing change events, resulting in much smaller message sizes;
      The Avro converter itself has also been added to the Debezium Docker image for Kafka Connect, so you can use it right away&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;&lt;a href=&quot;http://debezium.io/docs/configuration/topic-routing/&quot;&gt;Topic Routing&lt;/a&gt; describes how to use Debezium’s &lt;code&gt;ByLogicalTableRouter&lt;/code&gt; single message transformation (SMT) for routing the change events from multiple tables into a single topic, which for instance is very useful when working with sharded tables&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please refer to the &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/CHANGELOG.md#052&quot;&gt;changelog&lt;/a&gt; for an overview of all the 19 issues fixed in Debezium 0.5.2.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The following people from the community have sent in pull requests for this release:
      &lt;a href=&quot;https://github.com/emrul&quot;&gt;Emrul Islam&lt;/a&gt;, &lt;a href=&quot;https://github.com/ekreiser&quot;&gt;Eric S. Kreiser&lt;/a&gt;, &lt;a href=&quot;https://github.com/xenji&quot;&gt;Mario Mueller&lt;/a&gt;, &lt;a href=&quot;https://github.com/mcapitanio&quot;&gt;Matteo Capitanio&lt;/a&gt;, &lt;a href=&quot;https://github.com/omarsmak&quot;&gt;Omar Al-Safi&lt;/a&gt; and &lt;a href=&quot;https://github.com/Satyajitv&quot;&gt;Satyajit Vegesna&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks a lot to you and everyone else in the community for contributing to Debezium via feature requests, bug reports, discussions and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The next version of Debezium will be 0.6 (planned for September).
      This release is planned to bring the upgrade to Kafka 0.11.
      We’ll also look into an SMT for transforming the change events emitted by Debezium into a flat representation, which for instance will be very useful in conjunction with the JDBC sink connector.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;While 0.6 is planned to be more of a &quot;stabilization release&quot;, 0.7 should bring a long-awaited major feature:
      we’ve planned to explore support for Oracle and hopefully will do an initial release of a Debezium connector for that database.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In other words, exciting times are ahead!
      If you’d like to get involved, let us know.
      Check out the details below on how to get in touch.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/06/12/debezium-0-5-1-released/</id>
    <title>Debezium 0.5.1 Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2017-06-12T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/06/12/debezium-0-5-1-released/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      It&#8217;s my pleasure to announce the release of Debezium 0.5.1!
      
      
      This release fixes several bugs in the MySQL, Postgres and MongoDB connectors.
      There&#8217;s also support for some new datatypes: POINT on MySQL (DBZ-222) and TSTZRANGE on Postgres (DBZ-280).
      This release is a drop-in replacement for 0.5.0, upgrading is recommended to all users.
      
      
      Note that in the&#8201;&#8212;&#8201;rather unlikely&#8201;&#8212;&#8201;case that you happened to enable Debezium for all the system tables of MySQL,
      any configured table filters will be applied to these system tables now, too (DBZ-242).
      This may require an adjustment of your filters if you indeed wanted to capture all system tables but only selected non-system tables.
      
      
      Please...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s my pleasure to announce the release of &lt;strong&gt;Debezium 0.5.1&lt;/strong&gt;!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This release fixes several bugs in the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL&lt;/a&gt;, &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql/&quot;&gt;Postgres&lt;/a&gt; and &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/&quot;&gt;MongoDB&lt;/a&gt; connectors.
      There’s also support for some new datatypes: &lt;code&gt;POINT&lt;/code&gt; on MySQL (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-222&quot;&gt;DBZ-222&lt;/a&gt;) and &lt;code&gt;TSTZRANGE&lt;/code&gt; on Postgres (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-280&quot;&gt;DBZ-280&lt;/a&gt;).
      This release is a drop-in replacement for 0.5.0, upgrading is recommended to all users.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Note that in the — rather unlikely — case that you happened to enable Debezium for all the system tables of MySQL,
      any configured table filters will be applied to these system tables now, too (&lt;a href=&quot;https://issues.jboss.org/browse/DBZ-242&quot;&gt;DBZ-242&lt;/a&gt;).
      This may require an adjustment of your filters if you indeed wanted to capture all system tables but only selected non-system tables.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Please refer to the &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/CHANGELOG.md#051&quot;&gt;changelog&lt;/a&gt; for an overview of all the 29 issues fixed in Debezium 0.5.1.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Docker image containing &lt;a href=&quot;https://hub.docker.com/r/debezium/connect/&quot;&gt;Kafka Connect and all the Debezium 0.5.x connectors&lt;/a&gt;
      as well as the image containing &lt;a href=&quot;https://hub.docker.com/r/debezium/postgres/&quot;&gt;Postgres and the Debezium logical decoding plug-in&lt;/a&gt; have been updated to 0.5.1, too.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As Debezium continues to evolve and grow, the number of people contributing to the project is also going up.
      The following people have sent in pull requests for this release:
      &lt;a href=&quot;https://github.com/arosenber&quot;&gt;Aaron Rosenberg&lt;/a&gt;, &lt;a href=&quot;https://github.com/CyberDem0n&quot;&gt;Alexander Kukushkin&lt;/a&gt;, &lt;a href=&quot;https://github.com/brendanmaguire&quot;&gt;Brendan Maguire&lt;/a&gt;, &lt;a href=&quot;https://github.com/DuncanSands&quot;&gt;Duncan Sands&lt;/a&gt;, &lt;a href=&quot;https://github.com/dasl-&quot;&gt;David Leibovic&lt;/a&gt;, &lt;a href=&quot;https://github.com/jpechane&quot;&gt;Jiri Pechanec&lt;/a&gt;, &lt;a href=&quot;https://github.com/nacivida&quot;&gt;nacivida&lt;/a&gt;, &lt;a href=&quot;https://github.com/omarsmak&quot;&gt;Omar Al-Safi&lt;/a&gt;, &lt;a href=&quot;https://github.com/rhauch&quot;&gt;Randall Hauch&lt;/a&gt; and &lt;a href=&quot;https://github.com/tombentley&quot;&gt;Tom Bentley&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks a lot to you and everyone else in the community contributing via feature requests, bug reports, discussions and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve planned to do further bug fix releases for the 0.5.x line.
      Specifically, we’ll release a fix for &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-217&quot;&gt;DBZ-217&lt;/a&gt; shortly,
      which is about the MySQL connector stumbling when getting across a corrupt event in the binlog.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In parallel we’re looking into Debezium connectors for &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-40&quot;&gt;SQL Server&lt;/a&gt; and &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-137&quot;&gt;Oracle&lt;/a&gt;.
      While we cannot promise anything yet in terms of when these will be ready to be published, we hope to have at least one of them ready some time soon.
      Stay tuned and get involved!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
      so applications can see and respond almost instantly to each committed row-level change in the databases.
      Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
      Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
      ensuring that all events are processed correctly and completely.
      Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
      Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
      or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
      All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
      so build the code locally and help us improve ours existing connectors and add even more connectors.
      If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/04/27/hello-debezium/</id>
    <title>Hello Debezium!</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2017-04-27T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/04/27/hello-debezium/" rel="alternate" type="text/html" />
    <author>
      <name>Gunnar Morling</name>
    </author>
    <category term="community"></category>
    <category term="news"></category>
    <summary>
      
      When I first learned about the Debezium project last year, I was very excited about it right away.
      
      
      I could see how this project would be very useful for many people out there and I was very impressed by the professional way it was set up:
      a solid architecture for change data capture based on Apache Kafka, a strong focus on robustness and correctness also in the case of failures, the overall idea of creating a diverse eco-system of CDC connectors.
      All that based on the principles of open source, combined with extensive documentation from day one, a friendly and welcoming web site...
    </summary>
    <content type="html">
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When I first learned about the Debezium project last year, I was very excited about it right away.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I could see how this project would be very useful for many people out there and I was very impressed by the professional way it was set up:
      a solid architecture for change data capture based on Apache Kafka, a strong focus on robustness and correctness also in the case of failures, the overall idea of creating a diverse eco-system of CDC connectors.
      All that based on the principles of open source, combined with extensive documentation from day one, a friendly and welcoming web site and a great getting-started experience.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So you can imagine that I was more than enthusiastic about the opportunity to take over the role of Debezium’s project lead.
      Debezium and CDC have close links to some data-centric projects I’ve been previously working on and also tie in with ideas I’ve been pursuing around CQRS, even sourcing and denormalization.
      As core member of the &lt;a href=&quot;http://hibernate.org/&quot;&gt;Hibernate team&lt;/a&gt; at Red Hat, I’ve implemented the initial Elasticsearch support for &lt;a href=&quot;http://hibernate.org/search/&quot;&gt;Hibernate Search&lt;/a&gt;
      (which deals with full-text index updates via JPA/Hibernate).
      I’ve also contributed to &lt;a href=&quot;http://hibernate.org/ogm/&quot;&gt;Hibernate OGM&lt;/a&gt; - a project which connects JPA and the world of NoSQL.
      One of the plans for OGM is to create a declarative denormalization engine for creating read models optimized for specific use cases.
      It will be very interesting to see how this plays together with the capabilities provided by Debezium.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Currently I am serving as the lead of the &lt;a href=&quot;http://beanvalidation.org/&quot;&gt;Bean Validation 2.0&lt;/a&gt; specification (JSR 380) as well as its reference implementation &lt;a href=&quot;http://hibernate.org/validator/&quot;&gt;Hibernate Validator&lt;/a&gt;.
      Two other projects close to my heart are &lt;a href=&quot;http://mapstruct.org/&quot;&gt;MapStruct&lt;/a&gt; - a code generator for bean-to-bean mappings - and &lt;a href=&quot;https://github.com/moditect/moditect&quot;&gt;ModiTect&lt;/a&gt;, which is tooling for Java 9 modules and their descriptors.
      In general, I’m a strong believer into the idea of open source and I just love it to work with folks from all over the world to create useful tools and libraries.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Joining the Debezium community and working on change data capture is a great next step.
      There are so many things to do: connectors for Oracle, SQL Server and Cassandra,
      but also things like an entity join processor which would allow to step from row-level events to more aggregated business-level events (e.g. for updating a combined search index for an order and its order lines) or tooling for managing and visualizing histories of event schema changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;One thing I’d like to emphasize is that the project’s direction generally isn’t going to change very much.
      Red Hat is fully committed to maintaining and evolving the project together with you, the Debezium community.
      The ride really has just begun!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Finally, let me say a huge thank you to Randall for his excellent work!
      You’ve been a true role model for going from an idea over pitching it - within Red Hat as well as within the wider community - to building a steadily growing and evolving project.
      It’s stating the obvious, but it wouldn’t be for Debezium without you.
      Thanks for everything and looking forward very much to working with you and the community on this great project!&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Onwards,&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;--Gunnar&lt;/p&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/04/26/Debezium-evolving/</id>
    <title>Debezium Evolving</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2017-04-26T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/04/26/Debezium-evolving/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="community"></category>
    <category term="news"></category>
    <summary>
      
      Just before I started the Debezium project in early 2016, Martin Kleppmann gave several presentations about turning the database inside out and how his Bottled Water project demonstrated the importantance that change data capture can play in using Kafka for stream processing. Then Kafka Connect was announced, and at that point it seemed obvious to me that Kafka Connect was the foundation upon which practical and reusable change data capture can be built. As these techniques and technologies were becoming more important to Red Hat, I was given the opportunity to start a new open source project and community around...
    </summary>
    <content type="html">
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Just before I started the Debezium project in early 2016, &lt;a href=&quot;https://martin.kleppmann.com&quot;&gt;Martin Kleppmann&lt;/a&gt; gave several presentations about &lt;a href=&quot;https://martin.kleppmann.com/2015/03/04/turning-the-database-inside-out.html&quot;&gt;turning the database inside out&lt;/a&gt; and how his &lt;a href=&quot;https://martin.kleppmann.com/2015/04/23/bottled-water-real-time-postgresql-kafka.html&quot;&gt;Bottled Water&lt;/a&gt; project demonstrated the importantance that change data capture can play in using Kafka for stream processing. Then Kafka Connect was &lt;a href=&quot;https://www.confluent.io/blog/announcing-kafka-connect-building-large-scale-low-latency-data-pipelines/&quot;&gt;announced&lt;/a&gt;, and at that point it seemed obvious to me that Kafka Connect was the foundation upon which practical and reusable change data capture can be built. As these techniques and technologies were becoming more important to &lt;a href=&quot;https://www.redhat.com/&quot;&gt;Red Hat&lt;/a&gt;, I was given the opportunity to start a new open source project and community around building great CDC connectors for a variety of databases management systems.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Over the past few years, we have created Kafka Connect connectors for &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL&lt;/a&gt;, then &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/&quot;&gt;MongoDB&lt;/a&gt;, and most recently &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql/&quot;&gt;PostgreSQL&lt;/a&gt;. Each were initially limited and had a number of problems and issues, but over time more and more people have tried the connectors, asked questions, answered questions, mentioned &lt;a href=&quot;https://twitter.com/search?vertical=default&amp;amp;q=debezium&amp;amp;src=typd&quot;&gt;Debezium on Twitter&lt;/a&gt;, tested connectors in their own environments, reported problems, fixed bugs, discussed limitations and potential new features, implemented enhancements and new features, improved the documentation, and wrote blog posts. Simply put, people with similar needs and interests have worked together and have formed a community. Additional connectors for &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-137&quot;&gt;Oracle&lt;/a&gt; and &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-40&quot;&gt;SQL Server&lt;/a&gt; are in the works, but could use some help to move things along more quickly.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It’s really exciting to see how far we’ve come and how the Debezium community continues to evolve and grow. And it’s perhaps as good a time as any to hand the reigns over to someone else. In fact, after nearly 10 wonderful years at Red Hat, I’m making a bigger change and as of today am part of &lt;a href=&quot;https://www.confluent.io&quot;&gt;Confluent’s&lt;/a&gt; engineering team, where I expect to play a more active role in the broader &lt;a href=&quot;https://kafka.apache.org&quot;&gt;Kafka&lt;/a&gt; community and more directly with Kafka Connect and Kafka Streams. I &lt;strong&gt;definitely&lt;/strong&gt; plan to stay involved in the Debezium community, but will no longer be leading the project. That role will instead be filled by &lt;a href=&quot;https://github.com/gunnarmorling/&quot;&gt;Gunnar Morling&lt;/a&gt;, who’s recently joined the Debezium community but has extensive experience in open source, the &lt;a href=&quot;http://in.relation.to/gunnar-morling/&quot;&gt;Hibernate community&lt;/a&gt;, and the &lt;a href=&quot;http://beanvalidation.org&quot;&gt;Bean Validation&lt;/a&gt; specification effort. Gunnar is a great guy and an excellent developer, and will be an excellent lead for the Debezium community.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Will the Debezium project change? To some degree it will always continue to evolve just as it has from the very beginning, and that’s a healthy thing. But a lot is staying the same. Red Hat remains committed to the Debezium project, and will continue its sponsorship and community-oriented governance that has worked so well from the beginning. And just as importantly, we the community are still here and will continue building the best open source CDC connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;So keep up the great work, and look for and take advantage of opportunities to &lt;a href=&quot;http://debezium.io/community/&quot;&gt;become more involved&lt;/a&gt; in Debezium. Please give a warm welcome to Gunnar by introducing yourself in the &lt;a href=&quot;https://gitter.im/debezium/dev&quot;&gt;developer&lt;/a&gt; and / or &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;user&lt;/a&gt; chat rooms and mention how you’re using Debezium and what the Debezium community means to you.&lt;/p&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/03/27/Debezium-0-5-0-Released/</id>
    <title>Debezium 0.5.0 Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2017-03-27T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/03/27/Debezium-0-5-0-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.5.0 is now available for use with Kafka Connect 0.10.2.0. This release also includes a few fixes for the MySQL connector. See the release notes for specifics on these changes, and be sure to check out the Kafka documentation for compatibility with the version of the Kafka broker that you are using.
      
      
      Kafka Connect 0.10.2.0 comes with a significant new feature called Single Message Transforms, and you can now use them with Debezium connectors. SMTs allow you to modify the messages produced by Debezium connectors and any oher Kafka Connect source connectors, before those messages...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.5.0&lt;/strong&gt; is now available for use with &lt;strong&gt;Kafka Connect 0.10.2.0&lt;/strong&gt;. This release also includes a few fixes for the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL connector&lt;/a&gt;. See the &lt;a href=&quot;http://debezium.io/docs/releases/&quot;&gt;release notes&lt;/a&gt; for specifics on these changes, and be sure to check out the &lt;a href=&quot;https://kafka.apache.org/documentation/#upgrade&quot;&gt;Kafka documentation&lt;/a&gt; for compatibility with the version of the Kafka broker that you are using.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka Connect 0.10.2.0 comes with a significant new feature called &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-66%3A+Single+Message+Transforms+for+Kafka+Connect&quot;&gt;Single Message Transforms&lt;/a&gt;, and you can now use them with Debezium connectors. SMTs allow you to modify the messages produced by Debezium connectors and any oher Kafka Connect source connectors, before those messages are written to Kafka. SMTs can also be used with Kafka Connect sink connectors to modify the messages &lt;em&gt;before&lt;/em&gt; the sink connectors processes them. You can use SMTs to filter out or mask specific fields, add new fields, modify existing fields, change the topic and/or topic partition to which the messages are written, and even more. And you can even chain multiple SMTs together.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka Connect comes with a number of built-in SMTs that you can simply configure and use, but you can also create your own SMT implementations to do more complex and interesting things. For example, although Debezium connectors normally map all of the changes in each table (or collection) to separate topics, you can write a custom SMT that uses a completely different mapping between tables and topics and even add fields to message keys and/or values. Using your new SMT is also very easy - simply put it on the Kafka Connect classpath and update the connector configuration to use it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also added &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.5&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Sanjay and everyone in the community for their help with this release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll continue to improve the MongoDB, MySQL, and PostgreSQL connectors and pushing out 0.5.x releases with fixes. And we’re still working on connectors for &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-40&quot;&gt;SQL Server&lt;/a&gt; and &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-137&quot;&gt;Oracle&lt;/a&gt;. Stay tuned and get involved!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/03/17/Debezium-0-4-1-Released/</id>
    <title>Debezium 0.4.1 Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2017-03-17T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/03/17/Debezium-0-4-1-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="rds"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.4.1 is now available for use with Kafka Connect 0.10.1.1. This release includes several fixes for the MongoDB connector and MySQL connector, including improved support for Amazon RDS and Amazon Aurora (MySQL compatibility). See the release notes for specifics on these changes.
      
      
      We&#8217;ve also updated the Debezium Docker images labelled 0.4 and latest, which we use in our tutorial.
      
      
      Thanks to Jan, Horia, David, Josh, Johan, Sanjay, Saulius, and everyone in the community for their help with this release, issues, discussions, contributions, and questions!
      
      
      
      
      What&#8217;s next
      
      
      Kafka 0.10.2.0 is out, so we plan to release 0.5.0 next week...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.4.1&lt;/strong&gt; is now available for use with Kafka Connect 0.10.1.1. This release includes several fixes for the &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/&quot;&gt;MongoDB connector&lt;/a&gt; and &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL connector&lt;/a&gt;, including improved support for &lt;a href=&quot;https://aws.amazon.com/rds/mysql/&quot;&gt;Amazon RDS&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/rds/aurora/&quot;&gt;Amazon Aurora (MySQL compatibility)&lt;/a&gt;. See the &lt;a href=&quot;http://debezium.io/docs/releases/&quot;&gt;release notes&lt;/a&gt; for specifics on these changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.4&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Jan, Horia, David, Josh, Johan, Sanjay, Saulius, and everyone in the community for their help with this release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka 0.10.2.0 is out, so we plan to release 0.5.0 next week with all of the changes/fixes in 0.4.1 but with support for Kafka 0.10.2.0. We’ll then continue to improve the MongoDB, MySQL, and PostgreSQL connectors and pushing out 0.5.x releases. Stay tuned and get involved!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/02/22/Debezium-at-WePay/</id>
    <title>Streaming databases in realtime with MySQL, Debezium, and Kafka</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2017-02-22T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/02/22/Debezium-at-WePay/" rel="alternate" type="text/html" />
    <author>
      <name>Chris Riccomini</name>
    </author>
    <category term="mysql"></category>
    <summary>
      
      
      
      This post originally appeared on the WePay Engineering blog.
      
      
      Change data capture has been around for a while, but some recent developments in technology have given it new life. Notably, using Kafka as a backbone to stream your database data in realtime has become increasingly common.
      
      
      If you&#8217;re wondering why you might want to stream database changes into Kafka, I highly suggest reading The Hardest Part About Microservices: Your Data. At WePay, we wanted to integrate our microservices and downstream datastores with each other, so every system could get access to the data that it needed. We use Kafka as our data...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;strong&gt;&lt;em&gt;This post originally appeared on the &lt;a href=&quot;https://wecode.wepay.com/posts/streaming-databases-in-realtime-with-mysql-debezium-kafka&quot;&gt;WePay Engineering blog&lt;/a&gt;.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Change_data_capture&quot;&gt;Change data capture&lt;/a&gt; has been around for a while, but some recent developments in technology have given it new life. Notably, using &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; as a backbone to stream your database data in realtime has become &lt;a href=&quot;https://github.com/wushujames/mysql-cdc-projects/wiki&quot;&gt;increasingly common&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’re wondering why you might want to stream database changes into Kafka, I highly suggest reading &lt;a href=&quot;http://blog.christianposta.com/microservices/the-hardest-part-about-microservices-data/&quot;&gt;The Hardest Part About Microservices: Your Data&lt;/a&gt;. At WePay, we wanted to integrate our microservices and downstream datastores with each other, so every system could get access to the data that it needed. We use Kafka as our data integration layer, so we needed a way to get our database data into it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Last year, &lt;a href=&quot;https://www.yelp.com/engineering&quot;&gt;Yelp’s engineering team&lt;/a&gt; published an excellent &lt;a href=&quot;https://engineeringblog.yelp.com/2016/11/open-sourcing-yelps-data-pipeline.html&quot;&gt;series of posts&lt;/a&gt; on their data pipeline. These included a discussion on how they &lt;a href=&quot;https://engineeringblog.yelp.com/2016/08/streaming-mysql-tables-in-real-time-to-kafka.html&quot;&gt;stream MySQL data into Kafka&lt;/a&gt;. Their architecture involves a series of homegrown pieces of software to accomplish the task, notably &lt;a href=&quot;https://github.com/Yelp/schematizer&quot;&gt;schematizer&lt;/a&gt; and &lt;a href=&quot;https://github.com/Yelp/mysql_streamer&quot;&gt;MySQL streamer&lt;/a&gt;. The write-up triggered a thoughtful post on Debezium’s blog about a proposed equivalent architecture using &lt;a href=&quot;http://docs.confluent.io/3.1.1/connect/&quot;&gt;Kafka connect&lt;/a&gt;, &lt;a href=&quot;http://debezium.io/&quot;&gt;Debezium&lt;/a&gt;, and &lt;a href=&quot;http://docs.confluent.io/3.1.1/schema-registry/docs/&quot;&gt;Confluent’s schema registry&lt;/a&gt;. This proposed architecture is what we’ve been implementing at WePay, and this post describes how we leverage Debezium and Kafka connect to stream our MySQL databases into Kafka.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;architecture&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#architecture&quot;&gt;&lt;/a&gt;Architecture&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The flow of data starts with each microservice’s MySQL database. These databases run in &lt;a href=&quot;https://cloud.google.com/&quot;&gt;Google Cloud&lt;/a&gt; as &lt;a href=&quot;https://cloud.google.com/sql/&quot;&gt;CloudSQL&lt;/a&gt; MySQL instances &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/replication-gtids.html&quot;&gt;with GTIDs enabled&lt;/a&gt;. We’ve set up a downstream MySQL cluster specifically for Debezium. Each CloudSQL instance replicates its data into the Debezium cluster, which consists of two MySQL machines: a primary (active) server and secondary (passive) server. This single Debezium cluster is an operational trick to make it easier for us to operate Debezium. Rather than having Debezium connect to dozens of microservice databases directly, we can connect to just a single database. This also isolates Debezium from impacting the production OLTP workload that the master CloudSQL instances are handling.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We run one Debezium connector (in &lt;a href=&quot;http://docs.confluent.io/2.0.0/connect/userguide.html#distributed-mode&quot;&gt;distributed mode&lt;/a&gt; on the Kafka connect framework) for each microservice database. Again, the goal here is isolation. Theoretically, we could run a single Debezium connector that produces messages for all databases (since all microservice databases are in the Debezium cluster). This approach would actually be more resource efficient since each Debezium connector has to read MySQL’s entire &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/binary-log.html&quot;&gt;binlog&lt;/a&gt; anyway. We opted not to do this because we wanted to be able to bring Debezium connectors up and down, and configure them differently for each microservice DB.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The Debezium connectors feed the MySQL messages into Kafka (and add their schemas to the Confluent schema registry), where downstream systems can consume them. We use our Kafka connect &lt;a href=&quot;https://wecode.wepay.com/posts/kafka-bigquery-connector&quot;&gt;BigQuery connector&lt;/a&gt; to load the MySQL data into BigQuery using BigQuery’s &lt;a href=&quot;https://cloud.google.com/bigquery/streaming-data-into-bigquery&quot;&gt;streaming API&lt;/a&gt;. This gives us a data warehouse in BigQuery that is usually less than 30 seconds behind the data that’s in production. Other microservices, stream processors, and data infrastructure consume the feeds as well.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;imageblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;img src=&quot;https://wecode.wepay.com/assets/2017-02-21-streaming-databases-in-realtime-with-mysql-debezium-kafka/debezium-architecture.png&quot; alt=&quot;Debezium architecture&quot; /&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#debezium&quot;&gt;&lt;/a&gt;Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The remainder of this post will focus on Debezium (the DBZ boxes in the diagram above), and how we configure and operate it. Debezium works by connecting to MySQL and pretending to be a replica. MySQL sends its replication data to Debezium, thinking it’s actually funneling data to another downstream MySQL instance. Debezium then takes the data, converts the schemas from MySQL schemas to &lt;a href=&quot;https://kafka.apache.org/0100/javadoc/org/apache/kafka/connect/data/Struct.html&quot;&gt;Kafka connect structures&lt;/a&gt;, and forwards them to Kafka.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;adding_new_databases&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#adding_new_databases&quot;&gt;&lt;/a&gt;Adding new databases&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;When a new microservice with a CloudSQL database comes online, we want to get that data into Kafka. The first step in the process is to load the data into the Debezium MySQL cluster. This involves several steps:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;Take a MySQL dump of the data in the microservice DB.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Pause the secondary Debezium MySQL DB.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Load the MySQL dump into the secondary Debezium MySQL DB.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Reset &lt;code&gt;GTID_PURGED&lt;/code&gt; parameter to include the GTID from the new DB dump.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Unpause the secondary Debezium MySQL DB.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Update HA Proxy to point to the secondary, which now becomes the primary.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Follow steps 2-5 for the old primary instance (now secondary).&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The actual commands that we run are:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight nowrap&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;# (1) Take a dump of the database we wish to add.
      $ mydumper  --host=123.123.123.123 --port=3306 --user=foo --password=*********  -B log --trx-consistency-only  --triggers --routines -o /mysqldata/new_db/ -c -L mydumper.log
      
      # (2) Stop all replication on the secondary Debezium cluster.
      $ mysql&amp;gt; STOP SLAVE for channel 'foo';
      $ mysql&amp;gt; STOP SLAVE for channel 'bar';
      $ mysql&amp;gt; STOP SLAVE for channel 'baz';
      
      # Get the current GTID purged values from MySQL.
      $ mysql&amp;gt; SHOW GLOBAL VARIABLES like '%gtid_purged%';
      
      # (3) Load the dump of the database into the Debezium cluster.
      $ myloader -d /mysqldata/new_db/ -s new_db
      
      # (4) Clear out existing GTID_PURGED values so that we can overwrite it to include the GTID from the new dump file.
      $ mysql&amp;gt; reset master;
      
      # Set the new GTID_PURGED value, including the GTID_PURGED value from the MySQL dump file.
      $ mysql&amp;gt; set global GTID_PURGED=&quot;f3a44d1a-11e6-44ba-bf12-040bab830af0:1-10752,c627b2bc-b36a-11e6-a886-42010af00790:1-9052,01261abc3-6ade-11e6-9647-42010af0044a:1-375342&quot;;
      
      # (5) Start replication for the new DB.
      $ mysql&amp;gt; CHANGE MASTER TO MASTER_HOST='123.123.123.123', MASTER_USER='REPLICATION_USER', MASTER_PASSWORD='REPLICATION_PASSWORD',MASTER_AUTO_POSITION=1 for CHANNEL 'new_db';
      $ mysql&amp;gt; START SLAVE for channel 'new_db';
      
      # Start replication for the DBs that we paused.
      $ mysql&amp;gt; START SLAVE for channel 'foo';
      $ mysql&amp;gt; START SLAVE for channel 'bar';
      $ mysql&amp;gt; START SLAVE for channel 'baz';
      
      # Repeat steps 2-5 on the old primary (now secondary).&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;At the end of these steps, both the primary and secondary Debezium MySQL servers have the new database. Once finished, we can then add a new Debezium connector to the Kafka connect cluster. This connector will have configuration that looks roughly like this:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight nowrap&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
         &quot;name&quot;: &quot;debezium-connector-microservice1&quot;,
         &quot;config&quot;: {
             &quot;name&quot;: &quot;debezium-connector-microservice1&quot;,
             &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
             &quot;tasks.max&quot;: &quot;1&quot;,
             &quot;database.hostname&quot;: &quot;dbz-mysql01&quot;,
             &quot;database.port&quot;: &quot;3306&quot;,
             &quot;database.user&quot;: &quot;user&quot;,
             &quot;database.password&quot;: &quot;*******&quot;,
             &quot;database.server.id&quot;: &quot;101&quot;,
             &quot;database.server.name&quot;: &quot;db.debezium.microservice1&quot;,
             &quot;gtid.source.includes&quot;: &quot;c34aeb9e-89ad-11e6-877b-42010a93af2d&quot;,
             &quot;database.whitelist&quot;: &quot;microservice1_db&quot;,
             &quot;poll.interval.ms&quot;: &quot;2&quot;,
             &quot;table.whitelist&quot;: &quot;microservice1_db.table1,microservice1_db.table2&quot;,
             &quot;column.truncate.to.1024.chars&quot; : &quot;microservice1_db.table1.text_col&quot;,
             &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka01:9093,kafka02:9093,kafka03:9093&quot;,
             &quot;database.history.kafka.topic&quot;: &quot;debezium.history.microservice1&quot;,
             &quot;database.ssl.truststore&quot;: &quot;/certs/truststore&quot;,
             &quot;database.ssl.truststore.password&quot;: &quot;*******&quot;,
             &quot;database.ssl.mode&quot;: &quot;required&quot;,
             &quot;database.history.producer.security.protocol&quot;: &quot;SSL&quot;,
             &quot;database.history.producer.ssl.truststore.location&quot;: &quot;/certs/truststore&quot;,
             &quot;database.history.producer.ssl.truststore.password&quot;: &quot;*******&quot;,
             &quot;database.history.consumer.security.protocol&quot;: &quot;SSL&quot;,
             &quot;database.history.consumer.ssl.truststore.location&quot;: &quot;/certs/truststore&quot;,
             &quot;database.history.consumer.ssl.truststore.password&quot;: &quot;*******&quot;,
         }
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The details on these configuration fields are located &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/#connector-properties&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The new connector will start up and begin &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/#snapshots&quot;&gt;snapshotting&lt;/a&gt; the database, since this is the first time it’s been started. Debezium’s snapshot implementation (see &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-31&quot;&gt;DBZ-31&lt;/a&gt;) uses an approach very similar to MySQL’s mysqldump tool. Once the snapshot is complete, Debezium will switch over to using MySQL’s binlog to receive all future database updates.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka connect and Debezium work together to periodically commit Debezium’s location in the MySQL binlog described by a &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/replication-gtids-concepts.html&quot;&gt;MySQL global transaction ID&lt;/a&gt; (GTID). When Debezium restarts, Kafka connect will give it the last committed MySQL GTID, and Debezium will pick up from there.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;em&gt;Note that commits only happen periodically, so Debezium might start up from a location in the log prior to the last row that it received. In such a case, you will observe duplicate messages in Debezium Kafka topic. Debezium writes messages to Kafka with an at-least-once messaging guarantee.&lt;/em&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;high_availability&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#high_availability&quot;&gt;&lt;/a&gt;High availability&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;One of the difficulties we faced when we first began using Debezium was how to make it tolerant to machine failures (both the upstream MySQL server, and Debezium, itself). MySQL prior to version 5.6 modeled a replica’s location in its parent’s binlogs using a (binlog filename, file offset) tuple. The problem with this approach is that the binlog filenames are not the same between MySQL machines. This means that a replica reading from upstream MySQL machine 1 can’t easily fail over to MySQL machine 2. There is an entire ecosystem of tools (including &lt;a href=&quot;https://code.google.com/p/mysql-master-ha/&quot;&gt;MHA&lt;/a&gt;) to try and address this problem.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Starting with MySQL 5.6, MySQL introduced the concept of global transaction IDs. These GTIDs identify a specific location within the MySQL binlog &lt;em&gt;across machines&lt;/em&gt;. This means that a consumer reading from a binlog on one MySQL server can switch over to the other, provided that both servers have the data available. This is how we run our systems. Both the CloudSQL instances and the Debezium MySQL cluster run with GTIDs enabled. The Debezium MySQL servers also have replication binlogs enabled so that binlogs exist for Debezium to read (replicas don’t normally have binlogs enabled by default). All of this enables Debezium to consume from the primary Debezium MySQL server, but switch over to the secondary (via HA Proxy) if there’s a failure.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If the machine that Debezium, itself, is running on fails, then the Kafka connect framework fails the connector over to another machine in the cluster. When the failover occurs, Debezium receives its last committed offset (GTID) from Kafka connect, and picks up where it left off (with the same caveat as above: you might see some duplicate messages due to periodic commit frequency).&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;An important configuration that needs to be called out is the &lt;code&gt;gtid.source.includes&lt;/code&gt; field that we have set above. When we first set up the topology that’s described in the architecture section, we discovered that we could not fail over from the primary Debezium DB to the secondary DB even though they both were replicating exactly the same data. This is because, in addition to the GTIDs for the various upstream DBs that both primary and secondary machines are replicating, each machine has its &lt;em&gt;own&lt;/em&gt; server UUID for its various MySQL databases (e.g. information_schema). The fact that these two servers have different UUIDs in them led MySQL to get confused when we triggered a failover, because Debezium’s GTID would include the server UUID for the primary server, which the secondary server didn’t know about. The fix was to filter out all UUIDs that we don’t care about from the GTID. Each Debezium connector filters out all server UUIDs except for the UUID for the microservice DB that it cares about. This allows the connector to fail from primary to secondary without issue. This issue is documented in detail on &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-129&quot;&gt;DBZ-129&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;schemas&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#schemas&quot;&gt;&lt;/a&gt;Schemas&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium’s &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/#change-events-value&quot;&gt;message format&lt;/a&gt; includes both the &quot;before&quot; and &quot;after&quot; versions of a row. For inserts, the &quot;before&quot; is null. For deletes, the &quot;after&quot; is null. Updates have both the &quot;before&quot; and &quot;after&quot; fields filled out. The messages also include some server information such as the server ID that the message came from, the GTID of the message, the server timestamp, and so on.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
        &quot;before&quot;: {
          &quot;id&quot;: 1004,
          &quot;first_name&quot;: &quot;Anne&quot;,
          &quot;last_name&quot;: &quot;Kretchmar&quot;,
          &quot;email&quot;: &quot;annek@noanswer.org&quot;
        },
        &quot;after&quot;: {
          &quot;id&quot;: 1004,
          &quot;first_name&quot;: &quot;Anne Marie&quot;,
          &quot;last_name&quot;: &quot;Kretchmar&quot;,
          &quot;email&quot;: &quot;annek@noanswer.org&quot;
        },
        &quot;source&quot;: {
          &quot;name&quot;: &quot;mysql-server-1&quot;,
          &quot;server_id&quot;: 223344,
          &quot;ts_sec&quot;: 1465581,
          &quot;gtid&quot;: null,
          &quot;file&quot;: &quot;mysql-bin.000003&quot;,
          &quot;pos&quot;: 484,
          &quot;row&quot;: 0,
          &quot;snapshot&quot;: null
        },
        &quot;op&quot;: &quot;u&quot;,
        &quot;ts_ms&quot;: 1465581029523
      }&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The serialization format that Debezium sends to Kafka is configurable. We prefer Avro at WePay for its compact size, schema DDL, performance, and rich ecosystem. We’ve configured Kafka connect to use Confluent’s &lt;a href=&quot;https://github.com/confluentinc/schema-registry/tree/master/avro-serializer/src/main/java/io/confluent/kafka/serializers&quot;&gt;Avro encoder&lt;/a&gt; codec for Kafka. This encoder serializes messages to Avro, but also registers the schemas with Confluent’s schema registry.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If a MySQL table’s schema is changed, Debezium adapts to the change by updating the structure and schema of the &quot;before&quot; and &quot;after&quot; portions of its event messages. This will appear to the Avro encoder as a new schema, which it will register with the schema registry before the message is sent to Kafka. The registry runs full compatibility checks to make sure that downstream consumers don’t break due to a schema evolution.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;em&gt;Note that it’s still possible to make an incompatible change in the MySQL schema itself, which would break downstream consumers. We have not yet added automatic compatibility checks to MySQL table alters.&lt;/em&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;future_work&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#future_work&quot;&gt;&lt;/a&gt;Future work&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;monolithic_database&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#monolithic_database&quot;&gt;&lt;/a&gt;Monolithic database&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In addition to our microservices, we have a legacy monolithic database that’s much larger than our microservice databases. We’re in the process of upgrading this cluster to run with GTIDs enabled. Once this is done, we plan to replicate this cluster into Kafka with Debezium as well.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;large_table_snapshots&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#large_table_snapshots&quot;&gt;&lt;/a&gt;Large table snapshots&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re lucky that all of our microservice databases are of relatively manageable size. Our monolithic database has some tables that are much larger. We have yet to test Debezium with very large tables, so it’s unclear if any tuning or patches will be required in order to snapshot these tables on the initial Debezium load. We have heard community reports that larger tables (6 billion+ rows) do work, provided that the configuration exposed in &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-152&quot;&gt;DBZ-152&lt;/a&gt; is set. This is work we’re planning to do shortly.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;more_monitoring&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#more_monitoring&quot;&gt;&lt;/a&gt;More monitoring&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka connect doesn’t currently make it easy to expose metrics through the Kafka metrics framework. As a result, there are very few metrics available from the Kafka connect framework. Debezium does expose metrics via JMX (see &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-134&quot;&gt;DBZ-134&lt;/a&gt;), but we aren’t exposing them to our metrics system currently. We do monitor the system, but when things go wrong, it can be difficult to determine what’s going on. &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-2376&quot;&gt;KAFKA-2376&lt;/a&gt; is the open JIRA that’s meant to address the underlying Kafka connect issue.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;more_databases&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#more_databases&quot;&gt;&lt;/a&gt;More databases&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As we add more microservice databases, we’ll begin to put pressure on the two Debezium MySQL servers that we have. Eventually, we plan to split the single Debezium cluster that we have into more than one, with some microservices replicating only to one cluster, and the rest replicating to others.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;unify_compatibility_checks&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#unify_compatibility_checks&quot;&gt;&lt;/a&gt;Unify compatibility checks&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As I mentioned in the schema section, above, the Confluent schema registry runs schema compatibility checks out of the box right now. This makes it very easy for us to prevent backward and forward incompatible changes from making their way into Kafka. We don’t currently have an equivalent check at the MySQL layer. This is a problem because it means it’s possible for a DBA to make incompatible changes at the MySQL layer. Debezium will then fail when trying to produce the new messages into Kafka. We need to make sure this can’t happen by adding equivalent checks at the MySQL layer. &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-70&quot;&gt;DBZ-70&lt;/a&gt; discusses this more.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect2&quot;&gt;
      &lt;h3 id=&quot;automatic_topic_configuration&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#automatic_topic_configuration&quot;&gt;&lt;/a&gt;Automatic topic configuration&lt;/h3&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We currently run Kafka with topic auto-create enabled with a default of 6 partitions, and time-based/size-based retention. This configuration doesn’t make much sense for Debezium topics. At the very least, they should be using log-compaction as their retention. We plan to write a script that looks for mis-configured Debezium topics, and updates them to appropriate retention settings.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;conclusion&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#conclusion&quot;&gt;&lt;/a&gt;Conclusion&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve been running Debezium in production for the past 8 months. Initially, we ran it dark, and then enabled it for the realtime BigQuery pipeline shown in the architecture diagram above. Recently, we’ve begun consuming the messages in microservices and stream processing systems. We look forward to adding more data to the pipeline, and addressing some of the issues that were raised in the &lt;em&gt;Future work&lt;/em&gt; section.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;A special thanks to &lt;a href=&quot;https://www.linkedin.com/in/randallhauch&quot;&gt;Randall Hauch&lt;/a&gt;, who has been invaluable in addressing a number of bug fixes and feature requests.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/02/08/Support-for-Postgresql/</id>
    <title>PostgreSQL support added to Debezium</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2017-02-08T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/02/08/Support-for-Postgresql/" rel="alternate" type="text/html" />
    <author>
      <name>Horia Chiorean</name>
    </author>
    <category term="postgres"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      With the recent Debezium release, we&#8217;re happy to announce that a new PostgreSQL connector has been added alongside the already existing MySQL and MongoDB connectors.
      
      
      
      
      
      
      
      
      
      Make sure you read the connector documentation for an in-depth look at the different configuration options.
      
      
      
      
      
      
      
      
      Getting started
      
      
      The fastest way to check out the new connector is using Debezium&#8217;s Postgres docker image which is based on a vanilla Postgres docker image on top of which it compiles and installs a PostgreSQL logical decoding plugin
      and sets up the necessary permissions for streaming changes locally (on localhost)
      
      
      Once you fire up the Docker machine with the database server, starting up...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;With the &lt;a href=&quot;http://debezium.io/blog/2017/02/07/Debezium-0-4-0-Released&quot;&gt;recent Debezium release&lt;/a&gt;, we’re happy to announce that a new &lt;strong&gt;PostgreSQL connector&lt;/strong&gt; has been added alongside the already existing MySQL and MongoDB connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;admonitionblock tip&quot;&gt;
      &lt;table&gt;
      &lt;tr&gt;
      &lt;td class=&quot;icon&quot;&gt;
      &lt;i class=&quot;fa icon-tip&quot; title=&quot;Tip&quot;&gt;&lt;/i&gt;
      &lt;/td&gt;
      &lt;td class=&quot;content&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Make sure you &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql&quot;&gt;read the connector documentation&lt;/a&gt; for an in-depth look at the different configuration options.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/td&gt;
      &lt;/tr&gt;
      &lt;/table&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;getting_started&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#getting_started&quot;&gt;&lt;/a&gt;Getting started&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The fastest way to check out the new connector is using &lt;a href=&quot;https://hub.docker.com/r/debezium/postgres&quot;&gt;Debezium’s Postgres docker image&lt;/a&gt; which is based on a vanilla Postgres docker image on top of which it compiles and installs a PostgreSQL &lt;a href=&quot;https://github.com/debezium/postgres-decoderbufs&quot;&gt;logical decoding plugin&lt;/a&gt;
      and sets up the necessary permissions for streaming changes locally (on &lt;code&gt;localhost&lt;/code&gt;)&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once you fire up the Docker machine with the database server, starting up and configuring the connector to stream changes from that machine is exactly the same as described in detail by the &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;Debezium tutorial&lt;/a&gt;. The only obvious difference is that instead of the MySQL machine and MySQL connector configuration you need to use the PostgreSQL machine and the PostgreSQL connector configuration parameters.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;using_the_connector_in_your_own_environment&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#using_the_connector_in_your_own_environment&quot;&gt;&lt;/a&gt;Using the connector in your own environment&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Unlike the Mongo and MySQL connectors, getting the PostgreSQL connector up and running is a bit more complicated due to the fact that it requires a server-side logical decoding plugin running in the PostgreSQL server.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In general, there are three major steps involved in getting the connector running in your environment:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;Compiling and installing the &lt;a href=&quot;https://github.com/debezium/postgres-decoderbufs&quot;&gt;logical decoding plugin&lt;/a&gt; into your own server&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Setting up the PostgreSQL server with appropriate replication permissions&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Starting the Kafka Connect, Broker and Zookeeper machines&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For steps 1 and 2 you can check out our &lt;a href=&quot;https://github.com/debezium/docker-images/tree/master/postgres/9.6&quot;&gt;PostgreSQL Docker image&lt;/a&gt; together with the sources for the &lt;a href=&quot;https://github.com/debezium/postgres-decoderbufs&quot;&gt;logical decoding plugin&lt;/a&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;For step 3 you can either use Debezium’s &lt;a href=&quot;https://github.com/debezium/docker-images&quot;&gt;Kafka Docker images&lt;/a&gt; or perform a similar setup locally. The &lt;a href=&quot;http://debezium.io/docs/tutorial&quot;&gt;Debezium tutorial&lt;/a&gt; and the &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql&quot;&gt;the connector documentation&lt;/a&gt; are great resources for helping out with this task.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2017/02/07/Debezium-0-4-0-Released/</id>
    <title>Debezium 0.4.0 Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2017-02-07T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2017/02/07/Debezium-0-4-0-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.4.0 is now available for use with Kafka Connect 0.10.1.1. This release introduces a new PostgreSQL connector, and contains over a dozen fixes combined for the MongoDB connector and MySQL connector, including preliminar support for Amazon RDS and Amazon Aurora (MySQL compatibility). See the release notes for specifics on these changes.
      
      
      We&#8217;ve also created Debezium Docker images labelled 0.4 and latest, which we use in our tutorial.
      
      
      Thanks to Horia, Chris, Akshath, Ramesh, Matthias, Anton, Sagi, barton, and others for their help with this release, issues, discussions, contributions, and questions!
      
      
      
      
      What&#8217;s next
      
      
      We&#8217;ll continue to improve the MongoDB,...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.4.0&lt;/strong&gt; is now available for use with Kafka Connect 0.10.1.1. This release introduces a new &lt;a href=&quot;http://debezium.io/docs/connectors/postgresql/&quot;&gt;PostgreSQL connector&lt;/a&gt;, and contains over a dozen fixes combined for the &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/&quot;&gt;MongoDB connector&lt;/a&gt; and &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL connector&lt;/a&gt;, including preliminar support for &lt;a href=&quot;https://aws.amazon.com/rds/mysql/&quot;&gt;Amazon RDS&lt;/a&gt; and &lt;a href=&quot;https://aws.amazon.com/rds/aurora/&quot;&gt;Amazon Aurora (MySQL compatibility)&lt;/a&gt;. See the &lt;a href=&quot;http://debezium.io/docs/releases/&quot;&gt;release notes&lt;/a&gt; for specifics on these changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also created &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.4&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Horia, Chris, Akshath, Ramesh, Matthias, Anton, Sagi, barton, and others for their help with this release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll continue to improve the MongoDB, MySQL, and PostgreSQL connectors and pushing out 0.4.x releases. We’re also going to work on a few new connectors, though we’ll likely increase the minor version with each new connector. Stay tuned and get involved!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/12/21/Debezium-0-3-6-Released/</id>
    <title>Debezium 0.3.6 Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2016-12-21T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/12/21/Debezium-0-3-6-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.3.6 is now available for use with Kafka Connect 0.10.0.1. This release contains over a dozen fixes combined for the MySQL connector and MongoDB connectors. See the release notes for specifics on these changes.
      
      
      We&#8217;ve also updated the Debezium Docker images labelled 0.3 and latest, which we use in our tutorial.
      
      
      Thanks to Farid, RenZhu, Dongjun, Anton, Chris, Dennis, Sharaf, Rodrigo, Tim, and others for their help with this release, issues, discussions, contributions, and questions!
      
      
      
      
      What&#8217;s next
      
      
      We&#8217;ll continue to improve the MongoDB and MySQL connectors, and we also have a great PostgreSQL connector that is nearly ready...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.3.6&lt;/strong&gt; is now available for use with Kafka Connect 0.10.0.1. This release contains over a dozen fixes combined for the MySQL connector and MongoDB connectors. See the &lt;a href=&quot;http://debezium.io/docs/releases/&quot;&gt;release notes&lt;/a&gt; for specifics on these changes.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Farid, RenZhu, Dongjun, Anton, Chris, Dennis, Sharaf, Rodrigo, Tim, and others for their help with this release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;what_s_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_s_next&quot;&gt;&lt;/a&gt;What’s next&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ll continue to improve the MongoDB and MySQL connectors, and we also have a great PostgreSQL connector that is nearly ready to be released. With the new connector we’ll switch release numbers to 0.4.x and plan to stop issuing 0.3.x releases. Stay tuned for this next 0.4.0 release!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/11/14/Debezium-0-3-5-Released/</id>
    <title>Debezium 0.3.5 Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2016-11-14T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/11/14/Debezium-0-3-5-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.3.5 is now available for use with Kafka Connect 0.10.0.1. This release contains several fixes for the MySQL connector and adds the ability to use with multi-master MySQL servers as sources. See the release notes for specifics on these changes. We&#8217;ve also updated the Debezium Docker images labelled 0.3 and latest, which we use in our tutorial.
      
      
      One of the fixes is signficant, and so we strongly urge all users to upgrade to this release from all earlier versions. In prior versions, the MySQL connector may stop without completing all updates in a transaction, and...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.3.5&lt;/strong&gt; is now available for use with Kafka Connect 0.10.0.1. This release contains several fixes for the MySQL connector and adds the ability to use with &lt;a href=&quot;http://debezium.io/docs/mysql/#multi-master-mysql/&quot;&gt;multi-master MySQL servers&lt;/a&gt; as sources. See the &lt;a href=&quot;http://debezium.io/docs/releases/&quot;&gt;release notes&lt;/a&gt; for specifics on these changes. We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;One of the fixes is signficant, and so &lt;strong&gt;we strongly urge all users to upgrade to this release from all earlier versions.&lt;/strong&gt; In prior versions, the MySQL connector may stop without completing all updates in a transaction, and when the connector restarts it starts with the &lt;em&gt;next&lt;/em&gt; transaction and therefore might fail to capture some of the change events in the earlier transaction. This release fixes this issue so that when restarting it will always pick up where it left off, even if that point is in the middle of a transaction. Note that this fix only takes affect once a connector is upgraded and restarted. See &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-144&quot;&gt;the issue&lt;/a&gt; for more details.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Akshath, Anton, Chris, and others for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/10/25/Debezium-0-3-4-Released/</id>
    <title>Debezium 0.3.4 Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2016-10-25T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/10/25/Debezium-0-3-4-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.3.4 is now available for use with Kafka Connect 0.10.0.1. This release contains several new features for the MySQL connector: support for MySQL&#8217;s JSON datatype, a new snapshot mode called schema_only, and JMX metrics. Also, the Debezium Docker images for Zookeeper, Kafka, and Kafka Connect have all been updated to allow optionally expose JMX metrics in these services. And, one backward-incompatible fix was made to the change event&#8217;s ts_sec field. See the release notes for specifics.
      
      
      We&#8217;ve also updated the Debezium Docker images labelled 0.3 and latest, which we use in our tutorial.
      
      
      Thanks to Akshath,...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.3.4&lt;/strong&gt; is now available for use with Kafka Connect 0.10.0.1. This release contains several new features for the MySQL connector: support for MySQL’s &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/#data-types&quot;&gt;&lt;code&gt;JSON&lt;/code&gt;&lt;/a&gt; datatype, a new snapshot mode called &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/#snapshots&quot;&gt;&lt;code&gt;schema_only&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&quot;http://debezium.io/docs/monitoring&quot;&gt;JMX metrics&lt;/a&gt;. Also, the Debezium Docker images for Zookeeper, Kafka, and Kafka Connect have all been updated to allow optionally &lt;a href=&quot;http://debezium.io/docs/monitoring&quot;&gt;expose JMX metrics&lt;/a&gt; in these services. And, one backward-incompatible fix was made to the change event’s &lt;code&gt;ts_sec&lt;/code&gt; field. See the &lt;a href=&quot;http://debezium.io/docs/releases/&quot;&gt;release notes&lt;/a&gt; for specifics.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Akshath, Chris, Vitalii, Dennis, Prannoy, and others for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/10/19/Support-for-MySQL-JSON-typpe-coming-soon/</id>
    <title>Support for MySQL&#8217;s JSON type coming soon</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2016-10-19T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/10/19/Support-for-MySQL-JSON-typpe-coming-soon/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="mysql"></category>
    <category term="json"></category>
    <summary>
      
      MySQL 5.7 introduced a new data type for storing and working with JSON data. Clients can define tables with columns using the new JSON datatype, and they can store and read JSON data using SQL statements and new built-in JSON functions to construct JSON data from other relational columns, introspect the structure of JSON values, and search within and manipulate JSON data. It possible to define generated columns on tables whose values are computed from the JSON value in another column of the same table, and to then define indexes with those generated columns. Overall, this is really a very...
    </summary>
    <content type="html">
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;MySQL 5.7 introduced a new data type for &lt;a href=&quot;http://mysqlserverteam.com/whats-new-in-mysql-5-7-generally-available/&quot;&gt;storing and working with JSON data&lt;/a&gt;. Clients can define tables with columns using the new &lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/json.html&quot;&gt;&lt;code&gt;JSON&lt;/code&gt; datatype&lt;/a&gt;, and they can store and read JSON data using SQL statements and new built-in JSON functions to construct JSON data from other relational columns, introspect the structure of JSON values, and search within and manipulate JSON data. It possible to define generated columns on tables whose values are computed from the JSON value in another column of the same table, and to then define indexes with those generated columns. Overall, this is really a very powerful feature in MySQL.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium’s MySQL connector will support the &lt;code&gt;JSON&lt;/code&gt; datatype starting with the upcoming 0.3.4 release. JSON document, array, and scalar values will appear in change events as strings with &lt;code&gt;io.debezium.data.json&lt;/code&gt; for the schema name. This will make it natural for consumers to work with JSON data. BTW, this is the same semantic schema type used by the MongoDB connector to represent JSON data.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;This sounds straightforward, and we hope it is. But &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-126&quot;&gt;implementing this&lt;/a&gt; required a fair amount of work. That’s because although MySQL exposes JSON data as strings to client applications, &lt;em&gt;internally&lt;/em&gt; it stores all JSON data in a special binary form that allows the MySQL engine to efficiently access the JSON data in queries, JSON functions and generated columns. All JSON data appears in the binlog in this binary form as well, which meant that we had to parse the binary form ourselves if we wanted to extract the more useful string representation. Writing and testing this parser took a bit of time and effort, and ultimately we &lt;a href=&quot;https://github.com/shyiko/mysql-binlog-connector-java/issues/115&quot;&gt;donated it&lt;/a&gt; to the excellent &lt;a href=&quot;https://github.com/shyiko/mysql-binlog-connector-java&quot;&gt;MySQL binlog client library&lt;/a&gt; that the connector uses internally to read the binlog events.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’d like to thank &lt;a href=&quot;https://github.com/shyiko&quot;&gt;Stanley Shyiko&lt;/a&gt; for guiding us and helping us debug the final problems with parsing JSON in the binlog, for accepting our proposed changes into his library, for releasing his library quickly when needed, and for being so responsive on this and other issues!&lt;/p&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/10/18/Debezium-0-3-3-Released/</id>
    <title>Debezium 0.3.3 Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2016-10-18T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/10/18/Debezium-0-3-3-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.3.3 is now available for use with Kafka Connect 0.10.0.1. This release contains a handful of bug fixes and minor improvements for the MySQL connector, including better handling of BIT(n) values, ENUM and SET values, and GTID sets, This release also improves the log messages output by the MySQL connectors to better represent the ongoing activity when consuming the changes from the source database. See the release notes for specifics.
      
      
      We&#8217;ve also updated the Debezium Docker images labelled 0.3 and latest, which we use in our tutorial. We&#8217;ve also updated the tutorial to use the...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.3.3&lt;/strong&gt; is now available for use with Kafka Connect 0.10.0.1. This release contains a handful of bug fixes and minor improvements for the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL connector&lt;/a&gt;, including better handling of &lt;code&gt;BIT(n)&lt;/code&gt; values, &lt;code&gt;ENUM&lt;/code&gt; and &lt;code&gt;SET&lt;/code&gt; values, and GTID sets, This release also improves the log messages output by the MySQL connectors to better represent the ongoing activity when consuming the changes from the source database. See the &lt;a href=&quot;http://debezium.io/docs/releases/&quot;&gt;release notes&lt;/a&gt; for specifics.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;. We’ve also updated the tutorial to use the &lt;a href=&quot;https://docs.docker.com/engine/installation/&quot;&gt;latest Docker installations&lt;/a&gt; on Linux, Windows, and OS X.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Akshath, Chris, Randy, Prannoy, Umang, Horia, and others for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/09/26/Debezium-0-3-2-Released/</id>
    <title>Debezium 0.3.2 Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2016-09-26T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/09/26/Debezium-0-3-2-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.3.2 is now available for use with Kafka Connect 0.10.0.1. This release contains a handful of bug fixes and minor improvements for the MySQL connector and MongoDB connector. The MySQL connector better handles BIT(n) values and zero-value date and timestamp values. This release also improves the log messages output by the MySQL and MongoDB connectors to better represent the ongoing activity when consuming the changes from the source database. See the release notes for specifics.
      
      
      We&#8217;ve also updated the Debezium Docker images labelled 0.3 and latest, which we use in our tutorial. We&#8217;ve also updated...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.3.2&lt;/strong&gt; is now available for use with Kafka Connect 0.10.0.1. This release contains a handful of bug fixes and minor improvements for the &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL connector&lt;/a&gt; and &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/&quot;&gt;MongoDB connector&lt;/a&gt;. The MySQL connector better handles &lt;code&gt;BIT(n)&lt;/code&gt; values and &lt;a href=&quot;http://dev.mysql.com/doc/refman/5.7/en/date-and-time-types.html&quot;&gt;zero-value&lt;/a&gt; date and timestamp values. This release also improves the log messages output by the MySQL and MongoDB connectors to better represent the ongoing activity when consuming the changes from the source database. See the &lt;a href=&quot;http://debezium.io/docs/releases/&quot;&gt;release notes&lt;/a&gt; for specifics.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;. We’ve also updated the tutorial to use the &lt;a href=&quot;https://docs.docker.com/engine/installation/&quot;&gt;latest Docker installations&lt;/a&gt; on Linux, Windows, and OS X.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Akshath, Colum, Emmanuel, Konstantin, Randy, RenZhu, Umang, and others for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/09/19/Serializing-Debezium-events-with-Avro/</id>
    <title>Serializing Debezium events with Avro</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2016-09-19T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/09/19/Serializing-Debezium-events-with-Avro/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="kafka"></category>
    <category term="avro"></category>
    <category term="serialization"></category>
    <summary>
      
      
      
      Although Debezium makes it easy to capture database changes and record them in Kafka, one of the more important decisions you have to make is how those change events will be serialized in Kafka. Every message in Kafka has a key and a value, and to Kafka these are opaque byte arrays. But when you set up Kafka Connect, you have to say how the Debezium event keys and values should be serialized to a binary form, and your consumers will also have to deserialize them back into a usable form.
      
      
      Debezium event keys and values are both structured, so JSON...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Although Debezium makes it easy to capture database changes and record them in Kafka, one of the more important decisions you have to make is &lt;em&gt;how&lt;/em&gt; those change events will be serialized in Kafka. Every message in Kafka has a key and a value, and to Kafka these are opaque byte arrays. But when you set up Kafka Connect, you have to say how the Debezium event keys and values should be serialized to a binary form, and your consumers will also have to deserialize them back into a usable form.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium event keys and values are both structured, so JSON is certainly a reasonable option — it’s flexible, ubiquitous, and language agnostic, but on the other hand it’s quite verbose. One alternative is Avro, which is also flexible and language agnostic, but also faster and results in smaller binary representations. Using Avro requires a bit more setup effort on your part and some additional software, but the advantages are often worth it.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;kafka_serializers_and_deserializers&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#kafka_serializers_and_deserializers&quot;&gt;&lt;/a&gt;Kafka serializers and deserializers&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Before we get too far, let’s back up and review how Kafka producers and consumers normally do this serialization and deserialization. Because the keys and values are simple opaque byte arrays, you can use anything for your keys and values. For example, consider a case where we’re using simple whole numbers for the keys and strings for the values. Here, a producer of these messages would use a &lt;em&gt;long serializer&lt;/em&gt; to convert the &lt;code&gt;long&lt;/code&gt; keys to binary form and a &lt;em&gt;string serializer&lt;/em&gt; to convert the &lt;code&gt;String&lt;/code&gt; values to binary form. Meanwhile, the consumers use a &lt;em&gt;long deserializer&lt;/em&gt; to convert the binary keys into usable &lt;code&gt;long&lt;/code&gt; values, and a &lt;em&gt;string deserializer&lt;/em&gt; to convert the binary values back into &lt;code&gt;String&lt;/code&gt; objects.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;In cases where the keys and/or values need to be a bit more structured, the producers and consumers can be written to use JSON structures for keys and/or values, and the Kafka-provided &lt;em&gt;JSON serializer and deserializer&lt;/em&gt; to do the conversion to and from binary form stored within the Kafka messages. As we said earlier, using JSON for keys and/or values is very flexible and language agnostic, but it is also produces keys and values that are relatively large since the fields and structure of the JSON values need to be encoded as well.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;avro_serialization&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#avro_serialization&quot;&gt;&lt;/a&gt;Avro serialization&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://avro.apache.org/&quot;&gt;Avro&lt;/a&gt; is a data serialization mechanism that uses a &lt;em&gt;schema&lt;/em&gt; to define the structure of data. Avro relies upon this schema when writing the data to the binary format, and the schema allows it to encode the fields within the data in a much more compact form. Avro also relies upon the schema when &lt;em&gt;reading&lt;/em&gt; the data, too. But interestingly, Avro schemas are designed to evolve, so it is actually possible to use a slightly different schema for reading than what was used for writing. This feature makes Avro a great choice for Kafka serialization and deserialization.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://confluent.io&quot;&gt;Confluent&lt;/a&gt; provides a &lt;a href=&quot;http://docs.confluent.io/3.0.1/app-development.html&quot;&gt;Kafka serializer and deserializer that uses Avro&lt;/a&gt; and a separate &lt;a href=&quot;http://docs.confluent.io/3.0.1/schema-registry/docs/intro.html&quot;&gt;Schema Registry&lt;/a&gt;, and it works like this: when a numeric or string object are to be serialized, the &lt;em&gt;Avro serializer&lt;/em&gt; will determine the corresponding Avro Schema for the given type, register with the Schema Registry this schema and the topic its used on, get back the unique identifier for the schema, and then encode in the binary form the unique identifier of the schema and the encoded value. The next message is likely to have the same type and thus schema, so the serializer can quickly encode the schema identifier and value for this message without having to talk to the Schema Registry. Only when needing to serialize a schema it hasn’t already seen does the Avro serializer talk with the Schema Registry. So not only is this fast, but it also produces very compact binary forms and allows for the producer to &lt;em&gt;evolve&lt;/em&gt; its key and/or value schemas over time. The Schema Registry can also be configured to allow new versions of schemas to be registered only when they are &lt;em&gt;compatible&lt;/em&gt; with the Avro schema evolution rules, ensuring that producers do not produce messages that consumers will not be able to read.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Consumers, meanwhile, use the &lt;em&gt;Avro deserializer&lt;/em&gt;, which works in a similar manner, albeit backwards: when it reads the binary form of a key or value, it first looks for the schema identifier and, if it hasn’t seen it before asks the Schema Registry for the schema, and then uses that schema to decode the remainder of the binary representation into its object form. Again, if the deserializer has previously seen a particular schema identifier, it already has the schema needed to decode the data and doesn’t have to consult the Schema Registry.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;kafka_connect_converters&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#kafka_connect_converters&quot;&gt;&lt;/a&gt;Kafka Connect converters&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka Connect is a bit different than many Kafka producers/consumers, since the keys and values will often be structured. And rather than require connectors to work with JSON objects, Kafka Connect defines its own lightweight framework for defining data structures with a schema, making it much easier to write connectors to work with structured data. Kafka Connect defines its own &lt;em&gt;converters&lt;/em&gt; that are similar to Kafka (de)serializers, except that Kafka Connect’s converters know about these structures and schemas and can serialize the keys and values to binary form. Kafka Connect provides a &lt;em&gt;JSON converter&lt;/em&gt; that converts the structures into JSON and then uses the normal Kafka JSON serializer, so downstream consumers can just use the normal Kafka JSON deserializer and get a JSON representation of the Kafka Connect structs and schema. This is exactly what the &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;Debezium tutorial&lt;/a&gt; is using, and the &lt;code&gt;watch-topic&lt;/code&gt; consumer knows to use the JSON deserializer.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;One great feature of Kafka Connect is that the connectors simply provide the structured messages, and Kafka Connect takes care of serializing them using the configured converter. This means that you can use any Kafka Connect &lt;em&gt;converters&lt;/em&gt; with any Kafka Connect connector, including all of Debezium’s connectors.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Kafka Connect’s schema system was designed specifically with Avro in mind, so there is a one-to-one mapping between Kafka Connect schemas and Avro schemas. Confluent provides an &lt;em&gt;Avro Converter&lt;/em&gt; for Kafka Connect that serializes the Kafka Connect structs provided by the connectors into the compact Avro binary representation, again using the Schema Registry just like the Avro serializer. The consumer just uses the normal Avro deserializer as mentioned above.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Using Avro for serialization of Debezium events brings several significant advantages:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;olist arabic&quot;&gt;
      &lt;ol class=&quot;arabic&quot;&gt;
      &lt;li&gt;
      &lt;p&gt;The encoded binary forms of the Debezium events are &lt;em&gt;significantly&lt;/em&gt; smaller than the JSON representations. Not only is the structured data encoded in a more compact form, but the &lt;em&gt;schema&lt;/em&gt; associated with that structured data is represented in the binary form as a single integer.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Encoding the Debezium events into their Avro binary forms is fast. Only when the converter sees a new schema does it have to consult with the Schema Registry; otherwise, the schema has already been seen and its encoding logic already precomputed.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The Avro Converter for Kafka Connect produces messages with Avro-encoded keys and values that can be read by any Kafka consumers using the Avro deserializer.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Debezium event structures are based upon the structure of the table from which the changes were captured. When the structure of the source table changes (e.g., because an &lt;code&gt;ALTER&lt;/code&gt; statement was applied to it), the structure and schema of the events will also change. If this is done in a manner such that the new Avro schema is &lt;em&gt;compatible with&lt;/em&gt; the older Avro schema, then consumers will be able to process the events without disruption, even though the event structures evolve over time.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Avro’s schema mechanism is far more formal and rigorous than the free-form JSON structure, and the changes in the schemas are clearly identified when comparing any two messages.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;The Avro converter, Avro (de)serializers, and Schema Registry are all open source.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ol&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;It is true that using the Avro converter and deserializer requires a running Schema Registry, and that the registry becomes an integral part of your streaming infrastructure. However, this is a small price to pay for the benefits listed above.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;using_the_avro_converter_with_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#using_the_avro_converter_with_debezium&quot;&gt;&lt;/a&gt;Using the Avro Converter with Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As mentioned above, in the interest of keeping the Debezium tutorial as simple as possible, we avoid using the Schema Registry or the Avro converter in the tutorial. We also don’t (yet) include the Avro converter in our Docker images, though that &lt;a href=&quot;https://issues.jboss.org/browse/DBZ-59&quot;&gt;will change soon&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Nevertheless, it is absolutely possible to use the Avro Converter with the Debezium connectors when you are installing the connectors into either the Confluent Platform or into your own installation of Kafka Connect. Simply configure the &lt;a href=&quot;http://docs.confluent.io/3.0.1/connect/userguide.html&quot;&gt;Kafka Connect workers&lt;/a&gt; to use the Avro converter for the keys and values:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;key.converter=io.confluent.connect.avro.AvroConverter
      value.converter=io.confluent.connect.avro.AvroConverter&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;And, if you want to use the Avro Converter for Kafka Connect internal messages, then set these as well:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;listingblock&quot;&gt;
      &lt;div class=&quot;content&quot;&gt;
      &lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;internal.key.converter=io.confluent.connect.avro.AvroConverter
      internal.value.converter=io.confluent.connect.avro.AvroConverter&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Once again, there is no need to configure the Debezium connectors any differently.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/08/30/Debezium-0-3-1-Released/</id>
    <title>Debezium 0.3.1 Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2016-08-30T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/08/30/Debezium-0-3-1-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      We&#8217;re happy to announce that Debezium 0.3.1 is now available for use with Kafka Connect 0.10.0.1. This release contains an updated MySQL connector with a handful of bug fixes and two significant but backward-compatible changes. First, the MySQL connector now supports using secure connections to MySQL, adding to the existing ability to connect securely to Kafka. Second, the MySQL connector is able to capture MySQL string values using the proper character sets so that any values stored in the database can be captured correctly in events. See our release notes for details of these changes and for upgrading recommendations.
      
      
      We&#8217;ve also...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’re happy to announce that &lt;strong&gt;Debezium 0.3.1&lt;/strong&gt; is now available for use with Kafka Connect 0.10.0.1. This release contains an updated &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL connector&lt;/a&gt; with a handful of bug fixes and two significant but backward-compatible changes. First, the MySQL connector now supports using secure connections to MySQL, adding to the existing ability to connect securely to Kafka. Second, the MySQL connector is able to capture MySQL string values using the proper character sets so that any values stored in the database can be captured correctly in events. See our &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-3-1&quot;&gt;release notes&lt;/a&gt; for details of these changes and for upgrading recommendations.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; labelled &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;, which we use in our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Chris, Akshath, barten, and and others for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/08/16/Debezium-0-3-0-Released/</id>
    <title>Debezium 0.3.0 Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2016-08-16T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/08/16/Debezium-0-3-0-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="mongodb"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      After a few weeks delay, Debezium 0.3.0 is now available for use with Kafka Connect 0.10.0.1. This release contains an updated MySQL connector with quite a few bug fixes, and a new MongoDB connector that captures the changes made to a MongoDB replica set or MongoDB sharded cluster. See the documentation for details about how to configure these connectors and how they work.
      
      
      We&#8217;ve also updated the Debezium Docker images (with labels 0.3 and latest) used in our tutorial.
      
      
      Thanks to Andrew, Bhupinder, Chris, David, Horia, Konstantin, Tony, and others for their help with the release, issues, discussions, contributions, and questions!
      
      
      
      
      About Debezium
      
      
      Debezium...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;After a few weeks delay, &lt;strong&gt;Debezium 0.3.0 is now available&lt;/strong&gt; for use with Kafka Connect 0.10.0.1. This release contains an updated &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL connector&lt;/a&gt; with quite a few bug fixes, and a new &lt;strong&gt;&lt;em&gt;&lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/&quot;&gt;MongoDB connector&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt; that captures the changes made to a MongoDB replica set or MongoDB sharded cluster. See the &lt;a href=&quot;http://debezium.io/docs/connectors/&quot;&gt;documentation&lt;/a&gt; for details about how to configure these connectors and how they work.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; (with labels &lt;code&gt;0.3&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;) used in our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Andrew, Bhupinder, Chris, David, Horia, Konstantin, Tony, and others for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/08/16/Debezium-0-2-4-Released/</id>
    <title>Debezium 0.2.4 Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2016-08-16T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/08/16/Debezium-0-2-4-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m happy to announce that Debezium 0.2.4 is now available for use with Kafka Connect 0.9.0.1. This release adds more verbose logging during MySQL snapshots, enables taking snapshots of very large MySQL databases, and correct a potential exception during graceful shutdown. See our release notes for details of these changes and for upgrading recommendations.
      
      
      We&#8217;ve also updated the Debezium Docker images (with label 0.2 and latest) used in our tutorial.
      
      
      Thanks to David and wangshao for their help with the release, issues, discussions, contributions, and questions!
      Stay tuned for our next release, which will be 0.3 and will have a new MongoDB connector...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m happy to announce that &lt;strong&gt;Debezium 0.2.4 is now available&lt;/strong&gt; for use with Kafka Connect 0.9.0.1. This release adds more verbose logging during MySQL snapshots, enables taking snapshots of very large MySQL databases, and correct a potential exception during graceful shutdown. See our &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-2-4&quot;&gt;release notes&lt;/a&gt; for details of these changes and for upgrading recommendations.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; (with label &lt;code&gt;0.2&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;) used in our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to David and wangshao for their help with the release, issues, discussions, contributions, and questions!
      Stay tuned for our next release, which will be 0.3 and will have a new MongoDB connector and will support Kafka Connect 0.10.0.1.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/08/02/capturing-changes-from-mysql/</id>
    <title>Capturing changes from MySQL</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2016-08-02T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/08/02/capturing-changes-from-mysql/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="mysql"></category>
    <summary>
      
      
      
      Change data capture is a hot topic. Debezium&#8217;s goal is to make change data capture easy for multiple DBMSes, but admittedly we&#8217;re still a young open source project and so far we&#8217;ve only released a connector for MySQL with a connector for MongoDB that&#8217;s just around the corner. So it&#8217;s great to see how others are using and implementing change data capture. In this post, we&#8217;ll review Yelp&#8217;s approach and see how it is strikingly similar to Debezium&#8217;s MySQL connector.
      
      
      
      
      Streaming data at Yelp
      
      
      The Yelp Engineering Blog recently began a series describing their real-time streaming data infrastructure. The first post provides...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Change data capture is a hot topic. Debezium’s goal is to make change data capture easy for multiple DBMSes, but admittedly we’re still a young open source project and so far we’ve only released a &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;connector for MySQL&lt;/a&gt; with a &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/&quot;&gt;connector for MongoDB&lt;/a&gt; that’s just around the corner. So it’s great to see how others are using and implementing change data capture. In this post, we’ll review Yelp’s approach and see how it is strikingly similar to Debezium’s MySQL connector.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;streaming_data_at_yelp&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#streaming_data_at_yelp&quot;&gt;&lt;/a&gt;Streaming data at Yelp&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;The &lt;a href=&quot;http://engineeringblog.yelp.com/&quot;&gt;Yelp Engineering Blog&lt;/a&gt; recently began a series describing their real-time streaming data infrastructure. The &lt;a href=&quot;http://engineeringblog.yelp.com/2016/07/billions-of-messages-a-day-yelps-real-time-data-pipeline.html&quot;&gt;first post&lt;/a&gt; provides a good introduction and explains how moving from a monolith to a service-oriented architecture increased productivity, but also made it more challenging to work with data spread across the 100 services that own it. It’s totally worth your time to read it right now.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;As Justin writes in the post, several reasons prompted them to create their own real time streaming data pipeline:&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;ulist&quot;&gt;
      &lt;ul&gt;
      &lt;li&gt;
      &lt;p&gt;Ensuring data always remains consistent across services is always a difficult task, but especially so when things can and do go wrong. Transactions across services may be useful in some situations, but they’re not straightforward, are expensive, and can lead to request amplification where one service calls another, which coordinates with two others, etc.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Services that update data in multiple backend services suffer from the &lt;a href=&quot;http://www.confluent.io/blog/using-logs-to-build-a-solid-data-infrastructure-or-why-dual-writes-are-a-bad-idea/&quot;&gt;dual write problem&lt;/a&gt;, which is where a failure occurs after one backing service was updated but before the other could be updated and that always results in data inconsistencies that are difficult to track down and correct.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
      &lt;p&gt;Combining and integrating data spread across multiple services can also be difficult and expensive, but it is even harder when that data is continously changing. One approach is to use bulk APIs, but these can beprohibitive to create, can result in inconsistencies, and pose real scalability problems when services need to continually receive the never-ending updates to data.&lt;/p&gt;
      &lt;/li&gt;
      &lt;/ul&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Yelp’s Real-Time Data Pipeline records changes to data on totally ordered distributed logs so that downstream consumers can receive and process the same changes in exactly the same order. Services can consume changes made by other services, and can therefore stay in sync without explicit interservice communication. This system uses among other things Kafka for event logs, a homegrown system named &lt;a href=&quot;http://engineeringblog.yelp.com/2016/08/streaming-mysql-tables-in-real-time-to-kafka.html&quot;&gt;MySQLStreamer&lt;/a&gt; to capture committed changes to MySQL tables, &lt;a href=&quot;http://avro.apache.org&quot;&gt;Avro&lt;/a&gt; for message format and schemas, and a custom &lt;a href=&quot;http://engineeringblog.yelp.com/2016/07/billions-of-messages-a-day-yelps-real-time-data-pipeline.html#yelps-real-time-data-pipeline&quot;&gt;Schematizer&lt;/a&gt; service that tracks consumers and enforces the Avro schemas used for messages on every Kafka topic.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;how_yelp_captures_mysql_changes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#how_yelp_captures_mysql_changes&quot;&gt;&lt;/a&gt;How Yelp captures MySQL changes&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Perhaps most interesting for Debezium is how Yelp captures the committed changes in their MySQL databases and write them to Kafka topics. Their &lt;a href=&quot;http://engineeringblog.yelp.com/2016/08/streaming-mysql-tables-in-real-time-to-kafka.html&quot;&gt;second post in the series&lt;/a&gt; goes into a lot more detail about their MySQLStreamer process that reads the MySQL binary log and continously processes the DDL statements and DML operations that appear in the log, generating the corresponding &lt;em&gt;insert&lt;/em&gt;, &lt;em&gt;update&lt;/em&gt;, &lt;em&gt;delete&lt;/em&gt;, and &lt;em&gt;refresh&lt;/em&gt; events, and writing these event messages to a separate Kafka topic for each MySQL table. We’ve &lt;a href=&quot;http://debezium.io/blog/2016-04-15-parsing-ddl/&quot;&gt;mentioned before&lt;/a&gt; that MySQL’s row-level binlog events that result from the DML operation don’t include the full definition of the columns, so knowing what the columns mean in each event requires process the DDL statements that also appear in the binlog. Yelp uses a separate MySQL instance it calls the &lt;em&gt;schema tracker database&lt;/em&gt;, which behaves like a MySQL slave to which are applied only the DDL statements they read from the binlog. This technique lets Yelp’s MySQLStreamer system know the state of the database schema and the structure of its tables at the point in the binlog where they are processing events. This is pretty interesting, because it uses the MySQL engine to handle the DDL parsing.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Yelp’s MySQLStreamer process uses another MySQL database to track internal state describing its position in the binlog, what events have been successfully published to Kafka, and, because the binlog position varies on each replica, replica-independent information about each transaction. This latter information is similar to MySQL GTIDs, although Yelp is using earlier versions of MySQL that do not support GTIDs.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Of course, special consideration has to be taken for databases that have been around for a long time. The MySQL binlogs are capped and will not contain the &lt;em&gt;entire&lt;/em&gt; history of the databases, so Yelp’s MySQLStreamer process bootstraps the change data capture process of old databases by starting another clean MySQL replica, which will use the built-in MySQL replication mechanism with the &lt;a href=&quot;http://dev.mysql.com/doc/refman/5.7/en/blackhole-storage-engine.html&quot;&gt;MySQL blackhole database engine&lt;/a&gt; to obtain a consistent snapshot of the master and so that all activity is logged in the replica’s binlog while the replica actually stores no data.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Yelp’s MySQLStreamer mechanism is quite ingenious in its use of MySQL and multiple extra databases to capture changes from MySQL databases and write them to Kafka topics. The downside, of course, is that doing so does increase the operational complexity of the system.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;similar_purpose_similar_approach&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#similar_purpose_similar_approach&quot;&gt;&lt;/a&gt;Similar purpose, similar approach&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source project that is building a change data capture for a variety of DBMSes. Like Yelp’s MySQLStreamer, Debezium’s &lt;a href=&quot;http://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL Connector&lt;/a&gt; can continously capture the committed changes to MySQL database rows and record these events in a separate Kafka topic for each table. When first started, Debezium’s MySQL Connector can perform an initial consistent snapshot and then begin reading the MySQL binlog. It uses both DDL and DML operations that appear in the binlog, directly &lt;a href=&quot;http://debezium.io/blog/2016-04-15-parsing-ddl/&quot;&gt;parsing and using the DDL statements&lt;/a&gt; to learn the changes to each table’s structure and the mapping of each insert, update, and delete binlog event. And each resulting change event written to Kafka includes information about the originating MySQL server and its binlog position, as well as the before and/or after states of the affected row.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;However, unlike Yelp’s MySQLStreamer, the Debezium MySQL connector doesn’t need or use extra MySQL databases to parse DDL or to store the connector’s state. Instead, Debezium is built on top of Kafka Connect, which is a new Kafka library that provides much of the generic functionality of reliably pulling data from external systems, pushing it into Kafka topics, and tracking what data has already been processed. Kafka Connect stores this state inside Kafka itself, simplifying the operational footprint. Debezium’s MySQL connector can then focus on the details of performing a consistent snapshot when required, reading the binlog, and converting the binlog events into useful change events.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Yelp’s real time data pipeline makes use of a custom Avro schema registry, and uses those Avro schemas to encode each event into a compact binary representation while keeping the metadata about the structure of the event. It’s possible to do this with Debezium, too: simply run &lt;a href=&quot;http://docs.confluent.io/3.0.0/schema-registry/docs/index.html&quot;&gt;Confluent’s Schema Registry&lt;/a&gt; as a service and then configure the Kafka Connect worker to use the &lt;a href=&quot;http://debezium.io/docs/faq/#avro-converter/&quot;&gt;Avro Converter&lt;/a&gt;. As the converter serializes each event, it looks at the structure defined by the connector and, when that structure changes, generates an updated Avro Schema and registers it with the Schema Registry. That new Avro schema is then used to encode the event (and others with an identical structure) into a compact binary form written to Kafka. And of course, consumers then also use the same Avro converter so that as events are deserialized, the converter coordinates with the Schema Registry whenever it needs an Avro schema it doesn’t know about. As a result, the events are stored in a compact manner while the events' content and metadata remain available, while Schema Registry captures and maintains the history of the Avro schema for each table as it evolves over time.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;capturing_changes_from_mysql_with_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#capturing_changes_from_mysql_with_debezium&quot;&gt;&lt;/a&gt;Capturing changes from MySQL with Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;If you’re interested in change data capture with MySQL (or any other DBMSes), give Debezium a try by going through &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;our tutorial&lt;/a&gt; that walks you through starting Kafka, Kafka Connect, and Debezium’s MySQL Connector to see exactly what change data events look like and how they can be used. Best of all, it’s open source with a growing community of developers that has had the benefit of building on top of recently-created Kafka Connect framework. Our MySQL connector is ready now, but we’re working on &lt;a href=&quot;http://debezium.io/docs/connectors/&quot;&gt;connectors for other DBMSes&lt;/a&gt;. Specifically, our upcoming 0.3 release will include our &lt;a href=&quot;http://debezium.io/docs/connectors/mongodb/&quot;&gt;MongoDB Connector&lt;/a&gt;, with 0.4 including connectors for PostgreSQL and/or Oracle.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;&lt;em&gt;Correction: A previous version of this post incorrectly stated that Yelp was using a MySQL version that did support GTIDs, when in fact they are using a version that does &lt;strong&gt;not&lt;/strong&gt; support MySQL GTIDs. The post has been corrected, and the author regrets the mistake.&lt;/em&gt;&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/07/26/Debezium-0-2-3-Released/</id>
    <title>Debezium 0.2.3 Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2016-07-26T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/07/26/Debezium-0-2-3-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m happy to announce that Debezium 0.2.3 is now available for use with Kafka Connect 0.9.0.1. This release corrects the MySQL connector behavior when working with TINYINT and SMALLINT columns or with TIME, DATE, and TIMESTAMP columns. See our release notes for details of these changes and for upgrading recommendations.
      
      
      We&#8217;ve also updated the Debezium Docker images (with label 0.2 and latest) used in our tutorial.
      
      
      Thanks to Chris, Christian, Laogang, and Tony for their help with the release, issues, discussions, contributions, and questions!
      Stay tuned for our next release, which will be 0.3 and will have a new MongoDB connector and will...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m happy to announce that &lt;strong&gt;Debezium 0.2.3 is now available&lt;/strong&gt; for use with Kafka Connect 0.9.0.1. This release corrects the MySQL connector behavior when working with &lt;code&gt;TINYINT&lt;/code&gt; and &lt;code&gt;SMALLINT&lt;/code&gt; columns or with &lt;code&gt;TIME&lt;/code&gt;, &lt;code&gt;DATE&lt;/code&gt;, and &lt;code&gt;TIMESTAMP&lt;/code&gt; columns. See our &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-2-3&quot;&gt;release notes&lt;/a&gt; for details of these changes and for upgrading recommendations.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; (with label &lt;code&gt;0.2&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;) used in our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Chris, Christian, Laogang, and Tony for their help with the release, issues, discussions, contributions, and questions!
      Stay tuned for our next release, which will be 0.3 and will have a new MongoDB connector and will support Kafka Connect 0.10.0.0.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
  <entry>
    <id>http://debezium.io/blog/2016/06/22/Debezium-0-2-2-Released/</id>
    <title>Debezium 0.2.2 Released</title>
    <updated>2019-01-17T09:16:09+00:00</updated>
    <published>2016-06-22T00:00:00+00:00</published>
    <link href="http://debezium.io/blog/2016/06/22/Debezium-0-2-2-Released/" rel="alternate" type="text/html" />
    <author>
      <name>Randall Hauch</name>
    </author>
    <category term="releases"></category>
    <category term="mysql"></category>
    <category term="docker"></category>
    <summary>
      
      
      
      I&#8217;m happy to announce that Debezium 0.2.2 is now available. This release fixes several bugs in the MySQL connector that can produce change events with incorrect source metadata, and that eliminates the possibility a poorly-timed connector crash causing the connector to only process some of the rows in a multi-row MySQL event. See our release notes for details of these changes and for upgrading recommendations.
      
      
      Also, thanks to a community member for reporting that Debezium 0.2.x can only be used with Kafka Connect 0.9.0.1. Debezium 0.2.x cannot be used with Kafka Connect 0.10.0.0 because of its backward incompatible changes to the...
    </summary>
    <content type="html">
      &lt;div id=&quot;preamble&quot;&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;I’m happy to announce that &lt;strong&gt;Debezium 0.2.2 is now available&lt;/strong&gt;. This release fixes several bugs in the MySQL connector that can produce change events with incorrect &lt;code&gt;source&lt;/code&gt; metadata, and that eliminates the possibility a poorly-timed connector crash causing the connector to only process some of the rows in a multi-row MySQL event. See our &lt;a href=&quot;http://debezium.io/docs/releases/#release-0-2-2&quot;&gt;release notes&lt;/a&gt; for details of these changes and for upgrading recommendations.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Also, thanks to a community member for &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/DBZ-80&quot;&gt;reporting&lt;/a&gt; that Debezium 0.2.x can only be used with Kafka Connect 0.9.0.1. Debezium 0.2.x cannot be used with Kafka Connect 0.10.0.0 because of its &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-3006&quot;&gt;backward incompatible changes to the consumer API&lt;/a&gt;. Our next release of Debezium will support Kafka 0.10.x.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We’ve also updated the &lt;a href=&quot;https://hub.docker.com/r/debezium/&quot;&gt;Debezium Docker images&lt;/a&gt; (with label &lt;code&gt;0.2&lt;/code&gt; and &lt;code&gt;latest&lt;/code&gt;) used in our &lt;a href=&quot;http://debezium.io/docs/tutorial/&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot;&gt;&lt;/a&gt;About Debezium&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href=&quot;http://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;div class=&quot;sect1&quot;&gt;
      &lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot;&gt;&lt;/a&gt;Get involved&lt;/h2&gt;
      &lt;div class=&quot;sectionbody&quot;&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve the MySQL connector and add more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.jboss.org/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;. And stay tuned, because we’re hoping to add a MongoDB connector in our next release.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div class=&quot;paragraph&quot;&gt;
      &lt;p&gt;Thanks to Chris, Christian, Konstantin, James, and Bhupinder for their help with the release, issues, discussions, contributions, and questions!&lt;/p&gt;
      &lt;/div&gt;
      &lt;/div&gt;
      &lt;/div&gt;
    </content>
  </entry>
</feed>
